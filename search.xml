<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[排序算法]]></title>
    <url>%2F2018%2F06%2F09%2F%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[最近在学习数据结构和算法，这里总结一下学习的排序算法。 选择排序 - SelectionSort基本思路:假设有一个数组（如图所示），进行从小到大的排序。首先在整个数组范围里，找出要放在第一个位置的数，也就是最小的数：1，然后将1和现在的第一名的位置8进行换位，经过交换以后，1所处的位置就是最终排序所在的位置，这样就继续在剩下的部分找此时最小的数，也就是2，然后把2和相应的第二个位置所在的元素进行交换，此时1和2两个元素也已经是最终排好序的结果。整个过程以此类推，继续在剩下的部分中找此时最小的元素，然后进行交换位置。。。。。 代码实现： 12345678910111213141516//arr为要进行排序的数组，//n为数组的元素个数，即数组大小template&lt;typename T&gt;void selectionSort(T arr[],int n )&#123; for(int i=0;i&lt;n;i++)&#123; //寻找[i,n)区间里的最小值 int minIndex = i; //记录当前所找的最小值所处索引的位置，初始化在位置i for( int j= i+1;j&lt;n;j++)&#123; if(arr[j] &lt; arr[minIndex]) //比较j位置的元素是否小于minIndex位置的元素，如果小于则更新当前的minIndex minIndex = j; &#125; //此时已经找到了[i,n)区间里的最小值，并且记录其位置已经记录下来 swap(arr[i] , arr[minIndex]); //如果使用std标准库不行，有可能需要引入algorithm标准库 &#125;&#125; 插入排序 - InsertionSort基本思路：开始只考虑8这个元素的时候，它就已经排好序了。接着看6这个元素,接下来的步骤是把6与它前面的数组进行比较，放在合适的位置，当6与8比较时，6&lt;8，所以6在8前面位置。接着看2这个元素，2与它前面的数组进行比较，2&lt;8，所以2和8交换一次位置，2继续和6比较，2&lt;6，所以2和6交换位置，此时2在最前面的位置。接着看3这个元素，3比8小，所以交换一次位置，3又比6小，所以交换一次位置，3比2大所以不进行交换操作，3插入在2和6中间，这时前面的4个元素排序完成。以此类推。 代码实现： 1234567891011121314151617template&lt;typename T&gt;void insertionSort (T arr[] , int n)&#123; //从i=1开始，因为对插入排序来说，第0个位置不用考虑 for(int i = 1 ; i &lt; n ; i++)&#123; //寻找arr[i]合适的插入位置 for(int j = i ; j &gt; 0 ; j-- )&#123; if ( arr[j] &lt; arr[j-1] ) swap(arr[j] , arr[j-1]); else //如果arr[i]元素已经在合适的位置，则可以直接进入下一个循环 break; &#125; &#125; //下面是简化后的代码 // for(int j = i ; j &gt; 0 &amp;&amp; arr[j] &lt; arr[j-1] ; j-- )&#123; // swap(arr[j] , arr[j-1]); // &#125;&#125; 插入排序和选择排序相比，如果满足了条件就有机会提前结束，所以它的排序效率理论上要比选择排序高但是实际上它的运行时间比选择排序要慢，这是因为其swap操作较多，浪费了时间，所以针对这个地方进行改进 改进代码思路:首先,对位置0处的元素不作处理。 接着，看位置1处的元素6，首先对元素6做一个副本保存起来，然后看元素6是否应该在当前位置，即让他与前面的元素进行对比，如果小于前面的元素，则当前元素位置的值赋值为前一个元素的值，前一个元素位置的值赋值为刚才保存起来的副本（即当前元素的值）。（其实这也是相当于一个交换操作，在满足前一个元素大于当前元素值的情况下，进行交换值操作） 接着，看位置2处的元素2，首先对元素2做一个副本保存起来，然后元素2与前一个元素8比较，2&lt;8，则将元素2处的值赋值为8；接着再比较位置1处的元素8与位置1处的元素6的大小，将位置1处的值赋值为元素6，接着将元素2放在第一个位置。这样就少进行了交换的操作。 接着，看元素3，首先对元素3做一个副本保存起来，然后看元素3该不该放在当前位置，发现3&lt;8，所以这个位置赋值为8；然后看3是不是该放在刚才元素8的位置，发现元素3比元素6小，所以元素6放在刚才元素8的位置；然后看元素3是不是该放在刚才元素6的位置，发现元素3比元素2大，所以元素3应该放在这个位置。 这样很多交换操作就通过赋值进行取代了，所以性能更好。 改进代码：12345678910111213//改进代码template&lt;typename T&gt;void insertionSort (T arr[] , int n)&#123; //从i=1开始 for(int i = 1 ; i &lt; n ; i++)&#123; T e = arr[i]; //用e暂存i位置的数 int j; //保存元素e应该插入的位置 for(j = i ; j &gt; 0 &amp;&amp; arr[j-1] &gt; e ; j--)&#123; arr[j] = arr[j-1]; &#125; arr[j] = e; &#125;&#125; 归并排序 - MergeSort自顶向下的归并排序基本思路：所谓归并排序，有两大步，一步是归，一步是并。 当对一个数组进行排序的时候,先把这个数组分成一半，然后分别把左边的数组和右边的数组排序,之后再归并起来。在对左边数组和右边的数组进行排序的时候，再次分别把左边的数组和右边的数组分成一半，然后对每一个部分进行排序。一样的，对这每一个部分进行排序的时候，再次把他们分成一半，直到它们只含有一个元素的时候，已经是有序了。（蓝色部分） 这时候就对上面最终分割完成的各个部分进行归并。在归并到上一个层级之后，继续进行归并,逐层上升，进行归并，直到归并到最后一层的时候，整个数组就有序了。（红色部分） 可以看到，对图中的8各元素进行归并的时候，分成3级，第三级就可以把数组分成单个元素了，这样每次二分，就是log2 8 = 3，如果是N个元素，则有log2 N的层级。每一层要处理的元素个数是一样的，则整个归并过程是N*log(N)的时间复杂度。 那么问题是假设左半部分和右半部分已经排好了序，怎么把他们合并成一个有序的数组。这个过程需要为数组开辟一个相同大小的临时空间进行辅助,如下图。在合并成一个数组的过程中，需要3个索引：k是最终在归并过程中要跟踪的位置，i和j分别表示两个排好序的数组，当前要考虑的元素项。首先看1和2两个元素，谁应该放在k位置，进行对比后，较小的元素1放在k位置。这时候k++，j++。紧接着，元素2和元素4进行对比，较小的元素2放在当前的k位置，这时候k++,i++。继续元素3和元素4进行对比，较小的元素3放在当前的k位置，这时候k++,i++。继续元素6和元素4进行对比，较小的元素4放在当前的k位置，这时候k++,j++。。。。。算法过程中，需要维护k,i,j满足算法的定义。并且需要跟踪i,j的越界情况。这里定义l(left),r(right),和m(middle)分别为整个数组的最左边的元素，数组最右边的元素和中间位置的元素(这里指定为第一个数组的最后一个元素)。这里再做一个说明，这里的定义整个算法的数组是前闭后闭的数组。 代码实现：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859//将arr[l...mid]和arr[mid+1...r]两部分进行归并template&lt;typename T&gt;void __merge(T arr[] , int l , int mid , int r )&#123; //开辟一段临时空间，大小和arr[]的空间一样大 T aux[r-l+1]; for( int i = l ; i &lt;= r ; i++ ) //因为是闭区间,所以用i&lt;=r aux[i-l] = arr[i]; // aux[]是从0开始的，arr[]是从l开始的，所以有个i-l的偏移量 //设置两个索引，指向两部分的开头 int i = l , j = mid + 1; for( int k = l ; k &lt;= r ; k++)&#123; //使用k索引进行遍历，每一次决定arr[k]的位置应该是谁 //先判断i和j是否越界 //这时候说明左半部分已经遍历完,k没有遍历完，j索引所指数组的元素还没有归并回去,则剩下的k索引所指的元素就对应j-l所指的元素 if(i &gt; mid )&#123; arr[k] = aux[j-l]; j++; &#125; else if(j&gt;r)&#123;//同上 arr[k] = aux[i-l]; i++; &#125; else if(aux[i-l] &lt; aux[j-l])&#123; arr[k] = aux[i-l]; i++; &#125; else&#123; arr[k] = aux[j-l]; j++; &#125; &#125;&#125;//递归使用归并排序，对arr[l...r]的范围进行排序（r是最后一个元素的位置）template&lt;typename T&gt;void __mergeSort(T arr[] , int l , int r )&#123; if(l &gt;= r) return; int mid = (l+r)/2; //对左右两部分进行归并排序 __mergeSort(arr,l,mid); __mergeSort(arr,mid+1,r); //将归并排序好的两部分[l,mid]和[mid+1,r]进行merge操作 if( arr[mid] &gt; arr[mid+1] ) __merge(arr, l , mid , r);&#125;template&lt;typename T&gt;void mergeSort(T arr[],int n)&#123; //arr:当前要处理的数据，0:要处理的数据的开始位置，n-1:要处理的数据的结束位置 __mergeSort(arr , 0 , n-1 );&#125; 自底向上的归并排序基本思路：自底向上的归并排序的基本原理是，给定一个数组，从左向右将数组依次划分为小段，如两个元素一个小段，然后进行归并排序。当这一轮归并排序完成之后，再四个元素一个小段进行归并排序，最后八个元素一个小段进行归并排序。以上面的数组为例，这时候就排序完成了。这种方法并不需要递归，只需要迭代就可以完成排序操作。 代码实现：123456789101112131415161718192021222324252627282930313233343536373839404142434445template&lt;typename T&gt;void __merge(T arr[] , int l , int mid , int r )&#123; //开辟一段临时空间，大小和arr[]的空间一样大 T aux[r-l+1]; for( int i = l ; i &lt;= r ; i++ ) //因为是闭区间,所以用i&lt;=r aux[i-l] = arr[i]; // aux[]是从0开始的，arr[]是从l开始的，所以有个i-l的偏移量 //设置两个索引，指向两部分的开头 int i = l , j = mid + 1; for( int k = l ; k &lt;= r ; k++)&#123; //使用k索引进行遍历，每一次决定arr[k]的位置应该是谁 //先判断i和j是否越界 if(i &gt; mid )&#123; //这时候说明左半部分已经遍历完,k没有遍历完，j索引所指数组的元素还没有归并回去,则剩下的k索引所指的元素就对应j-l所指的元素 arr[k] = aux[j-l]; j++; &#125; else if(j&gt;r)&#123;//同上 arr[k] = aux[i-l]; i++; &#125; else if(aux[i-l] &lt; aux[j-l])&#123; arr[k] = aux[i-l]; i++; &#125; else&#123; arr[k] = aux[j-l]; j++; &#125; &#125;&#125;//自底向上的归并排序template&lt;typename T&gt;void mergeSortBU(T arr[] , int n)&#123; for(int sz = 1 ; sz &lt;= n ; sz += sz) //size从1开始，每次加一个size的大小，直到是n大小 //第二层循环是每次归并过程中起始的元素的位置 // i+sz&lt;n 保证了第二部分的存在，也保证了i+sz-1是不会越界的 for(int i = 0; i + sz &lt; n ; i += sz + sz ) //对 arr[i...i+sz-1]和arr[i+sz...i+2*sz-1]进行归并 //min()保证i+sz+sz-1越界的时候取n-1 __merge(arr , i , i + sz - 1 , min(i + sz + sz -1, n - 1); //自底向上的归并排序方法没有用到数组的索引，所以可以很好的对链表结构进行排序&#125; 快速排序 - QuickSort基本思路：以下图的数组为例，首先选定一个元素4，把它挪到当它在排好序的时候，应该处的位置，当它处在这个位置之后，使得这个数组有了一个性质：在4之前的数都小于4，在4之后的数都大于4。接下来，对小于4的子数组和大于4的子数组分别继续进行快速排序。这样逐渐递归，完成整个排序过程。 那么关键问题是怎么把4这个元素放在正确的改放的位置，这个过程称为Partition。通常使用数组的第一个元素作为分界的标志点，第一个位置记作l，然后逐渐遍历右边没有被访问的元素，在遍历的过程中，逐步整理让元素一部分是&gt;v一部分是&lt;v的,将大于v和小于v的分界点的索引记为j，当前访问的元素e的索引记作i。下面讨论i位置的e该如何处理，如图。如果e&gt;v则将它放在这个位置不变，i++继续讨论下一个元素。否则如果e&lt;v则将i位置的e与j+1位置的元素交换，j++,i++，继续考察下一个元素。依次继续进行操作，直至遍历完数组的所有元素。这时候要将v放在数组中合适的位置，即将索引l位置的v与索引j位置的元素进行交换。最终完成此次操作。 代码实现：1234567891011121314151617181920212223242526272829303132333435363738394041//对arr[l...r]部分进行partition操作//返回p,使得arr[l...p-1] &lt; arr[p] ; arr[p+1...r] &gt; arr[p]template&lt;typename T&gt;int __partition(T arr[] , int l , int r )&#123; //先取第一个元素作为标准 T v = arr[l]; //arr[l+1...j]&lt;v ; arr[j+1...i) &gt; v ，后面是开区间是因为i是当前要考察的元素 int j = l ; for( int i = l + 1 ;i &lt;= r ; i++ )&#123; if( arr[i] &lt; v )&#123; swap( arr[j+1] , arr[i] ); j++; &#125; &#125; swap( arr[l] , arr[j] ); return j;&#125;//对arr[l...r]部分进行快速排序template&lt;typename T&gt;void __quickSort(T arr[] , int l , int r )&#123; if(l &gt;= r) return; int p = __partition(arr, l , r ); __quickSort(arr , l , p-1 ); __quickSort(arr , p + 1 , r );&#125;template&lt;typename T&gt;void quickSort(T arr[] , int n )&#123; //调用递归函数 __quickSort(arr, 0 , n - 1);&#125;]]></content>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vue-cli 目录结构笔记 及 一个简单电商项目的网页架构思路]]></title>
    <url>%2F2018%2F06%2F05%2FVue-cli-%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84%E7%AC%94%E8%AE%B0-%E5%8F%8A-%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%94%B5%E5%95%86%E9%A1%B9%E7%9B%AE%E7%9A%84%E7%BD%91%E9%A1%B5%E6%9E%B6%E6%9E%84%E6%80%9D%E8%B7%AF%2F</url>
    <content type="text"><![CDATA[学习了vue有一小段时间，期间中断去学习了java并且补了一下数据结构的基础，有点断层。跟着视频用vue2.0做了一个电商的小项目。思路稍微清晰了一些，但是因为中途转学其他的缘故，有一些东西还是忘掉了，这里总结一下使用vue-cli搭建项目的一些经验和教训。 首先是vue-cli的目录结构，这个是基于webpack的脚手架目录： 12345678910111213141516171819202122232425262728293031.|-- build // 项目构建(webpack)相关代码| |-- build.js // 生产环境构建代码| |-- check-version.js // 检查node、npm等版本| |-- utils.js // 构建工具相关| |-- vue-loader.conf.js // css加载器配置| |-- webpack.base.conf.js // webpack基础配置| |-- webpack.dev.conf.js // webpack开发环境配置| |-- webpack.prod.conf.js // webpack生产环境配置|-- config // 项目开发环境配置| |-- dev.env.js // 开发环境变量| |-- index.js // 项目一些配置变量(包括监听变量，打包路径等)| |-- prod.env.js // 生产环境变量| |-- test.env.js // 测试环境变量|-- node_modules //存放依赖的目录|-- src // 源码目录| |-- assets // 静态资源（css文件，外部js文件）| |-- components // vue公共组件| |-- router // 路由配置| |-- App.vue // 根组件| |-- main.js // 入口文件，加载各种公共组件|-- static // 静态文件，比如一些图片，json数据等|-- test // 测试文件目录|-- .babelrc // ES6语法编译配置|-- .editorconfig // 定义代码格式|-- .gitignore // git上传需要忽略的文件格式|-- .postcssrc.js|-- README.md // 项目说明|-- index.html // 入口页面|-- package.json // 项目基本信息. 当然不同版本的项目目录或者文件大同小异，基本都包括在上面了。 接下来讲一个平时用的比较多的网页排版及vue的大体配置。在此之前先介绍几个文件： index.html一般只定义一个空的根节点，在main.js里面定义的实例将挂载在根节点下，内容都通过vue组件来填充。 App.vueApp.vue 是个根组件。一个vue文件包括template,script,style三部分。vue通常用es6来写，用export default导出。&lt;style&gt;&lt;/style&gt;默认是影响全局的，如需定义作用域只在该组件下起作用，需在标签上加scoped。如要引入外部css文件，首先需给项目安装css-loader依赖包。使用import引入，比如： 12345&lt;style&gt; import &apos;./assets/css/bootstrap.css&apos;&lt;/style&gt; main.jsmain.js是个入口文件。这里:template: &#39;&lt;App/&gt;&#39;表示用&lt;app&gt;&lt;/app&gt;替换index.html里面的&lt;div id=&quot;app&quot;&gt;&lt;/div&gt;。这么做的目的很简单，&lt;App /&gt;他就是App.vue，template就是选择vue实例要加载哪个模板。最新的vue-cli脚手架模板现在是这个形式。App.vue是主程序，其他所有的.vue都是放在App.vue中，所以只需要加载App.vue就完全可以把其他的东西加载出来。 routerrouter目录下的index.js即是路由配置文件 router中可以设置多个路由，但是这里要先引入相应的组件，在进行设置： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152//引入Vue框架import Vue from &apos;vue&apos;//引入路由依赖import Router from &apos;vue-router&apos;//引入各个页面组件import IndexPage from &apos;./components/index&apos;import DetailPage from &apos;./components/detail.vue&apos;import DetailAnaPage from &apos;./components/detail/analysis&apos;import DetailPubPage from &apos;./components/detail/publish&apos;import DetailCouPage from &apos;./components/detail/count&apos;import DetailForPage from &apos;./components/detail/forecast&apos;import OrderListPage from &apos;./components/orderList&apos;Vue.use(Router)export default new Router(&#123; mode: &apos;history&apos;, routes: [ &#123; path: &apos;/&apos;, component: IndexPage &#125;, &#123; path: &apos;/orderList&apos;, component: OrderListPage &#125;, &#123; path: &apos;/detail&apos;, component: DetailPage, redirect: &apos;detail/analysis&apos;, children: [ &#123; path: &apos;forecast&apos;, component: DetailForPage &#125;, &#123; path: &apos;analysis&apos;, component: DetailAnaPage &#125;, &#123; path: &apos;publish&apos;, component: DetailPubPage &#125;, &#123; path: &apos;count&apos;, component: DetailCouPage &#125; ] &#125; ]&#125;) 这里介绍一个基础的vue模板构建思路： App.vue如下，其中的router的配置可以参见上面的代码 当然这里只设置了简单的内容，具体的方法和数据及样式根据不同的需求进行补充即可。 当然这只是一种简单的设计思路，做项目时可以用这个做为参考，但是不要被限制。]]></content>
      <tags>
        <tag>前端</tag>
        <tag>Vue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据基础学习笔记（十）——图计算]]></title>
    <url>%2F2018%2F05%2F28%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%EF%BC%89%E2%80%94%E2%80%94%E5%9B%BE%E8%AE%A1%E7%AE%97%2F</url>
    <content type="text"><![CDATA[图结构数据•许多大数据都是以大规模图或网络的形式呈现•许多非图结构的大数据，也常常会被转换为图模型后进行分析•图数据结构很好地表达了数据之间的关联性•关联性计算是大数据计算的核心——通过获得数据的关联性，可以从噪音很多的海量数据中抽取有用的信息 传统图计算解决方案的不足之处很多传统的图计算算法都存在以下几个典型问题：（1）常常表现出比较差的内存访问局部性（2）针对单个顶点的处理工作过少（3）计算过程中伴随着并行度的改变 针对大型图（比如社交网络和网络图）的计算问题，可能的解决方案及其不足之处具体如下：• （1 ）为特定的图应用定制相应的分布式实现• （2 ）基于现有的分布式计算平台进行图计算• （3 ）使用单机的图算法库：比如BGL、LEAD、NetworkX、JDSL、Standford GraphBase和FGL等• （4 ）使用已有的并行图计算系统：比如，ParallelBGL和CGM Graph，实现了很多并行图算法 图计算通用软件• 针对大型图的计算，目前通用的图计算软件主要包括两种：– 第一种主要是 基于遍历算法 的、 实时的图数据库，如Neo4j、OrientDB、DEX和 Infinite Graph– 第二种则是 以图顶点为中心的、基于消息传递批处理的并行引擎，如GoldenOrb、Giraph、Pregel和Hama，这些图处理软件主要是基于BSP模型实现的并行图处理系统 一次BSP(Bulk Synchronous Parallel Computing Model，又称“大同步”模型)计算过程包括一系列全局超步（所谓的超步就是计算中的一次迭代），每个超步主要包括三个组件：• 局部计算：每个参与的处理器都有自身的计算任务• 通讯：处理器群相互交换数据• 栅栏同步(Barrier Synchronization)：当一个处理器遇到“路障”（或栅栏），会等到其他所有处理器完成它们的计算步骤 Pregel•谷歌公司在2003年到2004年公布了GFS、MapReduce和BigTable•谷歌在后Hadoop时代的新“三驾马车”•Caffeine•Dremel•Pregel•Pregel是一种基于BSP模型实现的并行图处理系统•为了解决大型图的分布式计算问题，Pregel搭建了一套可扩展的、有容错机制的平台，该平台提供了一套非常灵活的API，可以描述各种各样的图计算•Pregel作为分布式图计算的计算框架，主要用于图遍历、最短路径、PageRank计算等等 Pregel图计算模型有向图和顶点•Pregel计算模型以有向图作为输入•有向图的每个顶点都有一个String类型的顶点ID•每个顶点都有一个可修改的用户自定义值与之关联•每条有向边都和其源顶点关联，并记录了其目标顶点ID•边上有一个可修改的用户自定义值与之关联 •在每个超步S中，图中的所有顶点都会并行执行相同的用户自定义函数•每个顶点可以接收前一个超步(S-1)中发送给它的消息，修改其自身及其出射边的状态，并发送消息给其他顶点，甚至是修改整个图的拓扑结构•在这种计算模式中，“边”并不是核心对象，在边上面不会运行相应的计算，只有顶点才会执行用户自定义函数进行相应计算 顶点之间的消息传递采用消息传递模型主要基于以下两个原因：（1）消息传递具有足够的表达能力，没有必要使用远程读取或共享内存的方式（2）有助于提升系统整体性能 Pregel的计算过程•Pregel的计算过程是由一系列被称为“超步”的迭代组成的•在每个超步中，每个顶点上面都会并行执行用户自定义的函数，该函数描述了一个顶点V在一个超步S中需要执行的操作•该函数可以读取前一个超步(S-1)中其他顶点发送给顶点V的消息，执行相应计算后，修改顶点V及其出射边的状态，然后沿着顶点V的出射边发送消息给其他顶点，而且，一个消息可能经过多条边的传递后被发送到任意已知ID的目标顶点上去•这些消息将会在下一个超步(S+1)中被目标顶点接收，然后象上述过程一样开始下一个超步(S+1)的迭代过程•在Pregel计算过程中，一个算法什么时候可以结束，是由所有顶点的状态决定的•在第0个超步，所有顶点处于活跃状态•当一个顶点不需要继续执行进一步的计算时，就会把自己的状态设置为“停机”，进入非活跃状态•当一个处于非活跃状态的顶点收到来自其他顶点的消息时，Pregel计算框架必须根据条件判断来决定是否将其显式唤醒进入活跃状态•当图中所有的顶点都已经标识其自身达到“非活跃（inactive）”状态，并且没有消息在传送的时候，算法就可以停止运行 实例 Pregel的C++ APIPregel已经预先定义好一个基类——Vertex类：123456789101112template &lt;typename VertexValue, typename EdgeValue, typename MessageValue&gt;class Vertex &#123; public: virtual void Compute(MessageIterator* msgs) = 0; const string&amp; vertex_id() const; int64 superstep() const; const VertexValue&amp; GetValue(); VertexValue* MutableValue(); OutEdgeIterator GetOutEdgeIterator(); void SendMessageTo(const string&amp; dest_vertex, const MessageValue&amp; message); void VoteToHalt();&#125;; •在Vetex类中，定义了三个值类型参数，分别表示顶点、边和消息。每一个顶点都有一个给定类型的值与之对应•编写Pregel程序时，需要继承Vertex类，并且覆写Vertex类的虚函数Compute() 消息传递机制• 顶点之间的通讯是借助于消息传递机制来实现的，每条消息都包含了消息值和需要到达的目标顶点ID。用户可以通过Vertex类的模板参数来设定消息值的数据类型• 在一个超步S中，一个顶点可以发送任意数量的消息，这些消息将在下一个超步（S+1）中被其他顶点接收• 一个顶点V通过与之关联的出射边向外发送消息，并且，消息要到达的目标顶点并不一定是与顶点V相邻的顶点，一个消息可以连续经过多条连通的边到达某个与顶点V不相邻的顶点U，U可以从接收的消息中获取到与其不相邻的顶点V的ID Combiner• Pregel计算框架在消息发出去之前，Combiner可以将发往同一个顶点的多个整型值进行求和得到一个值，只需向外发送这个“求和结果”，从而实现了由多个消息合并成一个消息，大大减少了传输和缓存的开销• 在默认情况下，Pregel计算框架并不会开启Combiner功能• 当用户打算开启Combiner功能时，可以继承Combiner类并覆写虚函数Combine()• 此外，通常只对那些满足交换律和结合律的操作才可以去开启Combiner功能 Aggregator• Aggregator提供了一种全局通信、监控和数据查看的机制• 在一个超步S中，每一个顶点都可以向一个Aggregator提供一个数据，Pregel计算框架会对这些值进行聚合操作产生一个值，在下一个超步（S+1）中，图中的所有顶点都可以看见这个值• Aggregator的聚合功能，允许在整型和字符串类型上执行最大值、最小值、求和操作，比如，可以定义一个“Sum”Aggregator来统计每个顶点的出射边数量，最后相加可以得到整个图的边的数量• Aggregator还可以实现全局协同的功能，比如，可以设计“and”Aggregator来决定在某个超步中Compute()函数是否执行某些逻辑分支，只有当“and” Aggregator显示所有顶点都满足了某条件时，才去执行这些逻辑分支 拓扑改变• Pregel计算框架允许用户在自定义函数Compute()中定义操作，修改图的拓扑结构，比如在图中增加（或删除）边或顶点• 对于全局拓扑改变，Pregel采用了惰性协调机制• 对于本地的局部拓扑改变，是不会引发冲突的，顶点或边的本地增减能够立即生效，很大程度上简化了分布式编程 输入和输出• 在Pregel计算框架中，图的保存格式多种多样，包括文本文件、关系数据库或键值数据库等• 在Pregel中，“从输入文件生成得到图结构”和“执行图计算”这两个过程是分离的，从而不会限制输入文件的格式• 对于输出，Pregel也采用了灵活的方式，可以以多种方式进行输出 Pregel的体系结构Pregel的执行过程•在Pregel计算框架中，一个大型图会被划分成许多个分区，每个分区都包含了一部分顶点以及以其为起点的边•一个顶点应该被分配到哪个分区上，是由一个函数决定的，系统默认函数为hash(ID) mod N，其中，N为所有分区总数，ID是这个顶点的标识符；当然，用户也可以自己定义这个函数•这样，无论在哪台机器上，都可以简单根据顶点ID判断出该顶点属于哪个分区，即使该顶点可能已经不存在了 在理想的情况下（不发生任何错误），一个Pregel用户程序的执行过程如下：（1）选择集群中的多台机器执行图计算任务，有一台机器会被选为Master，其他机器作为Worker（2）Master把一个图分成多个分区，并把分区分配到多个Worker。一个Worker会领到一个或多个分区，每个Worker知道所有其他Worker所分配到的分区情况 （3）Master会把用户输入划分成多个部分。然后，Master会为每个Worker分配用户输入的一部分。如果一个Worker从输入内容中加载到的顶点，刚好是自己所分配到的分区中的顶点，就会立即更新相应的数据结构。否则，该Worker会根据加载到的顶点的ID，把它发送到其所属的分区所在的Worker上。当所有的输入都被加载后，图中的所有顶点都会被标记为“活跃”状态。 （4）Master向每个Worker发送指令，Worker收到指令后，开始运行一个超步。当一个超步中的所有工作都完成以后，Worker会通知Master，并把自己在下一个超步还处于“活跃”状态的顶点的数量报告给Master。上述步骤会被不断重复，直到所有顶点都不再活跃并且系统中不会有任何消息在传输，这时，执行过程才会结束。（5）计算过程结束后，Master会给所有的Worker发送指令，通知每个Worker对自己的计算结果进行持久化存储 容错性• Pregel采用检查点机制来实现容错。在每个超步的开始，Master会通知所有的Worker把自己管辖的分区的状态写入到持久化存储设备• Master会周期性地向每个Worker发送ping消息，Worker收到ping消息后会给Master发送反馈消息• 每个Worker上都保存了一个或多个分区的状态信息，当一个Worker发生故障时，它所负责维护的分区的当前状态信息就会丢失。Master监测到一个Worker发生故障“失效”后，会把失效Worker所分配到的分区，重新分配到其他处于正常工作状态的Worker集合上，然后，所有这些分区会从最近的某超步S开始时写出的检查点中，重新加载状态信息 Worker在一个Worker中，它所管辖的分区的状态信息是保存在内存中的。分区中的顶点的状态信息包括：•顶点的当前值•以该顶点为起点的出射边列表，每条出射边包含了目标顶点ID和边的值•消息队列，包含了所有接收到的、发送给该顶点的消息•标志位，用来标记顶点是否处于活跃状态在每个超步中，Worker会对自己所管辖的分区中的每个顶点进行遍历，并调用顶点上的Compute()函数，在调用时，会把以下三个参数传递进去：•该顶点的当前值•一个接收到的消息的迭代器•一个出射边的迭代器 •在Pregel中，为了获得更好的性能，“标志位”和输入消息队列是分开保存的•对于每个顶点而言，Pregel只保存一份顶点值和边值，但是，会保存两份“标志位”和输入消息队列，分别用于当前超步和下一个超步•如果一个顶点V在超步S接收到消息，那么，它表示V将会在下一个超步S+1中（而不是当前超步S中）处于“活跃”状态 •当一个Worker上的一个顶点V需要发送消息到其他顶点U时，该Worker会首先判断目标顶点U是否位于自己机器上•如果目标顶点U在自己的机器上，就直接把消息放入到与目标顶点U对应的输入消息队列中•如果发现目标顶点U在远程机器上，这个消息就会被暂时缓存到本地，当缓存中的消息数目达到一个事先设定的阈值时，这些缓存消息会被批量异步发送出去，传输到目标顶点所在的Worker上 Master•Master主要负责协调各个Worker执行任务，每个Worker会借助于名称服务系统定位到Master的位置，并向Master发送自己的注册信息，Master会为每个Worker分配一个唯一的ID•Master维护着关于当前处于“有效”状态的所有Worker的各种信息，包括每个Worker的ID和地址信息，以及每个Worker被分配到的分区信息•Master中保存这些信息的数据结构的大小，只与分区的数量有关，而与顶点和边的数量无关 •一个大规模图计算任务会被Master分解到多个Worker去执行，在每个超步开始时，Master都会向所有处于“有效”状态的Worker发送相同的指令，然后等待这些Worker的回应•如果在指定时间内收不到某个Worker的反馈，Master就认为这个Worker失效•如果参与任务执行的多个Worker中的任意一个发生了故障失效，Master就会进入恢复模式•在每个超步中，图计算的各种工作，比如输入、输出、计算、保存和从检查点中恢复，都会在“路障（barrier）”之前结束 •Master在内部运行了一个HTTP服务器来显示图计算过程的各种信息•用户可以通过网页随时监控图计算执行过程各个细节 •图的大小 •关于出度分布的柱状图 •处于活跃状态的顶点数量 •在当前超步的时间信息和消息流量 •所有用户自定义Aggregator的值 Aggregator• 每个用户自定义的Aggregator都会采用聚合函数对一个值集合进行聚合计算得到一个全局值• 每个Worker都保存了一个Aggregator的实例集，其中的每个实例都是由类型名称和实例名称来标识的• 在执行图计算过程的某个超步S中，每个Worker会利用一个Aggregator对当前本地分区中包含的所有顶点的值进行归约，得到一个本地的局部归约值• 在超步S结束时，所有Worker会将所有包含局部归约值的Aggregator的值进行最后的汇总，得到全局值，然后提交给Master• 在下一个超步S+1开始时，Master就会将Aggregator的全局值发送给每个Worker Pregel的应用实例——单源最短路径Dijkstra算法是解决单源最短路径问题的贪婪算法 Pregel非常适合用来解决单源最短路径问题，实现代码如下：12345678910111213141516class ShortestPathVertex : public Vertex&lt;int, int, int&gt; &#123; void Compute(MessageIterator* msgs) &#123; int mindist = IsSource(vertex_id()) ? 0 : INF; for (; !msgs-&gt;Done(); msgs-&gt;Next()) mindist = min(mindist, msgs-&gt;Value()); if (mindist &lt; GetValue()) &#123; *MutableValue() = mindist; OutEdgeIterator iter = GetOutEdgeIterator(); for (; !iter.Done(); iter.Next()) SendMessageTo(iter.Target(), mindist + iter.GetValue()); &#125; VoteToHalt(); &#125; &#125;; 超步1：•顶点0：没有收到消息，依然非活跃•顶点1：收到消息100（唯一消息），被显式唤醒，执行计算，mindist变为100，小于顶点值INF，顶点值修改为100，没有出射边，不需要发送消息，最后变为非活跃•顶点2：收到消息30，被显式唤醒，执行计算，mindist变为30，小于顶点值ZNF，顶点值修改为30，有两条出射边，向顶点3发送消息90（即：30+60），向顶点1发送消息90（即：30+60），最后变为非活跃•顶点3：没有收到消息，依然非活跃•顶点4：收到消息10，被显式唤醒，执行计算，mindist变为10，小于顶点值INF，顶点值修改为10，向顶点3发送消息60（即：10+50），最后变为非活跃剩余超步省略……当所有顶点非活跃，并且没有消息传递，就结束]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据基础学习笔记（九）——流计算]]></title>
    <url>%2F2018%2F05%2F22%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B9%9D%EF%BC%89%E2%80%94%E2%80%94%E6%B5%81%E8%AE%A1%E7%AE%97%2F</url>
    <content type="text"><![CDATA[什么是流数据• 近年来，在Web应用、网络监控、传感监测等领域，兴起了一种新的数据密集型应用——流数据，即数据以大量、快速、时变的流形式持续到达• 实例：PM2.5检测、电子商务网站用户点击流• 流数据具有如下特征：– 数据快速持续到达，潜在大小也许是无穷无尽的– 数据来源众多，格式复杂– 数据量大，但是不十分关注存储，一旦经过处理，要么被丢弃，要么被归档存储– 注重数据的整体价值，不过分关注个别数据– 数据顺序颠倒，或者不完整，系统无法控制将要处理的新到达的数据元素的顺序• 对静态数据和流数据的处理，对应着两种截然不同的计算模式：批量计算和实时计算•批量计算：充裕时间处理静态数据，如Hadoop•流数据不适合采用批量计算，因为流数据不适合用传统的关系模型建模•流数据必须采用实时计算，响应时间为秒级•在大数据时代，数据格式复杂、来源众多、数据量巨大，对实时计算提出了很大的挑战。因此，针对流数据的实时计算——流计算，应运而生 流计算的概念• 流计算秉承一个基本理念，即 数据的价值随着时间的流逝而降低，如用户点击流。因此，当事件出现时就应该立即进行处理，而不是缓存起来进行批量处理。为了及时处理流数据，就需要一个低延迟、可扩展、高可靠的处理引擎• 对于一个流计算系统来说，它应达到如下需求：– 高性能– 海量式– 实时性– 分布式– 易用性– 可靠性 流计算与Hadoop• Hadoop设计的初衷是面向大规模数据的批量处理• MapReduce是专门面向静态数据的批量处理的，内部各种实现机制都为批处理做了高度优化，不适合用于处理持续到达的动态数据• 可能会想到一种“变通”的方案来降低批处理的时间延迟——将基于MapReduce的批量处理转为小批量处理，将输入数据切成小的片段，每隔一个周期就启动一次MapReduce作业。但这种方式也无法有效处理流数据– 切分成小片段，可以降低延迟，但是也增加了附加开销，还要处理片段之间依赖关系– 需要改造MapReduce以支持流式处理结论：鱼和熊掌不可兼得，Hadoop擅长批处理，但是不适合流计算• 当前业界诞生了许多专门的流数据实时计算系统来满足各自需求：• 商业级：IBM InfoSphere Streams和IBM StreamBase• 开源流计算框架：– Twitter Storm：免费、开源的分布式实时计算系统，可简单、高效、可靠地处理大量的流数据– Yahoo! S4（Simple Scalable Streaming System）：开源流计算平台，是通用的、分布式的、可扩展的、分区容错的、可插拔的流式系统• 公司为支持自身业务开发的流计算框架：– Facebook Puma– Dstream（百度）– 银河流数据处理平台（淘宝） 流计算处理流程• 传统的数据处理流程，需要先采集数据并存储在关系数据库等数据管理系统中，之后由用户通过查询操作和数据管理系统进行交互• 传统的数据处理流程隐含了两个前提：– 存储的数据是旧的。存储的静态数据是过去某一时刻的快照，这些数据在查询时可能已不具备时效性了– 需要用户主动发出查询 • 流计算的处理流程一般包含三个阶段：数据实时采集、数据实时计算、实时查询服务 数据实时采集• 数据实时采集阶段通常采集多个数据源的海量数据，需要保证实时性、低延迟与稳定可靠• 以日志数据为例，由于分布式集群的广泛应用，数据分散存储在不同的机器上，因此需要实时汇总来自不同机器上的日志数据• 目前有许多互联网公司发布的开源分布式日志采集系统均可满足每秒数百MB的数据采集和传输需求，如： – Facebook的Scribe – LinkedIn的Kafka – 淘宝的Time Tunnel – 基于Hadoop的Chukwa和Flume • 数据采集系统的基本架构一般有以下三个部分：– Agent：主动采集数据，并把数据推送到Collector部分– Collector：接收多个Agent的数据，并实现有序、可靠、高性能的转发– Store：存储Collector转发过来的数据（对于流计算不存储数据） 数据实时计算• 数据实时计算阶段对采集的数据进行实时的分析和计算，并反馈实时结果• 经流处理系统处理后的数据，可视情况进行存储，以便之后再进行分析计算。在时效性要求较高的场景中，处理之后的数据也可以直接丢弃 实时查询服务• 实时查询服务：经由流计算框架得出的结果可供用户进行实时查询、展示或储存• 传统的数据处理流程，用户需要主动发出查询才能获得想要的结果。而在流处理流程中，实时查询服务可以不断更新结果，并将用户所需的结果实时推送给用户• 虽然通过对传统的数据处理系统进行定时查询，也可以实现不断地更新结果和结果推送，但通过这样的方式获取的结果，仍然是根据过去某一时刻的数据得到的结果，与实时结果有着本质的区别• 可见，流处理系统与传统的数据处理系统有如下不同：– 流处理系统处理的是实时的数据，而传统的数据处理系统处理的是预先存储好的静态数据– 用户通过流处理系统获取的是实时结果，而通过传统的数据处理系统，获取的是过去某一时刻的结果– 流处理系统无需用户主动发出查询，实时查询服务可以主动将实时结果推送给用户 开源流计算框架Storm• Twitter Storm是一个免费、开源的分布式实时计算系统，Storm对于实时计算的意义类似于Hadoop对于批处理的意义，Storm可以简单、高效、可靠地处理流数据，并支持多种编程语言 • Storm框架可以方便地与数据库系统进行整合，从而开发出强大的实时计算系统 • Twitter是全球访问量最大的社交网站之一，Twitter开发Storm流处理框架也是为了应对其不断增长的流数据实时处理需求 Storm的特点• Storm可用于许多领域中，如实时分析、在线机器学习、持续计算、远程RPC、数据提取加载转换等 • Storm具有以下主要特点：– 整合性– 简易的API– 可扩展性– 可靠的消息处理– 支持各种编程语言– 快速部署– 免费、开源 Storm设计思想• Storm主要术语包括Streams、Spouts、Bolts、Topology和Stream Groupings • Streams ：Storm将流数据Stream描述成一个无限的Tuple序列，这些Tuple序列会以分布式的方式并行地创建和处理 •每个tuple是一堆值，每个值有一个名字，并且每个值可以是任何类型•Tuple本来应该是一个Key-Value的Map，由于各个组件间传递的tuple的字段名称已经事先定义好了，所以Tuple只需要按序填入各个Value，所以就是一个Value List（值列表） • Spout：Storm认为每个Stream都有一个源头，并把这个源头抽象为Spout • 通常Spout会从外部数据源（队列、数据库等）读取数据，然后封装成Tuple形式，发送到Stream中。Spout是一个主动的角色，在接口内部有个nextTuple函，Storm框架会不停的调用该函数 • Bolt ：Storm将Streams的状态转换过程抽象为Bolt。Bolt即可以处理Tuple，也可以将处理后的Tuple作为新的Streams发送给其他Bolt • Bolt可以执行过滤、函数操作、Join、操作数据库等任何操作• Bolt是一个被动的角色，其接口中有一个execute(Tuple input)方法，在接收到消息之后会调用此函数，用户可以在此方法中执行自己的处理逻辑 • Topology ：Storm将Spouts和Bolts组成的网络抽象成Topology，它可以被提交到Storm集群执行。Topology可视为流转换图，图中节点是一个Spout或Bolt，边则表示Bolt订阅了哪个Stream。当Spout或者Bolt发送元组时，它会把元组发送到每个订阅了该Stream的Bolt上进行处理• Topology里面的每个处理组件（Spout或Bolt）都包含处理逻辑， 而组件之间的连接则表示数据流动的方向• Topology里面的每一个组件都是并行运行的•在Topology里面可以指定每个组件的并行度，Storm会在集群里面分配那么多的线程来同时计算•在Topology的具体实现上，Storm中的Topology定义仅仅是一些Thrift结构体（二进制高性能的通信中间件），支持各种编程语言进行定义 • Stream Groupings ：Storm中的Stream Groupings用于告知Topology如何在两个组件间（如Spout和Bolt之间，或者不同的Bolt之间）进行Tuple的传送。每一个Spout和Bolt都可以有多个分布式任务，一个任务在什么时候、以什么方式发送Tuple就是由Stream Groupings来决定的 目前，Storm中的Stream Groupings有如下几种方式： (1)ShuffleGrouping：随机分组，随机分发Stream中的Tuple，保证每个Bolt的Task接收Tuple数量大致一致(2)FieldsGrouping：按照字段分组，保证相同字段的Tuple分配到同一个Task中(3)AllGrouping：广播发送，每一个Task都会收到所有的Tuple(4)GlobalGrouping：全局分组，所有的Tuple都发送到同一个Task中(5)NonGrouping：不分组，和ShuffleGrouping类似，当前Task的执行会和它的被订阅者在同一个线程中执行(6)DirectGrouping：直接分组，直接指定由某个Task来执行Tuple的处理 Storm框架设计•Storm运行任务的方式与Hadoop类似：Hadoop运行的是MapReduce作业，而Storm运行的是“Topology”•但两者的任务大不相同，主要的不同是：MapReduce作业最终会完成计算并结束运行，而Topology将持续处理消息（直到人为终止） • Storm集群采用“Master—Worker”的节点方式：– Master节点运行名为“Nimbus”的后台程序（类似Hadoop中的“JobTracker”），负责在集群范围内分发代码、为Worker分配任务和监测故障– Worker节点运行名为“Supervisor”的后台程序，负责监听分配给它所在机器的工作，即根据Nimbus分配的任务来决定启动或停止Worker进程，一个Worker节点上同时运行若干个Worker进程• Storm使用Zookeeper来作为分布式协调组件，负责Nimbus和多个Supervisor之间的所有协调工作。借助于Zookeeper，若Nimbus进程或Supervisor进程意外终止，重启时也能读取、恢复之前的状态并继续工作，使得Storm极其稳定 worker进程 (1)Worker进程:每个worker进程都属于一个特定的Topology，每个Supervisor节点的worker可以有多个，每个worker对Topology中的每个组件（Spout或Bolt）运行一个或者多个executor线程来提供task的运行服务(2)Executor：executor是产生于worker进程内部的线程，会执行同一个组件的一个或者多个task。(3)Task:实际的数据处理由task完成Worker、Executor和Task的关系 • 基于这样的架构设计，Storm的工作流程如下图所示：•所有Topology任务的提交必须在Storm客户端节点上进行，提交后，由Nimbus节点分配给其他Supervisor节点进行处理•Nimbus节点首先将提交的Topology进行分片，分成一个个Task，分配给相应的Supervisor，并将Task和Supervisor相关的信息提交到Zookeeper集群上•Supervisor会去Zookeeper集群上认领自己的Task，通知自己的Worker进程进行Task的处理 Spark StreamingSpark Streaming设计•Spark Streaming可整合多种输入数据源，如Kafka、Flume、HDFS，甚至是普通的TCP套接字。经处理后的数据可存储至文件系统、数据库，或显示在仪表盘里 Spark Streaming的基本原理是将实时输入数据流以时间片（秒级）为单位进行拆分，然后经Spark引擎以类似批处理的方式处理每个时间片数据 Spark Streaming最主要的抽象是DStream（Discretized Stream，离散化数据流），表示连续不断的数据流。在内部实现上，Spark Streaming的输入数据按照时间片（如1秒）分成一段一段的DStream，每一段数据转换为Spark中的RDD，并且对DStream的操作都最终转变为对相应的RDD的操作 Spark Streaming与Storm的对比•Spark Streaming和Storm最大的区别在于，Spark Streaming无法实现毫秒级的流计算，而Storm可以实现毫秒级响应 •Spark Streaming构建在Spark上，一方面是因为Spark的低延迟执行引擎（100ms+）可以用于实时计算，另一方面，相比于Storm，RDD数据集更容易做高效的容错处理 •Spark Streaming采用的小批量处理的方式使得它可以同时兼容批量和实时数据处理的逻辑和算法，因此，方便了一些需要历史数据和实时数据联合分析的特定应用场合 Samza1.作业 一个作业（Job）是对一组输入流进行处理转化成输出流的程序。 2.分区 •Samza的流数据单位既不是Storm中的元组，也不是Spark Streaming中的DStream，而是一条条消息•Samza中的每个流都被分割成一个或多个分区，对于流里的每一个分区而言，都是一个有序的消息序列，后续到达的消息会根据一定规则被追加到其中一个分区里 3.任务 •一个作业会被进一步分割成多个任务（Task）来执行，其中，每个任务负责处理作业中的一个分区•分区之间没有定义顺序，从而允许每一个任务独立执行•YARN调度器负责把任务分发给各个机器，最终，一个工作中的多个任务会被分发到多个机器进行分布式并行处理 4.数据流图 •一个数据流图是由多个作业构成的，其中，图中的每个节点表示包含数据的流，每条边表示数据传输•多个作业串联起来就完成了流式的数据处理流程•由于采用了异步的消息订阅分发机制，不同任务之间可以独立运行 系统架构•Samza系统架构主要包括•流数据层（Kafka）•执行层（YARN）•处理层（Samza API）•流处理层和执行层都被设计成可插拔的，开发人员可以使用其他框架来替代YARN和Kafka 处理分析过程如下： •Samza客户端需要执行一个Samza作业时，它会向YARN的ResouceManager提交作业请求 •ResouceManager通过与NodeManager沟通为该作业分配容器（包含了CPU、内存等资源）来运行Samza ApplicationMaster •Samza ApplicationMaster进一步向ResourceManager申请运行任务的容器 •获得容器后，Samza ApplicationMaster与容器所在的NodeManager沟通，启动该容器，并在其中运行Samza Task Runner •Samza Task Runner负责执行具体的Samza任务，完成流数据处理分析 Storm、Spark Streaming和Samza的应用场景•从编程的灵活性来讲，Storm是比较理想的选择，它使用Apache Thrift，可以用任何编程语言来编写拓扑结构（Topology） •当需要在一个集群中把流计算和图计算、机器学习、SQL查询分析等进行结合时，可以选择Spark Streaming，因为，在Spark上可以统一部署Spark SQL，Spark Streaming、MLlib，GraphX等组件，提供便捷的一体化编程模型 •当有大量的状态需要处理时，比如每个分区都有数十亿个元组，则可以选择Samza。当应用场景需要毫秒级响应时，可以选择Storm和Samza，因为Spark Streaming无法实现毫秒级的流计算]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据基础学习笔记（八）——Spark]]></title>
    <url>%2F2018%2F05%2F17%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AB%EF%BC%89%E2%80%94%E2%80%94Spark%2F</url>
    <content type="text"><![CDATA[Spark的特点•运行速度快：使用DAG执行引擎以支持循环数据流与内存计算 •容易使用：支持使用Scala、Java、Python和R语言进行编程，可以通过Spark Shell进行交互式编程 •通用性：Spark提供了完整而强大的技术栈，包括SQL查询、流式计算、机器学习和图算法组件 •运行模式多样：可运行于独立的集群模式中，可运行于Hadoop中，也可运行于Amazon EC2等云环境中，并且可以访问HDFS、Cassandra、HBase、Hive等多种数据源 Scala简介Scala是一门现代的多范式编程语言，运行于Java平台（JVM，Java 虚拟机），并兼容现有的Java程序 Scala的特性： •Scala具备强大的并发性，支持函数式编程，可以更好地支持分布式系统 •Scala语法简洁，能提供优雅的API Scala兼容Java，运行速度快，且能融合到Hadoop生态圈中 Scala是Spark的主要编程语言，但Spark还支持Java、Python、R作为编程语言 Scala的优势是提供了REPL（Read-Eval-Print Loop，交互式解释器），提高程序开发效率 Spark与Hadoop的对比Hadoop存在如下一些缺点： •表达能力有限 •磁盘IO开销大 •延迟高 •任务之间的衔接涉及IO开销 •在前一个任务执行完成之前，其他任务就无法开始，难以胜任复杂、多阶段的计算任务 Spark在借鉴Hadoop MapReduce优点的同时，很好地解决了MapReduce所面临的问题 相比于Hadoop MapReduce，Spark主要具有如下优点： •Spark的计算模式也属于MapReduce，但不局限于Map和Reduce操作，还提供了多种数据集操作类型，编程模型比Hadoop MapReduce更灵活 •Spark提供了内存计算，可将中间结果放到内存中，对于迭代运算效率更高 Spark基于DAG的任务调度执行机制，要优于Hadoop MapReduce的迭代执行机制 •使用Hadoop进行迭代计算非常耗资源 •Spark将数据载入内存后，之后的迭代计算都可以直接使用内存中的中间结果作运算，避免了从磁盘中频繁读取数据 Spark生态系统在实际应用中，大数据处理主要包括以下三个类型： •复杂的批量数据处理：通常时间跨度在数十分钟到数小时之间 •基于历史数据的交互式查询：通常时间跨度在数十秒到数分钟之间 •基于实时数据流的数据处理：通常时间跨度在数百毫秒到数秒之间 当同时存在以上三种场景时，就需要同时部署三种不同的软件 •比如: MapReduce / Impala / Storm这样做难免会带来一些问题： •不同场景之间输入输出数据无法做到无缝共享，通常需要进行数据格式的转换 •不同的软件需要不同的开发和维护团队，带来了较高的使用成本 •比较难以对同一个集群中的各个系统进行统一的资源协调和分配 •Spark的设计遵循“一个软件栈满足不同应用场景”的理念，逐渐形成了一套完整的生态系统 •既能够提供内存计算框架，也可以支持SQL即席查询、实时流式计算、机器学习和图计算等 •Spark可以部署在资源管理器YARN之上，提供一站式的大数据解决方案 •因此，Spark所提供的生态系统足以应对上述三种场景，即同时支持批处理、交互式查询和流数据处理 Spark生态系统已经成为伯克利数据分析软件栈BDAS（Berkeley Data Analytics Stack）的重要组成部分 Spark的生态系统主要包含了Spark Core、Spark SQL、Spark Streaming、MLLib和GraphX 等组件 Spark运行架构•RDD：是Resillient Distributed Dataset（弹性分布式数据集）的简称，是分布式内存的一个抽象概念，提供了一种高度受限的共享内存模型 •DAG：是Directed Acyclic Graph（有向无环图）的简称，反映RDD之间的依赖关系 •Executor：是运行在工作节点（WorkerNode）的一个进程，负责运行Task •Application：用户编写的Spark应用程序 •Task：运行在Executor上的工作单元 •Job：一个Job包含多个RDD及作用于相应RDD上的各种操作 •Stage：是Job的基本调度单位，一个Job会分为多组Task，每组Task被称为Stage，或者也被称为TaskSet，代表了一组关联的、相互之间没有Shuffle依赖关系的任务组成的任务集 •Spark运行架构包括集群资源管理器（Cluster Manager）、运行作业任务的工作节点（Worker Node）、每个应用的任务控制节点（Driver）和每个工作节点上负责具体任务的执行进程（Executor） •资源管理器可以自带或Mesos或YARN 与Hadoop MapReduce计算框架相比，Spark所采用的Executor有两个优点： •一是利用多线程来执行具体的任务，减少任务的启动开销 •二是Executor中有一个BlockManager存储模块，会将内存和磁盘共同作为存储设备，有效减少IO开销 •一个Application由一个Driver和若干个Job构成，一个Job由多个Stage构成，一个Stage由多个没有Shuffle关系的Task组成 •当执行一个Application时，Driver会向集群管理器申请资源，启动xecutor，并向Executor发送应用程序代码和文件，然后在Executor上执行Task，运行结束后，执行结果会返回给Driver，或者写到HDFS或者其他数据库中 Spark运行基本流程（1）首先为应用构建起基本的运行环境，即由Driver创建一个SparkContext，进行资源的申请、任务的分配和监控 （2）资源管理器为Executor分配资源，并启动Executor进程 （3）SparkContext根据RDD的依赖关系构建DAG图，DAG图提交给DAGScheduler解析成Stage，然后把一个个TaskSet提交给底层调度器TaskScheduler处理；Executor向SparkContext申请Task，Task Scheduler将Task发放给Executor运行，并提供应用程序代码 （4）Task在Executor上运行，把执行结果反馈给TaskScheduler，然后反馈给DAGScheduler，运行完毕后写入数据并释放所有资源 总体而言，Spark运行架构具有以下特点： （1）每个Application都有自己专属的Executor进程，并且该进程在Application运行期间一直驻留。Executor进程以多线程的方式运行Task （2）Spark运行过程与资源管理器无关，只要能够获取Executor进程并保持通信即可 （3）Task采用了数据本地性和推测执行等优化机制 RDD1.设计背景 •许多迭代式算法（比如机器学习、图算法等）和交互式数据挖掘工具，共同之处是，不同计算阶段之间会重用中间结果 •目前的MapReduce框架都是把中间结果写入到HDFS中，带来了大量的数据复制、磁盘IO和序列化开销 •RDD就是为了满足这种需求而出现的，它提供了一个抽象的数据架构，我们不必担心底层数据的分布式特性，只需将具体的应用逻辑表达为一系列转换处理，不同RDD之间的转换操作形成依赖关系，可以实现管道化，避免中间数据存储 2.RDD 概念 •一个RDD就是一个分布式对象集合，本质上是一个只读的分区记录集合，每个RDD可分成多个分区，每个分区就是一个数据集片段，并且一个RDD的不同分区可以被保存到集群中不同的节点上，从而可以在集群中的不同节点上进行并行计算 •RDD提供了一种高度受限的共享内存模型，即RDD是只读的记录分区的集合，不能直接修改，只能基于稳定的物理存储中的数据集创建RDD，或者通过在其他RDD上执行确定的转换操作（如map、join和group by）而创建得到新的RDD •RDD提供了一组丰富的操作以支持常见的数据运算，分为“动作”（Action）和“转换”（Transformation）两种类型 •RDD提供的转换接口都非常简单，都是类似map、filter、groupBy、join等粗粒度的数据转换操作，而不是针对某个数据项的细粒度修改（不适合网页爬虫） •表面上RDD的功能很受限、不够强大，实际上RDD已经被实践证明可以高效地表达许多框架的编程模型（比如MapReduce、SQL、Pregel） •Spark用Scala语言实现了RDD的API，程序员可以通过调用API实现对RDD的各种操作 RDD典型的执行过程如下： •RDD读入外部数据源进行创建 •RDD经过一系列的转换（Transformation）操作，每一次都会产生不同的RDD，供给下一个转换操作使用 •最后一个RDD经过“动作”操作进行转换，并输出到外部数据源这一系列处理称为一个Lineage（血缘关系），即DAG拓扑排序的结果 优点：惰性调用、管道化、避免同步等待、不需要保存中间结果、每次操作变得简单 3.RDD特性 Spark采用RDD以后能够实现高效计算的原因主要在于： （1）高效的容错性 •现有容错机制：数据复制或者记录日志 •RDD：血缘关系、重新计算丢失分区、无需回滚系统、重算过程在不同节点之间并行、只记录粗粒度的操作 （2）中间结果持久化到内存，数据在内存中的多个RDD操作之间进行传递，避免了不必要的读写磁盘开销 （3）存放的数据可以是Java对象，避免了不必要的对象序列化和反序列化 4.RDD之间的依赖关系 窄依赖表现为一个父RDD的分区对应于一个子RDD的分区或多个父RDD的分区对应于一个子RDD的分区 •宽依赖则表现为存在一个父RDD的一个分区对应一个子RDD的多个分区 5.Stage的划分 Spark通过分析各个RDD的依赖关系生成了DAG，再通过分析各个RDD中的分区之间的依赖关系来决定如何划分Stage，具体划分方法是： •在DAG中进行反向解析，遇到宽依赖就断开 •遇到窄依赖就把当前的RDD加入到Stage中 •将窄依赖尽量划分在同一个Stage中，可以实现流水线计算被分成三个Stage，在Stage2中，从map到union都是窄依赖，这两步操作可以形成一个流水线操作 流水线操作实例分区7通过map操作生成的分区9，可以不用等待分区8到分区10这个map操作的计算结束，而是继续进行union操作，得到分区13，这样流水线执行大大提高了计算的效率 Stage的类型包括两种：ShuffleMapStage和ResultStage，具体如下： （1）ShuffleMapStage：不是最终的Stage，在它之后还有其他Stage，所以，它的输出一定需要经过Shuffle过程，并作为后续Stage的输入；这种Stage是以Shuffle为输出边界，其输入边界可以是从外部获取数据，也可以是另一个ShuffleMapStage的输出，其输出可以是另一个Stage的开始；在一个Job里可能有该类型的Stage，也可能没有该类型Stage； （2）ResultStage：最终的Stage，没有输出，而是直接产生结果或存储。这种Stage是直接输出结果，其输入边界可以是从外部获取数据，也可以是另一个ShuffleMapStage的输出。在一个Job里必定有该类型Stage。因此，一个Job含有一个或多个Stage，其中至少含有一个ResultStage。 6.RDD运行过程 通过上述对RDD概念、依赖关系和Stage划分的介绍，结合之前介绍的Spark运行基本流程，再总结一下RDD在Spark架构中的运行过程： （1）创建RDD对象； （2）SparkContext负责计算RDD之间的依赖关系，构建DAG； （3）DAGScheduler负责把DAG图分解成多个Stage，每个Stage中包含了多个Task，每个Task会被TaskScheduler分发给各个WorkerNode上的Executor去执行。 Spark SQL设计Spark SQL在Hive兼容层面仅依赖HiveQL解析、Hive元数据，也就是说，从HQL被解析成抽象语法树（AST）起，就全部由Spark SQL接管了。Spark SQL执行计划生成和优化都由Catalyst（函数式关系查询优化框架）负责 •Spark SQL增加了SchemaRDD（即带有Schema信息的RDD），使用户可以在Spark SQL中执行SQL语句，数据既可以来自RDD，也可以是Hive、HDFS、Cassandra等外部数据源，还可以是JSON格式的数据 •Spark SQL目前支持Scala、Java、Python三种语言，支持SQL-92规范]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据基础学习笔记（七）——数据仓库Hive]]></title>
    <url>%2F2018%2F05%2F01%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%83%EF%BC%89%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93Hive%2F</url>
    <content type="text"><![CDATA[数据仓库概念数据仓库（Data Warehouse）是一个面向主题的（Subject Oriented）、集成的（Integrated）、相对稳定的（Non-Volatile）、反映历史变化（Time Variant）的数据集合，用于支持管理决策。 Hive简介•Hive是一个构建于Hadoop顶层的数据仓库工具 •支持大规模数据存储、分析，具有良好的可扩展性 •某种程度上可以看作是用户编程接口，本身不存储和处理数据 •依赖分布式文件系统HDFS存储数据 •依赖分布式并行计算模型MapReduce处理数据 •定义了简单的类似SQL 的查询语言——HiveQL •用户可以通过编写的HiveQL语句运行MapReduce任务 •可以很容易把原来构建在关系数据库上的数据仓库应用程序移植到Hadoop平台上 •是一个可以提供有效、合理、直观组织和使用数据的分析工具 Hive具有的特点非常适用于数据仓库 1 采用批处理方式处理海量数据 •Hive需要把HiveQL语句转换成MapReduce任务进行运行 •数据仓库存储的是静态数据，对静态数据的分析适合采用批处理方式，不需要快速响应给出结果，而且数据本身也不会频繁变化 2 提供适合数据仓库操作的工具 •Hive本身提供了一系列对数据进行提取、转换、加载（ETL）的工具，可以存储、查询和分析存储在Hadoop中的大规模数据 •这些工具能够很好地满足数据仓库各种应用场景 Hive与Hadoop生态系统中其他组件的关系•Hive 依赖于HDFS 存储数据 •Hive 依赖于MapReduce 处理数据 • 在某些场景下Pig 可以作为Hive 的替代工具 •HBase 提供数据的实时访问 Hive 与传统数据库的对比分析Hive在很多方面和传统的关系数据库类似，但是它的底层依赖的是HDFS和MapReduce，所以在很多方面又有别于传统数据库 Hive 在企业中的部署和应用 Hive在企业大数据分析平台中的应用 Hive 在Facebook 公司中的应用 •基于Oracle的数据仓库系统已经无法满足激增的业务需求 •Facebook公司开发了数据仓库工具Hive，并在企业内部进行了大量部署 Hive系统架构•用户接口模块包括CLI、HWI、JDBC、ODBC、Thrift Server •驱动模块（Driver）包括编译器、优化器、执行器等，负责把HiveSQL语句转换成一系列MapReduce作业 •元数据存储模块（Metastore）是一个独立的关系型数据库（自带derby数据库，或MySQL数据库） Hive HA基本原理问题：在实际应用中，Hive也暴露出不稳定的问题 解决方案：Hive HA（High Availability） •由多个Hive实例进行管理的，这些Hive实例被纳入到一个资源池中，并由HAProxy提供一个统一的对外接口 •对于程序开发人员来说，可以把它认为是一台超强“Hive” Hive工作原理SQL语句转换成MapReduce作业的基本原理 join的实现原理 group by 的实现原理 存在一个分组（Group By）操作，其功能是把表Score的不同片段按照rank和level的组合值进行合并，计算不同rank和level的组合值分别有几条记录：select rank, level ,count(*) as value from score group by rank, level Hive中SQL查询转换成MapReduce作业的过程•当用户向Hive输入一段命令或查询时，Hive需要与Hadoop交互工作来完成该操作： •驱动模块接收该命令或查询编译器 •对该命令或查询进行解析编译 •由优化器对该命令或查询进行优化计算 •该命令或查询通过执行器进行执行 第1步：由Hive驱动模块中的编译器对用户输入的SQL语言进行词法和语法解析，将SQL语句转化为抽象语法树的形式 第2步：抽象语法树的结构仍很复杂，不方便直接翻译为MapReduce算法程序，因此，把抽象语法书转化为查询块 第3步：把查询块转换成逻辑查询计划，里面包含了许多逻辑操作符 第4步：重写逻辑查询计划，进行优化，合并多余操作，减少MapReduce任务数量 第5步：将逻辑操作符转换成需要执行的具体MapReduce任务 第6步：对生成的MapReduce任务进行优化，生成最终的MapReduce任务执行计划 第7步：由Hive驱动模块中的执行器，对最终的MapReduce任务进行执行输出 几点说明： • 当启动MapReduce程序时，Hive本身是不会生成MapReduce算法程序的 • 需要通过一个表示“Job执行计划”的XML文件驱动执行内置的、原生的Mapper和Reducer模块 • Hive通过和JobTracker通信来初始化MapReduce任务，不必直接部署在JobTracker所在的管理节点上执行 • 通常在大型集群上，会有专门的网关机来部署Hive工具。网关机的作用主要是远程操作和管理节点上的JobTracker通信来执行任务 • 数据文件通常存储在HDFS上，HDFS由名称节点管理 ImpalaImpala简介• Impala是由Cloudera公司开发的新型查询系统，它提供SQL语义，能查询存储在Hadoop的HDFS和HBase上的PB级大数据，在性能上比Hive高出3~30倍 • Impala的运行需要依赖于Hive的元数据 • Impala是参照 Dremel系统进行设计的 • Impala采用了与商用并行关系数据库类似的分布式查询引擎，可以直接与HDFS和HBase进行交互查询 • Impala和Hive采用相同的SQL语法、ODBC驱动程序和用户接口 Impala系统架构Impala和Hive、HDFS、HBase等工具是统一部署在一个Hadoop平台上的Impala主要由Impalad，State Store和CLI三部分组成 图中虚线组件是Impala的组件 Impala主要由Impalad，State Store和CLI三部分组成 Impalad • 负责协调客户端提交的查询的执行 • 包含Query Planner、Query Coordinator和Query Exec Engine三个模块 • 与HDFS的数据节点（HDFS DN）运行在同一节点上 • 给其他Impalad分配任务以及收集其他Impalad的执行结果进行汇总 • Impalad也会执行其他Impalad给其分配的任务，主要就是对本地HDFS和HBase里的部分数据进行操作 State Store • 会创建一个statestored进程 • 负责收集分布在集群中各个Impalad进程的资源信息，用于查询调度 CLI • 给用户提供查询使用的命令行工具 • 还提供了Hue、JDBC及ODBC的使用接口 说明：Impala中的元数据直接存储在Hive中。Impala采用与Hive相同的元数据、SQL语法、ODBC驱动程序和用户接口，从而使得在一个Hadoop平台上，可以统一部署Hive和Impala等分析工具，同时支持批处理和实时查询 Impala查询执行过程 Impala执行查询的具体过程： • 第0步，当用户提交查询前，Impala先创建一个负责协调客户端提交的查询的Impalad进程，该进程会向Impala State Store提交注册订阅信息，State Store会创建一个statestored进程，statestored进程通过创建多个线程来处理Impalad的注册订阅信息。 • 第1步，用户通过CLI客户端提交一个查询到impalad进程，Impalad的Query Planner对SQL语句进行解析，生成解析树；然后，Planner把这个查询的解析树变成若干PlanFragment，发送到Query Coordinator • 第2步，Coordinator通过从MySQL元数据库中获取元数据，从HDFS的名称节点中获取数据地址，以得到存储这个查询相关数据的所有数据节点。 • 第3步，Coordinator初始化相应impalad上的任务执行，即把查询任务分配给所有存储这个查询相关数据的数据节点。 • 第4步，Query Executor通过流式交换中间输出，并由Query Coordinator汇聚来自各个impalad的结果。 • 第5步，Coordinator把汇总后的结果返回给CLI客户端。 Impala与Hive的比较 Hive与Impala的 不同点总结如下： Hive适合于长时间的批处理查询分析，而Impala适合于实时交互式SQL查询 Hive依赖于MapReduce计算框架，Impala把执行计划表现为一棵完整的执行计划树，直接分发执行计划到各个Impalad执行查询 Hive在执行过程中，如果内存放不下所有数据，则会使用外存，以保证查询能顺序执行完成，而Impala在遇到内存放不下数据时，不会利用外存，所以Impala目前处理查询时会受到一定的限制 Hive与Impala的 相同点总结如下： Hive与Impala使用相同的存储数据池，都支持把数据存储于HDFS和HBase中 Hive与Impala使用相同的元数据 Hive与Impala中对SQL的解释处理比较相似，都是通过词法分析生成执行计划 总结 •Impala的目的不在于替换现有的MapReduce工具 •把Hive与Impala配合使用效果最佳 •可以先使用Hive进行数据转换处理，之后再使用Impala在Hive处理后的结果数据集上进行快速的数据分析 转自林子雨老师的公开课 视频地址：http://www.icourse163.org/learn/XMU-1002335004#/learn/content?type=detail&amp;id=1003836807&amp;cid=1004616536&amp;replay=true]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据基础学习笔记（六）——MapReduce]]></title>
    <url>%2F2018%2F04%2F24%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AD%EF%BC%89%E2%80%94%E2%80%94MapReduce%2F</url>
    <content type="text"><![CDATA[MapReduce体系结构MapReduce主要有以下4个部分组成： 1 ）Client •用户编写的MapReduce程序通过Client提交到JobTracker端 •用户可通过Client提供的一些接口查看作业运行状态 2 ）JobTracker •JobTracker负责资源监控和作业调度 •JobTracker 监控所有TaskTracker与Job的健康状况，一旦发现失败，就将相应的任务转移到其他节点 •JobTracker 会跟踪任务的执行进度、资源使用量等信息，并将这些信息告诉任务调度器（TaskScheduler），而调度器会在资源出现空闲时，选择合适的任务去使用这些资源 3 ）TaskTracker •TaskTracker 会周期性地通过“心跳”将本节点上资源的使用情况和任务的运行进度汇报给JobTracker，同时接收JobTracker 发送过来的命令并执行相应的操作（如启动新任务、杀死任务等） •TaskTracker 使用“slot”等量划分本节点上的资源量（CPU、内存等）。一个Task 获取到一个slot 后才有机会运行，而Hadoop调度器的作用就是将各个TaskTracker上的空闲slot分配给Task使用。slot 分为Map slot 和Reduce slot 两种，分别供MapTask 和Reduce Task使用 4 ）Task Task 分为Map Task 和Reduce Task 两种，均由TaskTracker 启动 MapReduce工作流程概述MapReduce把一个大的数据集拆分成多个小数据块在多台机器上并行处理，也就是说，一个大的MapReduce作业，首先会被拆分成许多个Map任务在多台机器上并行执行，每个Map任务通常运行在数据存储的节点上，这样，计算和数据就可以放在一起运行，不需要额外的数据传输开销。当Map任务结束后，会生成以形式表示的许多中间结果。然后，这些中间结果会被分发到多个Reduce任务在多台机器上并行执行，具有相同Key的会被发送到同一个Reduce任务那里，Reduce任务会对中间结果进行会中计算得到最后的结果，并输出到分布式文件系统中。 •不同的Map任务之间不会进行通信 •不同的Reduce任务之间也不会发生任何信息交换 •用户不能显式地从一台机器向另一台机器发送消息 •所有的数据交换都是通过MapReduce框架自身去实现的 MapReduce各个执行阶段MapReduce的算法执行过程： 1）MapReduce框架使用InputFormat模块做Map前的预处理，比如验证输入的格式是否符合输入定义，然后，将输入文件切分为逻辑上的多个InputSplit，这是MapReduce对文件进行处理和运算的输入单位，只是一个逻辑概念，每个InputSplit并没有对文件进行实际切割，只是记录了要处理的数据的位置和长度。 2）因为InuptSplit是逻辑切分而非物理切分，所以还需要通过RecordReader（RR）根据InputSplit中的信息来处理InputSplit中的具体记录，加载数据并转换为适合Map任务读取的键值对，输入给Map任务。 3）Map任务会根据用户自定义的映射规则，输出一系列的作为中间结果。 4）为了让Reduce可以并行处理Map的结果，需要对Map的输出进行一定的分区（Portition）、排序（Sort）、合并（Combine）、归并（Merge）等操作，得到形式的中间结果，在交给对应的Reduce进行处理，这个过程称为Shuffle。从无序的到有序的，这个过程用Shuffle（洗牌）来称呼是非常形象的。 5）Reduce以一系列中间结果作为输入，执行用户定义的逻辑，输出结果给OutputFormat模块。 6）OutputFormat模块会验证输出目录是否已经存在以及输出结果类型是否符合配置文件中的配置类型，如果都满足，就输出Reduce的结果到分布式文件系统。 HDFS 以固定大小的block 为基本单位存储数据，而对于MapReduce 而言，其处理单位是split。split 是一个逻辑概念，它只包含一些元数据信息，比如数据起始位置、数据长度、数据所在节点等。它的划分方法完全由用户自己决定。 Map 任务的数量 •Hadoop为每个split创建一个Map任务，split 的多少决定了Map任务的数目。大多数情况下，理想的分片大小是一个HDFS块 Reduce 任务的数量 •最优的Reduce任务个数取决于集群中可用的reduce任务槽(slot)的数目 •通常设置比reduce任务槽数目稍微小一些的Reduce任务个数（这样可以预留一些系统资源处理可能发生的错误 Shuffle过程原理1.Shuffle 过程简介 2.Map 端的Shuffle 过程 Map的输出结果首先被写入缓存，当缓存满时，就启动溢写操作，把缓存中的数据写入磁盘文件，并清空缓存。当启动溢写操作时，首先需要把缓存中的数据进行分区，然后对每个分区的数据进行排序（Sort）和合并（Combine），之后再写入磁盘文件。每次溢写操作会生成一个新的磁盘文件，随着Map任务的执行，磁盘中就会生成多个溢写文件。在Map任务全部结束之前，这些溢写文件会被归并（Merge）成一个大的磁盘文件，然后通知相应的Reduce任务来领取属于自己处理的数据。 •每个Map任务分配一个缓存 •MapReduce默认100MB缓存 •设置溢写比例0.8 •分区默认采用哈希函数 •排序是默认的操作 •排序后可以合并（Combine） •合并不能改变最终结果 •在Map任务全部结束之前进行归并 •归并得到一个大的文件，放在本地磁盘 •文件归并时，如果溢写文件数量大于预定值（默认是3）则可以再次启动Combiner，少于3不需要 •JobTracker会一直监测Map任务的执行，并通知Reduce任务来领取数据 合并（Combine）和归并（Merge）的区别：两个键值对&lt;“a”,1&gt;和&lt;“a”,1&gt;，如果合并，会得到&lt;“a”,2&gt;，如果归并，会得到&lt;“a”,&gt; 3.Reduce 端的Shuffle 过程 Reduce任务从Map端的不同Map及其领回属于自己处理的那部分数据，然后对数据进行归并（Merge）后交给Reduce处理。 •Reduce任务通过RPC向JobTracker询问Map任务是否已经完成，若完成，则领取数据 •Reduce领取数据先放入缓存，来自不同Map机器，先归并，再合并，写入磁盘 •多个溢写文件归并成一个或多个大文件，文件中的键值对是排序的 •当数据很少时，不需要溢写到磁盘，直接在缓存中归并，然后输出给Reduce MapReduce应用程序执行过程 参考资料：林子雨老师的MOOC课程：https://www.icourse163.org/learn/XMU-1002335004#/learn/content?type=detail&amp;id=1003836797&amp;cid=1004616527&amp;replay=true]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据基础学习笔记（五）——云数据库架构]]></title>
    <url>%2F2018%2F04%2F17%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89%E2%80%94%E2%80%94%E4%BA%91%E6%95%B0%E6%8D%AE%E5%BA%93%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[不同的云数据库产品采用的系统架构差异很大，这里以阿里巴巴集团核心系统数据库团队开发的UMP(Unified MySQL Platform)系统为例进行介绍。 UMP系统概述•UMP系统是低成本和高性能的MySQL云数据库方案 总的来说，UMP系统架构设计遵循了以下原则： •保持单一的系统对外入口，并且为系统内部维护单一的资源池（CPU、内存、带宽、磁盘等放在一个统一的资源池，供上部组件调用） •消除单点故障，保证服务的高可用性（设置多个管家（Controller）） •保证系统具有良好的可伸缩，能够动态地增加、删减计算与存储节点 •保证分配给用户的资源也是弹性可伸缩的，资源之间相互隔离，确保应用和数据安全（多租户之间隔离，当一个用户使用过多资源时，对其进行限制，以免影响其他用户的使用） UMP系统架构 Mnesia•Mnesia是一个分布式数据库管理系统 •Mnesia支持事务，支持透明的数据分片，利用两阶段锁实现分布式事务，可以线性扩展到至少50个节点 •Mnesia的数据库模式(schema)可在运行时动态重配置，表能被迁移或复制到多个节点来改进容错性 •Mnesia的这些特性，使其在开发云数据库时被用来提供分布式数据库服务 RabbitMQ •RabbitMQ是一个工业级的消息队列产品（功能类似于IBM公司的消息队列产品IBM Websphere MQ），作为消息传输中间件来使用，可以实现可靠的消息传送 •UMP集群中各个节点之间的通信，不需要建立专门的连接，都是通过读写队列消息来实现的 ZookeeperZookeeper是高效和可靠的协同工作系统，提供分布式锁之类的基本服务（比如统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等），用于构建分布式应用，减轻分布式应用程序所承担的协调任务 在UMP系统中，Zookeeper主要发挥三个作用： •作为全局的配置服务器。UMP系统把运行的应用系统的配置信息完全交给Zookeeper来管理，把配置信息保存在Zookeeper的某个目录节点中，然后将所有需要修改的服务器对这个目录节点设置监听，也就是监控配置信息的状态，一旦配置信息发生变化，每台服务器就会收到Zookeeper的通知，然后从Zookeeper获取新的配置信息。 •提供分布式锁（选出一个集群的“总管”）。UMP急群众部署了多个Controller服务器，为了保证系统的正确运行，对于有些操作，在某一时刻，只能由一个服务器去执行，而不能同时执行。礼物，一个MySQL实例发生故障以后，需要进行主备切换，有另一个正常的服务器来代替当前发生故障的服务器，如果这个时候所有的Controller服务器都去跟踪处理并且发起主备切换流程，那么，整个系统就会进入混乱状态。因此，在同一时间，必须从集群的多个Controller服务器中选举出一个“总管”，由这个“总管”负责发起各种系统任务。Zookeeper的分布式锁功能能够帮助选出一个“总管”，让这个“总管”来管理集群。 •监控所有MySQL实例。急群众运行MySQL实例的服务器发生故障时，必须被及时监听到，然后使用其他正常服务器来替代故障服务器。UMP系统借助Zookeeper实现对所有MySQL实例的监控。每个MySQL实例在启动时都会在Zookeeper上创建一个临时类型的目录节点，当某个MySQL实例挂掉时，这个临时类型的目录节点也随之被删除，后台监听进程可以捕获到这种变化，从而知道这个MySQL实例不再可用。 LVS•LVS(Linux Virtual Server)即Linux虚拟服务器，是一个虚拟的服务器集群系统 •UMP系统借助于LVS来实现集群内部的负载均衡 •LVS集群采用IP负载均衡技术和基于内容请求分发技术 •调度器是LVS集群系统的唯一入口点，调度器具有很好的吞吐率，将请求均衡地转移到不同的服务器上执行，且调度器自动屏蔽掉服务器的故障，从而将一组服务器构成一个高性能的、高可用的虚拟服务器 •整个服务器集群的结构对客户是透明的，而且无需修改客户端和服务器端的程序 Controller服务器•Controller服务器向UMP集群提供各种管理服务，实现集群成员管理、元数据存储、MySQL实例管理、故障恢复、备份、迁移、扩容等功能 •Controller服务器上运行了一组Mnesia分布式数据库服务，其中存储了各种系统元数据，主要包括集群成员、用户的配置和状态信息，以及用户名到后端MySQL实例地址的映射关系（或称为“路由表”）等 •当其它服务器组件需要获取用户数据时，可以向Controller服务器发送请求获取数据 •为了避免单点故障，保证系统的高可用性，UMP系统中部署了多台Controller服务器，然后，由Zookeeper的分布式锁功能来帮助选出一个“总管”，负责各种系统任务的调度和监控 Web 控制台Web控制台向用户提供系统管理界面 Proxy 服务器Proxy服务器向用户提供访问MySQL数据库的服务，它完全实现了MySQL协议，用户可以使用已有的MySQL客户端连接到Proxy服务器，Proxy服务器通过用户名获取到用户的认证信息、资源配额的限制(例如QPS、IOPS（I/O Per Second）、最大连接数等)，以及后台MySQL实例的地址，然后，用户的SQL查询请求会被转发到相应的MySQL实例上。 除了数据路由的基本功能外，Proxy服务器中还实现了很多重要的功能，主要包括屏蔽MySQL实例故障、读写分离、分库分表、资源隔离、记录用户访问日志等 Agent 服务器Agent服务器部署在运行MySQL进程的机器上，用来管理每台物理机上的MySQL实例，执行主从切换、创建、删除、备份、迁移等操作，同时，还负责收集和分析MySQL进程的统计信息、慢查询日志（Slow Query Log）和bin-log 日志分析服务器日志分析服务器存储和分析Proxy服务器传入的用户访问日志，并支持实时查询一段时间内的慢日志和统计报表 信息统计服务器信息统计服务器定期将采集到的用户的连接数、QPS数值以及MySQL实例的进程状态用RRDtool进行统计，可以在 Web界面上可视化展示统计结果，也可以把统计结果作为今后实现弹性的资源分配和自动化的MySQL实例迁移的依据 愚公系统愚公系统是一个全量复制结合bin-log分析进行增量复制的工具，可以实现在不停机的情况下动态扩容、缩容和迁移]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据基础学习笔记（四）——HBase相关知识（三）]]></title>
    <url>%2F2018%2F04%2F03%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89%E2%80%94%E2%80%94HBase%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%EF%BC%88%E4%B8%89%EF%BC%89%2F</url>
    <content type="text"><![CDATA[HBase应用方案HBase实际应用中的性能优化方法行键（Row Key ） 行键是按照 字典序存储，因此，设计行键时，要充分利用这个排序特点，将经常一起读取的数据存储到一块，将最近可能会被访问的数据放在一块。 举个例子：如果最近写入HBase表中的数据是最可能被访问的，可以考虑将时间戳作为行键的一部分，由于是字典序排序，所以可以使用Long.MAX_VALUE -timestamp作为行键，这样能保证新写入的数据在读取时可以被快速命中。 InMemory创建表的时候，可以通过HColumnDescriptor.setInMemory(true)将表放到Region服务器的缓存中，保证在读取的时候被cache命中。 Max Version创建表的时候，可以通过HColumnDescriptor.setMaxVersions(int maxVersions)设置表中数据的最大版本，如果只需要保存最新版本的数据，那么可以设置setMaxVersions(1)。 Time To Live创建表的时候，可以通过HColumnDescriptor.setTimeToLive(int timeToLive)设置表中数据的存储生命期，过期数据将自动被删除，例如如果只需要存储最近两天的数据，那么可以设置setTimeToLive(2 24 60 * 60)。 HBase性能监视Master-status(自带) •HBase Master默认基于Web的UI服务端口为60010，HBase region服务器默认基于Web的UI服务端口为60030.如果master运行在名为master.foo.com的主机中，mater的主页地址就是http://master.foo.com:60010，用户可以通过Web浏览器输入这个地址查看该页面 •可以查看HBase集群的当前状态 Ganglia Ganglia是UC Berkeley发起的一个开源集群监视项目，用于监控系统性能 OpenTSDB OpenTSDB可以从大规模的集群（包括集群中的网络设备、操作系统、应用程序）中获取相应的metrics并进行存储、索引以及服务，从而使得这些数据更容易让人理解，如web化，图形化等 Ambari Ambari 的作用就是创建、管理、监视 Hadoop 的集群 在HBase之上构建SQL引擎NoSQL区别于关系型数据库的一点就是NoSQL不使用SQL作为查询语言，至于为何在NoSQL数据存储HBase上提供SQL接口，有如下原因： 易使用。使用诸如SQL这样易于理解的语言，使人们能够更加轻松地使用HBase。 减少编码。使用诸如SQL这样更高层次的语言来编写，减少了编写的代码量。方案： 1.Hive整合HBase2.Phoenix 1.Hive 整合HBase Hive与HBase的整合功能从Hive0.6.0版本已经开始出现，利用两者对外的API接口互相通信，通信主要依靠hive_hbase-handler.jar工具包(Hive Storage Handlers)。由于HBase有一次比较大的版本变动，所以并不是每个版本的Hive都能和现有的HBase版本进行整合，所以在使用过程中特别注意的就是两者版本的一致性。 2.Phoenix Phoenix由Salesforce.com开源，是构建在Apache HBase之上的一个SQL中间层，可以让开发者在HBase上执行SQL查询。 构建HBase二级索引二级索引，又叫辅助索引 HBase只有一个针对行健的索引访问HBase表中的行，只有三种方式： •通过单个行健访问 •通过一个行健的区间来访问 •全表扫描 使用其他产品为HBase行健提供索引功能： •Hindex二级索引 •HBase+Redis •HBase+solr 原理：采用HBase0.92版本之后引入的Coprocessor特性 Coprocessor构建二级索引 •Coprocessor提供了两个实现：endpoint和observer，endpoint相当于关系型数据库的存储过程，而observer则相当于触发器 •observer允许我们在记录put前后做一些处理，因此，而我们可以在插入数据时同步写入索引表 •Coprocessor构建二级索引•缺点：每插入一条数据需要向索引表插入数据，即耗时是双倍的，对HBase的集群的压力也是双倍的 优点：非侵入性：引擎构建在HBase之上，既没有对HBase进行任何改动，也不需要上层应用做任何妥协 Hindex二级索引 Hindex 是华为公司开发的纯 Java 编写的HBase二级索引，兼容 Apache HBase 0.94.8。当前的特性如下： •多个表索引 •多个列索引 •基于部分列值的索引 HBase+Redis •Redis+HBase方案 •Coprocessor构建二级索引 •Redis做客户端缓存 •将索引实时更新到Redis等KV系统中，定时从KV更新索引到HBase的索引表中 Solr+HBase Solr是一个高性能，采用Java5开发，基于Lucene的全文搜索服务器。同时对其进行了扩展，提供了比Lucene更为丰富的查询语言，同时实现了可配置、可扩展并对查询性能进行了优化，并且提供了一个完善的功能管理界面，是一款非常优秀的全文搜索引擎。]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据基础学习笔记（三）——HBase相关知识（二）]]></title>
    <url>%2F2018%2F04%2F02%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89%E2%80%94%E2%80%94HBase%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[HBase功能组件• HBase的实现包括三个主要的功能组件：– （1）库函数：链接到每个客户端– （2）一个Master主服务器（充当管家的作用）– （3）许多个Region服务器 • 主服务器Master负责管理和维护HBase表的分区信息，维护Region服务器列表，分配Region，负载均衡 • 一个大的表会被分成很多个Region，Region服务器负责存储和维护分配给自己的Region，处理来自客户端的读写请求 • 客户端并不是直接从Master主服务器上读取数据，而是在获得Region的存储位置信息后，直接从Region服务器上读取数据 • 客户端并不依赖Master，而是通过Zookeeper来获得Region位置信息，大多数客户端甚至从来不和Master通信，这种设计方式使得Master负载很小 表和Region •开始只有一个Region，后来不断分裂 •Region拆分操作非常快，接近瞬间，因为拆分之后的Region读取的仍然是原存储文件，直到“合并”过程把存储文件异步地写到独立的文件之后，才会读取新文件 这种拆分只是逻辑上的拆分，只是数据的指向发生了变化，它的实际存储还是在原来的旧的Region中的数据。当读新的Region时，后台会有一个合并操作，会把拆分的数据进行重新操作，最终会写到新的文件中去。 一个Region只能存到一个Region服务器上。 Region的定位那么有一个问题，当一个Region被拆成很多个Region时，这些Region会把它打散，分布到不同的地方存储，那么怎么知道它被存到哪里去了呢？ •元数据表，又名.META.表，存储了Region和Region服务器的映射关系 •当HBase表很大时， .META.表也会被分裂成多个Region •根数据表，又名-ROOT-表，记录所有元数据的具体位置 •-ROOT-表只有唯一一个Region，名字是在程序中被写死的 •Zookeeper文件记录了-ROOT-表的位置 •一个-ROOT-表最多只能有一个Region，也就是最多只能有128MB，按照每行（一个映射条目）占用1KB内存计算，128MB空间可以容纳128MB/1KB=2^17 行，也就是说，一个-ROOT-表可以寻址2^17 个.META.表的Region。 •同理，每个.META.表的 Region可以寻址的用户数据表的Region个数是128MB/1KB=2^17 。 •最终，三层结构可以保存的Region数目是(128MB/1KB) × (128MB/1KB) = 2^34 个Region 所以三层架构能够满足企业的需求。 客户端访问数据时的“三级寻址”•为了加速寻址，客户端会缓存位置信息，同时，需要解决缓存失效问题 •寻址过程客户端只需要询问Zookeeper服务器，不需要连接Master服务器 这里的缓存机制采用的是惰性缓存，如果在使用缓存获取数据时，获取不到数据，那么就失效了，这时候再次进行三级寻址过程，以解决缓存失效问题。 HBase运行机制 • 1. 客户端– 客户端包含访问HBase的接口，同时在缓存中维护着已经访问过的Region位置信息，用来加快后续数据访问过程 • 2. Zookeeper服务器– Zookeeper可以帮助选举出一个Master作为集群的总管，并保证在任何时刻总有唯一一个Master在运行，这就避免了Master的“单点失效”问题Zookeeper是一个很好的集群管理工具，被大量用于分布式计算，提供配置维护、域名服务、分布式同步、组服务等。提供管家的功能，维护整个HBase集群。虽然有很多备用的Master，但是它保证只有一个Master是运行的。 • 3. Master• 主服务器Master主要负责表和Region的管理工作：– 管理用户对表的增加、删除、修改、查询等操作– 实现不同Region服务器之间的负载均衡– 在Region分裂或合并后，负责重新调整Region的分布– 对发生故障失效的Region服务器上的Region进行迁移 • 4. Region服务器– Region服务器是HBase中最核心的模块，负责维护分配给自己的Region，并响应用户的读写请求 Region服务器工作原理1.用户读写数据过程 •用户写入数据时，被分配到相应Region服务器去执行 •用户数据首先被写入到MemStore和Hlog中 •只有当操作写入Hlog之后，commit()调用才会将其返回给客户端 •当用户读取数据时，Region服务器会首先访问MemStore缓存，如果找不到，再去磁盘上面的StoreFile中寻找 2.缓存的刷新 •系统会周期性地把MemStore缓存里的内容刷写到磁盘的StoreFile文件中，清空缓存，并在Hlog里面写入一个标记 •每次刷写都生成一个新的StoreFile文件，因此，每个Store包含多个StoreFile文件 •每个Region服务器都有一个自己的HLog 文件，每次启动都检查该文件，确认最近一次执行缓存刷新操作之后是否发生新的写入操作；如果发现更新，则先写入MemStore，再刷写到StoreFile，最后删除旧的Hlog文件，开始为用户提供服务 3.StoreFile 的合并 •每次刷写都生成一个新的StoreFile，数量太多，影响查找速度 •调用Store.compact()把多个合并成一个 •合并操作比较耗费资源，只有数量达到一个阈值才启动合并 Store工作原理•Store是Region服务器的核心 •多个StoreFile合并成一个 •单个StoreFile过大时，又触发分裂操作，1个父Region被分裂成两个子Region HLog工作原理• 分布式环境必须要考虑系统出错。HBase采用HLog保证系统恢复 • HBase系统为每个Region服务器配置了一个HLog文件，它是一种预写式日志（Write Ahead Log） • 用户更新数据必须首先写入日志后，才能写入MemStore缓存，并且，直到MemStore缓存内容对应的日志已经写入磁盘，该缓存内容才能被刷写到磁盘 • Zookeeper会实时监测每个Region服务器的状态，当某个Region服务器发生故障时，Zookeeper会通知Master • Master首先会处理该故障Region服务器上面遗留的HLog文件，这个遗留的HLog文件中包含了来自多个Region对象的日志记录 • 系统会根据每条日志记录所属的Region对象对HLog数据进行拆分，分别放到相应Region对象的目录下，然后，再将失效的Region重新分配到可用的Region服务器中，并把与该Region对象相关的HLog日志记录也发送给相应的Region服务器 • Region服务器领取到分配给自己的Region对象以及与之相关的HLog日志记录以后，会重新做一遍日志记录中的各种操作，把日志记录中的数据写入到MemStore缓存中，然后，刷新到磁盘的StoreFile文件中，完成数据恢复 • 共用日志优点：提高对表的写操作性能；缺点：恢复时需要分拆日志 本笔记参考自厦门大学林子雨老师的公开课：https://www.icourse163.org/course/XMU-1002335004]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据基础学习笔记（二）——HBase相关知识（一）]]></title>
    <url>%2F2018%2F04%2F02%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94Hbase%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[Hbase简介HBase是一个高可靠、高性能、面向列、可伸缩的分布式数据库，是谷歌BigTable的开源实现，主要用来存储非结构化和半结构化的松散数据。HBase的目标是处理非常庞大的表，可以通过水平扩展的方式，利用廉价计算机集群处理由超过10亿行数据和数百万列元素组成的数据表。 底层的分布式文件系统用来存储完全非结构化的数据。 Hbase是架构在底层的分布式文件系统HDFS基础之上的同时MR可以对Hbase的数据进行处理。同时Hive和Pig等都可以访问Hbase中的数据。 从上图可以看出，BigTable和HBase的底层技术的对比。 为什么要设计HBase这个数据产品呢？•Hadoop可以很好地解决大规模数据的离线批量处理问题，但是，受限于HadoopMapReduce编程框架的高延迟数据处理机制，使得Hadoop无法满足大规模数据实时处理应用的需求 •HDFS面向批量访问模式，不是随机访问模式 •传统的通用关系型数据库无法应对在数据规模剧增时导致的系统扩展性和性能问题（分库分表也不能很好解决） •传统关系数据库在数据结构变化时一般需要停机维护；空列浪费存储空间 •因此，业界出现了一类面向半结构化数据存储和处理的高可扩展、低写入/查询延迟的系统，例如，键值数据库、文档数据库和列族数据库（如BigTable和HBase等） •HBase已经成功应用于互联网服务领域和传统行业的众多在线式数据分析处理系统中 HBase与传统关系数据库的对比分析• HBase与传统的关系数据库的区别主要体现在以下几个方面： • （1）数据类型：关系数据库采用关系模型，具有丰富的数据类型（整型，字符型等等）和存储方式，HBase则采用了更加简单的数据模型，它把数据存储为未经解释的字符串（也就是Bytes数组） • （2）数据操作：关系数据库中包含了丰富的操作（增删改查），其中会涉及复杂的多表连接。HBase操作则不存在复杂的表与表之间的关系，只有简单的插入、查询、删除、清空等，因为HBase在设计上就避免了复杂的表和表之间的关系 • （3）存储模式：关系数据库是基于行模式存储的。HBase是基于列存储的，每个列族都由几个文件保存，不同列族的文件是分离的 • （4）数据索引：关系数据库通常可以针对不同列构建复杂的多个索引，以提高数据访问性能。HBase只有一个索引——行键，通过巧妙的设计，HBase中的所有访问方法，或者通过行键访问，或者通过行键扫描，从而使得整个系统不会慢下来 • （5）数据维护：在关系数据库中，更新操作会用最新的当前值去替换记录中原来的旧值，旧值被覆盖后就不会存在。而在HBase中执行更新操作时，并不会删除数据旧的版本，而是生成一个新的版本，旧有的版本仍然保留，只有在过了设置的参数期限之后，在系统后台清理的时候才会清理掉 • （6）可伸缩性：关系数据库很难实现横向扩展，纵向扩展（如添加内存，改进CPU等等）的空间也比较有限。相反，HBase和BigTable这些分布式数据库就是为了实现灵活的水平扩展而开发的，能够轻易地通过在集群中增加或者减少硬件数量来实现性能的伸缩 HBase的访问接口以后在使用Hbase的时候，可以通过哪些方式访问HBase数据库？见下图： HBase数据模型• HBase是一个稀疏、多维度、排序的映射表，这张表的索引是行键、列族、列限定符和时间戳 • 每个值是一个未经解释的字符串，没有数据类型 • 用户在表中存储数据，每一行都有一个可排序的行键和任意多的列 • 表在水平方向由一个或者多个列族组成，一个列族中可以包含任意多个列，同一个列族里面的数据存储在一起 • 列族支持动态扩展，可以很轻松地添加一个列族或列，无需预先定义列的数量以及类型，所有列均以字符串形式存储，用户需要自行进行数据类型转换 • HBase中执行更新操作时，并不会删除数据旧的版本，而是生成一个新的版本，旧有的版本仍然保留（这是和HDFS只允许追加不允许修改的特性相关的） • 表：HBase采用表来组织数据，表由行和列组成，列划分为若干个列族 • 行：每个HBase表都由若干行组成，每个行由行键（row key）来标识。 • 列族：一个HBase表被分组成许多“列族”（Column Family）的集合，它是基本的访问控制单元（支持动态拓展） • 列限定符：列族里的数据通过列限定符（或列）来定位 • 单元格：在HBase表中，通过行、列族和列限定符确定一个“单元格”（cell），单元格中存储的数据没有数据类型，总被视为字节数组byte[] • 时间戳：每个单元格都保存着同一份数据的多个版本，这些版本采用时间戳进行索引 HBase的数据坐标HBase中需要根据行键、列族、列限定符和时间戳来确定一个单元格，因此，可以视为一个“四维坐标”，即[行键, 列族, 列限定符, 时间戳] 概念视图HBase在概念上和实际的底层存储是有区分的，在概念上HBase只是一个表，如下面只给了一个行键： 如这一个行键给了两个列族，第一个列族contents中冒号前面的contents是列族的名称，冒号后面的html是列的名称，引号中的内容就是这一列的数据。一个时间戳并不一定会在所有列族插入数据，从图中就可以看出。所以这就导致了HBase的稀疏表的特性。这只是在概念上的视图。 物理视图实际上在实际存储中，并不是按上述的方式去存的。在底层存储时，是按列族为单位进行存储的。 上图是在实际存储时，存储在底层的实际的表。并没有像概念视图中存储了很多的空数据。所以概念视图和物理视图上是有区分的。 面向列的存储 传统的数据库，以行为单位进行存储，一行包括ID,姓名，年龄，性别，IP，操作等。但是按列存储，里面的姓名、年龄等进行单独存储。 它们各自的优缺点： 另外，使用列式存储，数据可以达到很高的数据压缩率。而行式存储，很难压缩。 本笔记参考自厦门大学林子雨老师的公开课：https://www.icourse163.org/course/XMU-1002335004]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据基础学习笔记（一）——Hadoop相关知识]]></title>
    <url>%2F2018%2F04%2F01%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[HadoopHadoop的应用现状和构成简介下图为Hadoop在企业中的 应用架构 访问层不用多说，满足企业的数据分析、数据挖掘和数据实时查询功能。为了满足访问层的需求，大数据层的各个技术对其进行支撑。（1）离线分析：大量数据拿过来之后进行批量处理。其中MR是MapReduce的简称，Hive数据仓库和Pig也可以进行离线数据分析。（2）实时查询：其中Hbase是一个可以支持几十亿行数据的非常好的分布式数据库。（3）BI分析：Mahout是Hadoop平台上的一款数据挖掘应用。可以把各种数据挖掘，机器学习和商务智能的算法用MapReduce实现。否则开发人员要自己用MapReduce写决策树算法。 下图为一些大数据计算模式及其代表产品 下图为Hadoop项目结构 YARNz专门负责调度内存，CPU，带宽等计算资源。而上面的事完成具体的计算工作的。 Tez会把很多的MapReduce作业进行分析优化，构建成一个有向无环图，保证获得最好的处理效率。 Spark与MapReduce类似，也是进行相应的计算。但是Spark是基于内存的，而MapReduce是基于磁盘的计算。MR在计算时，先把数据写到磁盘中，然后c处理结束后再写到分布式文件系统中。所以Spark的性能要高。 Pig实现流数据处理，较MR属于轻量级。它也支持类似于SQL的语句。是一种轻量级的脚本语言。 Oozie是一个工作流管理系统，可以把一个工作分成不同的工作环节。 Zookeeper提供分布式协调一致性服务。 Hbase是一个非关系型数据库，可以支持随机读写。 Flume是专门负责日志收集的，分析一些实时生成的数据流。 Sqoopy用于在Hadoop与传统数据库之间进行数据传递（导入导出等）。可以把之前存到关系型数据库（如Oracle）中的数据导入到HDFS、Hive或者Hbase中，反之亦可。 Ambari是一个安装部署工具，可以在一个集群上面智能化的管理一整套Hadoop上的各个套件。 Hadoop各组件的功能如下： Hadoop集群的节点类型Hadoop框架中最核心的设计是为海量数据提供存储的HDFS和对数据进行计算的MapReduce MapReduce的作业主要包括：（1）从磁盘或从网络读取数据，即IO密集工作；（2）计算数据，即CPU密集工作 •Hadoop集群的整体性能取决于CPU、内存、网络以及存储之间的性能平衡。因此运营团队在选择机器配置时要针对不同的工作节点选择合适硬件类型•一个基本的Hadoop集群中的节点主要有: •NameNode：负责协调集群中的数据存储 •DataNode：存储被拆分的数据块 •JobTracker：协调数据计算任务 •TaskTracker：负责执行由JobTracker指派的任务 •SecondaryNameNode：帮助NameNode收集文件系统运行的状态信息 HDFS全称：Hadoop Distributed File System.解决海量数据的分布式存储问题。 分布式文件系统的结构分布式文件系统在物理结构上是由计算机集群中的多个节点构成的，这些节点分为两类，一类叫“主节点”(Master Node)或者也被称为“名称结点”(NameNode)，另一类叫“从节点”（Slave Node）或者也被称为“数据节点”(DataNode) HDFS的三个节点：Namenode，Datanode，Secondary Namenode Namenode：HDFS的守护进程，用来管理文件系统的命名空间，负责记录文件是如何分割成数据块，以及这些数据块分别被存储到那些数据节点上，它的主要功能是对内存及IO进行集中管理。 Datanode：文件系统的工作节点，根据需要存储和检索数据块，并且定期向namenode发送他们所存储的块的列表。 Secondary Namenode：辅助后台程序，与NameNode进行通信，以便定期保存HDFS元数据的快照。 HDFS采用了主从（Master/Slave）结构模型，一个HDFS集群包括一个名称节点（NameNode）和若干个数据节点（DataNode）（如图所示）。名称节点作为中心服务器，负责管理文件系统的命名空间及客户端对文件的访问。集群中的数据节点一般是一个节点运行一个数据节点进程，负责处理文件系统客户端的读/写请求，在名称节点的统一调度下进行数据块的创建、删除和复制等操作。每个数据节点的数据实际上是保存在本地Linux文件系统中的。 HDFS的缺点： 1.不适合低延迟的数据访问2.无法高效存储大量小文件3.不支持多用户写入及任意修改文件 名称节点和数据节点 在HDFS中，名称节点（NameNode）负责管理分布式文件系统的命名空间（Namespace），保存了两个核心的数据结构，即FsImage和EditLog。 •FsImage用于维护文件系统树以及文件树中所有的文件和文件夹的元数据 •操作日志文件EditLog中记录了所有针对文件的创建、删除、重命名等操作 •名称节点记录了每个文件中各个块所在的数据节点的位置信息。 客户端在访问数据时，先通过名称节点，获取元数据信息，从而知道被访问的数据存到哪些数据节点，获得数据块具体存储位置的信息之后，客户端就会到各个机器上去获取它所需要的数据。写入操作类似，客户端先访问名称节点，一个大文件（如1TB,2TB）要怎么写，然后名称节点会告诉它，把文件分成多少块，每个块放到哪个数据节点上。 FsImage 文件•FsImage文件包含文件系统中所有目录和文件inode的序列化形式。每个inode是一个文件或目录的元数据的内部表示，并包含此类信息：文件的复制等级、修改和访问时间、访问权限、块大小以及组成文件的块。对于目录，则存储修改时间、权限和配额元数据 •FsImage文件没有记录块存储在哪个数据节点。而是由名称节点把这些映射保留在内存中，当数据节点加入HDFS集群时，数据节点会把自己所包含的块列表告知给名称节点，此后会定期执行这种告知操作，以确保名称节点的块映射是最新的。 在名称节点启动的时候，它会将FsImage文件中的内容加载到内存中，之后再执行EditLog文件中的各项操作，使得内存中的元数据和实际的同步，存在内存中的元数据支持客户端的读操作。 •一旦在内存中成功建立文件系统元数据的映射，则创建一个新的FsImage文件和一个空的EditLog文件 •名称节点起来之后，HDFS中的更新操作会重新写到EditLog文件中，因为FsImage文件一般都很大（GB级别的很常见），如果所有的更新操作都往FsImage文件中添加，这样会导致系统运行的十分缓慢，但是，如果往EditLog文件里面写就不会这样，因为EditLog 要小很多。每次执行写操作之后，且在向客户端发送成功代码之前，edits文件都需要同步更新 第二名称节点第二名称节点是HDFS架构中的一个组成部分，它是用来保存名称节点中对HDFS 元数据信息的备份，并减少名称节点重启的时间。SecondaryNameNode一般是单独运行在一台机器上。 SecondaryNameNode的工作情况： （1）SecondaryNameNode会定期和NameNode通信，请求其停止使用EditLog文件，暂时将新的写操作写到一个新的文件edit.new上来，这个操作是瞬间完成，上层写日志的函数完全感觉不到差别； （2）SecondaryNameNode通过HTTP GET方式从NameNode上获取到FsImage和EditLog文件，并下载到本地的相应目录下； （3）SecondaryNameNode将下载下来的FsImage载入到内存，然后一条一条地执行EditLog文件中的各项更新操作，使得内存中的FsImage保持最新；这个过程就是EditLog和FsImage文件合并； （4）SecondaryNameNode执行完（3）操作之后，会通过post方式将新的FsImage文件发送到NameNode节点上； （5）NameNode将从SecondaryNameNode接收到的新的FsImage替换旧的FsImage文件，同时将edit.new替换EditLog文件，通过这个过程EditLog就变小。 数据节点（DataNode）数据节点是分布式文件系统HDFS的工作节点，负责数据的存储和读取，会根据客户端或者是名称节点的调度来进行数据的存储和检索，并且向名称节点定期发送自己所存储的块的列表 •每个数据节点中的数据会被保存在各自节点的本地Linux文件系统中 HDFS存储原理冗余数据保存作为一个分布式文件系统，为了保证系统的容错性和可用性，HDFS采用了多副本方式对数据进行冗余存储，通常一个数据块的多个副本会被分布到不同的数据节点上，数据块1被分别存放到数据节点A和C上，数据块2被存放在数据节点A和B上。这种多副本方式具有以下几个优点：（1） 加快数据传输速度 （2） 容易检查数据错误 （3） 保证数据可靠性 数据存取策略数据存放•第一个副本：放置在上传文件的数据节点；如果是集群外提交，则随机挑选一台磁盘不太满、CPU不太忙的节点 •第二个副本：放置在与第一个副本不同的机架的节点上 •第三个副本：与第一个副本相同机架的其他节点上 •更多副本：随机节点 数据读取•HDFS提供了一个API可以确定一个数据节点所属的机架ID，客户端也可以调用API获取自己所属的机架ID •当客户端读取数据时，从名称节点获得数据块不同副本的存放位置列表，列表中包含了副本所在的数据节点，可以调用API来确定客户端和这些数据节点所属的机架ID，当发现某个数据块副本对应的机架ID和客户端对应的机架ID相同时，就优先选择该副本读取数据，如果没有发现，就随机选择一个副本读取数据 数据错误与恢复HDFS具有较高的容错性，可以兼容廉价的硬件，它把硬件出错看作一种常态，而不是异常，并设计了相应的机制检测数据错误和进行自动恢复，主要包括以下几种情形：名称节点出错、数据节点出错和数据出错。 名称节点出错名称节点保存了所有的元数据信息，其中，最核心的两大数据结构是FsImage和Editlog，如果这两个文件发生损坏，那么整个HDFS实例将失效。因此，HDFS设置了备份机制，把这些核心文件同步复制到备份服务器SecondaryNameNode上。当名称节点出错时，就可以根据备份服务器SecondaryNameNode中的FsImage和Editlog数据进行恢复。 数据节点出错•每个数据节点会定期向名称节点发送“心跳”信息，向名称节点报告自己的状态 •当数据节点发生故障，或者网络发生断网时，名称节点就无法收到来自一些数据节点的心跳信息，这时，这些数据节点就会被标记为“宕机”，节点上面的所有数据都会被标记为“不可读”，名称节点不会再给它们发送任何I/O请求 •这时，有可能出现一种情形，即由于一些数据节点的不可用，会导致一些数据块的副本数量小于冗余因子•名称节点会定期检查这种情况，一旦发现某个数据块的副本数量小于冗余因子，就会启动数据冗余复制，为它生成新的副本 •HDFS和其它分布式文件系统的最大区别就是可以调整冗余数据的位置 数据出错•网络传输和磁盘错误等因素，都会造成数据错误 •客户端在读取到数据后，会采用md5和sha1对数据块进行校验，以确定读取到正确的数据 •在文件被创建时，客户端就会对每一个文件块进行信息摘录，并把这些信息写入到同一个路径的隐藏文件里面 •当客户端读取文件的时候，会先读取该信息文件，然后，利用该信息文件对每个读取的数据块进行校验，如果校验出错，客户端就会请求到另外一个数据节点读取该文件块，并且向名称节点报告这个文件块有错误，名称节点会定期检查并且重新复制这个块 本笔记的来源源自林子雨老师的MOOC课程和课件，地址：https://www.icourse163.org/course/XMU-1002335004]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
</search>
