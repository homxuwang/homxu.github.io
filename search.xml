<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[SpringMVC+Mybatis整合]]></title>
    <url>%2F2018%2F12%2F01%2FSpringMVC-Mybatis%E6%95%B4%E5%90%88%2F</url>
    <content type="text"><![CDATA[控制层采用springmvc、持久层使用mybatis实现。首先参考Spring与Mybatis整合 所需依赖包 配置数据库和逆向工程数据库表:使用逆向工程生成pojo和dao:这里可能和上一篇文章中User类和Items类的代码不同，需要进行修改 配置文件applicationContext.xml:123456789101112131415161718192021222324252627282930313233343536&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:context="http://www.springframework.org/schema/context" xmlns:p="http://www.springframework.org/schema/p" xmlns:aop="http://www.springframework.org/schema/aop" xmlns:tx="http://www.springframework.org/schema/tx" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.0.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.0.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-4.0.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-4.0.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util-4.0.xsd"&gt; &lt;!-- 加载配置文件 --&gt; &lt;context:property-placeholder location="classpath:db.properties"/&gt; &lt;!-- 配置数据库连接池 --&gt; &lt;bean id="dataSource" class="org.apache.commons.dbcp.BasicDataSource" destroy-method="close"&gt; &lt;property name="driverClassName" value="$&#123;jdbc.driver&#125;" /&gt; &lt;property name="url" value="$&#123;jdbc.url&#125;" /&gt; &lt;property name="username" value="$&#123;jdbc.username&#125;" /&gt; &lt;property name="password" value="$&#123;jdbc.password&#125;" /&gt; &lt;property name="maxActive" value="10" /&gt; &lt;property name="maxIdle" value="5" /&gt; &lt;/bean&gt; &lt;!-- 配置SqlSessionFactory --&gt; &lt;bean id="sqlSessionFactoryBean" class="org.mybatis.spring.SqlSessionFactoryBean"&gt; &lt;!-- 配置数据源 --&gt; &lt;property name="dataSource" ref="dataSource" /&gt; &lt;!-- 配置mybatis核心配置文件 --&gt; &lt;property name="configLocation" value="classpath:sqlmapConfig.xml"/&gt; &lt;/bean&gt; &lt;!-- Mapper动态代理开发 扫描包 --&gt; &lt;bean class="org.mybatis.spring.mapper.MapperScannerConfigurer"&gt; &lt;!-- 配置基本包 --&gt; &lt;property name="basePackage" value="my.study.springmvc.mapper"/&gt; &lt;/bean&gt;&lt;/beans&gt; db.properties:1234jdbc.driver=com.mysql.jdbc.Driverjdbc.url=jdbc:mysql://localhost:3306/springmvcjdbc.username=rootjdbc.password=12345 log4j.properties:123456# Global logging configurationlog4j.rootLogger=DEBUG, stdout# Console output...log4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=%5p [%t] - %m%n sqlmapConfit.xml:1234567891011121314&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;!DOCTYPE configurationPUBLIC "-//mybatis.org//DTD Config 3.0//EN""http://mybatis.org/dtd/mybatis-3-config.dtd"&gt;&lt;configuration&gt; &lt;!-- 设置别名 --&gt; &lt;typeAliases&gt; &lt;!-- 指定扫描包，会把保内所有的类都设置别名， 别名的名就是类名，大小写不敏感 --&gt; &lt;package name="my.study.springmvc.mapper"/&gt; &lt;/typeAliases&gt;&lt;/configuration&gt; 在web.xml中配置监听器，去读取配置：123456789&lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:applicationContext.xml&lt;/param-value&gt; &lt;/context-param&gt; &lt;!-- Spring监听器 --&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt; 配置至此，如果开启tomcat,那么就可以连接数据库了 配置事务在applicationContext.xml中配置注解事务,并开启注解:123456&lt;!-- 配置事务 --&gt;&lt;bean class="org.springframework.jdbc.datasource.DataSourceTransactionManager"&gt; &lt;property name="dataSource" ref="dataSource"/&gt;&lt;/bean&gt;&lt;!-- 开启注解 --&gt; &lt;tx:annotation-driven transaction-manager="transactionManager"/&gt; 从mysql中读取信息并显示书写service层代码:ItemService.java:123456789package my.study.springmvc.service;import java.util.List;import my.study.springmvc.pojo.Items;public interface ItemService &#123; public List&lt;Items&gt; selectItemsList();&#125; ItemServiceImpl.java:1234567891011121314151617181920212223242526package my.study.springmvc.service;import java.util.List;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import my.study.springmvc.dao.ItemsMapper;import my.study.springmvc.pojo.Items;/** * 查询商品信息 * @author homxu * */@Servicepublic class ItemServiceImpl implements ItemService &#123; @Autowired private ItemsMapper itemsmapper; public List&lt;Items&gt; selectItemsList()&#123; return itemsmapper.selectByExample(null); &#125;&#125; 将service代码注入到controller： 12345678910111213141516171819202122232425262728package my.study.springmvc.controller;import java.util.List;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.servlet.ModelAndView;import my.study.springmvc.pojo.Items;import my.study.springmvc.service.ItemService;@Controllerpublic class ItemController &#123; @Autowired private ItemService itemservice; @RequestMapping(value = "/item/itemlist.action") public ModelAndView itemList()&#123; //从mysql中查询 List&lt;Items&gt; items = itemservice.selectItemsList(); ModelAndView mav = new ModelAndView(); mav.addObject("itemList",items); mav.setViewName("itemList"); return mav; &#125; &#125; 其中@Autowired 注释，它可以对类成员变量、方法及构造函数进行标注，完成自动装配的工作。 通过 @Autowired的使用来消除 set ，get方法。关于此注释：https://www.cnblogs.com/caoyc/p/5626365.html 附录sql12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061/*Navicat MySQL Data TransferSource Server : localhost_3306Source Server Version : 50611Source Host : localhost:3306Source Database : springmvcTarget Server Type : MYSQLTarget Server Version : 50611File Encoding : 65001Date: 2016-05-09 19:45:13*/SET FOREIGN_KEY_CHECKS=0;-- ------------------------------ Table structure for items-- ----------------------------DROP TABLE IF EXISTS `items`;CREATE TABLE `items` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(32) NOT NULL COMMENT '商品名称', `price` float(10,1) NOT NULL COMMENT '商品定价', `detail` text COMMENT '商品描述', `pic` varchar(64) DEFAULT NULL COMMENT '商品图片', `createtime` datetime NOT NULL COMMENT '生产日期', PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8;-- ------------------------------ Records of items-- ----------------------------INSERT INTO `items` VALUES ('1', '台式机', '13000.0', '该电脑质量非常好！！！！', null, '2016-02-03 13:22:53');INSERT INTO `items` VALUES ('2', '笔记本', '16000.0', '笔记本性能好，质量好！！！！！', null, '2015-02-09 13:22:57');INSERT INTO `items` VALUES ('3', '背包', '200.0', '名牌背包，容量大质量好！！！！', null, '2015-02-06 13:23:02');-- ------------------------------ Table structure for user-- ----------------------------DROP TABLE IF EXISTS `user`;CREATE TABLE `user` ( `id` int(11) NOT NULL AUTO_INCREMENT, `username` varchar(32) NOT NULL COMMENT '用户名称', `birthday` date DEFAULT NULL COMMENT '生日', `sex` char(1) DEFAULT NULL COMMENT '性别', `address` varchar(256) DEFAULT NULL COMMENT '地址', PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=27 DEFAULT CHARSET=utf8;-- ------------------------------ Records of user-- ----------------------------INSERT INTO `user` VALUES ('1', '王五', null, '2', null);INSERT INTO `user` VALUES ('10', '张三', '2014-07-10', '1', '北京市');INSERT INTO `user` VALUES ('16', '张小明', null, '1', '河南郑州');INSERT INTO `user` VALUES ('22', '陈小明', null, '1', '河南郑州');INSERT INTO `user` VALUES ('24', '张三丰', null, '1', '河南郑州');INSERT INTO `user` VALUES ('25', '陈小明', null, '1', '河南郑州');INSERT INTO `user` VALUES ('26', '王五', null, null, null); dao逆向工程生成的dao：ItemsMapper.java:12345678910111213141516171819202122232425262728293031323334353637package my.study.springmvc.dao;import my.study.springmvc.pojo.Items;import my.study.springmvc.pojo.ItemsExample;import java.util.List;import org.apache.ibatis.annotations.Param;public interface ItemsMapper &#123; int countByExample(ItemsExample example); int deleteByExample(ItemsExample example); int deleteByPrimaryKey(Integer id); int insert(Items record); int insertSelective(Items record); List&lt;Items&gt; selectByExampleWithBLOBs(ItemsExample example); List&lt;Items&gt; selectByExample(ItemsExample example); Items selectByPrimaryKey(Integer id); int updateByExampleSelective(@Param("record") Items record, @Param("example") ItemsExample example); int updateByExampleWithBLOBs(@Param("record") Items record, @Param("example") ItemsExample example); int updateByExample(@Param("record") Items record, @Param("example") ItemsExample example); int updateByPrimaryKeySelective(Items record); int updateByPrimaryKeyWithBLOBs(Items record); int updateByPrimaryKey(Items record);&#125; UserMapper.java:123456789101112131415161718192021222324252627282930package my.study.springmvc.dao;import my.study.springmvc.pojo.User;import my.study.springmvc.pojo.UserExample;import java.util.List;import org.apache.ibatis.annotations.Param;public interface UserMapper &#123; int countByExample(UserExample example); int deleteByExample(UserExample example); int deleteByPrimaryKey(Integer id); int insert(User record); int insertSelective(User record); List&lt;User&gt; selectByExample(UserExample example); User selectByPrimaryKey(Integer id); int updateByExampleSelective(@Param("record") User record, @Param("example") UserExample example); int updateByExample(@Param("record") User record, @Param("example") UserExample example); int updateByPrimaryKeySelective(User record); int updateByPrimaryKey(User record);&#125; ItemsMapper.xml:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;!DOCTYPE mapper PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN" "http://mybatis.org/dtd/mybatis-3-mapper.dtd" &gt;&lt;mapper namespace="my.study.springmvc.dao.ItemsMapper" &gt; &lt;resultMap id="BaseResultMap" type="my.study.springmvc.pojo.Items" &gt; &lt;id column="id" property="id" jdbcType="INTEGER" /&gt; &lt;result column="name" property="name" jdbcType="VARCHAR" /&gt; &lt;result column="price" property="price" jdbcType="REAL" /&gt; &lt;result column="pic" property="pic" jdbcType="VARCHAR" /&gt; &lt;result column="createtime" property="createtime" jdbcType="TIMESTAMP" /&gt; &lt;/resultMap&gt; &lt;resultMap id="ResultMapWithBLOBs" type="my.study.springmvc.pojo.Items" extends="BaseResultMap" &gt; &lt;result column="detail" property="detail" jdbcType="LONGVARCHAR" /&gt; &lt;/resultMap&gt; &lt;sql id="Example_Where_Clause" &gt; &lt;where &gt; &lt;foreach collection="oredCriteria" item="criteria" separator="or" &gt; &lt;if test="criteria.valid" &gt; &lt;trim prefix="(" suffix=")" prefixOverrides="and" &gt; &lt;foreach collection="criteria.criteria" item="criterion" &gt; &lt;choose &gt; &lt;when test="criterion.noValue" &gt; and $&#123;criterion.condition&#125; &lt;/when&gt; &lt;when test="criterion.singleValue" &gt; and $&#123;criterion.condition&#125; #&#123;criterion.value&#125; &lt;/when&gt; &lt;when test="criterion.betweenValue" &gt; and $&#123;criterion.condition&#125; #&#123;criterion.value&#125; and #&#123;criterion.secondValue&#125; &lt;/when&gt; &lt;when test="criterion.listValue" &gt; and $&#123;criterion.condition&#125; &lt;foreach collection="criterion.value" item="listItem" open="(" close=")" separator="," &gt; #&#123;listItem&#125; &lt;/foreach&gt; &lt;/when&gt; &lt;/choose&gt; &lt;/foreach&gt; &lt;/trim&gt; &lt;/if&gt; &lt;/foreach&gt; &lt;/where&gt; &lt;/sql&gt; &lt;sql id="Update_By_Example_Where_Clause" &gt; &lt;where &gt; &lt;foreach collection="example.oredCriteria" item="criteria" separator="or" &gt; &lt;if test="criteria.valid" &gt; &lt;trim prefix="(" suffix=")" prefixOverrides="and" &gt; &lt;foreach collection="criteria.criteria" item="criterion" &gt; &lt;choose &gt; &lt;when test="criterion.noValue" &gt; and $&#123;criterion.condition&#125; &lt;/when&gt; &lt;when test="criterion.singleValue" &gt; and $&#123;criterion.condition&#125; #&#123;criterion.value&#125; &lt;/when&gt; &lt;when test="criterion.betweenValue" &gt; and $&#123;criterion.condition&#125; #&#123;criterion.value&#125; and #&#123;criterion.secondValue&#125; &lt;/when&gt; &lt;when test="criterion.listValue" &gt; and $&#123;criterion.condition&#125; &lt;foreach collection="criterion.value" item="listItem" open="(" close=")" separator="," &gt; #&#123;listItem&#125; &lt;/foreach&gt; &lt;/when&gt; &lt;/choose&gt; &lt;/foreach&gt; &lt;/trim&gt; &lt;/if&gt; &lt;/foreach&gt; &lt;/where&gt; &lt;/sql&gt; &lt;sql id="Base_Column_List" &gt; id, name, price, pic, createtime &lt;/sql&gt; &lt;sql id="Blob_Column_List" &gt; detail &lt;/sql&gt; &lt;select id="selectByExampleWithBLOBs" resultMap="ResultMapWithBLOBs" parameterType="my.study.springmvc.pojo.ItemsExample" &gt; select &lt;if test="distinct" &gt; distinct &lt;/if&gt; &lt;include refid="Base_Column_List" /&gt; , &lt;include refid="Blob_Column_List" /&gt; from items &lt;if test="_parameter != null" &gt; &lt;include refid="Example_Where_Clause" /&gt; &lt;/if&gt; &lt;if test="orderByClause != null" &gt; order by $&#123;orderByClause&#125; &lt;/if&gt; &lt;/select&gt; &lt;select id="selectByExample" resultMap="BaseResultMap" parameterType="my.study.springmvc.pojo.ItemsExample" &gt; select &lt;if test="distinct" &gt; distinct &lt;/if&gt; &lt;include refid="Base_Column_List" /&gt; from items &lt;if test="_parameter != null" &gt; &lt;include refid="Example_Where_Clause" /&gt; &lt;/if&gt; &lt;if test="orderByClause != null" &gt; order by $&#123;orderByClause&#125; &lt;/if&gt; &lt;/select&gt; &lt;select id="selectByPrimaryKey" resultMap="ResultMapWithBLOBs" parameterType="java.lang.Integer" &gt; select &lt;include refid="Base_Column_List" /&gt; , &lt;include refid="Blob_Column_List" /&gt; from items where id = #&#123;id,jdbcType=INTEGER&#125; &lt;/select&gt; &lt;delete id="deleteByPrimaryKey" parameterType="java.lang.Integer" &gt; delete from items where id = #&#123;id,jdbcType=INTEGER&#125; &lt;/delete&gt; &lt;delete id="deleteByExample" parameterType="my.study.springmvc.pojo.ItemsExample" &gt; delete from items &lt;if test="_parameter != null" &gt; &lt;include refid="Example_Where_Clause" /&gt; &lt;/if&gt; &lt;/delete&gt; &lt;insert id="insert" parameterType="my.study.springmvc.pojo.Items" &gt; insert into items (id, name, price, pic, createtime, detail ) values (#&#123;id,jdbcType=INTEGER&#125;, #&#123;name,jdbcType=VARCHAR&#125;, #&#123;price,jdbcType=REAL&#125;, #&#123;pic,jdbcType=VARCHAR&#125;, #&#123;createtime,jdbcType=TIMESTAMP&#125;, #&#123;detail,jdbcType=LONGVARCHAR&#125; ) &lt;/insert&gt; &lt;insert id="insertSelective" parameterType="my.study.springmvc.pojo.Items" &gt; insert into items &lt;trim prefix="(" suffix=")" suffixOverrides="," &gt; &lt;if test="id != null" &gt; id, &lt;/if&gt; &lt;if test="name != null" &gt; name, &lt;/if&gt; &lt;if test="price != null" &gt; price, &lt;/if&gt; &lt;if test="pic != null" &gt; pic, &lt;/if&gt; &lt;if test="createtime != null" &gt; createtime, &lt;/if&gt; &lt;if test="detail != null" &gt; detail, &lt;/if&gt; &lt;/trim&gt; &lt;trim prefix="values (" suffix=")" suffixOverrides="," &gt; &lt;if test="id != null" &gt; #&#123;id,jdbcType=INTEGER&#125;, &lt;/if&gt; &lt;if test="name != null" &gt; #&#123;name,jdbcType=VARCHAR&#125;, &lt;/if&gt; &lt;if test="price != null" &gt; #&#123;price,jdbcType=REAL&#125;, &lt;/if&gt; &lt;if test="pic != null" &gt; #&#123;pic,jdbcType=VARCHAR&#125;, &lt;/if&gt; &lt;if test="createtime != null" &gt; #&#123;createtime,jdbcType=TIMESTAMP&#125;, &lt;/if&gt; &lt;if test="detail != null" &gt; #&#123;detail,jdbcType=LONGVARCHAR&#125;, &lt;/if&gt; &lt;/trim&gt; &lt;/insert&gt; &lt;select id="countByExample" parameterType="my.study.springmvc.pojo.ItemsExample" resultType="java.lang.Integer" &gt; select count(*) from items &lt;if test="_parameter != null" &gt; &lt;include refid="Example_Where_Clause" /&gt; &lt;/if&gt; &lt;/select&gt; &lt;update id="updateByExampleSelective" parameterType="map" &gt; update items &lt;set &gt; &lt;if test="record.id != null" &gt; id = #&#123;record.id,jdbcType=INTEGER&#125;, &lt;/if&gt; &lt;if test="record.name != null" &gt; name = #&#123;record.name,jdbcType=VARCHAR&#125;, &lt;/if&gt; &lt;if test="record.price != null" &gt; price = #&#123;record.price,jdbcType=REAL&#125;, &lt;/if&gt; &lt;if test="record.pic != null" &gt; pic = #&#123;record.pic,jdbcType=VARCHAR&#125;, &lt;/if&gt; &lt;if test="record.createtime != null" &gt; createtime = #&#123;record.createtime,jdbcType=TIMESTAMP&#125;, &lt;/if&gt; &lt;if test="record.detail != null" &gt; detail = #&#123;record.detail,jdbcType=LONGVARCHAR&#125;, &lt;/if&gt; &lt;/set&gt; &lt;if test="_parameter != null" &gt; &lt;include refid="Update_By_Example_Where_Clause" /&gt; &lt;/if&gt; &lt;/update&gt; &lt;update id="updateByExampleWithBLOBs" parameterType="map" &gt; update items set id = #&#123;record.id,jdbcType=INTEGER&#125;, name = #&#123;record.name,jdbcType=VARCHAR&#125;, price = #&#123;record.price,jdbcType=REAL&#125;, pic = #&#123;record.pic,jdbcType=VARCHAR&#125;, createtime = #&#123;record.createtime,jdbcType=TIMESTAMP&#125;, detail = #&#123;record.detail,jdbcType=LONGVARCHAR&#125; &lt;if test="_parameter != null" &gt; &lt;include refid="Update_By_Example_Where_Clause" /&gt; &lt;/if&gt; &lt;/update&gt; &lt;update id="updateByExample" parameterType="map" &gt; update items set id = #&#123;record.id,jdbcType=INTEGER&#125;, name = #&#123;record.name,jdbcType=VARCHAR&#125;, price = #&#123;record.price,jdbcType=REAL&#125;, pic = #&#123;record.pic,jdbcType=VARCHAR&#125;, createtime = #&#123;record.createtime,jdbcType=TIMESTAMP&#125; &lt;if test="_parameter != null" &gt; &lt;include refid="Update_By_Example_Where_Clause" /&gt; &lt;/if&gt; &lt;/update&gt; &lt;update id="updateByPrimaryKeySelective" parameterType="my.study.springmvc.pojo.Items" &gt; update items &lt;set &gt; &lt;if test="name != null" &gt; name = #&#123;name,jdbcType=VARCHAR&#125;, &lt;/if&gt; &lt;if test="price != null" &gt; price = #&#123;price,jdbcType=REAL&#125;, &lt;/if&gt; &lt;if test="pic != null" &gt; pic = #&#123;pic,jdbcType=VARCHAR&#125;, &lt;/if&gt; &lt;if test="createtime != null" &gt; createtime = #&#123;createtime,jdbcType=TIMESTAMP&#125;, &lt;/if&gt; &lt;if test="detail != null" &gt; detail = #&#123;detail,jdbcType=LONGVARCHAR&#125;, &lt;/if&gt; &lt;/set&gt; where id = #&#123;id,jdbcType=INTEGER&#125; &lt;/update&gt; &lt;update id="updateByPrimaryKeyWithBLOBs" parameterType="my.study.springmvc.pojo.Items" &gt; update items set name = #&#123;name,jdbcType=VARCHAR&#125;, price = #&#123;price,jdbcType=REAL&#125;, pic = #&#123;pic,jdbcType=VARCHAR&#125;, createtime = #&#123;createtime,jdbcType=TIMESTAMP&#125;, detail = #&#123;detail,jdbcType=LONGVARCHAR&#125; where id = #&#123;id,jdbcType=INTEGER&#125; &lt;/update&gt; &lt;update id="updateByPrimaryKey" parameterType="my.study.springmvc.pojo.Items" &gt; update items set name = #&#123;name,jdbcType=VARCHAR&#125;, price = #&#123;price,jdbcType=REAL&#125;, pic = #&#123;pic,jdbcType=VARCHAR&#125;, createtime = #&#123;createtime,jdbcType=TIMESTAMP&#125; where id = #&#123;id,jdbcType=INTEGER&#125; &lt;/update&gt;&lt;/mapper&gt; UserMapper.xml:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;!DOCTYPE mapper PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN" "http://mybatis.org/dtd/mybatis-3-mapper.dtd" &gt;&lt;mapper namespace="my.study.springmvc.dao.UserMapper" &gt; &lt;resultMap id="BaseResultMap" type="my.study.springmvc.pojo.User" &gt; &lt;id column="id" property="id" jdbcType="INTEGER" /&gt; &lt;result column="username" property="username" jdbcType="VARCHAR" /&gt; &lt;result column="birthday" property="birthday" jdbcType="DATE" /&gt; &lt;result column="sex" property="sex" jdbcType="CHAR" /&gt; &lt;result column="address" property="address" jdbcType="VARCHAR" /&gt; &lt;/resultMap&gt; &lt;sql id="Example_Where_Clause" &gt; &lt;where &gt; &lt;foreach collection="oredCriteria" item="criteria" separator="or" &gt; &lt;if test="criteria.valid" &gt; &lt;trim prefix="(" suffix=")" prefixOverrides="and" &gt; &lt;foreach collection="criteria.criteria" item="criterion" &gt; &lt;choose &gt; &lt;when test="criterion.noValue" &gt; and $&#123;criterion.condition&#125; &lt;/when&gt; &lt;when test="criterion.singleValue" &gt; and $&#123;criterion.condition&#125; #&#123;criterion.value&#125; &lt;/when&gt; &lt;when test="criterion.betweenValue" &gt; and $&#123;criterion.condition&#125; #&#123;criterion.value&#125; and #&#123;criterion.secondValue&#125; &lt;/when&gt; &lt;when test="criterion.listValue" &gt; and $&#123;criterion.condition&#125; &lt;foreach collection="criterion.value" item="listItem" open="(" close=")" separator="," &gt; #&#123;listItem&#125; &lt;/foreach&gt; &lt;/when&gt; &lt;/choose&gt; &lt;/foreach&gt; &lt;/trim&gt; &lt;/if&gt; &lt;/foreach&gt; &lt;/where&gt; &lt;/sql&gt; &lt;sql id="Update_By_Example_Where_Clause" &gt; &lt;where &gt; &lt;foreach collection="example.oredCriteria" item="criteria" separator="or" &gt; &lt;if test="criteria.valid" &gt; &lt;trim prefix="(" suffix=")" prefixOverrides="and" &gt; &lt;foreach collection="criteria.criteria" item="criterion" &gt; &lt;choose &gt; &lt;when test="criterion.noValue" &gt; and $&#123;criterion.condition&#125; &lt;/when&gt; &lt;when test="criterion.singleValue" &gt; and $&#123;criterion.condition&#125; #&#123;criterion.value&#125; &lt;/when&gt; &lt;when test="criterion.betweenValue" &gt; and $&#123;criterion.condition&#125; #&#123;criterion.value&#125; and #&#123;criterion.secondValue&#125; &lt;/when&gt; &lt;when test="criterion.listValue" &gt; and $&#123;criterion.condition&#125; &lt;foreach collection="criterion.value" item="listItem" open="(" close=")" separator="," &gt; #&#123;listItem&#125; &lt;/foreach&gt; &lt;/when&gt; &lt;/choose&gt; &lt;/foreach&gt; &lt;/trim&gt; &lt;/if&gt; &lt;/foreach&gt; &lt;/where&gt; &lt;/sql&gt; &lt;sql id="Base_Column_List" &gt; id, username, birthday, sex, address &lt;/sql&gt; &lt;select id="selectByExample" resultMap="BaseResultMap" parameterType="my.study.springmvc.pojo.UserExample" &gt; select &lt;if test="distinct" &gt; distinct &lt;/if&gt; &lt;include refid="Base_Column_List" /&gt; from user &lt;if test="_parameter != null" &gt; &lt;include refid="Example_Where_Clause" /&gt; &lt;/if&gt; &lt;if test="orderByClause != null" &gt; order by $&#123;orderByClause&#125; &lt;/if&gt; &lt;/select&gt; &lt;select id="selectByPrimaryKey" resultMap="BaseResultMap" parameterType="java.lang.Integer" &gt; select &lt;include refid="Base_Column_List" /&gt; from user where id = #&#123;id,jdbcType=INTEGER&#125; &lt;/select&gt; &lt;delete id="deleteByPrimaryKey" parameterType="java.lang.Integer" &gt; delete from user where id = #&#123;id,jdbcType=INTEGER&#125; &lt;/delete&gt; &lt;delete id="deleteByExample" parameterType="my.study.springmvc.pojo.UserExample" &gt; delete from user &lt;if test="_parameter != null" &gt; &lt;include refid="Example_Where_Clause" /&gt; &lt;/if&gt; &lt;/delete&gt; &lt;insert id="insert" parameterType="my.study.springmvc.pojo.User" &gt; insert into user (id, username, birthday, sex, address) values (#&#123;id,jdbcType=INTEGER&#125;, #&#123;username,jdbcType=VARCHAR&#125;, #&#123;birthday,jdbcType=DATE&#125;, #&#123;sex,jdbcType=CHAR&#125;, #&#123;address,jdbcType=VARCHAR&#125;) &lt;/insert&gt; &lt;insert id="insertSelective" parameterType="my.study.springmvc.pojo.User" &gt; insert into user &lt;trim prefix="(" suffix=")" suffixOverrides="," &gt; &lt;if test="id != null" &gt; id, &lt;/if&gt; &lt;if test="username != null" &gt; username, &lt;/if&gt; &lt;if test="birthday != null" &gt; birthday, &lt;/if&gt; &lt;if test="sex != null" &gt; sex, &lt;/if&gt; &lt;if test="address != null" &gt; address, &lt;/if&gt; &lt;/trim&gt; &lt;trim prefix="values (" suffix=")" suffixOverrides="," &gt; &lt;if test="id != null" &gt; #&#123;id,jdbcType=INTEGER&#125;, &lt;/if&gt; &lt;if test="username != null" &gt; #&#123;username,jdbcType=VARCHAR&#125;, &lt;/if&gt; &lt;if test="birthday != null" &gt; #&#123;birthday,jdbcType=DATE&#125;, &lt;/if&gt; &lt;if test="sex != null" &gt; #&#123;sex,jdbcType=CHAR&#125;, &lt;/if&gt; &lt;if test="address != null" &gt; #&#123;address,jdbcType=VARCHAR&#125;, &lt;/if&gt; &lt;/trim&gt; &lt;/insert&gt; &lt;select id="countByExample" parameterType="my.study.springmvc.pojo.UserExample" resultType="java.lang.Integer" &gt; select count(*) from user &lt;if test="_parameter != null" &gt; &lt;include refid="Example_Where_Clause" /&gt; &lt;/if&gt; &lt;/select&gt; &lt;update id="updateByExampleSelective" parameterType="map" &gt; update user &lt;set &gt; &lt;if test="record.id != null" &gt; id = #&#123;record.id,jdbcType=INTEGER&#125;, &lt;/if&gt; &lt;if test="record.username != null" &gt; username = #&#123;record.username,jdbcType=VARCHAR&#125;, &lt;/if&gt; &lt;if test="record.birthday != null" &gt; birthday = #&#123;record.birthday,jdbcType=DATE&#125;, &lt;/if&gt; &lt;if test="record.sex != null" &gt; sex = #&#123;record.sex,jdbcType=CHAR&#125;, &lt;/if&gt; &lt;if test="record.address != null" &gt; address = #&#123;record.address,jdbcType=VARCHAR&#125;, &lt;/if&gt; &lt;/set&gt; &lt;if test="_parameter != null" &gt; &lt;include refid="Update_By_Example_Where_Clause" /&gt; &lt;/if&gt; &lt;/update&gt; &lt;update id="updateByExample" parameterType="map" &gt; update user set id = #&#123;record.id,jdbcType=INTEGER&#125;, username = #&#123;record.username,jdbcType=VARCHAR&#125;, birthday = #&#123;record.birthday,jdbcType=DATE&#125;, sex = #&#123;record.sex,jdbcType=CHAR&#125;, address = #&#123;record.address,jdbcType=VARCHAR&#125; &lt;if test="_parameter != null" &gt; &lt;include refid="Update_By_Example_Where_Clause" /&gt; &lt;/if&gt; &lt;/update&gt; &lt;update id="updateByPrimaryKeySelective" parameterType="my.study.springmvc.pojo.User" &gt; update user &lt;set &gt; &lt;if test="username != null" &gt; username = #&#123;username,jdbcType=VARCHAR&#125;, &lt;/if&gt; &lt;if test="birthday != null" &gt; birthday = #&#123;birthday,jdbcType=DATE&#125;, &lt;/if&gt; &lt;if test="sex != null" &gt; sex = #&#123;sex,jdbcType=CHAR&#125;, &lt;/if&gt; &lt;if test="address != null" &gt; address = #&#123;address,jdbcType=VARCHAR&#125;, &lt;/if&gt; &lt;/set&gt; where id = #&#123;id,jdbcType=INTEGER&#125; &lt;/update&gt; &lt;update id="updateByPrimaryKey" parameterType="my.study.springmvc.pojo.User" &gt; update user set username = #&#123;username,jdbcType=VARCHAR&#125;, birthday = #&#123;birthday,jdbcType=DATE&#125;, sex = #&#123;sex,jdbcType=CHAR&#125;, address = #&#123;address,jdbcType=VARCHAR&#125; where id = #&#123;id,jdbcType=INTEGER&#125; &lt;/update&gt;&lt;/mapper&gt;]]></content>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC学习笔记-一]]></title>
    <url>%2F2018%2F11%2F30%2FSpringMVC%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%B8%80%2F</url>
    <content type="text"><![CDATA[Spring web mvc和Struts2都属于表现层的框架,它是Spring框架的一部分,我们可以从Spring的整体结构中看得出来,如下图： SpringMVC简单的处理流程图:前端控制器可以说是SpringMVC的心脏，是核心部分。 创建入门程序创建动态Web工程，导入相关jar包: 配置前端控制器在web.xml中配置前端控制器:123456789101112131415&lt;!-- 前端控制器 --&gt;&lt;servlet&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;!-- 默认找 /WEB-INF/[servlet的名称]-servlet.xml --&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:springmvc.xml&lt;/param-value&gt; &lt;/init-param&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;url-pattern&gt;*.action&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 注意，在&lt;url-pattern&gt;配置时：/*拦截所有，包括jsp和各种静态资源*.action或者*.do只拦截以action和do结尾的请求/拦截所有，但是不包括jsp 创建springmvc.xml配置文件12345678910111213&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:p="http://www.springframework.org/schema/p" xmlns:context="http://www.springframework.org/schema/context" xmlns:mvc="http://www.springframework.org/schema/mvc" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.0.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc-4.0.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.0.xsd"&gt; &lt;!-- 扫描包下有@Controller @Service 注解的代码 --&gt; &lt;context:component-scan base-package="my.study.springmvc.controller"&gt; &lt;/context:component-scan&gt;&lt;/beans&gt; 创建Controller，书写入门程序1234567891011121314151617181920212223242526272829package my.study.springmvc.controller;import java.util.ArrayList;import java.util.Date;import java.util.List;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.servlet.ModelAndView;import my.study.springmvc.pojo.Items;@Controllerpublic class ItemController &#123; @RequestMapping(value = "/item/itemlist.action") public ModelAndView itemList()&#123; List&lt;Items&gt; list = new ArrayList&lt;Items&gt;(); list.add(new my.study.springmvc.pojo.Items(1, "1", 2399f, new Date(), "a")); list.add(new Items(2, "28", 2399f, new Date(), "d")); list.add(new Items(3, "38", 2399f, new Date(), "s")); list.add(new Items(4, "48", 2399f, new Date(), "f")); list.add(new Items(5, "58", 2399f, new Date(), "x")); ModelAndView mav = new ModelAndView(); mav.addObject("itemList", list); mav.setViewName("/WEB-INF/jsp/itemList.jsp"); return mav; &#125; &#125; 在类上添加@Controller注解，把Controller交由Spring管理@RequestMapping(value = &quot;请求地址&quot;)该方法请求的地址,其中.action可以加也可以不加。ModelAndView从字面意思就可以理解，是数据和视图 在/WEB-INF/jsp/路径下创建jsp，用于显示:1234567891011121314151617181920212223242526272829303132333435363738394041&lt;%@ page language="java" contentType="text/html; charset=UTF-8" pageEncoding="UTF-8"%&gt;&lt;%@ taglib uri="http://java.sun.com/jsp/jstl/core" prefix="c" %&gt;&lt;%@ taglib uri="http://java.sun.com/jsp/jstl/fmt" prefix="fmt"%&gt;&lt;!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd"&gt;&lt;html&gt;&lt;head&gt;&lt;meta http-equiv="Content-Type" content="text/html; charset=UTF-8"&gt;&lt;title&gt;查询商品列表&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;form action="$&#123;pageContext.request.contextPath &#125;/item/queryitem.action" method="post"&gt;查询条件：&lt;table width="100%" border=1&gt;&lt;tr&gt;&lt;td&gt;&lt;input type="submit" value="查询"/&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;商品列表：&lt;table width="100%" border=1&gt;&lt;tr&gt; &lt;td&gt;商品名称&lt;/td&gt; &lt;td&gt;商品价格&lt;/td&gt; &lt;td&gt;生产日期&lt;/td&gt; &lt;td&gt;商品描述&lt;/td&gt; &lt;td&gt;操作&lt;/td&gt;&lt;/tr&gt;&lt;c:forEach items="$&#123;itemList &#125;" var="item"&gt;&lt;tr&gt; &lt;td&gt;$&#123;item.name &#125;&lt;/td&gt; &lt;td&gt;$&#123;item.price &#125;&lt;/td&gt; &lt;td&gt;&lt;fmt:formatDate value="$&#123;item.createtime&#125;" pattern="yyyy-MM-dd HH:mm:ss"/&gt;&lt;/td&gt; &lt;td&gt;$&#123;item.detail &#125;&lt;/td&gt; &lt;td&gt;&lt;a href="$&#123;pageContext.request.contextPath &#125;/itemEdit.action?id=$&#123;item.id&#125;"&gt;修改&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/c:forEach&gt;&lt;/table&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 创建pojo：12345678910111213141516171819202122232425262728package my.study.springmvc.pojo;import java.util.Date;public class Items &#123; private Integer id; private String name; private Float price; private String pic; private Date createtime; private String detail; public Items(Integer id, String name, Float price, Date createtime, String detail) &#123; super(); this.id = id; this.name = name; this.price = price; this.createtime = createtime; this.detail = detail; &#125; //get/set...&#125; 启动服务器访问: SpringMVC架构架构流程：1、 用户发送请求至前端控制器DispatcherServlet2、 DispatcherServlet收到请求调用HandlerMapping处理器映射器。3、 处理器映射器根据请求url找到具体的处理器，生成处理器对象及处理器拦截器(如果有则生成)一并返回给DispatcherServlet。返回的是包名+类名+方法名4、 DispatcherServlet通过HandlerAdapter处理器适配器调用处理器5、 执行处理器(Controller，也叫后端控制器)。6、 Controller执行完成返回ModelAndView7、 HandlerAdapter将controller执行结果ModelAndView返回给DispatcherServlet8、 DispatcherServlet将ModelAndView传给ViewReslover视图解析器9、 ViewReslover解析后返回具体View10、 DispatcherServlet对View进行渲染视图（即将模型数据填充至视图中）。11、 DispatcherServlet响应用户 组件说明:以下组件通常使用框架提供实现： DispatcherServlet：前端控制器用户请求到达前端控制器，它就相当于mvc模式中的c，dispatcherServlet是整个流程控制的中心，由它调用其它组件处理用户的请求，dispatcherServlet的存在降低了组件之间的耦合性。 HandlerMapping：处理器映射器HandlerMapping负责根据用户请求url找到Handler即处理器，springmvc提供了不同的映射器实现不同的映射方式，例如：配置文件方式，实现接口方式，注解方式等。 Handler：处理器Handler 是继DispatcherServlet前端控制器的后端控制器，在DispatcherServlet的控制下Handler对具体的用户请求进行处理。由于Handler涉及到具体的用户业务请求，所以一般情况需要程序员根据业务需求开发Handler。 HandlAdapter：处理器适配器通过HandlerAdapter对处理器进行执行，这是适配器模式的应用，通过扩展适配器可以对更多类型的处理器进行执行。 ViewResolver：视图解析器View Resolver负责将处理结果生成View视图，View Resolver首先根据逻辑视图名解析成物理视图名即具体的页面地址，再生成View视图对象，最后对View进行渲染将处理结果通过页面展示给用户。 View：视图springmvc框架提供了很多的View视图类型的支持，包括：jstlView、freemarkerView、pdfView等。我们最常用的视图就是jsp。一般情况下需要通过页面标签或页面模版技术将模型数据通过页面展示给用户，需要由程序员根据业务需求开发具体的页面。 在springmvc的各个组件中，处理器映射器、处理器适配器、视图解析器称为springmvc的三大组件。需要用户开发的组件有handler、view 默认加载的组件我们没有做任何配置，就可以使用这些组件因为框架已经默认加载这些组件了，配置文件位置如下图： 123456789101112131415161718192021222324# Default implementation classes for DispatcherServlet&apos;s strategy interfaces.# Used as fallback when no matching beans are found in the DispatcherServlet context.# Not meant to be customized by application developers.org.springframework.web.servlet.LocaleResolver=org.springframework.web.servlet.i18n.AcceptHeaderLocaleResolverorg.springframework.web.servlet.ThemeResolver=org.springframework.web.servlet.theme.FixedThemeResolverorg.springframework.web.servlet.HandlerMapping=org.springframework.web.servlet.handler.BeanNameUrlHandlerMapping,\ org.springframework.web.servlet.mvc.annotation.DefaultAnnotationHandlerMappingorg.springframework.web.servlet.HandlerAdapter=org.springframework.web.servlet.mvc.HttpRequestHandlerAdapter,\ org.springframework.web.servlet.mvc.SimpleControllerHandlerAdapter,\ org.springframework.web.servlet.mvc.annotation.AnnotationMethodHandlerAdapterorg.springframework.web.servlet.HandlerExceptionResolver=org.springframework.web.servlet.mvc.annotation.AnnotationMethodHandlerExceptionResolver,\ org.springframework.web.servlet.mvc.annotation.ResponseStatusExceptionResolver,\ org.springframework.web.servlet.mvc.support.DefaultHandlerExceptionResolverorg.springframework.web.servlet.RequestToViewNameTranslator=org.springframework.web.servlet.view.DefaultRequestToViewNameTranslatororg.springframework.web.servlet.ViewResolver=org.springframework.web.servlet.view.InternalResourceViewResolverorg.springframework.web.servlet.FlashMapManager=org.springframework.web.servlet.support.SessionFlashMapManager 组件扫描器使用组件扫描器省去在spring容器配置每个Controller类的繁琐。使用&lt;context:component-scan&gt;自动扫描标记@Controller的控制器类，在springmvc.xml配置文件中配置如下：12345&lt;!-- 配置controller扫描包，多个包之间用,分隔 扫描包下有@Controller @Service 注解的代码--&gt; &lt;context:component-scan base-package="my.study.springmvc"&gt; &lt;/context:component-scan&gt; 注解映射器和适配器配置处理器映射器注解式处理器映射器，对类中标记了@ResquestMapping的方法进行映射。根据@ResquestMapping定义的url匹配@ResquestMapping标记的方法，匹配成功返回HandlerMethod对象给前端控制器。HandlerMethod对象中封装url对应的方法Method。 从spring3.1版本开始，废除了DefaultAnnotationHandlerMapping的使用，推荐使用RequestMappingHandlerMapping完成注解式处理器映射。 在springmvc.xml配置文件中配置如下：12&lt;!-- 配置处理器映射器 --&gt;&lt;bean class="org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping" /&gt; 注解描述：@RequestMapping：定义请求url到处理器功能方法的映射 配置处理器适配器注解式处理器适配器，对标记@ResquestMapping的方法进行适配。 从spring3.1版本开始，废除了AnnotationMethodHandlerAdapter的使用，推荐使用RequestMappingHandlerAdapter完成注解式处理器适配。 在springmvc.xml配置文件中配置如下：123&lt;!-- 配置处理器适配器 --&gt;&lt;bean class="org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter" /&gt; 注解驱动直接配置处理器映射器和处理器适配器比较麻烦，可以使用注解驱动来加载。SpringMVC使用&lt;mvc:annotation-driven&gt;自动加载RequestMappingHandlerMapping和RequestMappingHandlerAdapter可以在springmvc.xml配置文件中使用&lt;mvc:annotation-driven&gt;替代注解处理器和适配器的配置。 12&lt;!-- 注解驱动 --&gt;&lt;mvc:annotation-driven /&gt; 视图解析器视图解析器使用SpringMVC框架默认的InternalResourceViewResolver，这个视图解析器支持JSP视图解析在springmvc.xml配置文件中配置如下：12345678910&lt;!-- Example: prefix="/WEB-INF/jsp/", suffix=".jsp", viewname="test" -&gt; "/WEB-INF/jsp/test.jsp" --&gt; &lt;!-- 配置视图解析器 --&gt; &lt;bean class="org.springframework.web.servlet.view.InternalResourceViewResolver"&gt; &lt;!-- 配置逻辑视图的前缀 --&gt; &lt;property name="prefix" value="/WEB-INF/jsp/" /&gt; &lt;!-- 配置逻辑视图的后缀 --&gt; &lt;property name="suffix" value=".jsp" /&gt; &lt;/bean&gt; 逻辑视图名需要在controller中返回ModelAndView指定，比如逻辑视图名为itemList，则最终返回的jsp视图地址:“WEB-INF/jsp/itemList.jsp” 最终jsp物理地址：前缀+逻辑视图名+后缀]]></content>
  </entry>
  <entry>
    <title><![CDATA[Spring与Mybatis整合]]></title>
    <url>%2F2018%2F11%2F29%2FSpring%E4%B8%8EMybatis%E6%95%B4%E5%90%88%2F</url>
    <content type="text"><![CDATA[在不是用Spring的情况下，创建sqlSessionFactory需要两步:12345//1.加载核心配置文件String resource = "sqlMapConfig.xml";InputStream in = Resources.getResourceAsStream(resource);//2.创建SqlSessionFactorySqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(in); 原始Dao开发时，每一个实现类都要注入一个工厂，然后工厂调用openSession()方法创建SqlSession对象，然后sqlSession再执行sql语句，这一系列存在大量重复代码。 整合思路1、SqlSessionFactory对象应该放到spring容器中作为单例存在。2、传统dao的开发方式中，应该从spring容器中获得sqlsession对象。3、Mapper代理形式中，应该从spring容器中直接获得mapper的代理对象。4、数据库的连接以及数据库连接池事务管理都交给spring容器来完成。 依赖包 创建配置文件applicationContext.xml:1234567891011121314151617181920212223242526272829&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:context="http://www.springframework.org/schema/context" xmlns:p="http://www.springframework.org/schema/p" xmlns:aop="http://www.springframework.org/schema/aop" xmlns:tx="http://www.springframework.org/schema/tx" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.0.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.0.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-4.0.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-4.0.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util-4.0.xsd"&gt; &lt;!-- 加载配置文件 --&gt; &lt;context:property-placeholder location="classpath:db.properties"/&gt; &lt;!-- dbcp数据源(配置数据库连接池) --&gt; &lt;bean id="dataSource" class="org.apache.commons.dbcp.BasicDataSource" destroy-method="close"&gt; &lt;property name="driverClassName" value="$&#123;jdbc.driver&#125;" /&gt; &lt;property name="url" value="$&#123;jdbc.url&#125;" /&gt; &lt;property name="username" value="$&#123;jdbc.username&#125;" /&gt; &lt;property name="password" value="$&#123;jdbc.password&#125;" /&gt; &lt;property name="maxActive" value="10" /&gt; &lt;property name="maxIdle" value="5" /&gt; &lt;/bean&gt; &lt;!-- 配置SqlSessionFactory --&gt; &lt;bean id="sqlSessionFactory" class="org.mybatis.spring.SqlSessionFactoryBean"&gt; &lt;!-- 配置mybatis核心配置文件 --&gt; &lt;property name="configLocation" value="classpath:sqlmapConfig.xml"/&gt; &lt;!-- 配置数据源 --&gt; &lt;property name="dataSource" ref="dataSource" /&gt; &lt;/bean&gt;&lt;/beans&gt; maxActive表示连接池最大连接数，maxIdle表示最大空闲 db.properties:1234jdbc.driver=com.mysql.jdbc.Driverjdbc.url=jdbc:mysql://localhost:3306/mybatisjdbc.username=rootjdbc.password=12345 log4j.properties:123456# Global logging configurationlog4j.rootLogger=DEBUG, stdout# Console output...log4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=%5p [%t] - %m%n sqlmapConfig.xml Mapper代理形式开发dao配置mapper代理程序结构: 1234567package my.study.mybatis.mapper;import my.study.mybatis.pojo.User;public interface UserMapper &#123; public User selectUserById(Integer id);&#125; applicationContext.xml:12345&lt;!-- Mpper动态代理开发 --&gt;&lt;bean id="userMapper" class="org.mybatis.spring.mapper.MapperFactoryBean"&gt; &lt;property name="sqlSessionFactory" ref="sqlSessionFactoryBean"/&gt; &lt;property name="mapperInterface" value="my.study.mybatis.mapper.UserMapper"/&gt;&lt;/bean&gt; sqlmapConfig.xml:12345678910&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE mapperPUBLIC "-//mybatis.org//DTD Mapper 3.0//EN""http://mybatis.org/dtd/mybatis-3-mapper.dtd"&gt;&lt;mapper namespace="my.study.mybatis.mapper.UserMapper"&gt; &lt;!-- 根据用户id查询 --&gt; &lt;select id="selectUserById" parameterType="Integer" resultType="my.study.mybatis.pojo.User"&gt; select * from user where id = #&#123;id&#125; &lt;/select&gt;&lt;/mapper&gt; 123456789101112131415package my.study.mybatis.pojo;import java.io.Serializable;import java.util.Date;public class User implements Serializable &#123; private static final long serialVersionUID = 1L; private Integer id; private String username; private String sex; private Date birthday; private String address; //get/set...&#125; 测试代码：12345678910111213141516171819202122232425package my.study.mybatis.test;import org.junit.Before;import org.junit.Test;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;import my.study.mybatis.mapper.UserMapper;import my.study.mybatis.pojo.User;public class UserDaoTest &#123; private ApplicationContext context; @Before public void setUp() throws Exception &#123; this.context = new ClassPathXmlApplicationContext("classpath:applicationContext.xml"); &#125; @Test public void testQueryUserById() &#123; UserMapper mapper = (UserMapper) this.context.getBean("userMapper"); User user = mapper.selectUserById(1); System.out.println(user); &#125;&#125; 扫描包形式配置mapper使用扫描方式不需要手动再注入工厂，它会扫描spring中配置的工厂,只需制定基本包就可以12345&lt;!-- Mapper动态代理开发 扫描包 --&gt;&lt;bean class="org.mybatis.spring.mapper.MapperScannerConfigurer"&gt; &lt;!-- 配置基本包 --&gt; &lt;property name="basePackage" value="my.study.mybatis.mapper"/&gt;&lt;/bean&gt; 使用这种方式，每个mapper代理对象的id就是类名，首字母小写 注解掉之前配置的Mpper动态代理开发: 使用扫描方式进行注入，不需要指定id,没有id的情况下，无法指定id，则需要指定实现类:然后进行测试: 传统dao的开发方式 UserDao接口:1234567package my.study.mybatis.dao;import my.study.mybatis.pojo.User;public interface UserDao &#123; User selectUserById(int id);&#125; UserDaoImpl类:123456789101112131415161718192021package my.study.mybatis.dao;import org.apache.ibatis.session.SqlSession;import org.mybatis.spring.support.SqlSessionDaoSupport;import my.study.mybatis.pojo.User;//继承SqlSessionDaoSupport可以不用再在此类声明工厂，而是交给父类public class UserDaoImpl extends SqlSessionDaoSupport implements UserDao&#123; @Override public User selectUserById(int id) &#123; // 获取SqlSession SqlSession sqlSession = super.getSqlSession(); // 使用SqlSession执行操作 User user = sqlSession.selectOne("selectUserById", id); // 不要关闭sqlSession return user; &#125;&#125; UserDaoImpl继承SqlSessionDaoSupport可以使Spring在父类创建工厂 sqlmapConfig.xml配置:12345678910&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;!DOCTYPE configurationPUBLIC "-//mybatis.org//DTD Config 3.0//EN""http://mybatis.org/dtd/mybatis-3-config.dtd"&gt;&lt;configuration&gt; &lt;mappers&gt; &lt;package name="my.study.mybatis.mapper"/&gt; &lt;/mappers&gt;&lt;/configuration&gt; applicationContext.xml:12345&lt;!-- dao --&gt; &lt;bean id="userDao" class="my.study.mybatis.dao.UserDaoImpl"&gt; &lt;!-- 注入工厂 --&gt; &lt;property name="sqlSessionFactory" ref="sqlSessionFactoryBean"/&gt; &lt;/bean&gt; 书写测试类UserDaoTest.java:123456789101112131415161718192021222324252627package my.study.mybatis.test;import org.junit.Before;import org.junit.Test;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;import my.study.mybatis.dao.UserDao;import my.study.mybatis.pojo.User;public class UserDaoTest &#123; private ApplicationContext context; @Before public void setUp() throws Exception &#123; this.context = new ClassPathXmlApplicationContext("classpath:applicationContext.xml"); &#125; @Test public void testQueryUserById() &#123; // 获取userDao UserDao userDao = this.context.getBean(UserDao.class); User user = userDao.selectUserById(1); System.out.println(user); &#125;&#125; 逆向工程使用官方网站的Mapper自动生成工具mybatis-generator-core-1.3.2来生成po类和Mapper映射文件 运行时，主程序会读取配置文件，进行生成 配置文件generatorConfig.xml:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE generatorConfiguration PUBLIC "-//mybatis.org//DTD MyBatis Generator Configuration 1.0//EN" "http://mybatis.org/dtd/mybatis-generator-config_1_0.dtd"&gt;&lt;generatorConfiguration&gt; &lt;!-- id随便取 targetRuntime是MyBatis3版本--&gt; &lt;context id="testTables" targetRuntime="MyBatis3"&gt; &lt;commentGenerator&gt; &lt;!-- 是否去除自动生成的注释 true：是 ： false:否 --&gt; &lt;property name="suppressAllComments" value="true" /&gt; &lt;/commentGenerator&gt; &lt;!--数据库连接的信息：驱动类、连接地址、用户名、密码 --&gt; &lt;jdbcConnection driverClass="com.mysql.jdbc.Driver" connectionURL="jdbc:mysql://localhost:3306/mybatis" userId="root" password="12345"&gt; &lt;/jdbcConnection&gt; &lt;!-- &lt;jdbcConnection driverClass="oracle.jdbc.OracleDriver" connectionURL="jdbc:oracle:thin:@127.0.0.1:1521:yycg" userId="yycg" password="yycg"&gt; &lt;/jdbcConnection&gt; --&gt; &lt;!-- 默认false，把JDBC DECIMAL 和 NUMERIC 类型解析为 Integer，为 true时把JDBC DECIMAL 和 NUMERIC 类型解析为java.math.BigDecimal --&gt; &lt;javaTypeResolver&gt; &lt;property name="forceBigDecimals" value="false" /&gt; &lt;/javaTypeResolver&gt; &lt;!-- targetProject:生成PO类的位置 --&gt; &lt;javaModelGenerator targetPackage="my.study.mybatis.pojo" targetProject=".\src"&gt; &lt;!-- enableSubPackages:是否让schema作为包的后缀 --&gt; &lt;property name="enableSubPackages" value="false" /&gt; &lt;!-- 从数据库返回的值被清理前后的空格 --&gt; &lt;property name="trimStrings" value="true" /&gt; &lt;/javaModelGenerator&gt; &lt;!-- targetProject:mapper映射文件生成的位置 --&gt; &lt;sqlMapGenerator targetPackage="my.study.mybatis.mapper" targetProject=".\src"&gt; &lt;!-- enableSubPackages:是否让schema作为包的后缀 --&gt; &lt;property name="enableSubPackages" value="false" /&gt; &lt;/sqlMapGenerator&gt; &lt;!-- targetPackage：mapper接口生成的位置 --&gt; &lt;javaClientGenerator type="XMLMAPPER" targetPackage="my.study.mybatis.mapper" targetProject=".\src"&gt; &lt;!-- enableSubPackages:是否让schema作为包的后缀 --&gt; &lt;property name="enableSubPackages" value="false" /&gt; &lt;/javaClientGenerator&gt; &lt;!-- 指定数据库表 --&gt; &lt;table schema="" tableName="user"&gt;&lt;/table&gt; &lt;table schema="" tableName="orders"&gt;&lt;/table&gt; &lt;!-- 有些表的字段需要指定java类型 &lt;table schema="" tableName="user"&gt; &lt;columnOverride column="id" javaType="Long" /&gt; &lt;/table&gt; --&gt; &lt;/context&gt;&lt;/generatorConfiguration&gt; 主程序:12345678910111213141516171819202122232425262728293031323334353637383940import java.io.File;import java.io.IOException;import java.util.ArrayList;import java.util.List;import org.mybatis.generator.api.MyBatisGenerator;import org.mybatis.generator.config.Configuration;import org.mybatis.generator.config.xml.ConfigurationParser;import org.mybatis.generator.exception.XMLParserException;import org.mybatis.generator.internal.DefaultShellCallback;public class GeneratorSqlmap &#123; public void generator() throws Exception&#123; List&lt;String&gt; warnings = new ArrayList&lt;String&gt;(); boolean overwrite = true; //指定逆向工程配置文件 File configFile = new File("generatorConfig.xml"); ConfigurationParser cp = new ConfigurationParser(warnings); Configuration config = cp.parseConfiguration(configFile); DefaultShellCallback callback = new DefaultShellCallback(overwrite); MyBatisGenerator myBatisGenerator = new MyBatisGenerator(config, callback, warnings); myBatisGenerator.generate(null); &#125; public static void main(String[] args) throws Exception &#123; try &#123; GeneratorSqlmap generatorSqlmap = new GeneratorSqlmap(); generatorSqlmap.generator(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 在主程序运行,查看日志:刷新后，可以看到已经生成了对应的文件: 1.逆向工程生成的代码只能做单表查询2.不能在生成的代码上进行扩展，因为如果数据库变更，需要重新使用逆向工程生成代码，原来编写的代码就被覆盖了。3.一张表会生成4个文件4.*Example.java文件中封装了很多方便的方法可以用来调用]]></content>
  </entry>
  <entry>
    <title><![CDATA[Mybatis学习笔记-二]]></title>
    <url>%2F2018%2F11%2F21%2FMybatis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[parameterType(输入类型)传递简单类型和传递pojo对象简单类型：使用#{}占位符，或者${}进行sql拼接。 opjo对象：Mybatis使用ognl表达式解析对象字段的值，#{}或者${}括号中的值为pojo属性名称。 传递pojo包装对象开发中通过可以使用pojo传递查询条件。查询条件可能是综合的查询条件，不仅包括用户查询条件还包括其它的查询条件（比如查询用户信息的时候，将用户购买商品信息也作为查询条件），这时可以使用包装对象传递输入参数。 包装对象：Pojo类中的一个属性是另外一个pojo。 需求：根据用户名模糊查询用户信息，查询条件放到QueryVo的user属性中。 在Mybatis学习笔记-一的基础上，继续编写QueryVo类进行开发: 1234567891011121314package my.study.pojo;public class QueryVo &#123; //包含其他的pojo private User user; public User getUser() &#123; return user; &#125; public void setUser(User user) &#123; this.user = user; &#125;&#125; 在my.study.mapper包下的User.xml中书写sql语句:1234&lt;!-- 使用包装类型查询用户 --&gt; &lt;select id="findUserByQueryVo" parameterType="my.study.pojo.QueryVo" resultType="my.study.pojo.User"&gt; select * from user where username like "%"#&#123;user.username&#125;"%" &lt;/select&gt; 在UserMapper.java接口中添加方法:123456789101112package my.study.mapper;import java.util.List;import my.study.pojo.QueryVo;import my.study.pojo.User;public interface UserMapper &#123; public User findUserById(Integer id); //添加方法 public List&lt;User&gt; findUserByQueryVo(QueryVo vo);&#125; 书写测试代码：123456789101112131415161718192021@Testpublic void testfindUserByQueryVo() throws Exception&#123; //1.加载核心配置文件 String resource = "sqlMapConfig.xml"; InputStream in = Resources.getResourceAsStream(resource); //2.创建SqlSessionFactory SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(in); //3.创建SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); //4.SqlSession根据接口生成一个实现类 UserMapper mapper = sqlSession.getMapper(UserMapper.class); QueryVo vo =new QueryVo(); User u = new User(); u.setUsername("五"); vo.setUser(u); List&lt;User&gt; us = mapper.findUserByQueryVo(vo); for(User user:us) &#123; System.out.println(user); &#125; &#125; resultType(输出类型)输出简单类型需求：查询user的数量 在UserMapper.java接口中添加方法:1234... //查询数据条数 public Integer findUserCount();... Mapper.xml中添加查询语句:1234&lt;!-- 查询用户条数 --&gt; &lt;select id="findUserCount" resultType="Integer"&gt; select count(*) from user &lt;/select&gt; 书写测试类：12345678910111213141516//查询用户条数@Testpublic void testfindUserCount() throws Exception&#123; //1.加载核心配置文件 String resource = "sqlMapConfig.xml"; InputStream in = Resources.getResourceAsStream(resource); //2.创建SqlSessionFactory SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(in); //3.创建SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); //4.SqlSession根据接口生成一个实现类 UserMapper mapper = sqlSession.getMapper(UserMapper.class); Integer count = mapper.findUserCount(); System.out.println(count);&#125; 输出pojo对象和对象列表见 https://homxuwang.github.io/2018/11/19/Mybatis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%B8%80/ resultMapresultType可以指定将查询结果映射为pojo，但需要pojo的属性名和sql查询的列名一致方可映射成功。 如果sql查询字段名和pojo的属性名不一致，可以通过resultMap将字段名和属性名作一个对应关系 ，resultMap实质上还需要将查询结果映射到pojo对象中。 resultMap可以实现将查询结果映射为复杂类型的pojo，比如在查询结果映射对象中包括pojo和list实现一对一查询和一对多查询。 需求：查询订单表order的所有数据,订单表创建见:Mybatis学习笔记-一sql：SELECT id, user_id, number, createtime, note FROM order 创建POJO类:1234567891011121314151617181920212223242526272829package my.study.pojo;import java.io.Serializable;import java.util.Date;public class Orders implements Serializable&#123; private static final long serialVersionUID = 1L; private Integer id; private Integer userId; private String number; private Date createtime; private String note; private User user; 添加 get/set 方法 @Override public String toString() &#123; return "Orders [id=" + id + ", userId=" + userId + ", number=" + number + ", createtime=" + createtime + ", note=" + note + "]"; &#125;&#125; 可以看到类中userId字段和数据库表中的user_id字段名不一致 在my.study.mapper包中添加OrderMapper接口:1234567891011package my.study.mapper;import java.util.List;import my.study.pojo.Orders;import my.study.pojo.User;public interface OrderMapper &#123; // 查询订单表order的所有数据 public List&lt;Orders&gt; selectOrdersList(); &#125; 定义OrderMapper.xml:123456789101112&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE mapperPUBLIC "-//mybatis.org//DTD Mapper 3.0//EN""http://mybatis.org/dtd/mybatis-3-mapper.dtd"&gt;&lt;mapper namespace="my.study.mapper.OrderMapper"&gt; &lt;!-- 查询所有的订单数据 --&gt; &lt;select id="selectOrdersList" resultType="my.study.pojo.Orders"&gt; SELECT id, user_id, number, createtime, note FROM orders &lt;/select&gt;&lt;/mapper&gt; 记得在核心配置文件中引入OrderMapper.xml：1234&lt;mappers&gt; &lt;mapper resource="my/study/mapper/User.xml"/&gt; &lt;mapper resource="my/study/mapper/OrderMapper.xml"/&gt;&lt;/mappers&gt; 书写测试类:123456789101112131415161718//查询订单表的所有数据 @Test public void testfindUserCount() throws Exception&#123; //1.加载核心配置文件 String resource = "sqlMapConfig.xml"; InputStream in = Resources.getResourceAsStream(resource); //2.创建SqlSessionFactory SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(in); //3.创建SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); //4.SqlSession根据接口生成一个实现类 OrderMapper mapper = sqlSession.getMapper(OrderMapper.class); List&lt;Orders&gt; orderlist = mapper.selectOrdersList(); for(Orders order:orderlist) &#123; System.out.println(order); &#125; &#125; 可以看到，对于POJO类中和数据库表中字段不一致的，将会查询出null.除了修改为一样的字段值外，可以使用resultMap:其实，与数据库字段不一样的只有user_id和userId，没有必要都写出来，所以可以简写为:123&lt;resultMap type="my.study.pojo.Orders" id="orders"&gt; &lt;result column="user_id" property="userId"/&gt;&lt;/resultMap&gt; column对应数据库表字段;property对应POJO字段;javaType对应java类型(可以省略);jdbcType对应数据库中的类型(可以省略) 再次执行测试代码: 动态sql通过mybatis提供的各种标签方法实现动态拼接sql if标签需求：根据性别和名字查询用户查询sql：SELECT id, username, birthday, sex, address FROM user WHERE sex = 1 AND username LIKE ‘%张%’ 在UserMapper.java接口中添加方法:1234...//根据性别和名称查询用户 public List&lt;User&gt; selectUserBySexAndUsername(User user);... 在User.xml中书写sql语句:1234567891011&lt;!-- 使用if和where标签 --&gt; &lt;select id="selectUserBySexAndUsername" parameterType="my.study.pojo.User" resultType="my.study.pojo.User"&gt; select * from user where &lt;if test="sex != null and sex != ''"&gt; sex = #&#123;sex&#125; &lt;/if&gt; &lt;if test="username != null and username != ''"&gt; and username = #&#123;username&#125; &lt;/if&gt; &lt;/select&gt; 书写测试类:1234567891011121314151617181920//查询用户条数 @Test public void testselectUserBySexAndUsername() throws Exception&#123; //1.加载核心配置文件 String resource = "sqlMapConfig.xml"; InputStream in = Resources.getResourceAsStream(resource); //2.创建SqlSessionFactory SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(in); //3.创建SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); //4.SqlSession根据接口生成一个实现类 UserMapper mapper = sqlSession.getMapper(UserMapper.class); User u = new User(); u.setSex("1"); u.setUsername("张小明"); List&lt;User&gt; users = mapper.selectUserBySexAndUsername(u); for(User user:users) &#123; System.out.println(user); &#125; &#125; 查询结果： 因为通过user实例查询的sex和username字段都不为空，所以可以看到sql语句是正常的. 如果注释掉u.setUsername(&quot;张小明&quot;);再进行测试 :可以看到查询出的是sex为1的所有数据，因为username=null，所以在if标签中没有添加后面的and username = #{username} 但是如果注释掉u.setSex(&quot;1&quot;);，将u.setUsername(&quot;张小明&quot;);还原，再次进行测试：可以看到，查询错误，sql语句是拼接错误的.这时候就需要用到where标签 where标签where标签可以自动添加where，同时处理sql语句中第一个前and关键字对上面的查询语句做处理:1234567891011&lt;select id="selectUserBySexAndUsername" parameterType="my.study.pojo.User" resultType="my.study.pojo.User"&gt; select * from user &lt;where&gt; &lt;if test="sex != null and sex != ''"&gt; sex = #&#123;sex&#125; &lt;/if&gt; &lt;if test="username != null and username != ''"&gt; and username = #&#123;username&#125; &lt;/if&gt; &lt;/where&gt; &lt;/select&gt; 再次进行测试：上面提到where标签可以处理sql语句中第一个前and关键字 SQL片段Sql中可将重复的sql提取出来，使用时用include引用即可，最终达到sql重用的目的。先定义一个用于共用的sql片段,然后使用include进行引用:12345678910111213141516&lt;sql id="selectFromUser"&gt; select * from user&lt;/sql&gt;&lt;!--使用sql片段--&gt;&lt;select id="selectUserBySexAndUsername" parameterType="my.study.pojo.User" resultType="my.study.pojo.User"&gt; &lt;include refid="selectFromUser"&gt;&lt;/include&gt; &lt;where&gt; &lt;if test="sex != null and sex != ''"&gt; sex = #&#123;sex&#125; &lt;/if&gt; &lt;if test="username != null and username != ''"&gt; and username = #&#123;username&#125; &lt;/if&gt; &lt;/where&gt; &lt;/select&gt; foreach标签需求：根据多个id查询用户信息在UserMapper接口中添加方法:12//根据多个id查询用户信息public List&lt;User&gt; selectUserByIds(QueryVo vo); 在QueryVo中改造：12private List&lt;Integer&gt; idsList;//添加get/set方法 书写sql：123456789&lt;select id="selectUserByIds" parameterType="my.study.pojo.QueryVo" resultType="my.study.pojo.User"&gt; &lt;include refid="selectFromUser"/&gt; &lt;where&gt; id in &lt;foreach collection="idsList" item="id" separator="," open="(" close=")"&gt; #&#123;id&#125; &lt;/foreach&gt; &lt;/where&gt;&lt;/select&gt; collection：遍历的集合，这里是QueryVo的idsList属性item：遍历的项目，可以随便写，但是和后面的#{}里面要一致open：在前面添加的sql片段close：在结尾处添加的sql片段separator：指定遍历的元素之间使用的分隔符注意:如果接口的方法参数处传递的是数组类型的参数，则需要将collection处值设置为array;同理，如果接口的方法参数处传递的是list类型的参数，则需要将collection处值设置为list测试:1234567891011121314151617181920212223//多个id @Test public void testselectUserByidsList() throws Exception&#123; //1.加载核心配置文件 String resource = "sqlMapConfig.xml"; InputStream in = Resources.getResourceAsStream(resource); //2.创建SqlSessionFactory SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(in); //3.创建SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); //4.SqlSession根据接口生成一个实现类 UserMapper mapper = sqlSession.getMapper(UserMapper.class); List&lt;Integer&gt; ids = new ArrayList&lt;&gt;(); ids.add(16); ids.add(22); ids.add(24); QueryVo vo = new QueryVo(); vo.setIdsList(ids); List&lt;User&gt; users = mapper.selectUserByIds(vo); for(User user:users) &#123; System.out.println(user); &#125; &#125; 测试结果: 关联查询 一对一关联查询改造UserMapper,在接口中添加方法12//一对一关联 查询 以订单为中心 关联用户public List&lt;Orders&gt; selectOrders(); 在User.xml中书写sql(需要用到嵌套映射):12345678910111213141516171819202122232425262728&lt;resultMap type="my.study.pojo.Orders" id="orderUserResultMap"&gt; &lt;id column="id" property="id"/&gt; &lt;result column="user_id" property="userId"/&gt; &lt;result column="number" property="number"/&gt; &lt;result column="createtime" property="createtime"/&gt; &lt;result column="note" property="note"/&gt; &lt;!-- association ：配置一对一属性 --&gt; &lt;!-- property:order里面的User属性名 --&gt; &lt;!-- javaType:属性类型 --&gt; &lt;association property="user" javaType="my.study.pojo.User"&gt; &lt;!-- id:声明主键，表示user_id是关联查询对象的唯一标识--&gt; &lt;id column="user_id" property="id"/&gt; &lt;result column="username" property="username"/&gt; &lt;/association&gt; &lt;/resultMap&gt; &lt;select id="selectOrders" resultMap="orderUserResultMap"&gt; select o.id, o.user_id, o.number, o.createtime, o.note, u.username from orders o LEFT JOIN user u on o.user_id = u.id &lt;/select&gt; 测试:12345678910111213141516171819//一对一@Testpublic void testselectOrders() throws Exception&#123; //1.加载核心配置文件 String resource = "sqlMapConfig.xml"; InputStream in = Resources.getResourceAsStream(resource); //2.创建SqlSessionFactory SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(in); //3.创建SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); //4.SqlSession根据接口生成一个实现类 UserMapper mapper = sqlSession.getMapper(UserMapper.class); List&lt;Orders&gt; orderlist = mapper.selectOrders(); for(Orders order:orderlist) &#123; System.out.println(order); System.out.println(order.getUser()); &#125; &#125; 查询结果: 一对多关联查询一个用户有可能对应多个订单，所以以user表作为左表进行查询，是一对多的关系。 在User类中添加订单列表属性字段,并添加get和set方法:1234...private List&lt;Orders&gt; ordersList;//get/set... OrderMapper接口中添加方法:12//一对多关联public List&lt;User&gt; selectUserList(); 在OrderMapper.xml中书写sql代码:123456789101112131415161718192021222324252627&lt;resultMap type="my.study.pojo.User" id="userResultMap"&gt; &lt;id column="user_id" property="id"/&gt; &lt;result column="username" property="username"/&gt; &lt;!-- 配置一对多属性 ofType是指List中每个元素的类型 --&gt; &lt;collection property="ordersList" ofType="my.study.pojo.Orders"&gt; &lt;id column="id" property="id"/&gt; &lt;result column="user_id" property="userId"/&gt; &lt;result column="number" property="number"/&gt; &lt;result column="createtime" property="createtime"/&gt; &lt;result column="note" property="note"/&gt; &lt;/collection&gt; &lt;/resultMap&gt; &lt;select id="selectUserList" resultMap="userResultMap"&gt; select o.id, o.user_id, o.number, o.createtime, o.note, u.username from user u LEFT JOIN orders o on o.user_id = u.id &lt;/select&gt; 测试代码：12345678910111213141516171819//一对多@Testpublic void testselectUserList() throws Exception&#123; //1.加载核心配置文件 String resource = "sqlMapConfig.xml"; InputStream in = Resources.getResourceAsStream(resource); //2.创建SqlSessionFactory SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(in); //3.创建SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); //4.SqlSession根据接口生成一个实现类 OrderMapper mapper = sqlSession.getMapper(OrderMapper.class); List&lt;User&gt; userlist = mapper.selectUserList(); for(User user:userlist) &#123; System.out.println(user); System.out.println(user.getOrdersList()); &#125; &#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[Mybatis学习笔记-一]]></title>
    <url>%2F2018%2F11%2F19%2FMybatis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%B8%80%2F</url>
    <content type="text"><![CDATA[Mybatis架构图 准备测试数据库1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/*Navicat MySQL Data TransferSource Server : localhost_3306Source Server Version : 50521Source Host : localhost:3306Source Database : mybatisTarget Server Type : MYSQLTarget Server Version : 50521File Encoding : 65001Date: 2015-04-09 16:03:53*/SET FOREIGN_KEY_CHECKS=0;-- ------------------------------ Table structure for `orders`-- ----------------------------DROP TABLE IF EXISTS `orders`;CREATE TABLE `orders` ( `id` int(11) NOT NULL AUTO_INCREMENT, `user_id` int(11) NOT NULL COMMENT '下单用户id', `number` varchar(32) NOT NULL COMMENT '订单号', `createtime` datetime NOT NULL COMMENT '创建订单时间', `note` varchar(100) DEFAULT NULL COMMENT '备注', PRIMARY KEY (`id`), KEY `FK_orders_1` (`user_id`), CONSTRAINT `FK_orders_id` FOREIGN KEY (`user_id`) REFERENCES `user` (`id`) ON DELETE NO ACTION ON UPDATE NO ACTION) ENGINE=InnoDB AUTO_INCREMENT=6 DEFAULT CHARSET=utf8;-- ------------------------------ Records of orders-- ----------------------------INSERT INTO `orders` VALUES ('3', '1', '1000010', '2015-02-04 13:22:35', null);INSERT INTO `orders` VALUES ('4', '1', '1000011', '2015-02-03 13:22:41', null);INSERT INTO `orders` VALUES ('5', '10', '1000012', '2015-02-12 16:13:23', null);-- ------------------------------ Table structure for `user`-- ----------------------------DROP TABLE IF EXISTS `user`;CREATE TABLE `user` ( `id` int(11) NOT NULL AUTO_INCREMENT, `username` varchar(32) NOT NULL COMMENT '用户名称', `birthday` date DEFAULT NULL COMMENT '生日', `sex` char(1) DEFAULT NULL COMMENT '性别', `address` varchar(256) DEFAULT NULL COMMENT '地址', PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=27 DEFAULT CHARSET=utf8;-- ------------------------------ Records of user-- ----------------------------INSERT INTO `user` VALUES ('1', '王五', null, '2', null);INSERT INTO `user` VALUES ('10', '张三', '2014-07-10', '1', '北京市');INSERT INTO `user` VALUES ('16', '张小明', null, '1', '河南郑州');INSERT INTO `user` VALUES ('22', '陈小明', null, '1', '河南郑州');INSERT INTO `user` VALUES ('24', '张三丰', null, '1', '河南郑州');INSERT INTO `user` VALUES ('25', '陈小明', null, '1', '河南郑州');INSERT INTO `user` VALUES ('26', '王五', null, null, null); 环境搭建导入所需依赖包： 根据1中的架构图，首先准备配置文件：SqlMapConfig.xml:123456789101112131415161718192021&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE configurationPUBLIC "-//mybatis.org//DTD Config 3.0//EN""http://mybatis.org/dtd/mybatis-3-config.dtd"&gt;&lt;configuration&gt; &lt;!-- 用于配置数据库连接环境 和spring整合后 environments配置将废除 --&gt; &lt;environments default="development"&gt; &lt;environment id="development"&gt; &lt;!-- 使用jdbc事务管理 --&gt; &lt;transactionManager type="JDBC" /&gt; &lt;!-- 数据库连接池 --&gt; &lt;dataSource type="POOLED"&gt; &lt;property name="driver" value="com.mysql.jdbc.Driver" /&gt; &lt;property name="url" value="jdbc:mysql://localhost:3306/mybatis" /&gt; &lt;property name="username" value="root" /&gt; &lt;property name="password" value="12345" /&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt;&lt;/configuration&gt; 其中environments标签是用来配置数据库连接环境的，因为这里是单独是mybatis工程，所以要进行配置。后期在整合Spring工程时，数据库的配置由Spring来负责，可以将environments标签除掉。 在src包下创建log4j.properties.log4j.properties:123456# Global logging configurationlog4j.rootLogger=DEBUG, stdout# Console output...log4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=%5p [%t] - %m%n 创建user表中对应的pojo类.pojo类作为mybatis进行sql映射使用，po类通常与数据库表对应.123456789101112131415161718192021222324package my.study.pojo;import java.io.Serializable;import java.util.Date;public class User implements Serializable &#123; private static final long serialVersionUID = 1L; private Integer id; private String username; private String sex; private Date birthday; private String address; //省略get和set方法 //... @Override public String toString() &#123; return "User [id=" + id + ", username=" + username + ", sex=" + sex + ", birthday=" + birthday + ", address=" + address + "]"; &#125;&#125; 在my.study.mapper包下创建user.xml:12345678&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE mapperPUBLIC "-//mybatis.org//DTD Mapper 3.0//EN""http://mybatis.org/dtd/mybatis-3-mapper.dtd"&gt;&lt;!-- 书写sql语句 --&gt;&lt;mapper&gt;&lt;/mapper&gt; 在xml中引入约束后，其根标签就是&lt;mapper&gt;&lt;/mapper&gt;,用来在其中书写sql语句。 在这里定义了user.xml文件后，根据架构图，需要和核心配置文件结合，所以要在核心配置文件中使用&lt;mappers&gt;&lt;/mappers&gt;标签：1234&lt;!-- Mapper的位置 Mapper.xml 写Sql语句的文件的位置 --&gt; &lt;mappers&gt; &lt;mapper resource="my/study/mapper/User.xml"/&gt; &lt;/mappers&gt; 这样准备工作就基本完成了，接下来就要进行架构中的第二部，创建工厂和session并进行测试。 操作数据库在配置文件中书写sql在User.xml配置文件中书写查询sql: 其中&lt;&gt;cache和&lt;&gt;cache-ref表示缓存,它们和&lt;&gt;parameterMap都很少用到.&lt;&gt;sql表示sql片段,&lt;&gt;result表示手动映射。剩下的增删改查和更新。 书写查询sql：1234&lt;!-- 查询用户 --&gt; &lt;select id="findUserById" parameterType="Integer" resultType="my.study.pojo.User"&gt; select * from user where id = #&#123;v&#125; &lt;/select&gt; 标签中的id表示这条sql语句在配置文件中的唯一标识;parameterType表示传参的类型;resultType表示自动映射查询到的结果，填写pojo的全路径.自动映射要求所创建的类要和数据库表中的各个字段和类型相对应,如果不对应则需要手动映射.#{}表示占位符，类似于jdbc中的?,大括号中的值可以随便写. 如果有另一个配置文件中的查询语句的id和已存在的重复，这时候可以在&lt;mapper&gt;添加namespace=&quot;&quot;,namespace表示命名空间，用于隔离sql,避免发生这种情况.比如修改为:123&lt;mapper namespace="user"&gt;...&lt;/mapper&gt; 这时候可以使用user.findUserById访问这个sql. 书写测试类12345678910111213141516171819202122232425262728package my.study.test;import java.io.InputStream;import org.apache.ibatis.io.Resources;import org.apache.ibatis.session.SqlSession;import org.apache.ibatis.session.SqlSessionFactory;import org.apache.ibatis.session.SqlSessionFactoryBuilder;import org.junit.Test;import my.study.pojo.User;public class MybatisTest &#123; @Test public void test1() throws Exception &#123; //1.加载核心配置文件 String resource = "sqlMapConfig.xml"; InputStream in = Resources.getResourceAsStream(resource); //2.创建SqlSessionFactory SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(in); //3.创建SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); //4.执行Sql语句 User user = sqlSession.selectOne("user.findUserById", 10); System.out.println(user); &#125;&#125; 这里sqlSession.selectOne()方法的第一个参数就是传入&lt;mapper&gt;的namespace+sql语句的id;第二个就是传入要进行查询的参数(也就是占位符位置的值); 测试结果:因为加入了log日志，这里会打印出各个信息 实现模糊查询书写sql语句:1234&lt;!-- 模糊查询 --&gt; &lt;select id="findUserByUsername" parameterType="String" resultType="my.study.pojo.User"&gt; select * from user where username like '%$&#123;value&#125;%' &lt;/select&gt; 注意，这里的模糊查询中不是用占位符#{}，而是使用${}。类比jdbc中的占位符，#{}相当于?而在需要使用字符串拼接功能时，使用${}.并且其中要填写value,而#{}则没有这种限制。占位符可以防止sql注入，而字符串拼接不能防止sql注入，#{}和${}也是一个道理。在测试类中添加测试方法：12345678910111213141516//实现模糊查询 @Test public void test2() throws Exception &#123; //1.加载核心配置文件 String resource = "sqlMapConfig.xml"; InputStream in = Resources.getResourceAsStream(resource); //2.创建SqlSessionFactory SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(in); //3.创建SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); //4.执行Sql语句 List&lt;User&gt; users = sqlSession.selectList("user.findUserByUsername", "五"); for(User user2:users) &#123; System.out.println(user2); &#125; &#125; 因为模糊查询查出来的可能有很多结果，所以用List&lt;&gt;.测试结果: 上面提了这样不能防止sql注入，所以可以这样写select标签中的sql语句1select * from user where username like "%"#&#123;v&#125;"%" 可以看到打印结果中成功出现了占位符，其实最终的sql语句相当于1select * from user where username like "%"'五'"%" 这种写法是正确的，只是平时很少这么用 新增数据书写sql语句123456&lt;!-- 新增用户 --&gt; &lt;insert id="insertUser" parameterType="my.study.pojo.User"&gt; insert into user (username,birthday,address,sex) values (#&#123;username&#125;,#&#123;birthday&#125;,#&#123;address&#125;,#&#123;sex&#125;) &lt;/insert&gt; #{}中的参数要和User.java类中的字段对应，这样才能知道传参是和哪个字段对应 测试类:12345678910111213141516171819//新增用户 @Test public void testInsert() throws Exception &#123; //1.加载核心配置文件 String resource = "sqlMapConfig.xml"; InputStream in = Resources.getResourceAsStream(resource); //2.创建SqlSessionFactory SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(in); //3.创建SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); //4.执行Sql语句 User u = new User(); u.setUsername("李雷"); u.setBirthday(new Date()); u.setAddress("火星"); u.setSex("男"); int i =sqlSession.insert("user.insertUser", u); sqlSession.commit(); &#125; 其中insert的返回值为int型，表示影响的行数.注意最后要提交事务才能成功插入. 新增数据后返回主键ID如果想实现在新增完用户后立刻返回这条新增数据的主键，则需要在&lt;insert&gt;语句中再嵌套一层&lt;selectKey&gt;用来查询主键.123&lt;selectKey keyProperty="id" resultType="Integer" order="AFTER"&gt; select LAST_INSERT_ID()&lt;/selectKey&gt; keyProperty表示返回值为User的id属性，resultType表示类型是Integer型;注意：在sql中，如果主键是自增的int型，那么主键是在数据插入完成后再生成主键;如果不是自增类型，比如如果是varchar类型，则会先插入主键，再插入数据。order表示执行顺序,所以这里要用AFTER其中1select LAST_INSERT_ID() 是由sql提供的查询语句，它跟在insert语句后面的话，会返回刚插入的数据的主键,比如:12insert into ....select LAST_INSERT_ID() 在上面的测试类中添加1System.out.println(u.getId()); 即可查看到已经获取到最新插入的主键ID 修改/更新数据123456&lt;!-- 更新 --&gt; &lt;update id="updateUserById" parameterType="my.study.pojo.User"&gt; update user set username = #&#123;username&#125;,sex =#&#123;sex&#125;,address = #&#123;address&#125;,birthday = #&#123;birthday&#125; where id =#&#123;id&#125; &lt;/update&gt; 1234567891011121314151617181920//更新用户 @Test public void testUpdate() throws Exception &#123; //1.加载核心配置文件 String resource = "sqlMapConfig.xml"; InputStream in = Resources.getResourceAsStream(resource); //2.创建SqlSessionFactory SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(in); //3.创建SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); //4.执行Sql语句 User u = new User(); u.setId(29); u.setUsername("李雷update"); u.setBirthday(new Date()); u.setAddress("火星update"); u.setSex("2"); int i =sqlSession.update("user.updateUserById", u); sqlSession.commit(); &#125; 删除12345&lt;!-- 删除 --&gt; &lt;delete id="deleteUserById" parameterType="Integer"&gt; delete from user where id = #&#123;id&#125; &lt;/delete&gt; 1234567891011121314//删除用户 @Test public void testdelete() throws Exception &#123; //1.加载核心配置文件 String resource = "sqlMapConfig.xml"; InputStream in = Resources.getResourceAsStream(resource); //2.创建SqlSessionFactory SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(in); //3.创建SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); //4.执行Sql语句 int d = sqlSession.delete("user.deleteUserById", 29); sqlSession.commit(); &#125; Mapper动态代理开发使用动态代理方式，动态生成代码，避免书写重复代码.原始Dao开发中存在以下问题：Dao方法体存在重复代码：通过SqlSessionFactory创建SqlSession，调用SqlSession的数据库操作方法调用sqlSession的数据库操作方法需要指定statement的id，这里存在硬编码，不便于开发维护。 Mapper接口开发方法只需要程序员编写Mapper接口（相当于Dao接口），由Mybatis框架根据接口定义创建接口的动态代理对象。 Mapper接口开发需要遵循以下规范：1、Mapper.xml文件中的namespace与mapper接口的类路径相同。2、Mapper接口方法名和Mapper.xml中定义的每个statement的id相同3、Mapper接口方法的输入参数类型和mapper.xml中定义的每个sql 的parameterType的类型相同4、Mapper接口方法的输出参数类型和mapper.xml中定义的每个sql的resultType的类型相同 这里定义一个UserMapper接口:123456789101112package my.study.mapper;import my.study.pojo.User;public interface UserMapper &#123; //遵循四个原则 //接口方法名 == User.xml 中的 id名 //返回值类型 与 User.xml文件中返回值的类型一致 //方法的入参类型与User.xml中入参的类型一致 //mapper标签的命名空间绑定此接口 public User findUserById(Integer id);&#125; 命名空间与接口绑定： 书写测试类：12345678910111213141516171819202122232425262728293031package my.study.test;import java.io.InputStream;import org.apache.ibatis.io.Resources;import org.apache.ibatis.session.SqlSession;import org.apache.ibatis.session.SqlSessionFactory;import org.apache.ibatis.session.SqlSessionFactoryBuilder;import org.junit.Test;import my.study.mapper.UserMapper;import my.study.pojo.User;public class MybatisMapperTest &#123; @Test public void test1() throws Exception&#123; //1.加载核心配置文件 String resource = "sqlMapConfig.xml"; InputStream in = Resources.getResourceAsStream(resource); //2.创建SqlSessionFactory SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(in); //3.创建SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); //4.SqlSession根据接口生成一个实现类 UserMapper mapper = sqlSession.getMapper(UserMapper.class); User user = mapper.findUserById(10); System.out.println(user); &#125;&#125; 这样使用动态开发，只需要书写一个接口和一个mapper.xml配置文件就可以了. Mybatis相比于JDBC的优点1、数据库连接创建、释放频繁造成系统资源浪费从而影响系统性能，如果使用数据库连接池可解决此问题。解决：在SqlMapConfig.xml中配置数据连接池，使用连接池管理数据库链接。2、Sql语句写在代码中造成代码不易维护，实际应用sql变化的可能较大，sql变动需要改变java代码。解决：将Sql语句配置在XXXXmapper.xml文件中与java代码分离。3、向sql语句传参数麻烦，因为sql语句的where条件不一定，可能多也可能少，占位符需要和参数一一对应。解决：Mybatis自动将java对象映射至sql语句，通过statement中的parameterType定义输入参数的类型。4、对结果集解析麻烦，sql变化导致解析代码变化，且解析前需要遍历，如果能将数据库记录封装成pojo对象解析比较方便。解决：Mybatis自动将sql执行结果映射至java对象，通过statement中的resultType定义输出结果的类型。 Mybatis和Hibernate区别Mybatis和hibernate不同，它不完全是一个ORM框架，因为MyBatis需要程序员自己编写Sql语句。mybatis可以通过XML或注解方式灵活配置要运行的sql语句，并将java对象和sql语句映射生成最终执行的sql，最后将sql执行的结果再映射生成java对象。 Mybatis学习门槛低，简单易学，程序员直接编写原生态sql，可严格控制sql执行性能，灵活度高，非常适合对关系数据模型要求不高的软件开发，例如互联网软件、企业运营类软件等，因为这类软件需求变化频繁，一但需求变化要求成果输出迅速。但是灵活的前提是mybatis无法做到数据库无关性，如果需要实现支持多种数据库的软件则需要自定义多套sql映射文件，工作量大。 Hibernate对象/关系映射能力强，数据库无关性好，对于关系模型要求高的软件（例如需求固定的定制化软件）如果用hibernate开发可以节省很多代码，提高效率。但是Hibernate的学习门槛高，要精通门槛更高，而且怎么设计O/R映射，在性能和对象模型之间如何权衡，以及怎样用好Hibernate需要具有很强的经验和能力才行。]]></content>
  </entry>
  <entry>
    <title><![CDATA[整合Struts2+Spring+Hibernate]]></title>
    <url>%2F2018%2F11%2F16%2F%E6%95%B4%E5%90%88Struts2-Spring-Hibernate%2F</url>
    <content type="text"><![CDATA[项目所需依赖包： 单独配置Spring创建配置文件 导入xml约束约束包括beans|context|aop|tx 创建UserAction,并在Spring配置文件中配置.1234567package my.study.web.action;import com.opensymphony.xwork2.ActionSupport;public class UserAction extends ActionSupport &#123;&#125; 1&lt;bean name="userAction" class="my.study.web.action.UserAction"&gt;&lt;/bean&gt; 配置tomcat的web.xml在web.xml中配置让spring随web启动而创建的监听器,并配置spring配置文件配置参数:123456789&lt;!-- 让spring随web启动而创建的监听器 --&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;!-- 配置spring配置文件参数 --&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:applicationContext.xml&lt;/param-value&gt; &lt;/context-param&gt; 此时可以启动服务看看有没有报错，如果没有报错，这一步就配置成功了。 单独配置Struts2配置struts2主配置文件首先创建struts.xml，添加约束,并配置UserAction(约束可以在struts2-core-2.3.35.jar包下的struts-2.3.dtd中查找到)：123456789101112&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE struts PUBLIC "-//Apache Software Foundation//DTD Struts Configuration 2.3//EN" "http://struts.apache.org/dtds/struts-2.3.dtd"&gt; &lt;struts&gt; &lt;package name="ssh" namespace="/" extends="struts-default"&gt; &lt;action name="UserAction_*" class="my.study.web.action.UserAction" method="&#123;1&#125;"&gt; &lt;result name="success"&gt;/success.jsp&lt;/result&gt; &lt;/action&gt; &lt;/package&gt;&lt;/struts&gt; jsp文件自己定义一个放在WebContent目录下即可. 配置struts2核心过滤器到web.xml在web.xml中配置struts2核心过滤器123456789101112...&lt;!-- struts2核心过滤器 --&gt; &lt;filter&gt; &lt;filter-name&gt;struts2&lt;/filter-name&gt; &lt;filter-class&gt;org.apache.struts2.dispatcher.ng.filter.StrutsPrepareAndExecuteFilter&lt;/filter-class&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;struts2&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt;... url-pattern为/*表示所有路径都要经过核心过滤器如果启动服务器没有错误就说明配置成功 Spring和Struts2整合整合的目的是让struts的Action不再自己创建，而是由spring创建(依赖包是struts2-spring-plugin-2.3.35.jar)。 配置常量在struts2-core-2.3.35.jar-&gt;org.apache.struts2-&gt;default.properties中查找常量。 struts.objectFactory = spring是将action的创建交给spring容器struts.objectFactory.spring.autoWire = namespring负责装配Action一般只需配置struts.objectFactory第二个自动开启 在&lt;struts&gt;&lt;/struts&gt;标签中配置&lt;constant name=&quot;struts.objectFactory&quot; value=&quot;spring&quot;&gt; 两种方案创建各个类进行测试：12345678910111213141516package my.study.web.action;import com.opensymphony.xwork2.ActionSupport;import my.study.web.service.UserService;public class UserAction extends ActionSupport &#123; private UserService userService; public void setUserService(UserService userService) &#123; this.userService = userService; &#125; public void login() throws Exception&#123; System.out.println(userService); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344package my.study.web.domain;public class User &#123; private Long user_id; private String user_code; private String user_name; private String user_password; private Character user_state; public Long getUser_id() &#123; return user_id; &#125; public void setUser_id(Long user_id) &#123; this.user_id = user_id; &#125; public String getUser_code() &#123; return user_code; &#125; public void setUser_code(String user_code) &#123; this.user_code = user_code; &#125; public String getUser_name() &#123; return user_name; &#125; public void setUser_name(String user_name) &#123; this.user_name = user_name; &#125; public String getUser_password() &#123; return user_password; &#125; public void setUser_password(String user_password) &#123; this.user_password = user_password; &#125; public Character getUser_state() &#123; return user_state; &#125; public void setUser_state(Character user_state) &#123; this.user_state = user_state; &#125; @Override public String toString() &#123; return "User [user_id=" + user_id + ", user_code=" + user_code + ", user_name=" + user_name + ", user_password=" + user_password + "]"; &#125; &#125; 12345678910111213package my.study.web.impl;import my.study.web.domain.User;import my.study.web.service.UserService;public class UserServiceImpl implements UserService &#123; @Override public User getUserByCodePassword(User u) &#123; System.out.println("getUserByCodePassword"); return null; &#125;&#125; 1234567package my.study.web.service;import my.study.web.domain.User;public interface UserService &#123; User getUserByCodePassword(User u);&#125; 在spring配置文件中配置service:1234 &lt;!-- action配置 --&gt;&lt;bean name="userAction" class="my.study.web.action.UserAction"&gt;&lt;/bean&gt;&lt;!-- servie配置 --&gt;&lt;bean name="userService" class="my.study.web.impl.UserServiceImpl"&gt;&lt;/bean&gt; 整合方案1:class属性上仍然配置action的完整类名，这样struts2仍然创建action,由spring负责组装Action中的依赖属性UserAction中的login()方法就是为了检测是否是由spring装配属性。访问http://localhost:8080/SSH/UserAction_login看到打印结果:当然这种方法并不推荐，最好由spring完整管理action的生命周期。spring中功能才应用到Action上. 整合方案2:spring负责创建action以及组装.首先配置applicationContext.xml中的action，然后配置struts.xml这种方式下在class属性处填写spring中action对象的BeanName.完全由spring管理action生命周期，这样spring需要手动组装依赖属性。即需要在applicationContext.xml中action的&lt;bean&gt;&lt;/bean&gt;中使用&lt;property&gt;&lt;/property&gt;进行手动注入：同时，要注意，Action对象作用范围一定是多例的，这样才符合Struts2的架构。所以将scope属性配置为prototype 访问http://localhost:8080/SSH/UserAction_login依然可以打印出my.study.web.impl.UserServiceImpl至此，struts与spring就整合结束了。 单独配置Hibernate导入实体类和orm元数据User.hbm.xml:12345678910111213141516&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE hibernate-mapping PUBLIC "-//Hibernate/Hibernate Mapping DTD 3.0//EN" "http://www.hibernate.org/dtd/hibernate-mapping-3.0.dtd"&gt;&lt;hibernate-mapping package="my.study.web.domain" &gt; &lt;class name="User" table="sys_user" &gt; &lt;id name="user_id" &gt; &lt;generator class="native"&gt;&lt;/generator&gt; &lt;/id&gt; &lt;property name="user_code" &gt;&lt;/property&gt; &lt;property name="user_name" &gt;&lt;/property&gt; &lt;property name="user_password" &gt;&lt;/property&gt; &lt;property name="user_state" &gt;&lt;/property&gt; &lt;/class&gt;&lt;/hibernate-mapping&gt; Customer.hbm.xml:123456789101112131415161718192021222324252627282930313233343536&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE hibernate-mapping PUBLIC "-//Hibernate/Hibernate Mapping DTD 3.0//EN" "http://www.hibernate.org/dtd/hibernate-mapping-3.0.dtd"&gt;&lt;hibernate-mapping package="my.study.web.domain" &gt; &lt;class name="LinkMan" table="cst_linkman" &gt; &lt;id name="lkm_id" &gt; &lt;generator class="native"&gt;&lt;/generator&gt; &lt;/id&gt; &lt;property name="lkm_gender" &gt;&lt;/property&gt; &lt;property name="lkm_name" &gt;&lt;/property&gt; &lt;property name="lkm_phone" &gt;&lt;/property&gt; &lt;property name="lkm_email" &gt;&lt;/property&gt; &lt;property name="lkm_qq" &gt;&lt;/property&gt; &lt;property name="lkm_mobile" &gt;&lt;/property&gt; &lt;property name="lkm_memo" &gt;&lt;/property&gt; &lt;property name="lkm_position" &gt;&lt;/property&gt; &lt;!-- 多对一 --&gt; &lt;!-- name属性:引用属性名 column属性: 外键列名 class属性: 与我关联的对象完整类名 --&gt; &lt;!-- 级联操作: cascade save-update: 级联保存更新 delete:级联删除 all:save-update+delete 级联操作: 简化操作.目的就是为了少些两行代码. --&gt; &lt;!-- 多的一方: 不能放弃维护关系的.外键字段就在多的一方. --&gt; &lt;many-to-one name="customer" column="lkm_cust_id" class="Customer" &gt; &lt;/many-to-one&gt; &lt;/class&gt;&lt;/hibernate-mapping&gt; Customer.hbm.xml:1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE hibernate-mapping PUBLIC "-//Hibernate/Hibernate Mapping DTD 3.0//EN" "http://www.hibernate.org/dtd/hibernate-mapping-3.0.dtd"&gt; &lt;!-- 配置表与实体对象的关系 --&gt; &lt;!-- package属性:填写一个包名.在元素内部凡是需要书写完整类名的属性,可以直接写简答类名了. --&gt;&lt;hibernate-mapping package="my.study.web.domain" &gt; &lt;!-- class元素: 配置实体与表的对应关系的 name: 完整类名 table:数据库表名 --&gt; &lt;class name="Customer" table="cst_customer" &gt; &lt;!-- id元素:配置主键映射的属性 name: 填写主键对应属性名 column(可选): 填写表中的主键列名.默认值:列名会默认使用属性名 type(可选):填写列(属性)的类型.hibernate会自动检测实体的属性类型. 每个类型有三种填法: java类型|hibernate类型|数据库类型 not-null(可选):配置该属性(列)是否不能为空. 默认值:false length(可选):配置数据库中列的长度. 默认值:使用数据库类型的最大长度 --&gt; &lt;id name="cust_id" &gt; &lt;!-- generator:主键生成策略(明天讲) --&gt; &lt;generator class="native"&gt;&lt;/generator&gt; &lt;/id&gt; &lt;!-- property元素:除id之外的普通属性映射 name: 填写属性名 column(可选): 填写列名 type(可选):填写列(属性)的类型.hibernate会自动检测实体的属性类型. 每个类型有三种填法: java类型|hibernate类型|数据库类型 not-null(可选):配置该属性(列)是否不能为空. 默认值:false length(可选):配置数据库中列的长度. 默认值:使用数据库类型的最大长度 --&gt; &lt;property name="cust_name" column="cust_name" &gt; &lt;!-- &lt;column name="cust_name" sql-type="varchar" &gt;&lt;/column&gt; --&gt; &lt;/property&gt; &lt;property name="cust_source" column="cust_source" &gt;&lt;/property&gt; &lt;property name="cust_industry" column="cust_industry" &gt;&lt;/property&gt; &lt;property name="cust_level" column="cust_level" &gt;&lt;/property&gt; &lt;property name="cust_linkman" column="cust_linkman" &gt;&lt;/property&gt; &lt;property name="cust_phone" column="cust_phone" &gt;&lt;/property&gt; &lt;property name="cust_mobile" column="cust_mobile" &gt;&lt;/property&gt; &lt;/class&gt;&lt;/hibernate-mapping&gt; LinkMan.java:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485package my.study.web.domain;public class LinkMan &#123; private Long lkm_id; private Character lkm_gender; private String lkm_name; private String lkm_phone; private String lkm_email; private String lkm_qq; private String lkm_mobile; private String lkm_memo; private String lkm_position; private Customer customer ; private Long cust_id; public Long getCust_id() &#123; return cust_id; &#125; public void setCust_id(Long cust_id) &#123; this.cust_id = cust_id; &#125; public Customer getCustomer() &#123; return customer; &#125; public void setCustomer(Customer customer) &#123; this.customer = customer; &#125; public Long getLkm_id() &#123; return lkm_id; &#125; public void setLkm_id(Long lkm_id) &#123; this.lkm_id = lkm_id; &#125; public Character getLkm_gender() &#123; return lkm_gender; &#125; public void setLkm_gender(Character lkm_gender) &#123; this.lkm_gender = lkm_gender; &#125; public String getLkm_name() &#123; return lkm_name; &#125; public void setLkm_name(String lkm_name) &#123; this.lkm_name = lkm_name; &#125; public String getLkm_phone() &#123; return lkm_phone; &#125; public void setLkm_phone(String lkm_phone) &#123; this.lkm_phone = lkm_phone; &#125; public String getLkm_email() &#123; return lkm_email; &#125; public void setLkm_email(String lkm_email) &#123; this.lkm_email = lkm_email; &#125; public String getLkm_qq() &#123; return lkm_qq; &#125; public void setLkm_qq(String lkm_qq) &#123; this.lkm_qq = lkm_qq; &#125; public String getLkm_mobile() &#123; return lkm_mobile; &#125; public void setLkm_mobile(String lkm_mobile) &#123; this.lkm_mobile = lkm_mobile; &#125; public String getLkm_memo() &#123; return lkm_memo; &#125; public void setLkm_memo(String lkm_memo) &#123; this.lkm_memo = lkm_memo; &#125; public String getLkm_position() &#123; return lkm_position; &#125; public void setLkm_position(String lkm_position) &#123; this.lkm_position = lkm_position; &#125; &#125; Customer.java:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package my.study.web.domain;public class Customer &#123; private Long cust_id; private String cust_name; private String cust_source; private String cust_industry; private String cust_level; private String cust_linkman; private String cust_phone; private String cust_mobile; public Long getCust_id() &#123; return cust_id; &#125; public void setCust_id(Long cust_id) &#123; this.cust_id = cust_id; &#125; public String getCust_name() &#123; return cust_name; &#125; public void setCust_name(String cust_name) &#123; this.cust_name = cust_name; &#125; public String getCust_source() &#123; return cust_source; &#125; public void setCust_source(String cust_source) &#123; this.cust_source = cust_source; &#125; public String getCust_industry() &#123; return cust_industry; &#125; public void setCust_industry(String cust_industry) &#123; this.cust_industry = cust_industry; &#125; public String getCust_level() &#123; return cust_level; &#125; public void setCust_level(String cust_level) &#123; this.cust_level = cust_level; &#125; public String getCust_linkman() &#123; return cust_linkman; &#125; public void setCust_linkman(String cust_linkman) &#123; this.cust_linkman = cust_linkman; &#125; public String getCust_phone() &#123; return cust_phone; &#125; public void setCust_phone(String cust_phone) &#123; this.cust_phone = cust_phone; &#125; public String getCust_mobile() &#123; return cust_mobile; &#125; public void setCust_mobile(String cust_mobile) &#123; this.cust_mobile = cust_mobile; &#125; @Override public String toString() &#123; return "Customer [cust_id=" + cust_id + ", cust_name=" + cust_name + "]"; &#125;&#125; 在hibernate的配置文件中，不用配置隔离级别，在spring中进行配置。也不需配置session与当前线程绑定的配置，因为spring有管理session的机制。hibernate.cfg.xml:1234567891011121314151617181920212223242526272829303132333435363738&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE hibernate-configuration PUBLIC "-//Hibernate/Hibernate Configuration DTD 3.0//EN" "http://www.hibernate.org/dtd/hibernate-configuration-3.0.dtd"&gt;&lt;hibernate-configuration&gt; &lt;session-factory&gt; &lt;property name="hibernate.connection.driver_class"&gt;com.mysql.jdbc.Driver&lt;/property&gt; &lt;property name="hibernate.connection.url"&gt;jdbc:mysql:///customer1&lt;/property&gt; &lt;property name="hibernate.connection.username"&gt;root&lt;/property&gt; &lt;property name="hibernate.connection.password"&gt;12345&lt;/property&gt; &lt;!-- 数据库方言 注意: MYSQL在选择方言时,请选择最短的方言. --&gt; &lt;property name="hibernate.dialect"&gt;org.hibernate.dialect.MySQLDialect&lt;/property&gt; &lt;!-- #hibernate.show_sql true #hibernate.format_sql true --&gt; &lt;property name="hibernate.show_sql"&gt;true&lt;/property&gt; &lt;property name="hibernate.format_sql"&gt;true&lt;/property&gt; &lt;!-- ## auto schema export 自动导出表结构. 自动建表 #hibernate.hbm2ddl.auto create 自动建表.每次框架运行都会创建新的表.以前表将会被覆盖,表数据会丢失.(开发环境中测试使用) #hibernate.hbm2ddl.auto create-drop 自动建表.每次框架运行结束都会将所有表删除.(开发环境中测试使用) #hibernate.hbm2ddl.auto update(推荐使用) 自动生成表.如果已经存在不会再生成.如果表有变动.自动更新表(不会删除任何数据). #hibernate.hbm2ddl.auto validate 校验.不自动生成表.每次启动会校验数据库中表是否正确.校验失败. --&gt; &lt;property name="hibernate.hbm2ddl.auto"&gt;update&lt;/property&gt; &lt;!-- 引入orm元数据 路径书写: 填写src下的路径 --&gt; &lt;mapping resource="my/study/web/domain/Customer.hbm.xml"/&gt; &lt;mapping resource="my/study/web/domain/LinkMan.hbm.xml"/&gt; &lt;mapping resource="my/study/web/domain/User.hbm.xml"/&gt; &lt;/session-factory&gt;&lt;/hibernate-configuration&gt; 创建类进行测试：1234567891011121314151617181920212223242526272829303132333435package my.study.web.test;import org.hibernate.Session;import org.hibernate.SessionFactory;import org.hibernate.Transaction;import org.hibernate.cfg.Configuration;import org.junit.Test;import my.study.web.domain.User;public class HibernateTest &#123; @Test public void test() &#123; Configuration conf = new Configuration().configure(); SessionFactory sf = conf.buildSessionFactory(); Session session = sf.openSession(); Transaction tx = session.beginTransaction(); User u = new User(); u.setUser_code("hey"); u.setUser_name("哈喽"); u.setUser_password("12345"); session.save(u); tx.commit(); session.close(); sf.close(); &#125;&#125; 单独配置Hibernate就成功了 hibernate和spring整合整合原理：将SessionFactory对象交给spring容器管理 在spring中配置sessionFactory也就是要在配置文件中配置sessionFactory,然后sessionFactory加载配置有两种方案: 1.仍然使用外部的hibernate.cfg.xml配置信息 1234 &lt;!-- 将sessionFactory配置到spring容器中 --&gt;&lt;bean name="sessionFactory" class="org.springframework.orm.hibernate5.LocalSessionFactoryBean"&gt; &lt;property name="configLocation" value="classpath:hibernate.cfg.xml"&gt;&lt;/property&gt;&lt;/bean&gt; 注意选择和自己的hibernate版本一样的(我的是hibernate5) 书写代码进行测试，直接使用spring注入的sessionFactory: 1234567891011121314151617181920212223242526272829303132333435363738394041424344package my.study.web.test;import javax.annotation.Resource;import org.hibernate.Session;import org.hibernate.SessionFactory;import org.hibernate.Transaction;import org.hibernate.cfg.Configuration;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.test.context.ContextConfiguration;import org.springframework.test.context.junit4.SpringJUnit4ClassRunner;import my.study.web.domain.User;@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration("classpath:applicationContext.xml")public class HibernateTest &#123; @Resource(name="sessionFactory") private SessionFactory sf; @Test public void test1() &#123; .... &#125; @Test public void test2() &#123; Session session = sf.openSession(); Transaction tx = session.beginTransaction(); User u = new User(); u.setUser_code("hello"); u.setUser_name("你好"); u.setUser_password("8765"); session.save(u); tx.commit(); session.close(); &#125;&#125; 其中 sessionFactory的关闭可以交给spring来管理,所以close()方法可以不用写 2.在spring配置中放置hibernate配置信息第二种配置方法就是在spring中配置hibernate配置信息，包括必选配置，可选配置和引入orm元数据。12345678910111213141516171819&lt;bean name="sessionFactory" class="org.springframework.orm.hibernate5.LocalSessionFactoryBean"&gt; &lt;!-- 配置hibernate基本信息 --&gt; &lt;property name="hibernateProperties"&gt; &lt;props&gt; &lt;!-- 必选配置 --&gt; &lt;prop key="hibernate.connection.driver_class"&gt;com.mysql.jdbc.Driver&lt;/prop&gt; &lt;prop key="hibernate.connection.url"&gt;jdbc:mysql:///customer1&lt;/prop&gt; &lt;prop key="hibernate.connection.username"&gt;root&lt;/prop&gt; &lt;prop key="hibernate.connection.password"&gt;12345&lt;/prop&gt; &lt;prop key="hibernate.dialect"&gt;org.hibernate.dialect.MySQLDialect&lt;/prop&gt; &lt;!-- 可选配置 --&gt; &lt;prop key="hibernate.show_sql"&gt;true&lt;/prop&gt; &lt;prop key="hibernate.format_sql"&gt;true&lt;/prop&gt; &lt;prop key="hibernate.hbm2ddl.auto"&gt;update&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;!-- 引入orm元数据，指定orm元数据所在的包路径，spring会读取包中的所有配置 --&gt; &lt;property name="mappingDirectoryLocations" value="classpath:my/study/web/domain"&gt;&lt;/property&gt;&lt;/bean&gt; 将方法1中的配置换成上面的配置，进行测试:这样就完成了spring和hibernate的整合 重点总结，整合原理就是将SessionFactory对象交给spring容器管理 spring整合c3p0连接池c3p0的配置如下：123456789&lt;!-- 读取db.properties --&gt;&lt;context:property-placeholder location="classpath:db.properties"/&gt;&lt;!-- 配置c3p0连接池 --&gt;&lt;bean name="dataSource" class="com.mchange.v2.c3p0.ComboPooledDataSource"&gt; &lt;property name="JdbcUrl" value="$&#123;jdbc.JdbcUrl&#125;"&gt;&lt;/property&gt; &lt;property name="DriverClass" value="$&#123;jdbc.DriverClass&#125;"&gt;&lt;/property&gt; &lt;property name="User" value="$&#123;jdbc.User&#125;"&gt;&lt;/property&gt; &lt;property name="Password" value="$&#123;jdbc.Password&#125;"&gt;&lt;/property&gt;&lt;/bean&gt; 将c3p0注入到sessionFactory中,hibernate会通过连接池获得连接:12345678910111213141516&lt;bean name="sessionFactory" class="org.springframework.orm.hibernate5.LocalSessionFactoryBean"&gt; &lt;!-- 配置hibernate基本信息 --&gt; &lt;!-- 将连接池注入到sessionFactory中 --&gt; &lt;property name="dataSource" ref="dataSource"&gt;&lt;/property&gt; &lt;property name="hibernateProperties"&gt; &lt;props&gt; &lt;prop key="hibernate.dialect"&gt;org.hibernate.dialect.MySQLDialect&lt;/prop&gt; &lt;!-- 可选配置 --&gt; &lt;prop key="hibernate.show_sql"&gt;true&lt;/prop&gt; &lt;prop key="hibernate.format_sql"&gt;true&lt;/prop&gt; &lt;prop key="hibernate.hbm2ddl.auto"&gt;update&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;!-- 引入orm元数据，指定orm元数据所在的包路径，spring会读取包中的所有配置 --&gt; &lt;property name="mappingDirectoryLocations" value="classpath:my/study/web/domain"&gt;&lt;/property&gt; &lt;/bean&gt; spring整合hibernate环境操作数据库先创建测试类。创建UserDao接口：12345678910package my.study.web.dao;import my.study.web.domain.User;public interface UserDao &#123; //根据登陆名称查询user对象 User getByUserCode(String usercode); //保存用户 void save(User u);&#125; 然后创建UserDaoImpl实现类，在整合环境中操作数据库，spring提供了一个hibernate模板对象HibernateTemplate.可以让实现类继承HibernateDaoSupport,从而简少Dao的依赖关系,不用再去配置HibernateTemplate.这样在使用模板时，直接使用getHibernateTemplate()方法就可以了.要明确其中的依赖关系：HibernateTemplate通过HibernateDaoSupport创建出来，需要依赖SessionFactory,这个模板中封装的操作都是session中的操作.所以要先为dao注入SessionFactory.1234567891011121314151617181920212223242526272829303132333435363738394041package my.study.web.impl;import org.hibernate.HibernateException;import org.hibernate.Query;import org.hibernate.Session;import org.springframework.orm.hibernate5.HibernateCallback;import org.springframework.orm.hibernate5.support.HibernateDaoSupport;import my.study.web.dao.UserDao;import my.study.web.domain.User;//HibernateDaoSupport 要为dao注入sessionFactorypublic class UserDaoImpl extends HibernateDaoSupport implements UserDao &#123; @Override public User getByUserCode(final String usercode) &#123; //HQL return getHibernateTemplate().execute(new HibernateCallback&lt;User&gt;() &#123; @Override public User doInHibernate(Session session) throws HibernateException &#123; String hql = "from User where user_code = ? "; Query query = session.createQuery(hql); query.setParameter(0, usercode); User user = (User) query.uniqueResult(); return user; &#125; &#125;); //Criteria /*DetachedCriteria dc = DetachedCriteria.forClass(User.class); dc.add(Restrictions.eq("user_code", usercode)); List&lt;User&gt; list = (List&lt;User&gt;) getHibernateTemplate().findByCriteria(dc); if(list != null &amp;&amp; list.size()&gt;0)&#123; return list.get(0); &#125;else&#123; return null; &#125;*/ &#125;&#125; 在spring中配置UserDao,并注入sessionFactory：12345&lt;!-- 配置UserDao --&gt;&lt;bean name="userDao" class="my.study.web.impl.UserDaoImpl"&gt; &lt;!-- 注入sessionFactory --&gt; &lt;property name="sessionFacroty" ref="sessionFactory"&gt;&lt;/property&gt;&lt;/bean&gt; 在HibernateTest.java中添加测试代码：12345678910...@Resource(name="userDao") private UserDao ud; @Test //测试Dao Hibernate模板 public void test3() &#123; User user = ud.getByUserCode("hello"); System.out.println(user); &#125; Spring中的aop事务准备配置事务，最核心的是要配置核心事务管理器.其中包含了所有事务的打开提交关闭操作.所以要先在spring配置核心事务管理器.ctrl+shift+h,搜索一下TransactionManager,可以看到HibernateTransactionManager就是需要的: 将其配置到spring配置中,并且，使用它管理事务，要通过sessionFactory进行管理，所以要配置注入sessionFactory1234&lt;!-- 配置核心事务管理器 --&gt; &lt;bean name="transactionManager" class="org.springframework.orm.hibernate5.HibernateTransactionManager"&gt; &lt;property name="sessionFactory" ref="sessionFactory"&gt;&lt;/property&gt; &lt;/bean&gt; xml配置aop事务准备目标对象-&gt;配置通知-&gt;将配置织入目标对象(配置切点和切面)。在前面已经有了目标对象，所以这里只需要配置通知和织入目标对象.具体可见博客上一篇文章123456789101112131415161718192021&lt;!-- 配置通知 --&gt;&lt;tx:advice id="txAdvice" transaction-manager="transactionManager"&gt; &lt;tx:attributes&gt; &lt;tx:method name="save*" isolation="REPEATABLE_READ" propagation="REQUIRED" read-only="false" /&gt; &lt;tx:method name="persist*" isolation="REPEATABLE_READ" propagation="REQUIRED" read-only="false" /&gt; &lt;tx:method name="update*" isolation="REPEATABLE_READ" propagation="REQUIRED" read-only="false" /&gt; &lt;tx:method name="modify*" isolation="REPEATABLE_READ" propagation="REQUIRED" read-only="false" /&gt; &lt;tx:method name="delete*" isolation="REPEATABLE_READ" propagation="REQUIRED" read-only="false" /&gt; &lt;tx:method name="remove*" isolation="REPEATABLE_READ" propagation="REQUIRED" read-only="false" /&gt; &lt;tx:method name="get*" isolation="REPEATABLE_READ" propagation="REQUIRED" read-only="true" /&gt; &lt;tx:method name="find*" isolation="REPEATABLE_READ" propagation="REQUIRED" read-only="true" /&gt; &lt;/tx:attributes&gt;&lt;/tx:advice&gt;&lt;!-- 配置将通知织入目标对象 --&gt;&lt;aop:config&gt; &lt;!-- 配置切点 --&gt; &lt;aop:pointcut expression="execution(* my.study.web.impl.*ServiceImpl.*(..))" id="transactionPoincut"/&gt; &lt;!-- 配置切面 --&gt; &lt;aop:advisor advice-ref="txAdvice" pointcut-ref="transactionPoincut"/&gt;&lt;/aop:config&gt; 测试：在UserService接口中添加方法:1void saveUser(User u); 在UserDao接口中添加方法:12//保存用户 void save(User u); 在UserDaoImpl中添加方法:1234@Overridepublic void save(User u) &#123; getHibernateTemplate().save(u);&#125; 在UserServiceImpl中添加方法:123456789private UserDao ud;@Overridepublic void saveUser(User u) &#123; ud.save(u); &#125;//为ud添加setter方法public void setUd(UserDao ud) &#123; this.ud = ud;&#125; 在此之前，要在spring中的userService下将userDao注入进去:1234&lt;!-- servie配置 --&gt; &lt;bean name="userService" class="my.study.web.impl.UserServiceImpl"&gt; &lt;property name="ud" ref="userDao"&gt;&lt;/property&gt; &lt;/bean&gt; 测试代码：123456789101112@Resource(name="userService")private UserService us;@Test//测试aop事务public void test4() &#123; User u = new User(); u.setUser_code("aoptest"); u.setUser_name("aop测试"); u.setUser_password("aop123"); us.saveUser(u);&#125; 注解配置aop事务在spring配置文件中，首先开启注解事务:12&lt;!-- 开启注解事务 --&gt;&lt;tx:annotation-driven transaction-manager="transactionManager"/&gt; 改写UserServiceImpl:12345678910111213141516171819202122232425262728293031package my.study.web.impl;import org.springframework.transaction.annotation.Isolation;import org.springframework.transaction.annotation.Propagation;import org.springframework.transaction.annotation.Transactional;import my.study.web.dao.UserDao;import my.study.web.domain.User;import my.study.web.service.UserService;@Transactional(isolation=Isolation.REPEATABLE_READ,propagation=Propagation.REQUIRED,readOnly=true)public class UserServiceImpl implements UserService &#123; private UserDao ud; @Override public User getUserByCodePassword(User u) &#123; System.out.println("getUserByCodePassword"); return null; &#125; @Override @Transactional(isolation=Isolation.REPEATABLE_READ,propagation=Propagation.REQUIRED,readOnly=false) public void saveUser(User u) &#123; ud.save(u); &#125; public void setUd(UserDao ud) &#123; this.ud = ud; &#125; &#125; 测试:123456789101112@Resource(name="userService") private UserService us; @Test //测试aop事务 public void test4() &#123; User u = new User(); u.setUser_code("aop注解test"); u.setUser_name("aop测试注解"); u.setUser_password("aop123"); us.saveUser(u); &#125; 扩大session作用范围为了避免使用懒加载时出现no-session问题.需要扩大session的作用范围 在web.xml配置filter:123456789101112 &lt;!-- 扩大session作用域范围 注意:任何filter一定要在struts的filter之前调用 --&gt;&lt;filter&gt; &lt;filter-name&gt;openSessionInView&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.orm.hibernate5.support.OpenSessionInViewFilter&lt;/filter-class&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;openSessionInView&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 注意:任何filter一定要在struts的filter之前调用,所以要放在struts配置前面 配置完毕 实现用户登陆功能Web层改写UserAction方法:12345678910111213141516171819202122232425262728293031package my.study.web.action;import com.opensymphony.xwork2.ActionContext;import com.opensymphony.xwork2.ActionSupport;import com.opensymphony.xwork2.ModelDriven;import my.study.web.domain.User;import my.study.web.service.UserService;public class UserAction extends ActionSupport implements ModelDriven&lt;User&gt; &#123; private User user = new User(); private UserService userService; public void setUserService(UserService userService) &#123; this.userService = userService; &#125; public String login() throws Exception&#123; //1. 调用service执行登陆逻辑 User u = userService.getUserByCodePassword(user); //2. 将返回的user对象放到session域中 ActionContext.getContext().getSession().put("user", u); //3. 重定向到项目首页 return "toHome"; &#125; @Override public User getModel() &#123; return user; &#125;&#125; 改写struts.xml配置，改变跳转地址:1234567891011121314&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE struts PUBLIC "-//Apache Software Foundation//DTD Struts Configuration 2.3//EN" "http://struts.apache.org/dtds/struts-2.3.dtd"&gt; &lt;struts&gt; &lt;constant name="struts.objectFactory" value="spring"&gt; &lt;/constant&gt; &lt;package name="ssh" namespace="/" extends="struts-default"&gt; &lt;action name="UserAction_*" class="my.study.web.action.UserAction" method="&#123;1&#125;"&gt; &lt;result name="toHome"&gt;/index.html&lt;/result&gt; &lt;/action&gt; &lt;/package&gt;&lt;/struts&gt; Service层改写UserServiceImpl中的方法:12345678910111213141516171819... @Override public User getUserByCodePassword(User u) &#123; //1. 根据登录名称查询登陆用户 User exitU = ud.getByUserCode(u.getUser_code()); //2. 判断用户是否存在-&gt;不存在，抛出异常，提示用户名不存在 // -&gt;存在 // -&gt;密码错误，抛出异常，提示密码错误 if(exitU==null) &#123; throw new RuntimeException("用户不存在！"); &#125; if(!exitU.getUser_password().equals(u.getUser_password())) &#123; throw new RuntimeException("密码错误！"); &#125; //3.返回查询到的用户对象 return exitU; &#125;... 注意如果需要抛出异常，需要在struts.xml中配置:12345678910111213&lt;struts&gt; &lt;constant name="struts.objectFactory" value="spring"&gt; &lt;/constant&gt; &lt;package name="ssh" namespace="/" extends="struts-default"&gt; &lt;global-exception-mappings &gt; &lt;exception-mapping result="error" exception="java.lang.RuntimeException"&gt;&lt;/exception-mapping&gt; &lt;/global-exception-mappings&gt; &lt;action name="UserAction_*" class="my.study.web.action.UserAction" method="&#123;1&#125;"&gt; &lt;result name="toHome"&gt;/index.html&lt;/result&gt; &lt;result name="error"&gt;/login.jsp&lt;/result&gt; &lt;/action&gt; &lt;/package&gt;&lt;/struts&gt; dao层dao层在上面已经实现好了 测试用的页面login.jsp:12345678910111213141516171819&lt;%@ page language="java" contentType="text/html; charset=UTF-8" pageEncoding="ISO-8859-1"%&gt;&lt;%@ taglib prefix="s" uri="/struts-tags" %&gt;&lt;!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd"&gt;&lt;html&gt;&lt;head&gt;&lt;meta http-equiv="Content-Type" content="text/html; charset=UTF-8"&gt;&lt;title&gt;Insert title here&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;form action="$&#123;pageContext.request.contextPath&#125;/UserAction_login" method=post&gt; &lt;label&gt;username:&lt;/label&gt;&lt;input type="text" name="user_code"/&gt; &lt;br&gt; &lt;label&gt;password:&lt;/label&gt;&lt;input type=password name="user_password"/&gt; &lt;input type="submit" value="submit"/&gt; &lt;/form&gt; &lt;font color="red"&gt;&lt;s:property value="exception.message"&gt;&lt;/s:property&gt;&lt;/font&gt;&lt;/body&gt;&lt;/html&gt; 12345678910&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset="UTF-8"&gt;&lt;title&gt;Insert title here&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;首页&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt; 项目总览]]></content>
      <tags>
        <tag>Spring</tag>
        <tag>SSH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring学习笔记-三]]></title>
    <url>%2F2018%2F11%2F13%2FSpring%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%B8%89%2F</url>
    <content type="text"><![CDATA[Spring结合JDBC其实，Spring中提供了一个可以操作数据库的对象，这个对象封装了各种JDBC技术，可以用它来操作数据库.——JDBCtemplate需要的依赖包： 不使用Spring操作数据库123456789101112131415161718192021222324252627package my.study.g_jdbcTemplate;import java.beans.PropertyVetoException;import org.junit.Test;import org.springframework.jdbc.core.JdbcTemplate;import com.mchange.v2.c3p0.ComboPooledDataSource;public class Demo &#123; @Test public void test1() throws Exception &#123; //1. 准备连接池 ComboPooledDataSource dataSource = new ComboPooledDataSource(); dataSource.setDriverClass("com.mysql.jdbc.Driver"); dataSource.setJdbcUrl("jdbc:mysql:///你的数据库名"); dataSource.setUser("root"); dataSource.setPassword("12345"); //2. 创建JDBC模板对象 JdbcTemplate jt = new JdbcTemplate(); jt.setDataSource(dataSource); //3. 书写SQL语句 String sql = "insert into t_user values(null,'hey')"; jt.update(sql); &#125;&#125; 可以去数据库中自己查看是否添加成功 使用Spring操作数据库1.使用JDBC模板完成增删改查定义UserDao接口123456789101112package my.study.g_jdbcTemplate;import java.util.List;public interface UserDao &#123; void save(User u); void delete(Integer id); void update(User u); User getById(Integer id); int getTotalCount(); List&lt;User&gt; getAll();&#125; 定义User类123456789101112131415161718package my.study.g_jdbcTemplate;public class User &#123; private Integer id; private String name; public Integer getId() &#123; return id; &#125; public void setId(Integer id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125;&#125; 定义UserDaoImpl类，书写方法:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475package my.study.g_jdbcTemplate;import java.sql.ResultSet;import java.sql.SQLException;import java.util.List;import org.springframework.jdbc.core.JdbcTemplate;import org.springframework.jdbc.core.RowMapper;public class UserDaoImpl implements UserDao &#123; //定义一个JdbcTemplate用来对SQL语句进行操作 private JdbcTemplate jt; @Override public void save(User u) &#123; String sql = "insert into t_user values(null,?)"; jt.update(sql, u.getName()); &#125; @Override public void delete(Integer id) &#123; String sql = "delete from t_user where id = ?"; jt.update(sql, id); &#125; @Override public void update(User u) &#123; String sql = "update t_user set name = ? where id = ?"; jt.update(sql, u.getName(),u.getId()); &#125; @Override public User getById(Integer id) &#123; String sql = "select * from t_user where id = ?"; User u = jt.queryForObject(sql, new RowMapper&lt;User&gt;() &#123; @Override public User mapRow(ResultSet rs, int arg1) throws SQLException &#123; User u = new User(); u.setId(rs.getInt("id")); u.setName(rs.getString("name")); return u; &#125; &#125;, id); return u; &#125; @Override public int getTotalCount() &#123; String sql = "select count(*) from t_user"; Integer count = jt.queryForObject(sql,Integer.class); return count; &#125; @Override public List&lt;User&gt; getAll() &#123; String sql = "select * from t_user"; List&lt;User&gt; list = jt.query(sql, new RowMapper&lt;User&gt;() &#123; @Override public User mapRow(ResultSet rs, int arg1) throws SQLException &#123; User u = new User(); u.setId(rs.getInt("id")); u.setName(rs.getString("name")); return u; &#125; &#125;); return list; &#125; public void setJt(JdbcTemplate jt) &#123; this.jt = jt; &#125;&#125; 然后需要配置到Spring中，让Spring管理：在配置文件中配置：123456789101112131415161718&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.2.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.2.xsd &quot;&gt; &lt;!-- 1.将DataSource连接池放入Spring容器 --&gt; &lt;bean name=&quot;dataSource&quot; class=&quot;com.mchange.v2.c3p0.ComboPooledDataSource&quot;&gt; &lt;property name=&quot;JdbcUrl&quot; value=&quot;jdbc:mysql:///hibernate_32&quot;&gt;&lt;/property&gt; &lt;property name=&quot;DriverClass&quot; value=&quot;com.mysql.jdbc.Driver&quot;&gt;&lt;/property&gt; &lt;property name=&quot;User&quot; value=&quot;root&quot;&gt;&lt;/property&gt; &lt;property name=&quot;Password&quot; value=&quot;12345&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 2.将JDBCTemplate放入Spring容器 --&gt; &lt;bean name=&quot;JdbcTemplate&quot; class=&quot;org.springframework.jdbc.core.JdbcTemplate&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 3.将UserDao放入Spring容器 --&gt; &lt;bean name=&quot;userDao&quot; class=&quot;my.study.g_jdbcTemplate.UserDaoImpl&quot;&gt; &lt;property name=&quot;jt&quot; ref=&quot;JdbcTemplate&quot;&gt;&lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; 测试代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172package my.study.g_jdbcTemplate;import java.beans.PropertyVetoException;import javax.annotation.Resource;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.jdbc.core.JdbcTemplate;import org.springframework.test.context.ContextConfiguration;import org.springframework.test.context.junit4.SpringJUnit4ClassRunner;import com.mchange.v2.c3p0.ComboPooledDataSource;@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration("classpath:my/study/g_jdbcTemplate/applicationContext.xml")public class Demo &#123; @Resource(name="userDao") private UserDao ud; @Test public void test1() throws Exception &#123; //1. 准备连接池 ComboPooledDataSource dataSource = new ComboPooledDataSource(); dataSource.setDriverClass("com.mysql.jdbc.Driver"); dataSource.setJdbcUrl("jdbc:mysql:///hibernate_32"); dataSource.setUser("root"); dataSource.setPassword("12345"); //2. 创建JDBC模板对象 JdbcTemplate jt = new JdbcTemplate(); jt.setDataSource(dataSource); //3. 书写SQL语句 String sql = "insert into t_user values(null,'hey')"; jt.update(sql); &#125; @Test public void test2() throws Exception &#123; User u = new User(); u.setName("hello"); ud.save(u); &#125; @Test public void test3() throws Exception &#123; User u = new User(); u.setId(2); u.setName("nihao"); ud.update(u); &#125; @Test public void test4() throws Exception &#123; ud.delete(2); &#125; @Test public void test5() throws Exception &#123; int totalCount = ud.getTotalCount(); System.out.println(totalCount); &#125; @Test public void test6() throws Exception &#123; System.out.println(ud.getById(2)); &#125; @Test public void test7() throws Exception &#123; System.out.println(ud.getAll()); &#125;&#125; 另外，可以让UserDaoImpl继承JDBCDaoSupport，父类可以根据连接池自己创建JDBC模板.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package my.study.g_jdbcTemplate;import java.sql.ResultSet;import java.sql.SQLException;import java.util.List;import org.springframework.jdbc.core.JdbcTemplate;import org.springframework.jdbc.core.RowMapper;import org.springframework.jdbc.core.support.JdbcDaoSupport;public class UserDaoImpl extends JdbcDaoSupport implements UserDao &#123; @Override public void save(User u) &#123; String sql = "insert into t_user values(null,?) "; super.getJdbcTemplate().update(sql, u.getName()); &#125; @Override public void delete(Integer id) &#123; String sql = "delete from t_user where id = ? "; super.getJdbcTemplate().update(sql,id); &#125; @Override public void update(User u) &#123; String sql = "update t_user set name = ? where id=? "; super.getJdbcTemplate().update(sql, u.getName(),u.getId()); &#125; @Override public User getById(Integer id) &#123; String sql = "select * from t_user where id = ? "; return super.getJdbcTemplate().queryForObject(sql,new RowMapper&lt;User&gt;()&#123; @Override public User mapRow(ResultSet rs, int arg1) throws SQLException &#123; User u = new User(); u.setId(rs.getInt("id")); u.setName(rs.getString("name")); return u; &#125;&#125;, id); &#125; @Override public int getTotalCount() &#123; String sql = "select count(*) from t_user "; Integer count = super.getJdbcTemplate().queryForObject(sql, Integer.class); return count; &#125; @Override public List&lt;User&gt; getAll() &#123; String sql = "select * from t_user "; List&lt;User&gt; list = super.getJdbcTemplate().query(sql, new RowMapper&lt;User&gt;()&#123; @Override public User mapRow(ResultSet rs, int arg1) throws SQLException &#123; User u = new User(); u.setId(rs.getInt("id")); u.setName(rs.getString("name")); return u; &#125;&#125;); return list; &#125;&#125; 那么在配置文件中可以简化配置，不需要配置JDBCTemplate。1234567891011121314&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.2.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.2.xsd &quot;&gt; &lt;bean name=&quot;dataSource&quot; class=&quot;com.mchange.v2.c3p0.ComboPooledDataSource&quot;&gt; &lt;property name=&quot;JdbcUrl&quot; value=&quot;jdbc:mysql:///hibernate_32&quot;&gt;&lt;/property&gt; &lt;property name=&quot;DriverClass&quot; value=&quot;com.mysql.jdbc.Driver&quot;&gt;&lt;/property&gt; &lt;property name=&quot;User&quot; value=&quot;root&quot;&gt;&lt;/property&gt; &lt;property name=&quot;Password&quot; value=&quot;12345&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;bean name=&quot;userDao&quot; class=&quot;my.study.g_jdbcTemplate.UserDaoImpl&quot; &gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot; &gt;&lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; Spring中整合properties配置:新建db.properties：1234JdbcUrl=jdbc:mysql:///hibernate_32DriverClass=com.mysql.jdbc.DriverUser=rootPassword=12345 更改配置文件中的配置:1234567891011... &lt;!-- 指定Properties的位置 --&gt; &lt;context:property-placeholder location=&quot;classpath:db.properties&quot;/&gt; &lt;!-- 1.将DataSource连接池放入Spring容器 --&gt; &lt;bean name=&quot;dataSource&quot; class=&quot;com.mchange.v2.c3p0.ComboPooledDataSource&quot;&gt; &lt;property name=&quot;JdbcUrl&quot; value=&quot;$&#123;jdbc.JdbcUrl&#125;&quot;&gt;&lt;/property&gt; &lt;property name=&quot;DriverClass&quot; value=&quot;$&#123;jdbc.DriverClass&#125;&quot;&gt;&lt;/property&gt; &lt;property name=&quot;User&quot; value=&quot;$&#123;jdbc.User&#125;&quot;&gt;&lt;/property&gt; &lt;property name=&quot;Password&quot; value=&quot;$&#123;jdbc.Password&#125;&quot;&gt;&lt;/property&gt; &lt;/bean&gt;... Spring中的aop事务事务相关概念事务的特性：ACID ⑴ 原子性（Atomicity） 原子性是指事务包含的所有操作要么全部成功，要么全部失败回滚，因此事务的操作如果成功就必须要完全应用到数据库，如果操作失败则不能对数据库有任何影响。 ⑵ 一致性（Consistency） 一致性是指事务必须使数据库从一个一致性状态变换到另一个一致性状态，也就是说一个事务执行之前和执行之后都必须处于一致性状态。 拿转账来说，假设用户A和用户B两者的钱加起来一共是5000，那么不管A和B之间如何转账，转几次账，事务结束后两个用户的钱相加起来应该还得是5000，这就是事务的一致性。 ⑶ 隔离性（Isolation） 隔离性是当多个用户并发访问数据库时，比如操作同一张表时，数据库为每一个用户开启的事务，不能被其他事务的操作所干扰，多个并发事务之间要相互隔离。 即要达到这么一种效果：对于任意两个并发的事务T1和T2，在事务T1看来，T2要么在T1开始之前就已经结束，要么在T1结束之后才开始，这样每个事务都感觉不到有其他事务在并发地执行。 关于事务的隔离性数据库提供了多种隔离级别。 ⑷ 持久性（Durability） 持久性是指一个事务一旦被提交了，那么对数据库中的数据的改变就是永久性的，即便是在数据库系统遇到故障的情况下也不会丢失提交事务的操作。 例如我们在使用JDBC操作数据库时，在提交事务方法后，提示用户事务操作完成，当我们程序执行完成直到看到提示后，就可以认定事务以及正确提交，即使这时候数据库出现了问题，也必须要将我们的事务完全执行完成，否则就会造成我们看到提示事务处理完毕，但是数据库因为故障而没有执行事务的重大错误。 事务并发问题： 1.脏读 脏读是指在一个事务处理过程里读取了另一个未提交的事务中的数据。2.不可重复读 不可重复读是指在对于数据库中的某个数据，一个事务范围内多次查询却返回了不同的数据值，这是由于在查询间隔，被另一个事务修改并提交了。 例如事务T1在读取某一数据，而事务T2立马修改了这个数据并且提交事务给数据库，事务T1再次读取该数据就得到了不同的结果，发送了不可重复读。 不可重复读和脏读的区别是，脏读是某一事务读取了另一个事务未提交的脏数据，而不可重复读则是读取了前一事务提交的数据。3.虚读(幻读) 幻读是事务非独立执行时发生的一种现象。例如事务T1对一个表中所有的行的某个数据项做了从“1”修改为“2”的操作，这时事务T2又对这个表中插入了一行数据项，而这个数据项的数值还是为“1”并且提交给数据库。而操作事务T1的用户如果再查看刚刚修改的数据，会发现还有一行没有修改，其实这行是从事务T2中添加的，就好像产生幻觉一样，这就是发生了幻读。 幻读和不可重复读都是读取了另一条已经提交的事务（这点就脏读不同），所不同的是不可重复读查询的都是同一个数据项，而幻读针对的是一批数据整体（比如数据的个数）。 现在来看看MySQL数据库为我们提供的四种隔离级别： ① Serializable (串行化)：可避免脏读、不可重复读、幻读的发生。 ② Repeatable read (可重复读)：可避免脏读、不可重复读的发生。 ③ Read committed (读已提交)：可避免脏读的发生。 ④ Read uncommitted (读未提交)：最低级别，任何情况都无法保证。 以上四种隔离级别最高的是Serializable级别，最低的是Read uncommitted级别，当然级别越高，执行效率就越低。像Serializable这样的级别，就是以锁表的方式(类似于Java多线程中的锁)使得其他的线程只能在锁外等待，所以平时选用何种隔离级别应该根据实际情况。在MySQL数据库中默认的隔离级别为Repeatable read (可重复读)。 Spring提供了PlatformTransactionManager接口，封装了事务操作对象的方法.对不同的平台，有不同的实现类.如:JDBC平台–DataSourceTransactionManager;Hibernate平台—TransactionManager..等.在Spring中使用事务管理，最核心的就是使用PlatformTransactionManager对象。 Spring事务管理可以管理事务的隔离级别、是否只读、事务的传播行为(比如在一个service方法中调用另一个service方法)、 事务的传播行为包括： 保证同一个事务中 PROPAGATION_REQUIRED 支持当前事务，如果不存在 就新建一个(默认) PROPAGATION_SUPPORTS 支持当前事务，如果不存在，就不使用事务 PROPAGATION_MANDATORY 支持当前事务，如果不存在，抛出异常 保证没有在同一个事务中 PROPAGATION_REQUIRES_NEW 如果有事务存在，挂起当前事务，创建一个新的事务 PROPAGATION_NOT_SUPPORTED 以非事务方式运行，如果有事务存在，挂起当前事务 PROPAGATION_NEVER 以非事务方式运行，如果有事务存在，抛出异常 PROPAGATION_NESTED 如果当前事务存在，则嵌套事务执行 定义场景并书写代码不添加事务部分定义一个转钱的场景，自行建一个表，包含id,姓名和钱数。定义接口,包含加钱和减钱两个方法：1234567package my.study.dao;public interface AccountDao &#123; public void increaseMoney(Integer id,Double money); public void decreaseMoney(Integer id,Double money);&#125; 定义实现类：12345678910111213141516package my.study.dao;import org.springframework.jdbc.core.support.JdbcDaoSupport;public class AccountDaoImpl extends JdbcDaoSupport implements AccountDao &#123; @Override public void increaseMoney(Integer id, Double money) &#123; getJdbcTemplate().update("update t_account set money = money + ? where id = ?", money,id); &#125; @Override public void decreaseMoney(Integer id, Double money) &#123; getJdbcTemplate().update("update t_account set money = money-? where id = ? ", money,id); &#125;&#125; 定义service方法,包含转账方法：12345package my.study.service;public interface AccountService &#123; void transfer(Integer from,Integer to,Double money);&#125; 定义service方法实现类：123456789101112131415161718package my.study.service;import my.study.dao.AccountDao;public class AccountServiceImpl implements AccountService &#123; private AccountDao ad ; @Override public void transfer(final Integer from,final Integer to,final Double money) &#123; //减钱 ad.decreaseMoney(from, money); //加钱 ad.increaseMoney(to, money); &#125; public void setAd(AccountDao ad) &#123; this.ad = ad; &#125;&#125; 书写配置文件:1234567891011121314151617181920&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.2.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.2.xsd &quot;&gt; &lt;!-- 指定Properties的位置 --&gt; &lt;context:property-placeholder location=&quot;classpath:db.properties&quot;/&gt; &lt;!-- 1.将DataSource连接池放入Spring容器 --&gt; &lt;bean name=&quot;dataSource&quot; class=&quot;com.mchange.v2.c3p0.ComboPooledDataSource&quot;&gt; &lt;property name=&quot;JdbcUrl&quot; value=&quot;$&#123;jdbc.JdbcUrl&#125;&quot;&gt;&lt;/property&gt; &lt;property name=&quot;DriverClass&quot; value=&quot;$&#123;jdbc.DriverClass&#125;&quot;&gt;&lt;/property&gt; &lt;property name=&quot;User&quot; value=&quot;$&#123;jdbc.User&#125;&quot;&gt;&lt;/property&gt; &lt;property name=&quot;Password&quot; value=&quot;$&#123;jdbc.Password&#125;&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 2.AccountDao --&gt; &lt;bean name=&quot;accountDao&quot; class=&quot;my.study.dao.AccountDaoImpl&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 3.将AccountService放入Spring容器 --&gt; &lt;bean name=&quot;accountService&quot; class=&quot;my.study.service.AccountServiceImpl&quot;&gt; &lt;property name=&quot;ad&quot; ref=&quot;accountDao&quot;&gt;&lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; 书写代码测试:1234567891011121314151617181920package my.study.dao;import javax.annotation.Resource;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.test.context.ContextConfiguration;import org.springframework.test.context.junit4.SpringJUnit4ClassRunner;import my.study.service.AccountService;@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration("classpath:my/study/dao/applicationContext.xml")public class Demo &#123; @Resource(name="accountService") private AccountService as; @Test public void test1()&#123; as.transfer(1, 2, 100d); &#125;&#125; 刷新查看数据库结果: 这是没有添加事务的程序.没有添加事务，就容易发生各种问题(前面提到的事务的问题)。 添加事务首先需要配置核心事务管理器,它依赖于连接池：123456... &lt;!-- 事务核心管理器,封装了所有事务操作. 依赖于连接池 --&gt; &lt;bean name=&quot;transactionManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot; &gt;&lt;/property&gt; &lt;/bean&gt;... 编码式需要在代码中进行事务管理，需要书写重复代码。并不推荐.编码式需要事务模板对象，在xml中配置,要配置tt属性:12345678910...&lt;!-- 事务模板对象 --&gt; &lt;bean name=&quot;transactionTemplate&quot; class=&quot;org.springframework.transaction.support.TransactionTemplate&quot; &gt; &lt;property name=&quot;transactionManager&quot; ref=&quot;transactionManager&quot; &gt;&lt;/property&gt; &lt;/bean&gt;... &lt;bean name=&quot;accountService&quot; class=&quot;my.study.service.AccountServiceImpl&quot; &gt; &lt;property name=&quot;ad&quot; ref=&quot;accountDao&quot; &gt;&lt;/property&gt; &lt;property name=&quot;tt&quot; ref=&quot;transactionTemplate&quot; &gt;&lt;/property&gt;&lt;/bean&gt; 使用编码式，需要在service中声明一个TransactionTemplate,并生成set方法,然后定义execute()方法，实现接口，在doInTransactionWithoutResult方法中书写事务操作的代码:123456789101112131415161718192021222324252627282930313233343536package my.study.service;import org.springframework.transaction.TransactionStatus;import org.springframework.transaction.support.TransactionCallbackWithoutResult;import org.springframework.transaction.support.TransactionTemplate;import my.study.dao.AccountDao;public class AccountServiceImpl implements AccountService &#123; private AccountDao ad ; private TransactionTemplate tt; @Override public void transfer(final Integer from,final Integer to,final Double money) &#123; tt.execute(new TransactionCallbackWithoutResult() &#123; @Override protected void doInTransactionWithoutResult(TransactionStatus arg0) &#123; //减钱 ad.decreaseMoney(from, money); //加钱 ad.increaseMoney(to, money); &#125; &#125;); &#125; public void setAd(AccountDao ad) &#123; this.ad = ad; &#125; public void setTt(TransactionTemplate tt) &#123; this.tt = tt; &#125;&#125; execute方法的执行步骤：打开事务-&gt;执行doInTransactionWithoutResult方法-&gt;提交事务.在doInTransactionWithoutResult方法中提供了try catch语句，如果发生错误则进行回滚操作。 xml配置配置AOP事务，AOP事务的配置和效果如图：进行xml配置之前，还需要导入新的命名空间约束，tx约束,并在XMLEditor下添加这个约束和aop约束:导入方法见Spring学习笔记-一和Spring学习笔记-二中相关部分。最终需要的配置如下： xml中的各个配置：beans:最基本的根元素context:读取properties配置文件aop:配置AOP，将通知织入目标对象tx:配置事务通知 配置事务通知接下来配置事务通知，是以方法为单位进行配置的，在配置文件中新增配置事务的部分：123456&lt;!-- 配置事务通知 --&gt; &lt;tx:advice id=&quot;txAdvice&quot; transaction-manager=&quot;transactionManager&quot;&gt; &lt;tx:attributes&gt; &lt;tx:method name=&quot;transfer&quot; isolation=&quot;REPEATABLE_READ&quot; propagation=&quot;REQUIRED&quot; read-only=&quot;false&quot;&gt;&lt;/tx:method&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; name为事务的方法名isolation:隔离级别: - DEFAULT - READ_UNCOMMITTED - READ_COMMITTED - REPEATABLE_READ - SERIALIZABLE propagation:传播行为： - REQUIRED - SUPPORTS - MANDATORY - REQUIRES_NEW - NOT_SUPPORTED - NEVER - NESTED read-only:”false”,这个就不多解释了吧 当然，配置的name方法名可以用*通配符来进行匹配相似的方法名，可以使用的例子如下：123456789101112&lt;tx:advice id=&quot;txAdvice&quot; transaction-manager=&quot;transactionManager&quot;&gt; &lt;tx:attributes&gt; &lt;tx:method name=&quot;save*&quot; isolation=&quot;REPEATABLE_READ&quot; propagation=&quot;REQUIRED&quot; read-only=&quot;false&quot; /&gt; &lt;tx:method name=&quot;persist*&quot; isolation=&quot;REPEATABLE_READ&quot; propagation=&quot;REQUIRED&quot; read-only=&quot;false&quot; /&gt; &lt;tx:method name=&quot;update*&quot; isolation=&quot;REPEATABLE_READ&quot; propagation=&quot;REQUIRED&quot; read-only=&quot;false&quot; /&gt; &lt;tx:method name=&quot;modify*&quot; isolation=&quot;REPEATABLE_READ&quot; propagation=&quot;REQUIRED&quot; read-only=&quot;false&quot; /&gt; &lt;tx:method name=&quot;delete*&quot; isolation=&quot;REPEATABLE_READ&quot; propagation=&quot;REQUIRED&quot; read-only=&quot;false&quot; /&gt; &lt;tx:method name=&quot;remove*&quot; isolation=&quot;REPEATABLE_READ&quot; propagation=&quot;REQUIRED&quot; read-only=&quot;false&quot; /&gt; &lt;tx:method name=&quot;get*&quot; isolation=&quot;REPEATABLE_READ&quot; propagation=&quot;REQUIRED&quot; read-only=&quot;true&quot; /&gt; &lt;tx:method name=&quot;find*&quot; isolation=&quot;REPEATABLE_READ&quot; propagation=&quot;REQUIRED&quot; read-only=&quot;true&quot; /&gt; &lt;tx:method name=&quot;transfer&quot; isolation=&quot;REPEATABLE_READ&quot; propagation=&quot;REQUIRED&quot; read-only=&quot;false&quot;&gt;&lt;/tx:method&gt; &lt;/tx:advice&gt; 注意其中get和find的只读为true 配置将事务织入目标对象配置完事务通知要进行配置织入:1234567&lt;!-- 配置织入 --&gt; &lt;aop:config &gt; &lt;!-- 配置切点表达式 --&gt; &lt;aop:pointcut expression=&quot;execution(* my.study.service.*ServiceImpl.*(..))&quot; id=&quot;txPc&quot;/&gt; &lt;!-- 配置切面 --&gt; &lt;aop:advisor advice-ref=&quot;txAdvice&quot; pointcut-ref=&quot;txPc&quot;/&gt; &lt;/aop:config&gt; 其中切面的构成是通知加切点,advice-ref：通知的名称;pointcut-ref：切点的名称.完整的xml配置:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.2.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.2.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-4.2.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-4.2.xsd &quot;&gt; &lt;!-- 指定Properties的位置 --&gt; &lt;context:property-placeholder location=&quot;classpath:db.properties&quot;/&gt; &lt;!-- 事务核心管理器,封装了所有事务操作. 依赖于连接池 --&gt; &lt;bean name=&quot;transactionManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot; &gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 事务模板对象 --&gt; &lt;bean name=&quot;transactionTemplate&quot; class=&quot;org.springframework.transaction.support.TransactionTemplate&quot; &gt; &lt;property name=&quot;transactionManager&quot; ref=&quot;transactionManager&quot; &gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 配置事务通知 --&gt; &lt;tx:advice id=&quot;txAdvice&quot; transaction-manager=&quot;transactionManager&quot;&gt; &lt;tx:attributes&gt; &lt;!-- name为方法名 isolation:隔离级别: - DEFAULT - READ_UNCOMMITTED - READ_COMMITTED - REPEATABLE_READ - SERIALIZABLE propagation:传播行为： - REQUIRED - SUPPORTS - MANDATORY - REQUIRES_NEW - NOT_SUPPORTED - NEVER - NESTED --&gt; &lt;tx:method name=&quot;transfer&quot; isolation=&quot;REPEATABLE_READ&quot; propagation=&quot;REQUIRED&quot; read-only=&quot;false&quot; &gt;&lt;/tx:method&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; &lt;!-- 配置织入 --&gt; &lt;aop:config &gt; &lt;!-- 配置切点表达式 --&gt; &lt;aop:pointcut expression=&quot;execution(* my.study.service.*ServiceImpl.*(..))&quot; id=&quot;txPc&quot;/&gt; &lt;!-- 配置切面 advice-ref：通知的名称 pointcut-ref：切点的名称 --&gt; &lt;aop:advisor advice-ref=&quot;txAdvice&quot; pointcut-ref=&quot;txPc&quot;/&gt; &lt;/aop:config&gt; &lt;!-- 1.将DataSource连接池放入Spring容器 --&gt; &lt;bean name=&quot;dataSource&quot; class=&quot;com.mchange.v2.c3p0.ComboPooledDataSource&quot;&gt; &lt;property name=&quot;JdbcUrl&quot; value=&quot;$&#123;jdbc.JdbcUrl&#125;&quot;&gt;&lt;/property&gt; &lt;property name=&quot;DriverClass&quot; value=&quot;$&#123;jdbc.DriverClass&#125;&quot;&gt;&lt;/property&gt; &lt;property name=&quot;User&quot; value=&quot;$&#123;jdbc.User&#125;&quot;&gt;&lt;/property&gt; &lt;property name=&quot;Password&quot; value=&quot;$&#123;jdbc.Password&#125;&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 2.AccountDao --&gt; &lt;bean name=&quot;accountDao&quot; class=&quot;my.study.dao.AccountDaoImpl&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 3.将AccountService放入Spring容器 --&gt; &lt;bean name=&quot;accountService&quot; class=&quot;my.study.service.AccountServiceImpl&quot;&gt; &lt;property name=&quot;ad&quot; ref=&quot;accountDao&quot;&gt;&lt;/property&gt; &lt;property name=&quot;tt&quot; ref=&quot;transactionTemplate&quot;&gt;&lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; 注解配置注解配置只需将上面的编制织入和配置事务通知替换为:12&lt;!-- 开启注解管理AOP事务 --&gt; &lt;tx:annotation-driven/&gt; 配置完成后可以使用注解配置事务：1234567891011121314151617181920212223242526272829303132package my.study.service;import org.springframework.transaction.TransactionStatus;import org.springframework.transaction.annotation.Isolation;import org.springframework.transaction.annotation.Propagation;import org.springframework.transaction.annotation.Transactional;import org.springframework.transaction.support.TransactionCallbackWithoutResult;import org.springframework.transaction.support.TransactionTemplate;import my.study.dao.AccountDao;public class AccountServiceImpl implements AccountService &#123; private AccountDao ad ; private TransactionTemplate tt; @Override @Transactional(isolation=Isolation.REPEATABLE_READ,propagation=Propagation.REQUIRED,readOnly=false) public void transfer(final Integer from,final Integer to,final Double money) &#123; //减钱 ad.decreaseMoney(from, money); // int i = 1/0; //加钱 ad.increaseMoney(to, money); &#125; public void setAd(AccountDao ad) &#123; this.ad = ad; &#125; public void setTt(TransactionTemplate tt) &#123; this.tt = tt; &#125;&#125; 可以在类之前加上注解配置，那么这个类的所有方法都遵循这个配置。如果有某个方法和类声明的注解配置不一样，则再这个方法前面声明一下注解配置即可。 可以再书写一个测试类进行测试，注意要读取正确的配置文件（即开启了注解配置的文件）]]></content>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring学习笔记-二]]></title>
    <url>%2F2018%2F11%2F10%2FSpring%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[使用注解配置Spring导入新的命名空间和依赖包导入方法参见Spring学习笔记-一，与导入beans命名空间类似。先导入spring-context-4.2.xsd，然后进行配置,还要在xml的Dsign视图下进行配置，在Prefix中填写context.同时需要导入依赖包spring-aop-4.2.4.RELEASE.jar. 使用注解代理配置文件12345&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://www.springframework.org/schema/beans" xmlns:context="http://www.springframework.org/schema/context" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.2.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.2.xsd "&gt; &lt;!-- 扫描指定包名及其子包下的所有注解 --&gt; &lt;context:component-scan base-package="my.study.bean2"&gt;&lt;/context:component-scan&gt;&lt;/beans&gt; 在类中使用注解进行配置1234567891011121314151617181920212223242526272829303132333435package my.study.bean2;import org.springframework.stereotype.Component;@Component("user")public class User &#123; private String name; private Integer age; private Car car; public User()&#123; System.out.println("User 的空参构造方法被调用"); &#125; public Car getCar() &#123; return car; &#125; public void setCar(Car car) &#123; this.car = car; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public Integer getAge() &#123; return age; &#125; public void setAge(Integer age) &#123; this.age = age; &#125; @Override public String toString() &#123; return "User [name=" + name + ", age=" + age + ", car=" + car + "]"; &#125; &#125; 注解@Component(&quot;user&quot;)相当于在配置文件中添加了&lt;bean name=&quot;user&quot; class=&quot;my.study.bean2.User&quot;&gt;&lt;/bean&gt;书写测试类，可以打印出：1User 的空参构造方法被调用 除了@Component注解，还有@Serviece,@Controller,@Repository注解，它们和@Component没有什么区别，只是为了区分注解的对象是在哪一层，便于解读。@Serviece—Service层@Controller—Web层@Repository—DAO层 其他注解关键字@Scope(scopeName=&quot;singleton|prototype&quot;)—与&lt;bean&gt;中的scope关键字一样，默认为singleton. @Value()—值类型注入 1.—放在属性前面，里面填写要注入的值，表示要注入的属性的值例如:12345678910@Component("user")@Scope(scopeName="prototype")public class User &#123; @Value("test1") private String name; @Value("11") private Integer age; private Car car; .....&#125; 就等于1234&lt;bean name=&quot;User&quot; class=&quot;my.study.bean2.User&quot; scope=&quot;prototype&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;test1&quot;&gt;&lt;/property&gt; &lt;property name=&quot;age&quot; value=&quot;11&quot;&gt;&lt;/property&gt;&lt;/bean&gt; 可以看打印结果：12User 的空参构造方法被调用User [name=test1, age=11, car=null] 这种方式是通过反射的Field对属性赋值,破坏了封装性 2.—放在set方法前面，里面填写要注入的值，表示要注入的属性的值如：123456789101112131415@Component("user")@Scope(scopeName="prototype")public class User &#123; ... @Value("test1") public void setName(String name) &#123; this.name = name; &#125; ... @Value(value="11") public void setAge(Integer age) &#123; this.age = age; &#125; ...&#125; 这种方式通过set方法赋值,没有破坏封装性. @Autowired—自动装配引用类型注入 先将要注入的对象注册到容器中，可以在这个对象中先注入各种类型，然后使用@Autowired注入到当前类。缺点：如果是匹配到多个类型一致的对象，无法选择注入哪一个 @Qualifier()—指定要注入的对象的名称(即name=&quot;&quot;) @Resource(name=&quot;&quot;)—手动注入，指定注入哪个名称的对象 @PostContruct—在初始化方法前添加此注解,对象被创建后调用,类似于init-method @PreDestory—在销毁前调用,相当于destory-method Spring中的AOP手动实现AOP简要概括AOP思想就是面向切面编程,横向重复,纵向抽取. 首先Spring能够为容器中管理的对象生成动态代理对象。以前要使用Proxy.newProxyInstance(xx,xx,xx)生成代理对象.而现在Spring只需通过配置就可以生成代理对象。通过这个功能，就可以使用AOP思想进行开发。 Srping实现AOP的原理：1.动态代理—但是，被代理对象必须要实现接口，才能产生代理对象.没有借口不能实现动态代理.如果有接口，优先使用动态代理 2.cglib代理—cglib代理属于第三方代理技术.可以对任何类生成代理.代理的原理是对目标对象进行继承代理. 如果目标对象被final修饰.那么该类无法被cglib代理.如果没有借口，实现此代理 手动实现AOP:1.创建一个UserService接口，定义UserServiceImpl类要实现这个接口的方法..创建UserService代理工厂UserServiceProxyFactory,提供一个工厂方法，返回UserService对象.在方法中生成代理并返回UserService对象. 现在需要在调用UserServiceImpl的每一个方法(save,delete,update,find)之前都进行一个操作(比如打开事务),之后都进行另一个操作(比如提交事务).12345678package my.study.service;public interface UserService &#123; void save(); void delete(); void update(); void find();&#125; 1234567891011121314151617181920package my.study.service;public class UserServiceImpl implements UserService &#123; @Override public void save() &#123; System.out.println("保存用户!"); &#125; @Override public void delete() &#123; System.out.println("删除用户!"); &#125; @Override public void update() &#123; System.out.println("更新用户!"); &#125; @Override public void find() &#123; System.out.println("查找用户!"); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940package my.study.proxy;import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;import cn.itcast.service.UserService;import cn.itcast.service.UserServiceImpl;public class UserServiceProxyFactory implements InvocationHandler&#123; //定义构造方法，必须传入当前代理对象的实例 public UserServiceProxyFactory(UserService us) &#123; super(); this.us = us; &#125; //定义传入当前代理对象的实例 private UserService us; //返回代理对象 // 编写工具方法：生成动态代理 public UserService getUserServiceProxy()&#123; //第一个参数传入类加载器,第二个参数传入被代理对象所实现的接口,第三个参数要说明这次代理要怎么增强，即增强的内容，要传入一个InvocationHandler UserService usProxy = (UserService) Proxy.newProxyInstance(UserServiceProxyFactory.class.getClassLoader(), UserServiceImpl.class.getInterfaces(), this); //返回 return usProxy; &#125; //实现接口 @Override //第一个参数是当前代理对象的实例,第二个参数是当前调用的方法,第三个参数是当前方法执行时的参数 public Object invoke(Object arg0, Method method, Object[] arg2) throws Throwable &#123; //业务方法执行前需要进行的操作，这里是打开事务 System.out.println("打开事务!"); //传入当前代理对象的实例us作为运行所在类 //业务方法的执行 Object invoke = method.invoke(us, arg2); //业务方法执行后要执行的操作，这里是提交事务 System.out.println("提交事务!"); return invoke; &#125;&#125; 书写测试代码：1234567891011121314151617...@Test //动态代理 public void test1()&#123; //创建被代理对象 UserService us = new UserServiceImpl(); //创建Factory,传入被代理对象 UserServiceProxyFactory factory = new UserServiceProxyFactory(us); //调用getUserServiceProxy()方法，返回代理对象 UserService usProxy = factory.getUserServiceProxy(); //调用代理对象的增加方法 usProxy.save(); //代理对象与被代理对象实现了相同的接口 //代理对象 与 被代理对象没有继承关系 System.out.println(usProxy instanceof UserServiceImpl );//false &#125; 打印结果:1234打开事务！保存用户！提交事务！false 2.使用cglib方式实现AOP 123456789101112131415161718192021222324252627282930313233343536373839404142package my.study.proxy;import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;import org.springframework.cglib.proxy.Callback;import org.springframework.cglib.proxy.Enhancer;import org.springframework.cglib.proxy.MethodInterceptor;import org.springframework.cglib.proxy.MethodProxy;import cn.itcast.service.UserService;import cn.itcast.service.UserServiceImpl;//cglib代理public class UserServiceProxyFactory2 implements MethodInterceptor &#123; public UserService getUserServiceProxy()&#123; // 创建 Cglib 的核心类: Enhancer en = new Enhancer();//帮我们生成代理对象 // 设置父类: en.setSuperclass(UserServiceImpl.class);//设置对谁进行代理 // 设置父类: en.setCallback(this);//代理要做什么 // 生成代理： UserService us = (UserService) en.create();//创建代理对象 return us; &#125; @Override public Object intercept(Object prxoyobj, Method method, Object[] arg, MethodProxy methodProxy) throws Throwable &#123; //打开事务 System.out.println("打开事务!"); //调用原有方法 Object returnValue = methodProxy.invokeSuper(prxoyobj, arg); //提交事务 System.out.println("提交事务!"); return returnValue; &#125;&#125; 书写测试代码：123456789101112131415...@Testpublic void test2()&#123; UserServiceProxyFactory2 factory = new UserServiceProxyFactory2(); UserService usProxy = factory.getUserServiceProxy(); usProxy.save(); //判断代理对象是否属于被代理对象类型 //代理对象继承了被代理对象=&gt;true System.out.println(usProxy instanceof UserServiceImpl );//true&#125;... 打印结果:1234打开事务！保存用户！提交事务！true AOP相关术语Joinpoint(连接点):所谓连接点是指那些被拦截到的点。在 spring 中,这些点指的是方法,因为 spring 只支持方法类型的连接点.即目标对象中，所有可以增强的方法。 Pointcut(切入点):所谓切入点是指我们要对哪些 Joinpoint 进行拦截的定义.即目标对象，已经增强的方法(已经增强了的连接点)。 Advice(通知/增强):所谓通知是指拦截到 Joinpoint 之后所要做的事情就是通知.通知分为前置通知,后置通知,异常通知,最终通知,环绕通知(切面要完成的功能)。即增强的代码,也就是上面的例子中的System.out.println(&quot;打开事务!&quot;);和System.out.println(&quot;提交事务!&quot;); Introduction(引介):引介是一种特殊的通知在不修改类代码的前提下, ntroduction 可以在运行期为类动态地添加一些方法或 Field. Target(目标对象):代理的目标对象.即被代理对象 Weaving(织入):是指把增强应用到目标对象来创建新的代理对象的过程.spring 采用动态代理织入，而 AspectJ 采用编译期织入和类装在期织入.指的是将通知应用到连接点，形成切入点的过程(成形代理过程). Proxy（代理）:一个类被 AOP 织入增强后，就产生一个结果代理类.(将通知织入到目标对象后，形成代理对象) Aspect(切面): 是切入点和通知（引介）的结合,即切入点+通知 Spring中AOP的使用(xml配置)Spring中封装了动态代理的代码，所以不需要自己手写动态代理代码来实现AOP了。 需要第三方依赖包com.springsource.org.aopalliance-1.0.0.jar、com.springsource.org.aspectj.weaver-1.6.8.RELEASE.jar(具体版本可自己选择其他的) 准备目标对象使用上一节中的UserServiceImpl类. 准备通知通知类1234567891011121314151617181920212223242526272829package my.study.e_springaop;import org.aspectj.lang.ProceedingJoinPoint;public class MyAdvice &#123; //前置通知 目标方法运行之前调用 public void before() &#123; System.out.println("前置通知"); &#125; //后置通知(如果出现异常不调用) 目标方法运行之后调用 public void afterNoExc() &#123; System.out.println("后置通知(如果出现异常不调用)"); &#125; //环绕通知 目标方法之前和之后都调用 public Object around(ProceedingJoinPoint pjp) throws Throwable &#123; System.out.println("环绕通知,前置部分"); Object proceed = pjp.proceed(); System.out.println("环绕通知,后置部分"); return proceed; &#125; //异常拦截通知 如果出现异常则调用 public void exc() &#123; System.out.println("异常拦截通知"); &#125; //后置通知(无论是否出现异常都会调用) public void afterWithExc() &#123; System.out.println("后置通知(无论是否出现异常都会调用)"); &#125;&#125; 配置进行织入,将通知织入目标对象中首先导入aop的xml约束（命名空间），方法和之前的类似。 在my.study.e_springaop包内书写配置文件:12345678910111213141516171819202122232425262728293031323334353637&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:p="http://www.springframework.org/schema/p" xmlns="http://www.springframework.org/schema/beans" xmlns:aop="http://www.springframework.org/schema/aop" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.2.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-4.2.xsd "&gt;&lt;!-- 配置目标对象 --&gt; &lt;bean name="userServiceTarget" class="my.study.service.UserServiceImpl"&gt;&lt;/bean&gt;&lt;!-- 配置通知对象 --&gt; &lt;bean name="myAdvice" class="my.study.e_springaop.MyAdvice"&gt;&lt;/bean&gt;&lt;!-- 配置将通知织入目标对象 --&gt; &lt;aop:config&gt; &lt;!-- 配置切入点 --&gt; &lt;!-- id可以自定义 expression="execute()"为固定格式，传参为： public void my.study.service.UserServiceImpl.save() 默认为public: void my.study.service.UserServiceImpl.save() 返回值为任意的函数: * my.study.service.UserServiceImpl.save() UserServiceImpl类下的所有方法: * my.study.service.UserServiceImpl.*() 参数为任意: * my.study.service.*ServiceImpl.*(..) my.study.service包下及其子包下的所有以ServiceImpl为结尾的类的所有方法: * my.study.service..*ServiceImpl.*() --&gt; &lt;aop:pointcut expression="execution(* my.study.service.*ServiceImpl.*(..))" id="demo"/&gt; &lt;aop:aspect ref="myAdvice"&gt; &lt;!-- 指定myAdvice中的before方法切入到demo这个切入点中 --&gt; &lt;aop:before method="before" pointcut-ref="demo"/&gt; &lt;!-- 其他几个类似 --&gt; &lt;aop:after-returning method="afterNoexc" pointcut-ref="demo"/&gt; &lt;aop:around method="around" pointcut-ref="demo"/&gt; &lt;aop:after-throwing method="exc" pointcut-ref="demo"/&gt; &lt;aop:after method="afterWithExc" pointcut-ref="demo" /&gt; &lt;/aop:aspect&gt; &lt;/aop:config&gt;&lt;/beans&gt; 书写测试类:使用了spring与junit整合测试:12345678910111213141516171819202122package my.study.e_springaop;import javax.annotation.Resource;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.test.context.ContextConfiguration;import org.springframework.test.context.junit4.SpringJUnit4ClassRunner;import my.study.service.UserService;//创建容器@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration("classpath:my/study/e_springaop/applicationContext.xml")public class Demo &#123; @Resource(name="userServiceTarget") private UserService us; @Test public void test1()&#123; us.save(); &#125;&#125; 打印结果:123456前置通知环绕通知,前置部分保存用户!后置通知(无论是否出现异常都会调用)环绕通知,后置部分后置通知(如果出现异常不调用) Spring中AOP的使用(注解配置)注解配置的步骤和使用xml配置的步骤类似，只是在配置文件中不需要使用&lt;aop:config /&gt;标签进行配置，而是在配置文件中开启注解，继续接下来的配置。 12345678910&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:p="http://www.springframework.org/schema/p" xmlns="http://www.springframework.org/schema/beans" xmlns:aop="http://www.springframework.org/schema/aop" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.2.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-4.2.xsd "&gt;&lt;!-- 配置目标对象 --&gt; &lt;bean name="userServiceTarget" class="my.study.service.UserServiceImpl"&gt;&lt;/bean&gt;&lt;!-- 配置通知对象 --&gt; &lt;bean name="myAdvice" class="my.study.f_springannotionaop.MyAdvice"&gt;&lt;/bean&gt;&lt;!-- 开启使用注解完成织入 --&gt; &lt;aop:aspectj-autoproxy&gt;&lt;/aop:aspectj-autoproxy&gt;&lt;/beans&gt; 在my.study.f_springannotionaop包下的MyAdvice类中，添加注解：12345678910111213141516171819202122232425262728293031323334353637383940414243444546package my.study.f_springannotionaop;import org.aspectj.lang.ProceedingJoinPoint;import org.aspectj.lang.annotation.After;import org.aspectj.lang.annotation.AfterReturning;import org.aspectj.lang.annotation.AfterThrowing;import org.aspectj.lang.annotation.Around;import org.aspectj.lang.annotation.Aspect;import org.aspectj.lang.annotation.Before;import org.aspectj.lang.annotation.Pointcut;@Aspectpublic class MyAdvice &#123; @Pointcut("execution(* my.study.service.*ServiceImpl.*(..))") public void cut() &#123;&#125; //前置通知 目标方法运行之前调用 @Before("MyAdvice.cut()") public void before() &#123; System.out.println("前置通知"); &#125; //后置通知(如果出现异常不调用) 目标方法运行之后调用 @AfterReturning("execution(* my.study.service.*ServiceImpl.*(..))") public void afterNoExc() &#123; System.out.println("后置通知(如果出现异常不调用)"); &#125; //环绕通知 目标方法之前和之后都调用 @Around("execution(* my.study.service.*ServiceImpl.*(..))") public Object around(ProceedingJoinPoint pjp) throws Throwable &#123; System.out.println("环绕通知,前置部分"); Object proceed = pjp.proceed(); System.out.println("环绕通知,后置部分"); return proceed; &#125; //异常拦截通知 如果出现异常则调用 @AfterThrowing("execution(* my.study.service.*ServiceImpl.*(..))") public void exc() &#123; System.out.println("异常拦截通知"); &#125; //后置通知(无论是否出现异常都会调用) @After("execution(* my.study.service.*ServiceImpl.*(..))") public void afterWithExc() &#123; System.out.println("后置通知(无论是否出现异常都会调用)"); &#125;&#125; @Aspect表示该类是一个通知类@Before(&quot;execution(* my.study.service.*ServiceImpl.*(..))&quot;)表示该通知是一个前置通知，参数为execution(* my.study.service.*ServiceImpl.*(..))&quot;)进行指定切入点.@AfterReturning(&quot;execution(* my.study.service.*ServiceImpl.*(..))&quot;)表示该通知是一个后置通知.@Around(&quot;execution(* my.study.service.*ServiceImpl.*(..))&quot;)表示该通知是一个环绕通知@AfterThrowing(&quot;execution(* my.study.service.*ServiceImpl.*(..))&quot;)表示该通知是一个异常拦截通知@After(&quot;execution(* my.study.service.*ServiceImpl.*(..))&quot;)表示该通知是一个后置通知1234567@Pointcut("execution(* my.study.service.*ServiceImpl.*(..))") public void cut() &#123;&#125; @Before("MyAdvice.cut()") public void before() &#123; System.out.println("前置通知"); &#125; 其中 @Before(&quot;MyAdvice.cut()&quot;)就等于@Before(&quot;execution(* my.study.service.*ServiceImpl.*(..))&quot;)，这样方便管理切入点 书写测试类，并打印结果:123456环绕通知,前置部分前置通知保存用户!环绕通知,后置部分后置通知(无论是否出现异常都会调用)后置通知(如果出现异常不调用)]]></content>
      <tags>
        <tag>Spring</tag>
        <tag>JavaWeb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring学习笔记-一]]></title>
    <url>%2F2018%2F11%2F09%2FSpring%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%B8%80%2F</url>
    <content type="text"><![CDATA[Spring是一个一站式框架。Spring可以看做是一个容器，所有对象都可以在其中进行管理 Spring 搭建基本环境首先导入Spring所需依赖包： 创建一个对象用于测试： 123456789101112131415161718192021package my.study.bean;public class User &#123; private String name; private Integer age; public User()&#123; System.out.println("User 的空参构造方法被调用"); &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public Integer getAge() &#123; return age; &#125; public void setAge(Integer age) &#123; this.age = age; &#125; &#125; 然后需要书写配置文件，并将创建好的对象注册到Spring容器中。建议将配置文件放到源码文件夹根目录(/src)下，文件名可以任意取。 导入配置文件的约束：Preference中搜索XML Catlog，打开后点击Add-&gt;FileSystem...选择Spring解压目录下的schema-&gt;beans-&gt;spring-beans-4.2.xsd(选择最新版本(我的是4.2)即可) 继续选择Kye Type-&gt;Scheme location：把Location中spring-beans-4.2.xsd文件名复制，追加到Key中(注意补一个/). 然后在配置文件中，输入&lt;beans&gt;&lt;/beans&gt;后，在Eclipse下切换到设计视图，选中beans-&gt;右键选中Edit Namespaces-&gt;点击Add-&gt;勾选xsi-&gt;点击ok-&gt;继续点击Add-&gt;选择Specify New Namespace-&gt;选择Browse-&gt;选择Select XML Catalog entry-&gt;找到刚才上图的Key-&gt;Namespace Name填写Key中http://www.spri.....beans的内容，Prefix不用写 导入成功 最后将对象注册到容器进行测试： ApplicationContext&amp;BeanFactory 简单介绍：其中BeanFactory作为最顶层的接口，功能较为单一。BeanFactory接口实现类的容器，特点是每次获得对象时才会创建对象。 ApplicationContext在每次容器启动时，就会创建容器中的所有对象 Spring的配置bean 元素以前面配置User对象为例。 bean标签：使用该元素描述需要spring容器管理的对象class属性:被管理对象的完整类名.name属性:给被管理的对象起个名字.获得对象时根据该名称获得对象. 特点:可以重复.可以使用特殊字符.id属性: 与name属性一样.特点:名称不可重复.不能使用特殊字符. Scope属性singleton(默认值)单例对象.被标识为单例的对象在spring容器中只会存在一个实例，即默认为：1&lt;bean name=&quot;User&quot; class=&quot;my.study.bean.User&quot; scope=&quot;singleton&quot;&gt;&lt;/bean&gt; 测试：12345678910@Testpublic void test1() &#123; //1.创建容器对象 ApplicationContext ac = new ClassPathXmlApplicationContext("my/study/b_create/applicationContext.xml"); //2.在容器中寻找"User"对象 User u = (User) ac.getBean("User"); User u1 = (User) ac.getBean("User"); //3打印user对象 System.out.println(u == u1);&#125; 结果1true prototype多例原型.被标识为多例的对象,每次再获得才会创建.每次创建都是新的对象.注意，当整合struts2时,ActionBean必须配置为多例的。因为Action对象是由Spring容器管理的，struts2每次请求都会创建一个Action将配置文件改为:1&lt;bean name=&quot;User&quot; class=&quot;my.study.bean.User&quot; scope=&quot;prototype&quot;&gt;&lt;/bean&gt; 继续用test1()测试，打印结果为false request在web环境下.对象与request生命周期一致.即每当请求完成，对象就会在Spring中移除 session在web环境下,对象与session生命周期一致.即在一次会话中，会话完成后,对象就会在Spring中被移除 生命周期属性如果希望Bean在创建时有初始化方法，可以指定Bean中的一个方法为其初始化方法，Spring会在对象创建完成后调用。即init-method同样可以配置一个销毁方法，Spring容器在关闭前调用此方法。destory-method注意scope=&quot;prototype&quot;不能和destory-method标签一起使用 Spring的分模块配置在主配置文件中引入其他配置文件，1&lt;import resource=&quot;&quot;/&gt; 目录写在src目录下的全包名和配置文件名 Spring创建对象的方式创建新的配置文件和类进行测试 UserFactory:1234567891011121314package my.study.bean;public class UserFactory &#123; public static User createUser() &#123; System.out.println("静态工厂方式创建User"); return new User(); &#125; public User createUser2() &#123; System.out.println("实例工厂方式创建User"); return new User(); &#125;&#125; applicationContext.xml:123456789101112&lt;!-- 方式1：空参构造创建 --&gt; &lt;bean name=&quot;User&quot; class=&quot;my.study.bean.User&quot;&gt;&lt;/bean&gt; &lt;!-- 方式2：静态工厂创建 调用UserFactory的createUser方法创建名为user2的对象，放入容器 --&gt; &lt;bean name=&quot;User2&quot; class=&quot;my.study.bean.UserFactory&quot; factory-method=&quot;createUser&quot;&gt;&lt;/bean&gt; &lt;!-- 方式3：静态工厂创建 调用UserFactory的createUser方法创建名为user2的对象，放入容器 --&gt; &lt;bean name=&quot;User3&quot; factory-bean=&quot;userFactory&quot; factory-method=&quot;createUser2&quot;&gt;&lt;/bean&gt; &lt;bean name=&quot;userFactory&quot; class=&quot;my.study.bean.UserFactory&quot;&gt;&lt;/bean&gt; 空参构造方式空参构造方式就是最开始配置bean时的构造方式，前面已经有结果。当然这里要注意，创建容器对象时，传参时要传入的配置文件需要加上包名1ApplicationContext ac = new ClassPathXmlApplicationContext(&quot;my/study/b_create/applicationContext.xml&quot;); 静态工厂方式12345678910@Test//静态工厂方式public void test2() &#123; //1.创建容器对象 ApplicationContext ac = new ClassPathXmlApplicationContext("my/study/b_create/applicationContext.xml"); //2.向容器请求"user"对象 User u = (User) ac.getBean("User2"); //3打印user对象 System.out.println(u);&#125; 实例工厂方式12345678910@Test//动态工厂方式public void test3() &#123; //1.创建容器对象 ApplicationContext ac = new ClassPathXmlApplicationContext("my/study/b_create/applicationContext.xml"); //2.向容器请求"user"对象 User u = (User) ac.getBean("User3"); //3打印user对象 System.out.println(u);&#125; Spring属性注入注入方式set方法注入新建一个包，在配置文件&lt;bean&gt;&lt;/bean&gt;中添加property标签，其中name表示要注入的属性名,value表示要注入的值 12345&lt;!-- set方式注入： --&gt;&lt;bean name=&quot;User&quot; class=&quot;my.study.bean.User&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;jay&quot;&gt;&lt;/property&gt; &lt;property name=&quot;age&quot; value=&quot;17&quot;&gt;&lt;/property&gt;&lt;/bean&gt; 书写测试类：1234567891011121314151617package my.study.c_injection;import org.junit.Test;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;import my.study.bean.User;public class Demo &#123; @Test public void test() &#123; //1.创建容器对象 ApplicationContext ac = new ClassPathXmlApplicationContext("my/study/c_injection/applicationContext.xml"); User u = (User) ac.getBean("User"); System.out.println(u.getName() + " , " +u.getAge()); &#125;&#125; 打印出1User [name=jay, age=17] 这是值类型注入。 下面是引用类型注入。然后新建一个Car类，12345678910111213141516171819202122package my.study.bean;public class Car &#123; private String name; private String color; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getColor() &#123; return color; &#125; public void setColor(String color) &#123; this.color = color; &#125; @Override public String toString() &#123; return "Car [name=" + name + ", color=" + color + "]"; &#125;&#125; 在User中添加car属性，并添加get和set方法如果要注入Car类型，需要先把Car配置到容器中。在配置文件中添加：1234&lt;bean name=&quot;Car&quot; class=&quot;my.study.bean.Car&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;Q5&quot;&gt;&lt;/property&gt; &lt;property name=&quot;color&quot; value=&quot;black&quot;&gt;&lt;/property&gt;&lt;/bean&gt; 然后为User的car属性注入刚才配置的Car对象：12345&lt;bean name=&quot;User&quot; class=&quot;my.study.bean.User&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;jay&quot;&gt;&lt;/property&gt; &lt;property name=&quot;age&quot; value=&quot;17&quot;&gt;&lt;/property&gt; &lt;property name=&quot;car&quot; ref=&quot;Car&quot;&gt;&lt;/property&gt;&lt;/bean&gt; 重新书写User的toString()方法，重新运行test()打印结果：1User [name=jay, age=17, car=Car [name=Q5, color=black]] 构造函数注入在User类中添加有参构造函数1234public User(String name, Car car) &#123; this.name = name; this.car = car;&#125; 然后是bean的配置，添加如下配置:1234&lt;bean name=&quot;User2&quot; class=&quot;my.study.bean.User&quot;&gt; &lt;constructor-arg name=&quot;name&quot; value=&quot;chou&quot;&gt;&lt;/constructor-arg&gt; &lt;constructor-arg name=&quot;car&quot; ref=&quot;Car&quot;&gt;&lt;/constructor-arg&gt;&lt;/bean&gt; ref依然传入上面定义的Car.然后书写代码测试：12345public void test() &#123; ApplicationContext ac = new ClassPathXmlApplicationContext("my/study/c_injection/applicationContext.xml"); User u = (User) ac.getBean("User2"); System.out.println(u);&#125; 打印结果为:1User [name=chou, age=null, car=Car [name=Q5, color=black]] 如果定义一个重载构造函数：1234public User(Car car,String name) &#123; this.name = name; this.car = car;&#125; 想让Spring在使用构造函数注入时，使用User(String name, Car car)来创建对象，可以在&lt;contructor-arg&gt;&lt;/contructor-arg&gt;中使用index.使用index来表示构造函数的参数索引,index越小表示越靠前的位置.1234&lt;bean name=&quot;User2&quot; class=&quot;my.study.bean.User&quot;&gt; &lt;constructor-arg name=&quot;name&quot; value=&quot;chou&quot; index=&quot;0&quot;&gt;&lt;/constructor-arg&gt; &lt;constructor-arg name=&quot;car&quot; ref=&quot;Car&quot; index=&quot;1&quot;&gt;&lt;/constructor-arg&gt;&lt;/bean&gt; 如果是构造函数是同样的注入顺序，但是是不同的类型，在index后面继续追加一个type，表示构造函数的参数类型：1234public User(Integer name, Car car) &#123; this.name = name+""; this.car = car;&#125; 要注意value中的传参要和type对应1234&lt;bean name=&quot;User2&quot; class=&quot;my.study.bean.User&quot;&gt; &lt;constructor-arg name=&quot;name&quot; value=&quot;123&quot; index=&quot;0&quot; type=&quot;java.lang.Integer&quot;&gt;&lt;/constructor-arg&gt; &lt;constructor-arg name=&quot;car&quot; ref=&quot;Car&quot; index=&quot;1&quot;&gt;&lt;/constructor-arg&gt;&lt;/bean&gt; 打印结果：1User [name=123, age=null, car=Car [name=Q5, color=black]] p名称空间注入首先在&lt;beans&gt;&lt;/beans&gt;中导入名称空间,添加xmlns:p=&quot;http://www.springframework.org/schema/p&quot;即添加后的beans名称空间为：12345&lt;beans xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:p=&quot;http://www.springframework.org/schema/p&quot; xmlns=&quot;http://www.springframework.org/schema/beans&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.2.xsd &quot;&gt;&lt;/beans&gt; 注入方法，使用p:属性完成注入，如果是值类型，则用p:属性名=&quot;值&quot;，如果是引用类型，则用p:属性名-ref=&quot;bean名称&quot;：12&lt;bean name=&quot;User3&quot; class=&quot;my.study.bean.User&quot; p:name=&quot;hey&quot; p:age=&quot;22&quot; p:car-ref=&quot;Car&quot;&gt;&lt;/bean&gt; 书写代码测试：12345public void test() &#123; ApplicationContext ac = new ClassPathXmlApplicationContext("my/study/c_injection/applicationContext.xml"); User u = (User) ac.getBean("User3"); System.out.println(u);&#125; 打印结果：1User [name=hey, age=22, car=Car [name=Q5, color=black]] spel注入即Spring Expression Language.有点类似于el和ognl的写法. 例如:12345678&lt;bean name=&quot;User4&quot; class=&quot;my.study.bean.User&quot;&gt; &lt;!-- 使用User的name属性值作为value的值 --&gt; &lt;property name=&quot;name&quot; value=&quot;#&#123;User.name&#125;&quot;&gt;&lt;/property&gt; &lt;!-- 使用User3的value属性值作为value的值 --&gt; &lt;property name=&quot;age&quot; value=&quot;#&#123;User3.age&#125;&quot;&gt;&lt;/property&gt; &lt;!-- 引用类型不支持spel表达式 --&gt; &lt;property name=&quot;car&quot; ref=&quot;Car&quot;&gt;&lt;/property&gt;&lt;/bean&gt; 打印结果为：1User [name=jay, age=22, car=Car [name=Q5, color=black]] 复杂类型注入新建CollectionBean类进行测试：1234567891011121314151617181920212223242526272829303132333435363738394041package my.study.c_injection;import java.util.List;import java.util.Map;import java.util.Properties;public class CollectionBean &#123; private Object[] arr; private List list; private Map map; private Properties prop; public Object[] getArr() &#123; return arr; &#125; public void setArr(Object[] arr) &#123; this.arr = arr; &#125; public List getList() &#123; return list; &#125; public void setList(List list) &#123; this.list = list; &#125; public Map getMap() &#123; return map; &#125; public void setMap(Map map) &#123; this.map = map; &#125; public Properties getProp() &#123; return prop; &#125; public void setProp(Properties prop) &#123; this.prop = prop; &#125; @Override public String toString() &#123; return "CollectionBean [arr=" + Arrays.toString(arr) + ", list=" + list + ", map=" + map + ", prop=" + prop + "]"; &#125;&#125; 数组在配置文件中书写配置：1234567891011121314&lt;bean name="CollectionBean_arr" class="my.study.c_injection.CollectionBean"&gt; &lt;!-- 如果数组中只准备注入一个元素，直接使用value|ref --&gt; &lt;!-- &lt;property name="arr" value="11"&gt;&lt;/property&gt; --&gt; &lt;!-- 多元素注入 --&gt; &lt;property name="arr"&gt; &lt;array&gt; &lt;value&gt;1&lt;/value&gt; &lt;value&gt;2&lt;/value&gt; &lt;ref bean="User4"&gt;&lt;/ref&gt; &lt;/array&gt; &lt;/property&gt;&lt;/bean&gt; 测试并打印结果：1CollectionBean [arr=[1, 2, User [name=jay, age=22, car=Car [name=Q5, color=black]]], list=null, map=null, prop=null] List在配置文件中添加配置：12345678910111213141516171819202122232425&lt;bean name=&quot;CollectionBean_arr&quot; class=&quot;my.study.c_injection.CollectionBean&quot;&gt;&lt;!-- arr注入 --&gt; &lt;!-- 如果数组中只准备注入一个元素，直接使用value|ref --&gt; &lt;!-- &lt;property name=&quot;arr&quot; value=&quot;11&quot;&gt;&lt;/property&gt; --&gt; &lt;!-- 多元素注入 --&gt; &lt;property name=&quot;arr&quot;&gt; &lt;array&gt; &lt;value&gt;1&lt;/value&gt; &lt;value&gt;2&lt;/value&gt; &lt;ref bean=&quot;User4&quot;&gt;&lt;/ref&gt; &lt;/array&gt; &lt;/property&gt;&lt;!-- list注入 --&gt; &lt;!-- 如果List中只准备注入一个元素，直接使用value|ref --&gt; &lt;!-- 多元素注入 --&gt; &lt;property name=&quot;list&quot;&gt; &lt;list&gt; &lt;value&gt;list1&lt;/value&gt; &lt;value&gt;list2&lt;/value&gt; &lt;ref bean=&quot;User3&quot;&gt;&lt;/ref&gt; &lt;/list&gt; &lt;/property&gt;&lt;/bean&gt; 打印结果：1CollectionBean [arr=[1, 2, User [name=jay, age=22, car=Car [name=Q5, color=black]]], list=[list1, list2, User [name=hey, age=22, car=Car [name=Q5, color=black]]], map=null, prop=null] Map在配置文件中添加配置：123456789101112131415161718192021222324252627282930313233&lt;bean name=&quot;CollectionBean_arr&quot; class=&quot;my.study.c_injection.CollectionBean&quot;&gt;&lt;!-- arr注入 --&gt; &lt;!-- 如果数组中只准备注入一个元素，直接使用value|ref --&gt; &lt;!-- &lt;property name=&quot;arr&quot; value=&quot;11&quot;&gt;&lt;/property&gt; --&gt; &lt;!-- 多元素注入 --&gt; &lt;property name=&quot;arr&quot;&gt; &lt;array&gt; &lt;value&gt;1&lt;/value&gt; &lt;value&gt;2&lt;/value&gt; &lt;ref bean=&quot;User4&quot;&gt;&lt;/ref&gt; &lt;/array&gt; &lt;/property&gt;&lt;!-- list注入 --&gt; &lt;!-- 如果List中只准备注入一个元素，直接使用value|ref --&gt; &lt;!-- 多元素注入 --&gt; &lt;property name=&quot;list&quot;&gt; &lt;list&gt; &lt;value&gt;list1&lt;/value&gt; &lt;value&gt;list2&lt;/value&gt; &lt;ref bean=&quot;User3&quot;&gt;&lt;/ref&gt; &lt;/list&gt; &lt;/property&gt;&lt;!-- map注入 --&gt; &lt;property name=&quot;map&quot;&gt; &lt;map&gt; &lt;entry key=&quot;key1&quot; value=&quot;value1&quot;&gt;&lt;/entry&gt; &lt;entry key=&quot;user&quot; value-ref=&quot;User4&quot;&gt;&lt;/entry&gt; &lt;entry key-ref=&quot;User3&quot; value-ref=&quot;User2&quot;&gt;&lt;/entry&gt; &lt;/map&gt; &lt;/property&gt;&lt;/bean&gt; 测试并打印结果：1CollectionBean [arr=[1, 2, User [name=jay, age=22, car=Car [name=Q5, color=black]]], list=[list1, list2, User [name=hey, age=22, car=Car [name=Q5, color=black]]], map=&#123;key1=value1, user=User [name=jay, age=22, car=Car [name=Q5, color=black]], User [name=hey, age=22, car=Car [name=Q5, color=black]]=User [name=123, age=null, car=Car [name=Q5, color=black]]&#125;, prop=null] Properties配置文件中的配置：12345678910111213141516171819202122232425262728293031323334353637383940&lt;bean name=&quot;CollectionBean_arr&quot; class=&quot;my.study.c_injection.CollectionBean&quot;&gt;&lt;!-- arr注入 --&gt; &lt;!-- 如果数组中只准备注入一个元素，直接使用value|ref --&gt; &lt;!-- &lt;property name=&quot;arr&quot; value=&quot;11&quot;&gt;&lt;/property&gt; --&gt; &lt;!-- 多元素注入 --&gt; &lt;property name=&quot;arr&quot;&gt; &lt;array&gt; &lt;value&gt;1&lt;/value&gt; &lt;value&gt;2&lt;/value&gt; &lt;ref bean=&quot;User4&quot;&gt;&lt;/ref&gt; &lt;/array&gt; &lt;/property&gt;&lt;!-- list注入 --&gt; &lt;!-- 如果List中只准备注入一个元素，直接使用value|ref --&gt; &lt;!-- 多元素注入 --&gt; &lt;property name=&quot;list&quot;&gt; &lt;list&gt; &lt;value&gt;list1&lt;/value&gt; &lt;value&gt;list2&lt;/value&gt; &lt;ref bean=&quot;User3&quot;&gt;&lt;/ref&gt; &lt;/list&gt; &lt;/property&gt;&lt;!-- map注入 --&gt; &lt;property name=&quot;map&quot;&gt; &lt;map&gt; &lt;entry key=&quot;key1&quot; value=&quot;value1&quot;&gt;&lt;/entry&gt; &lt;entry key=&quot;user&quot; value-ref=&quot;User4&quot;&gt;&lt;/entry&gt; &lt;entry key-ref=&quot;User3&quot; value-ref=&quot;User2&quot;&gt;&lt;/entry&gt; &lt;/map&gt; &lt;/property&gt;&lt;!-- Properties注入 --&gt; &lt;property name=&quot;prop&quot;&gt; &lt;props&gt; &lt;prop key=&quot;propkey1&quot;&gt; propvalue1&lt;/prop&gt; &lt;prop key=&quot;propkey2&quot;&gt; propvalue2&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt;&lt;/bean&gt; 打印结果：1CollectionBean [arr=[1, 2, User [name=jay, age=22, car=Car [name=Q5, color=black]]], list=[list1, list2, User [name=hey, age=22, car=Car [name=Q5, color=black]]], map=&#123;key1=value1, user=User [name=jay, age=22, car=Car [name=Q5, color=black]], User [name=hey, age=22, car=Car [name=Q5, color=black]]=User [name=123, age=null, car=Car [name=Q5, color=black]]&#125;, prop=&#123;propkey2=propvalue2, propkey1=propvalue1&#125;]]]></content>
      <tags>
        <tag>Spring</tag>
        <tag>JavaWeb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实现线段树]]></title>
    <url>%2F2018%2F08%2F13%2F%E5%AE%9E%E7%8E%B0%E7%BA%BF%E6%AE%B5%E6%A0%91%2F</url>
    <content type="text"><![CDATA[定义当更关心某个区间上的问题时，使用线段树（区间树）会更方便。 线段树是一种二叉搜索树 线段树每个节点存放一个区间内相应的信息 一般用静态数组表示 线段树不一定是一棵完全二叉树 线段树是平衡二叉树（最大深度和最小深度的差最大为1） 例如，如果线段树想表示区间的和，那么每个节点存放的不是对应的数组，而是这个区间的和。 线段树依然可以使用数组表示。那么对于一个区间有n个元素，数组的大小该如何确定？对于一个满二叉树，如果有h层（从0层到h-1层），那么h层就有2^h-1个节点，差不多是2^h，最后一层(h-1)层，有2^(h-1)个节点，最后一层的节点的数目大约是前面的几点数目之和。 所以，如果用数组开辟空间，那么如果n=2^k(即恰好为2的整数次幂),需要2n的空间（这是满二叉树的情况），但是如果n=2^k+1（即n&gt;2^k,也就是最坏的情况），则需要4n的空间。 结论：因为线段树不考虑添加元素，也就是区间的大小是固定的，所以使用4n的静态空间就可以满足所有情况。（这里有空间浪费） 创建线段树线段树的根节点的信息，是两个孩子节点的信息的综合。比如求和，根节点的值就是左右孩子节点的值之和，依次类推，那么可以采用递归的方法进行求值。 另外，在进行线段树的创建时，因为不知道要采取什么样的方法去创建（比如求和，求积，求最大值，求最小值等），所以可以定义一个Merger接口，要求在创建线段树时，其构造函数不但要传入一个初始的数组，也要传入一个merger，即相应的要采取的操作。 Merger: 1234//融合器，即线段树进行什么操作(求和或者求乘积等操作)public interface Merger&lt;E&gt; &#123; E merge(E a,E b);&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980public class SegmentTree&lt;E&gt; &#123; private E[] data;//数组arr的副本 private E[] tree; // 将数据以树的形式表示出来,看成一个满二叉树 private Merger&lt;E&gt; merger; //传入一个merger，定义用户要进行的操作 public SegmentTree(E[] arr,Merger&lt;E&gt; merger)&#123; this.merger = merger; data = (E[]) new Object[arr.length]; for(int i = 0 ; i &lt; arr.length ; i++)&#123; data[i] = arr[i]; &#125; tree = (E[]) new Object[4 * arr.length]; buildSegmentTree(0,0,data.length - 1 );//创建SegmentTree &#125; public int getSize()&#123; return data.length; &#125; public E get(int index)&#123; if(index &lt; 0 || index &gt;= data.length) throw new IllegalArgumentException("Index is illegal."); return data[index]; &#125; //返回以完全二叉树的数组表示中，一个索引表示的元素的左孩子所在节点的索引 private int leftChild(int index)&#123; return index * 2 + 1; &#125; //返回以完全二叉树的数组表示中，一个索引表示的元素的右孩子所在节点的索引 private int rightChild(int index)&#123; return index * 2 + 2; &#125; /** *在treeIndex的位置创建表示区间[l,r]的线段树 * @param treeIndex 要创建的线段树根节点对应的索引 * @param l 对于此节点对应的左端点 * @param r 对于此节点对应的右端点 */ private void buildSegmentTree(int treeIndex,int l , int r)&#123; if(l == r)&#123; tree[treeIndex] = data[l]; return; &#125; //l &lt; r //左右子树的index，即在数组中的索引 int leftTreeIndex = leftChild(treeIndex); int rightTreeIndex = rightChild(treeIndex); //左右子树相应的区间的中间位置 int mid = l + (r - l) / 2; //为了防止(l + r) / 2 溢出 buildSegmentTree(leftTreeIndex,l,mid); buildSegmentTree(rightTreeIndex,mid+1,r); //调用merger接口类对象，进行相应的操作 tree[treeIndex] = merger.merge(tree[leftTreeIndex],tree[rightTreeIndex]); &#125; @Override public String toString()&#123; StringBuilder res = new StringBuilder(); res.append('['); for(int i = 0 ;i &lt; tree.length ; i++)&#123; if(tree[i] != null) res.append(tree[i]); else res.append("null"); if(i != tree.length - 1) res.append(","); &#125; res.append(']'); return res.toString(); &#125;&#125; 在Main函数中进行测试，并打印结果：12345678910111213public class Main &#123; public static void main(String[] args)&#123; Integer[] nums = &#123;-2,0,3,-5,2,-1&#125;; SegmentTree&lt;Integer&gt; segTree = new SegmentTree&lt;&gt;(nums, new Merger&lt;Integer&gt;() &#123; @Override public Integer merge(Integer a, Integer b) &#123; return a + b; &#125; &#125;); System.out.println(segTree); &#125;&#125; 生成如下的线段树： 线段树的查询操作用户可以输入要查询的某个区间，返回这个区间内的对应的值。 相应的方法：1234567891011121314151617181920212223242526272829303132333435/* 返回要查询的区间[queryStart,queryEnd]的值 */ public E query(int queryStart,int queryEnd)&#123; if(queryStart &lt; 0 || queryStart &gt;= data.length || queryEnd &lt; 0 || queryEnd &gt;= data.length || queryStart &gt; queryEnd) throw new IllegalArgumentException("Illegal Index"); return query(0,0,data.length-1,queryStart,queryEnd); &#125; /** * 定义私有函数，在以treeIndex为根的线段树中[l,r]的范围里，搜索区间[queryStart,queryEnd]的值 * @param treeIndex 要查询的树的根节点 * @param l 树对应的数组的左范围 * @param r 树对应的数组的右范围 * @param queryStart 要查询的区间的左端 * @param queryEnd 要查询的区间的右端 */ private E query(int treeIndex,int l,int r,int queryStart,int queryEnd)&#123; if(l == queryStart &amp;&amp; r == queryEnd) return tree[treeIndex]; int mid = l + (r - l) / 2; int leftIndex = leftChild(treeIndex); int rightIndex = rightChild(treeIndex); if(queryStart &gt;= mid+1) return query(rightIndex,mid+1,r,queryStart,queryEnd); else if(queryEnd &lt;= mid) return query(leftIndex,l,mid,queryStart,queryEnd); //否则，跨左右区间，分别求左右区间的值，然后merg，返回 E leftResult = query(leftIndex,l,mid,queryStart,mid); E rightResult = query(rightIndex,mid+1,r,mid+1,queryEnd); return merger.merge(leftResult,rightResult); &#125; 线段树的更新操作线段树的更新，是针对某个index位置的数据进行更新，使用线段树进行更新操作，其时间有优势，时间复杂度为O(logn) 更新操作的代码如下：1234567891011121314151617181920212223242526272829/** * 将index位置的值更新为e * @param index 待更新的位置索引 * @param e 更新后的值 */public void set(int index,E e)&#123; if(index &lt; 0 || index &gt;= data.length) throw new IllegalArgumentException("Illegal index."); set(0,0,data.length - 1,index,e);&#125;//在以treeIndex为根的线段树中，更新index的值为eprivate void set(int treeIndex,int l , int r,int index,E e)&#123; if(l == r)&#123; tree[treeIndex] = e; return; &#125; int mid = l +(r - l) / 2; int leftTreeIndex = leftChild(treeIndex); int rightTreeIndex = rightChild(treeIndex); if(index &gt;= mid+1) set(rightTreeIndex,mid+1,r,index,e); else //index &lt;= mid set(leftTreeIndex,l,mid,index,e); tree[treeIndex] = merger.merge(tree[leftTreeIndex],tree[rightTreeIndex]);&#125;]]></content>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实现优先队列]]></title>
    <url>%2F2018%2F08%2F12%2F%E5%AE%9E%E7%8E%B0%E4%BC%98%E5%85%88%E9%98%9F%E5%88%97%2F</url>
    <content type="text"><![CDATA[优先队列一般都是基于堆的，所以先写堆的实现。 堆 堆中某个节点的值总是不大于或不小于其父节点的值(所以分为最大堆和最小堆) 堆总是一棵完全二叉树 另外要注意：节点值的大小和节点所处的层次是没有关系的 本文以最大堆为例，进行书写。 堆的表示方法——数组堆的一种底层表示方法是使用动态数组实现，关于动态数组的实现见：https://homxuwang.github.io/2018/07/17/%E6%95%B0%E7%BB%84/ 堆的实现代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899public class MaxHeap&lt;E extends Comparable&lt;E&gt;&gt; &#123; private Array&lt;E&gt; data; public MaxHeap(int capacity)&#123; data = new Array&lt;&gt;(capacity); &#125; public MaxHeap()&#123; data = new Array&lt;&gt;(); &#125; //返回堆中的元素个数 public int size() &#123; return data.getSize(); &#125; //判断堆是否为空 public boolean isEmpty() &#123; return data.isEmpty(); &#125; //返回一个完全二叉树的数组表示中，一个索引所表示的元素的父亲节点的索引 private int parent(int index)&#123; if(index == 0) throw new IllegalArgumentException("index 0 doesn't have parent;"); return (index-1) / 2; &#125; //返回一个完全二叉树的数组表示中，一个索引所表示的元素的左孩子的索引 private int leftChild(int index)&#123; return index * 2 + 1; &#125; //返回一个完全二叉树的数组表示中，一个索引所表示的元素的右孩子的索引 private int rightChild(int index)&#123; return index * 2 + 2; &#125; //向堆中添加一个元素 public void add(E e)&#123; data.addLast(e); //先向末尾添加元素 //调用siftUp函数，进行上浮操作,以维护堆的性质 siftUp(data.getSize() - 1); &#125; //需要上浮的元素的索引 private void siftUp(int k)&#123; //如果k满足索引大于零,且其父亲节点的值小于它的值，则进行上浮操作 while (k &gt; 0 &amp;&amp; data.get(parent(k)).compareTo(data.get(k)) &lt; 0)&#123; swap(k,parent(k)); k = parent(k); &#125; &#125; //定义一个交换的函数 private void swap(int i , int j)&#123; if(i &lt; 0 || i &gt;= data.getSize() || j &lt; 0 || j&gt;= data.getSize())&#123; throw new IllegalArgumentException("Index is illegal."); &#125; E temp = data.get(i); data.set(i,data.get(j)); data.set(j,temp); &#125; //查看堆中的最大元素 public E findMax()&#123; if(data.getSize() == 0) throw new IllegalArgumentException("Can not findMac in a empty heap"); return data.get(0); &#125; //取出堆中的最大元素 public E extractMax()&#123; E ret = findMax(); swap(0,data.getSize() - 1); data.removeLast(); siftDown(0); return ret; &#125; //下沉操作 private void siftDown(int k)&#123; while (leftChild(k) &lt; data.getSize())&#123;//如果k没有左右孩子，则循环终止 int j = leftChild(k);//j为k的左孩子的索引 if(j + 1 &lt; data.getSize() &amp;&amp; data.get( j + 1 ).compareTo(data.get(j)) &gt; 0)&#123;//如果k有右孩子,并且右孩子的节点的值大于左孩子的节点的值 j = rightChild(k); //此时，data[j]是leftChild和rightChild中的最大值 &#125; if(data.get(k).compareTo(data.get(j)) &gt;= 0) // 如果k的值大于其左右孩子的值，则满足了最大堆的另一个性质，可以退出循环体 break; //否则交换k和j位置,并且k赋值为j，继续进行下一个循环 swap(k,j); k = j; &#125; &#125;&#125; 在数组中，一个父亲节点其所有节点的索引(假设索引以0开始)为： 12Index_leftChild = index_Father * 2 + 1;Index_rightChild = index_Father * 2 + 2; 如果知道一个孩子节点的索引(假设索引以0开始)求其父亲节点的索引: 1index_Father = ( index_Child - 1 ) / 2; 上面两个求法可以用数学归纳法进行证明。 Sift Up操作很简单，即新添加的元素先放到数组的末尾位置，这时候满足了完全二叉树的性质。但是它不一定满足总是不大于或者不小于其父亲节点的值。所以这时候的操作是，这个值要与它的父亲节点、爷爷节点……对比，直到放在合适的位置。因为方法中总结了找到一个节点的父亲节点的方法:faterIndex = (index-1) / 2。以一个最大堆为例，让新加入的节点和父亲节点对比，如果它大于其父亲节点，则交换，继续对比其父亲节点，直到它小于等于其父亲节点为止。 Sift Down操作，即取出最大堆的堆顶的元素（取出操作只能取出这个最大的元素，而不能取出别的元素）。因为最大的元素取出后，其左右的树结构就是两个单独的子树，那么要给这两个子树找一个新的父节点，操作如下： 将堆中的最后一个元素放在堆顶 删除掉最后一个元素（这时候满足完全二叉树的性质） 将堆顶元素与左右孩子中大于它且较大的数进行交换 交换后的新位置继续与孩子节点中大于它且较大的数进行交换 继续操作直到它大于其左后孩子或者它没有左右孩子 优点：add操作和extractMax操作的时间复杂度都是O(logn) 其中swap交换函数我写在了堆这个类中，可以在Array类中定义交换函数，在堆类中直接调用即可。 Heapify 和 replace操作replacereplace：取出最大元素后放入新元素 step1: extractMax -&gt; step2: add (2O(logn)) setp1: 替换堆顶元素 -&gt; step2: Sift Down (O(logn))1234567//取出堆中的最大元素，并替换成epublic E replace(E e)&#123; E ret = findMax(); data.set(0,e); siftDown(0); return ret;&#125; Heapify将任意数组整理成堆的形状 扫描数组，放如堆的新的对象中再返回 (O(nlogn)) 可以先把数组看成一棵完全二叉树,从最后一个非叶子节点开始进行Sift Down操作。(找到最后一个非叶子节点的方法：拿到最后一个节点的索引，然后计算他的父亲节点的索引即可) step1: 找到最后一个非叶子节点，进行Sift Down操作 step2: 倒数第二个非叶子节点进行Sift Down操作 … 以此类推 直到索引为0的非叶子节点完成Sift Down操作 O(n) Heapify一般可以在构造函数中进行(用一个数组初始化堆)),所以Array类也要支持一个用数组初始化动态数组的构造函数 Array类的构造函数： 1234567public Array(E[] arr) &#123; data = (E[])new Object[arr.length]; for(int i = 0 ; i &lt; arr.length ; i ++)&#123; data[i] = arr[i]; &#125; size = arr.length;&#125; MaxHeap的构造函数：12345public MaxHeap(E[] arr)&#123; data = new Array&lt;&gt;(arr); for(int i = parent(arr.length - 1) ; i &gt;= 0 ; i--) siftDown(i);&#125; 优先队列优先队列就是一个队列，要满足队列的所有属性方法，所以要实现队列接口，关于队列的实现见：https://homxuwang.github.io/2018/07/17/%E5%AE%9E%E7%8E%B0%E6%A0%88%E5%92%8C%E9%98%9F%E5%88%97/ 优先队列的底层是使用堆来实现，同样还是以最大堆为例。 123456789101112131415161718192021222324252627282930public class PriorityQueue&lt;E extends Comparable&lt;E&gt;&gt; implements Queue&lt;E&gt; &#123; private MaxHeap&lt;E&gt; maxHeap; public PriorityQueue()&#123; maxHeap = new MaxHeap&lt;&gt;(); &#125; @Override public int getSize() &#123; return maxHeap.size(); &#125; @Override public boolean isEmpty() &#123; return maxHeap.isEmpty(); &#125; @Override public void enqueue(E e) &#123; maxHeap.add(e); &#125; @Override public E dequeue() &#123; return maxHeap.extractMax(); &#125; @Override public E getFront() &#123; return maxHeap.findMax(); &#125;&#125; java.util中的PriorityQueue默认是用的最小堆，具体方法名也有一些区别]]></content>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用户日志统计分析]]></title>
    <url>%2F2018%2F08%2F04%2F%E7%94%A8%E6%88%B7%E6%97%A5%E5%BF%97%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[离线数据处理架构 用户通过APP或者PC访问服务器 这期间的操作日志信息用nginx记录下来(如记录在/var/log/access.log)，记录在nginx服务器 借助Flume框架，将日志信息抽取到HDFS中（命名规则根据自己的需要进行修改） 离线数据处理的第一步就是清理脏数据，清理完脏数据后继续写入HDFS 继续进行Hive（这里的Hive是一个统称，如Spark Sql） 处理完的结果写入RDBMS中或者NO SQL中 进行相应的数据输出(如表格，图形化输出) 离线处理的作业过程可以使用任务调度框架（Oozie或Azkaban）进行任务调度，安排每天进行作业的时间。可以将4.5.步骤串联起来 Kafka可以将日志进行实时处理，数据来了后进行立即处理（这是实时流处理） 本篇是写离线处理的一些代码分析和实现。 首先看数据(数据来自粉丝日志的教程，地址：http://blog.fens.me/hadoop-mapreduce-log-kpi/)： 摘取某一行的内容如下：1222.68.172.190 - - [18/Sep/2013:06:49:57 +0000] &quot;GET /images/my.jpg HTTP/1.1&quot; 200 19939 &quot;http://www.angularjs.cn/A00n&quot; &quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/29.0.1547.66 Safari/537.36&quot; 12345678remote_addr: 记录客户端的ip地址, 222.68.172.190remote_user: 记录客户端用户名称, –time_local: 记录访问时间与时区, [18/Sep/2013:06:49:57 +0000]request: 记录请求的url与http协议, “GET /images/my.jpg HTTP/1.1”status: 记录请求状态,成功是200, 200body_bytes_sent: 记录发送给客户端文件主体内容大小, 19939http_referer: 用来记录从那个页面链接访问过来的, “http://www.angularjs.cn/A00n”http_user_agent: 记录客户浏览器的相关信息, “Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/29.0.1547.66 Safari/537.36” 目的： 根据日志信息抽取出浏览器的信息 针对不同的浏览器进行统计 单机版代码分析和实现新建一个类，进行单机的测试下载配置userAgentParser工具类在github上下载工具类：https://github.com/LeeKemp/UserAgentParser 然后在本地用maven进行编译，编译的时候我出现了一些问题，见文章最后的踩坑记。 新建一个单元测试方法：输入一行数据进行测试 进行日志读取分析，统计访问网站的浏览器的数据相应的引入的文件：12345678910111213141516171819202122232425262728293031323334353637383940 public void readFileTest() throws IOException&#123; //文件路径 String path = "/home/hadoop/study/data/access.log.10"; BufferedReader reader = new BufferedReader( new InputStreamReader(new FileInputStream(new File(path))) ); String line = ""; UserAgentParser userAgentParser = new UserAgentParser(); UserAgent agent ; Map&lt;String,Integer&gt; broswerMap = new HashMap&lt;String,Integer&gt;(); int i = 0 ; while(line != null)&#123; line = reader.readLine(); //一次读一行数据 i ++; if(StringUtils.isNotBlank(line))&#123;//判断是否为空 String source = line.substring(getCharacterPosition(line,"\"",5) + 1); agent = userAgentParser.parse(source); String browser = agent.getBrowser(); String engine = agent.getEngine(); String engineVersion = agent.getEngineVersion(); String os = agent.getOs(); String platform = agent.getPlatform(); boolean isMobile = agent.isMobile(); Integer broswerValue = broswerMap.get(browser); if(broswerMap.get(browser) != null)&#123; broswerMap.put(browser,broswerValue + 1); &#125;else&#123; broswerMap.put(browser,1); &#125;// System.out.println(browser + "," + engine + " " + engineVersion + " " + os + " " + platform); &#125; &#125; //一共有多少条数据 System.out.println(i-1); System.out.println("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"); for(Map.Entry&lt;String,Integer&gt; entry : broswerMap.entrySet())&#123; System.out.println(entry.getKey() + " : " + entry.getValue()); &#125; &#125; 定义私有方法获取指定字符串中指定标识的字符串出现的索引，分析某一行的用户日志，可以使用&quot;进行分割，获取访问的客户端等信息在第5个&quot;，所以operator = &quot;,index = 5123456789101112131415/** * 获取指定字符串中指定标识的字符串出现的索引 * @return */private int getCharacterPosition(String value,String operator,int index)&#123; Matcher slashMatcher = Pattern.compile(operator).matcher(value); int mIndex = 0; while(slashMatcher.find())&#123; mIndex ++; if(mIndex == index)&#123; break; &#125; &#125; return slashMatcher.start();&#125; 可以查看打印结果： MapReduce实现需求统计新建类 代码编写可以参考 MapReduce的补充和WordCount简单实战(二):https://homxuwang.github.io/2018/07/29/MapReduce%E7%9A%84%E8%A1%A5%E5%85%85%E5%92%8CWordCount%E7%AE%80%E5%8D%95%E5%AE%9E%E6%88%982/123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115package com.hadoop.project;import com.kumkee.userAgent.UserAgent;import com.kumkee.userAgent.UserAgentParser;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;import java.util.regex.Matcher;import java.util.regex.Pattern;/** * 使用MapReduce 统计浏览器的访问次数 */public class LogApp &#123; public static class MyMapper extends Mapper&lt;LongWritable,Text,Text,LongWritable&gt; &#123; LongWritable plusone = new LongWritable(1); private UserAgentParser userAgentParser ; @Override protected void setup(Context context) throws IOException, InterruptedException &#123; userAgentParser = new UserAgentParser(); &#125; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; UserAgent agent ; //先读入没一行数据 String line = value.toString(); String source = line.substring(getCharacterPosition(line,"\"",5) + 1); agent = userAgentParser.parse(source); String browser = agent.getBrowser(); context.write(new Text(browser),plusone); &#125; @Override protected void cleanup(Context context) throws IOException, InterruptedException &#123; userAgentParser = null; &#125; &#125; public static class MyReducer extends Reducer&lt;Text,LongWritable,Text,LongWritable&gt;&#123; @Override protected void reduce(Text key, Iterable&lt;LongWritable&gt; values, Context context) throws IOException, InterruptedException &#123; long sum = 0 ; for(LongWritable value : values)&#123; sum += value.get();//通过get()转化为java中的类型 &#125; //最终统计结果输出 context.write(key,new LongWritable(sum)); &#125; &#125; /** * 获取指定字符串中指定标识的字符串出现的索引 * @return */ private static int getCharacterPosition(String value,String operator,int index)&#123; Matcher slashMatcher = Pattern.compile(operator).matcher(value); int mIndex = 0; while(slashMatcher.find())&#123; mIndex ++; if(mIndex == index)&#123; break; &#125; &#125; return slashMatcher.start(); &#125; public static void main(String[] args) throws IOException,ClassNotFoundException,InterruptedException &#123; Configuration configuration = new Configuration(); Path outputPath = new Path(args[1]); FileSystem fileSystem = FileSystem.get(configuration); if(fileSystem.exists(outputPath))&#123; fileSystem.delete(outputPath,true); System.out.println("filePath exists,but it has deleted"); &#125; Job job = Job.getInstance(configuration,"LogBrowser"); job.setJarByClass(LogApp.class); FileInputFormat.setInputPaths(job,new Path(args[0])); job.setMapperClass(LogApp.MyMapper.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(LongWritable.class); job.setCombinerClass(LogApp.MyReducer.class); job.setReducerClass(LogApp.MyReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(LongWritable.class); FileOutputFormat.setOutputPath(job,new Path(args[1])); System.out.println(job.waitForCompletion(true)? 0 : 1); &#125;&#125; 配置pom.xml文件在生产环境中hadoop的包是不需要的，所以加一个&lt;scope&gt;provided&lt;/scope&gt; 而因为UserAgentParser是第三方的包，在mvn打包的时候需要进行配置才能够添加进去，使用maven-assembly-plugin12345678910111213&lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;/configuration&gt;&lt;/plugin&gt; 打包命令使用1mvn assembly:assembly 可以看到打包成功的信息hdfs-1.0-SNAPSHOT-jar-with-dependencies.jar就是打包成功的文件 然后传到指定位置（或者服务器），我这里单机的伪分布式框架，所以传到本地一个目录就可以：1scp hdfs-1.0-SNAPSHOT-jar-with-dependencies.jar ~/lib 然后将日志数据上传到hdfs的根目录: 执行命令1hadoop jar /home/hadoop/lib/hdfs-1.0-SNAPSHOT-jar-with-dependencies.jar com.hadoop.project.LogApp hdfs://localhost:9000/access.log.10 hdfs://localhost:9000/Log/logBrowser 可以看到成功信息在目录中查看：查看结果：这与在单元测试中看到的信息是一样的： 下一步可以写到RDBMS中或者NOSQL数据库中，进一步进行可视化展示这只是单纯的统计了浏览器的记录，在实际应用中，应该根据日志中的时间记录统计每一天或者没一星期的时间周期中的访问记录。 踩到的坑mvn打包时的错误在github仓库下载了UserAgentParser工具类后，需要在自己电脑上打包（mvn clean package -DskipTests）。打包过程中报错：这里报错为(请使用 -source 5 或更高版本以启用泛型)和-source1.3 中不支持泛型.后来发现原来是下载的项目年代过久（已经7年了）。我本地的环境是maven 3.0.5和java 1.7.所以在下载的项目的pom.xml中添加一段代码，用来指定打包的项目所使用的java版本：123456789101112&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.7&lt;/source&gt; &lt;target&gt;1.7&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 这样打包就可以了 maven引入刚才打包的文件出现的问题在打包完成后,还需要将打包后的文件放入maven的本地仓库以便使用(mvn clean install -DskipTest).打包完成后的信息： 可以看到打包完成后的目录在/home/hadoop/.m2/respository/com/kumkee/下，这个是maven的本地默认仓库目录。而我在项目中使用的目录是后来指定的目录，所以在引用时一直不行，后来终于找到了这个问题 可以看到本地的仓库指定在/home/hadoop/文档/maven_repos，所以在打包的时候可以指定打包到本地的maven目录，即mvn clean install -DskipTest -Dmaven.repo.local=/home/hadoop/文档/maven_repos 这样就可以在项目中引入了 MapReduce作业时报错错误信息： 1TaskAttemptListenerImpl: Task: attempt_1533460421274_0001_m_000000_0 - exited : java.io.IOException: Type mismatch in key from map: expected org.apache.hadoop.io.LongWritable, received org.apache.hadoop.io.Text 原来是写成了两个job.setMapOutpuKeyClass()方法，改正后重新编译jar包并运行即可]]></content>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实现映射]]></title>
    <url>%2F2018%2F08%2F01%2F%E5%AE%9E%E7%8E%B0%E6%98%A0%E5%B0%84%2F</url>
    <content type="text"><![CDATA[映射也就是Map,这里是指一对一的映射。也成为字典。 映射迎来存储键值数据对(Key,Value) 根据Key寻找对应的Value 用链表或者二分搜索树实现比较简单 BST的结构:123456class Node &#123; K key; V value; Node left; Node right;&#125; 链表的结构:12345class Node &#123; K key; V value; Node next;&#125; 即本来存储一个数据的节点现在存储两个(Key,Value). 定义Map接口:123456789public interface Map&lt;K,V&gt; &#123; void add(K key,V value); V remove(K key); //删除key对应的键值对,并返回值 boolean contains(K key); //是否存已在key V get(K key); //获得key对应的value值 void set(K key,V newValue); //更新值 int getSize(); boolean isEmpty();&#125; 基于链表实现Map123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100public class LinkedListMap&lt;K,V&gt; implements Map&lt;K,V&gt; &#123; //定义节点 private class Node &#123; public K key; public V value; public Node next; public Node(K key,V value,Node node)&#123; this.key = key; this.value = value; this.next = null; &#125; public Node(K key)&#123; this(key,null,null); &#125; public Node()&#123; this(null,null,null); &#125; public String toString()&#123; return key.toString() + ":" + value.toString(); &#125; &#125; private Node dummyHead; private int size; public LinkedListMap()&#123; dummyHead = new Node(); size = 0; &#125; @Override public void add(K key, V value) &#123; Node node = getNode(key); if(node == null)&#123; dummyHead.next = new Node(key,value,dummyHead.next); size ++; &#125; else //如果已经存在,则进行值的覆盖 node.value = value; &#125; @Override public V remove(K key) &#123; Node prev = dummyHead; while(prev.next != null)&#123; if(prev.next.key.equals(key)) break; prev = prev.next; &#125; Node delNode; if(prev.next != null)&#123; delNode = prev.next; prev.next = delNode.next; delNode.next = null; return delNode.value; &#125; return null; &#125; @Override public boolean contains(K key) &#123; return getNode(key) != null; &#125; @Override public V get(K key) &#123; Node result = getNode(key); return result == null ? null : result.value; &#125; @Override public void set(K key, V newValue) &#123; Node result = getNode(key); if(result != null) result.value = newValue; else throw new IllegalArgumentException(key + "doesn't exists"); &#125; @Override public int getSize() &#123; return size; &#125; @Override public boolean isEmpty() &#123; return size == 0; &#125; private Node getNode(K k)&#123; Node cur = dummyHead.next; while(cur != null)&#123; if(cur.key.equals(k)) return cur; cur = cur.next; &#125; return null; &#125; &#125; 基于二分搜索树实现Map123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158public class BSTMap&lt;K extends Comparable&lt;K&gt;,V&gt; implements Map&lt;K,V&gt; &#123; private class Node &#123; public K key; public V value; public Node left,right; public Node(K key,V value)&#123; this.key = key; this.value = value; this.left = null; this.right = null; &#125; &#125; private Node root; private int size; @Override //向二分搜索树中添加新元素(key,value) public void add(K key, V value) &#123; root = add(root,key,value); &#125; @Override //借助辅助函数remove(Node node,K key) public V remove(K key) &#123; Node node = getNode(root,key); if(node != null)&#123; root = remove(root,key); return node.value; &#125; return null; &#125; @Override public boolean contains(K key) &#123; return getNode(root,key) != null ? true : false; &#125; @Override public V get(K key) &#123; Node node = getNode(root,key); return node == null ? null : node.value; &#125; @Override public void set(K key, V newValue) &#123; Node node = getNode(root,key); if(node == null) throw new IllegalArgumentException(key + "doesn't exist!"); node.value = newValue; &#125; @Override public int getSize() &#123; return size; &#125; @Override public boolean isEmpty() &#123; return size == 0; &#125; //向以node为根的二分搜索树中插入元素,采用递归算法 //返回插入新节点后二分搜索树的根 private Node add(Node node,K key,V value)&#123; if(node == null)&#123; size ++; return new Node(key,value); &#125; if(key.compareTo(node.key) &lt; 0)&#123; node.left = add(node.left,key,value); &#125; if(key.compareTo(node.key) &gt; 0)&#123; node.right = add(node.right,key,value); &#125; else //key.compareTo(node.key)==0 node.value = value; return node; &#125; //返回以node为根节点的二分搜索树中,key所在的节点 private Node getNode(Node node,K key)&#123; if(node == null)&#123; return null; &#125; if(key.compareTo(node.key) == 0)&#123; return node; &#125; if(key.compareTo(node.key) &lt; 0)&#123; return getNode(node.left,key); &#125; else&#123; //key.compareTo(node.key) &gt; 0) return getNode(node.right,key); &#125; &#125; //删除掉以node为根的二分搜索树中键为key的节点 //返回删除节点后新的二分搜索树的根 private Node remove(Node node,K key)&#123; if(node == null) return null; if(key.compareTo(node.key) &lt; 0)&#123; node.left = remove(node.left,key); return node; &#125; else if(key.compareTo(node.key) &gt; 0 )&#123; node.right = remove(node.right,key); return node; &#125; else&#123;//key.compareTo(node.key) == 0 //待删除左子树为空 if(node.left == null)&#123; Node rightNode = node.right; node.right = null; size --; return rightNode; &#125; //待删除右子树为空 if(node.right == null)&#123; Node leftNode = node.left; node.left = null; size --; return leftNode; &#125; //待删除左右子树均不为空 //先找到比待删除节点大的最小节点,即待删除节点右子树的最小节点 //用这个节点顶替待删除节点的位置 Node successor = minmum(node.right); successor.right = removeMin(node.right); successor.left = node.left; node.left = node.right = null; return successor; &#125; &#125; //返回以node为根的二分搜索树的最小值所在的节点 private Node minmum(Node node)&#123; if(node.left == null)&#123; return node; &#125; return minmum(node.left); &#125; //删除掉以node为根的二分搜索树中的最小节点 //返回删除节点后的最新的二分搜索树的根 private Node removeMin(Node node)&#123; if(node.left == null)&#123; Node rightNode = node.right; node.right = null; size --; return rightNode; &#125; node.left = removeMin(node.left); return node; &#125;&#125; 其实明显可以感受到BSTMap的存储速度要快于LinkedListMap 基于链表的Map和基于二分搜索树的Map的时间复杂度对比 操作 LinkedListMap BSTMap(平均) BSTMap最差) 增add O(n) O(logn) O(n) 删remove O(n) O(logn) O(n) 改set O(n) O(logn) O(n) 查get O(n) O(logn) O(n) 查contains O(n) O(logn) O(n)]]></content>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实现集合]]></title>
    <url>%2F2018%2F08%2F01%2F%E5%AE%9E%E7%8E%B0%E9%9B%86%E5%90%88%2F</url>
    <content type="text"><![CDATA[因为集合是不存放重复元素的，所以使用二分搜索树来实现集合很合适。集合的几个简单方法: void add(E) //添加元素，但不能添加重复元素 void remove(E) //删除元素E boolean contains(E) //是否包含元素E int getSize() //获得集合的size boolean isEmpty() //判断集合是否为空 基于二分搜索树的实现首先使用二分搜索树做底层实现集合类。二分搜索树的实现代码：https://homxuwang.github.io/2018/07/18/%E4%BA%8C%E5%88%86%E6%90%9C%E7%B4%A2%E6%A0%91/ 创建集合接口类Set：1234567public interface Set&lt;E&gt; &#123; void add(E e); void remove(E e); boolean isEmpty(); boolean containes(E e); int getSize();&#125; 然后创建实现集合的类BSTSet:123456789101112131415161718192021222324252627282930public class BSTSet&lt;E extends Comparable&lt;E&gt;&gt; implements Set&lt;E&gt; &#123; private BST&lt;E&gt; bst; public BSTSet() &#123; bst = new BST&lt;&gt;(); &#125; @Override public void add(E e) &#123; bst.add(e); &#125; @Override public void remove(E e) &#123; bst.remove(e); &#125; @Override public boolean isEmpty() &#123; return bst.isEmpty(); &#125; @Override public boolean containes(E e) &#123; return bst.contains(e); &#125; @Override public int getSize() &#123; return bst.size(); &#125;&#125; 其实BSTSet集合类只是简单的调用了其基础类BST类的方法。就可以实现集合类的各个功能。 基于链表的实现链表结构的实现首先链表的实现代码:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180public class LinkedList&lt;E&gt; &#123; private class Node&#123; public E e; public Node next; /** * 构造函数 * @param e * @param next */ public Node(E e,Node next)&#123; this.e = e; this.next = next; &#125; public Node(E e)&#123; this(e,null); &#125; public Node()&#123; this(null,null); &#125; @Override public String toString()&#123; return e.toString(); &#125; &#125; private Node dummyHead; private int size; public LinkedList()&#123; dummyHead = new Node(); size = 0; &#125; //返回链表中的元素个数 public int getSize()&#123; return size; &#125; //返回链表是否为空 public boolean isEmpty()&#123; return size == 0; &#125; //在链表头添加新的元素e public void addFirst(E e)&#123;// Node node = new Node(e);// node.next = head;// head = node; //和上面三行相等 add(0,e); &#125; //在链表末尾添加新的元素e public void addLast(E e)&#123; add(size,e); &#125; //在中间位置插入新的元素e public void add(int index,E e)&#123; if(index &lt; 0 || index &gt; size )&#123; throw new IllegalArgumentException("illegal index"); &#125; Node prev = dummyHead; for(int i = 0 ; i &lt; index ; i++)&#123; prev = prev.next; &#125; Node node = new Node(e); node.next = prev.next; prev.next = node; size ++; &#125; //获得链表中index位置的元素 public E get(int index)&#123; if(index &lt; 0 || index &gt;= size )&#123; throw new IllegalArgumentException("Get failed.Illegal index."); &#125; Node cur = dummyHead.next; for(int i = 0 ; i &lt; index ; i++)&#123; cur = cur.next; &#125; return cur.e; &#125; //获得链表的第一个元素 public E getFirst()&#123; return get(0); &#125; //获得链表的最后一个元素 public E getLast()&#123; return get(size - 1); &#125; //修改链表index位置的元素为e public void set(int index,E e)&#123; if(index &lt; 0 || index &gt;= size )&#123; throw new IllegalArgumentException("Set failed.Illegal index."); &#125; Node cur = dummyHead.next; for(int i = 0 ; i &lt; index ; i++ )&#123; cur = cur.next; &#125; cur.e = e; &#125; //检查链表中是否有元素e public boolean contains(E e)&#123; Node cur = dummyHead.next; while(cur != null)&#123; if(cur.e.equals(e)) return true; cur = cur.next; &#125; return false; &#125; // 从链表中删除index(0-based)位置的元素, 返回删除的元素 // 在链表中不是一个常用的操作，练习用：） public E remove(int index)&#123; if(index &lt; 0 || index &gt;= size) throw new IllegalArgumentException("Remove failed. Index is illegal."); Node prev = dummyHead; for(int i = 0 ; i &lt; index ; i ++) prev = prev.next; Node retNode = prev.next; prev.next = retNode.next; retNode.next = null; size --; return retNode.e; &#125; // 从链表中删除第一个元素, 返回删除的元素 public E removeFirst()&#123; return remove(0); &#125; // 从链表中删除最后一个元素, 返回删除的元素 public E removeLast()&#123; return remove(size - 1); &#125; // 从链表中删除元素e public void removeElement(E e)&#123; Node prev = dummyHead; while(prev.next != null)&#123; if(prev.next.e.equals(e)) break; prev = prev.next; &#125; if(prev.next != null)&#123; Node delNode = prev.next; prev.next = delNode.next; delNode.next = null; &#125; &#125; @Override public String toString()&#123; StringBuilder res = new StringBuilder(); Node cur = dummyHead.next; while (cur != null)&#123; res.append(cur.e + "-&gt;"); cur = cur.next; &#125; res.append("NULL"); return res.toString(); &#125;&#125; 基于链表的集合的实现12345678910111213141516171819202122232425262728293031public class LinkedListSet&lt;E&gt; implements Set&lt;E&gt; &#123; private LinkedList&lt;E&gt; list; public LinkedListSet() &#123; list = new LinkedList&lt;&gt;(); &#125; @Override public void add(E e) &#123; if(!list.contains(e)) //判断是否已存在元素e list.addFirst(e); //在链表头添加元素比较省时间 &#125; @Override public void remove(E e) &#123; list.removeElement(e); &#125; @Override public boolean isEmpty() &#123; return list.isEmpty(); &#125; @Override public boolean containes(E e) &#123; return list.contains(e); &#125; @Override public int getSize() &#123; return list.getSize(); &#125;&#125; 时间复杂度对比 操作 LinkedListSet BSTSet 增add O(n) O(logn) 删remove O(n) O(logn) 查contains O(n) O(logn) BSTSet的时间复杂度O(logn)的情况是指平均情况，在极端情况下,有可能就是一个链表，这时候的时间复杂度就是O(n). 测试用例用代码比较两种数据结构的时间,测试类FileOperation,找一个英文电子书进行统计分析,统计其词汇总数和词汇量.1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import java.io.BufferedInputStream;import java.io.File;import java.io.FileInputStream;import java.io.IOException;import java.util.ArrayList;import java.util.Locale;import java.util.Scanner;public class FileOperation &#123; //读取文件名称为filename中的内容，并且将其中包含的所有词语放进words中 public static boolean readFiles(String filename,ArrayList&lt;String&gt; words)&#123; if(filename == null || words == null)&#123; System.out.println("filename is null or words is null"); return false; &#125; //文件读取 Scanner scanner; try&#123; File file = new File(filename); if(file.exists())&#123; FileInputStream fis = new FileInputStream(file); scanner = new Scanner(new BufferedInputStream(fis),"UTF-8"); scanner.useLocale(Locale.ENGLISH); &#125;else return false; &#125;catch (IOException e)&#123; System.out.println("cannot open "+ filename); return false; &#125; ///简单分词 if(scanner.hasNextLine())&#123; String contents = scanner.useDelimiter("\\A").next(); int start = firstCharacterIndex(contents,0); for(int i = start + 1; i &lt; contents.length();)&#123; if(i == contents.length() || !Character.isLetter(contents.charAt(i)))&#123; String word = contents.substring(start,i).toLowerCase(); words.add(word); start = firstCharacterIndex(contents,i); i = start +1; &#125;else i++; &#125; &#125; return true; &#125; private static int firstCharacterIndex(String s,int start)&#123; for(int i = start;i&lt;s.length();i++)&#123; if(Character.isLetter(s.charAt(i))) return i; &#125; return s.length(); &#125;&#125; Main函数：1234567891011121314151617181920212223242526import java.util.ArrayList;public class Main &#123; public static void main(String[] srgs)&#123; System.out.println("Harry"); ArrayList&lt;String&gt; words1 = new ArrayList&lt;&gt;(); FileOperation.readFiles("C:\\Users\\homxu\\Desktop\\Harry Potter and the Sorcerer'_one .mobi",words1); System.out.println("Total words:"+words1.size()); BSTSet&lt;String&gt; set1 = new BSTSet&lt;&gt;(); for(String word:words1) set1.add(word); System.out.println("Dif Words:" + set1.getSize()); ArrayList&lt;String&gt; words2 = new ArrayList&lt;&gt;(); FileOperation.readFiles("C:\\Users\\homxu\\Desktop\\Harry Potter and the Sorcerer'_one .mobi",words2); System.out.println("Total words:"+words2.size()); LinkedListSet&lt;String&gt; set2 = new LinkedListSet&lt;&gt;(); for(String word:words2)&#123; set2.add(word); &#125; System.out.println("Dif Words:" + set2.getSize()); &#125;&#125; 我的电脑上的运行结果，使用两种底层的数据结构实现的集合类结果是一样的。但是运行过程中明显感觉到BSTSet的速度要比LinkedListSet的速度快]]></content>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce的补充和WordCount简单实战(二)]]></title>
    <url>%2F2018%2F07%2F29%2FMapReduce%E7%9A%84%E8%A1%A5%E5%85%85%E5%92%8CWordCount%E7%AE%80%E5%8D%95%E5%AE%9E%E6%88%982%2F</url>
    <content type="text"><![CDATA[MapReduce编程之WordCount基于JAVA开发 Text参数继承了BinaryComparable并实现了WritableComparable接口，可以把它理解为JAVA里面的字符串。 首先看一些基本的知识： Mapper类 Context代表的上下文 setup代表任务开始的时候执行的操作，且只执行一次。可以在这里进行数据库链接等操作 map方法，对于输入的input在每个键值对出发的时候就调用 cleanup方法是表示在任务结束的时候执行一次 run方法不需要手动调用 关键点是重写map方法 Reducer类 setup和cleanup方法同mapper方法类似 reduce方法，每个键值对都会被调用一次 run是写好的模板模式 关键点是重写reduce方法 先写好基本框架 然后复写map和reduce方法 完整代码：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697package hadoop.hdfs.mapreduce;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;/** * 使用MapReduce开发WordCount应用程序 */public class WordCountApp &#123; /** * Map:读取输入的文件 */ public static class MyMapper extends Mapper&lt;LongWritable,Text,Text,LongWritable&gt;&#123; //定义一个count LongWritable plusone = new LongWritable(1); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; //先读入每一行数据 String line = value.toString(); //然后按照指定分隔符分割 String[] words = line.split(" "); for(String word:words)&#123; //使用上下文context进行输出 context.write(new Text(word),plusone); &#125; &#125; &#125; /** * Reduce:归并操作 */ public static class MyReducer extends Reducer&lt;Text,LongWritable,Text,LongWritable&gt;&#123; @Override //Iterable&lt;LongWritable&gt; values是一个集合，因为一个单词交到一个reduce去处理，所以会出现多次 protected void reduce(Text key, Iterable&lt;LongWritable&gt; values, Context context) throws IOException, InterruptedException &#123; long sum = 0; for(LongWritable value:values)&#123; sum += value.get();//通过get()转化为java中的类型 &#125; //最终统计结果的输出 写入上下文 context.write(key,new LongWritable(sum)); &#125; &#125; /** * 定义Driver方法，封装MapReduce作业的所有信息 * @param args */ public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; //step1 创建configuration()(org.apache.hadoop.conf.Configuration) Configuration configuration = new Configuration(); //step2 Job.getInstance()拿到一个实例 (org.apache.hadoop.mapreduce.Job;) Job job = Job.getInstance(configuration,"wordcount"); //step3 设置job的处理类(即创建的这个类) job.setJarByClass(WordCountApp.class); //step4 设置作业处理的输入路径 FileInputFormat.setInputPaths(job,new Path(args[0])); //step5 设置Map相关参数 //step5.1 设置map处理类 job.setMapperClass(MyMapper.class); //step5.2 设置map的key输出的类型和map的value输出的类型那个 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(LongWritable.class); //step6 设置reduce相关参数 //step6.1 设置reduce处理类 job.setReducerClass(MyReducer.class); //step6.2 设置reduce的key输出类型和value输出类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(LongWritable.class); //step7 设置作业处理的输出路径 FileOutputFormat.setOutputPath(job,new Path(args[1])); //step8 提交作业并输出结果 System.exit(job.waitForCompletion(true)?0:1); &#125;&#125; 然后进行打包编译切换到项目目录运行mvn clean package -DskipTests 可以看到构建成功 编译成功后在target目录下 如果mvn命令不能使用需要安装相应环境 这里由于网络原因我下了好久 orz 将jar包拷贝到指定目录scp hdfs-1.0-SNAPSHOT.jar ~/lib 查看hdfs上的文件,并查看全路径 运行 jar包hadoop jar jar包目录 组类 (在idea中右键选中然后 CopyReference) 要传入的文件路径 要输出的文件路径 我的机器上的指令：hadoop jar /home/hadoop/lib/hdfs-1.0-SNAPSHOT.jar hadoop.hdfs.mapreduce.WordCountApp hdfs://localhost:9000/hello.txt hdfs://localhost:9000/hdfsapi/wordcountresult 输入文件的内容： 可以在浏览器 http://localhost:8088/cluster 查看yarn的作业 查看结果，操作成功 相同的代码和脚本再次运行会报错，因为在MR中，输出文件是不能存在的。所以要在每次运行后换新的路径或者删除旧的文件可以手工建一个shell脚本或者在JAVA代码中进行操作shell 脚本12hdfs dfs rm -r 文件路径hadoop jar jar包目录 组类 (在idea中右键选中然后 CopyReference) 要传入的文件路径 要输出的文件路径 要给脚本加上相应权限chmod u+x 脚本名称 在JAVA中完成自动删除功能在//step1 和 //step2 中间清理已经存在的目录1234567//step1.5 准备删除已经存在的文件目录 Path outputPath = new Path(args[1]); FileSystem fileSystem = FileSystem.get(configuration); if(fileSystem.exists(outputPath))&#123; fileSystem.delete(outputPath,true); System.out.println("filePath exists,but it has deleted"); &#125; 重新编译代码执行，就可以了 可以看到又一行输出ilePath exists,but it has deleted,同时执行成功 MapReduce编程之Combiner 本地的reducer 减少MapTask输出的数据量及网络传输量 在Map过程中先对key相同的值进行一个归并，然后在传输到reduce上，这样就减少了传输的数据数量。可以看成是在map端的一个小的reduce操作。 在上面代码的基础上，在step5的map操作和step6的reduce操作中间价一个combiner操作。12//step5.5 设置Combiner处理类，其实在逻辑上和reduce是一样的job.setCombinerClass(MyReducer.class); 重新编译，拷贝到hdfs目录下，执行 可以看到执行过程中有Combine信息，Combin input records=6和Combin output records=5,说明是生效了的(可以对比之前的操作，combine操作records=0) 虽然它减少了一些数据量，但是它是有适应场景的。在求和、求次数的时候适用，但是在求平均数等操作的时候，结果就会有问题，这里要注意 MapReduce编程之Partitioner Partitioner决定MapTask输出的数据交由哪个ReduceTask处理 默认实现：分发的key的hash值对ReduceTask个数取模 新建一个用来测试的文档：animal.txt 上传到hdfs中： 在ieda中，拷贝一份WordCountApp的代码，命名为PartitionerApp.按照空格拆分其实就是动物名字和动物数量，所以在Mapper类的map方法中修改代码：12345678910111213141516/** * Map:读取输入的文件 */ public static class MyMapper extends Mapper&lt;LongWritable,Text,Text,LongWritable&gt;&#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; //先读入每一行数据 String line = value.toString(); //然后按照指定分隔符分割 String[] words = line.split(" "); context.write(new Text(words[0]),new LongWritable(Long.parseLong(words[1]))); &#125; &#125; 还要新建一个Partition类123456789101112131415public static class MyPartitioner extends Partitioner&lt;Text,LongWritable&gt;&#123; @Override public int getPartition(Text key, LongWritable value, int numPartitions) &#123; if(key.equals("cat"))&#123; return 0; &#125; if (key.equals("dog"))&#123; return 1; &#125; if(key.equals("bird"))&#123; return 2; &#125; return 3; &#125; &#125; 在step6 和 step7之间添加代码：1234//step6.5 设置job的Partition job.setPartitionerClass(MyPartitioner.class); //step6.5.1 设置reduce的数量，不然不生效,这里设置4个，因为MyPartitioner类中有4种情况 job.setNumReduceTasks(4); 然后编译，将执行的类名和输入的数据改一下hadoop jar /home/hadoop/lib/hdfs-1.0-SNAPSHOT.jar hadoop.hdfs.mapreduce.PartitionerApphadoop.hdfs.mapreduce.PartitionerApp hdfs://localhost:9000//hdfsapi/animal.txt hdfs://localhost:9000/hdfsapi/Partitionresult 运行后查看结果 Partition会把符合规则的key送到指定的reduce处理，分别生成相应的结果。 配置jobHistory默认情况下这个功能是不开启的 找到mapreduce的配置mapred-site.xml 在&lt;configuration&gt;&lt;/configuration&gt;中间增加12345678910111213141516171819202122&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;localhost:10020&lt;/value&gt; &lt;description&gt;MR JobHistory Server管理的日志的存放位置&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;localhost:19888&lt;/value&gt; &lt;description&gt;查看历史服务器已经运行完的Mapreduce作业记录的web地址，需要启动该服务才行&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.done-dir&lt;/name&gt; &lt;value&gt;mr-history/done&lt;/value&gt; &lt;description&gt;MR JobHistory Server管理的日志的存放位置,默认:/mr-history/done&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.intermediate-done-dir&lt;/name&gt; &lt;value&gt;mr-history/mapred/tmp&lt;/value&gt; &lt;description&gt;MapReduce作业产生的日志存放位置，默认值:/mr-history/tmp&lt;/description&gt; 保存后先停掉yarn再重启，启动之后还要再启动mr-jobhistory-daemon.sh使用mr-jobhistory-daemon.sh start historyserver这样就启动成功了可以看到这时候多了一个进程JobHistoryServer 还要配置yarn-site.xml.在&lt;configuration&gt;&lt;/configuration&gt;中添加1234&lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; 再次重新启动yarn 使用mapreduce下的例子进行测试1/usr/local/hadoop/share/hadoop/mapreduce$ hadoop jar hadoop-mapreduce-examples-2.9.0.jar pi 2 3 我在启动后仍然出现问题，重启了hdfs和yarn都没用，不过后来尝试关闭jobhistory1./sbin/mr-jobhistory-daemon.sh stop historyserver 再重启1./sbin/mr-jobhistory-daemon.sh start historyserver 竟然可以了]]></content>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce的补充和WordCount简单实战(一)]]></title>
    <url>%2F2018%2F07%2F28%2FMapReduce%E7%9A%84%E8%A1%A5%E5%85%85%E5%92%8CWordCount%E7%AE%80%E5%8D%95%E5%AE%9E%E6%88%981%2F</url>
    <content type="text"><![CDATA[官网介绍：http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html 首先回顾一些MapReduce的基础知识：https://homxuwang.github.io/2018/04/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AD%EF%BC%89%E2%80%94%E2%80%94MapReduce/ Hadoop MapReduce是Google MapReduce的实现 MapReduce的优点： 海量数据离线处理 易开发(JAVA API) 易运行 MapReduce的缺点： 实时流式计算（MR是根据请求服务的方式进行计算；多个应用程序存在依赖关系，MR的作业，数据需要落地到HDFS或者磁盘。所以不能实现实时流式计算） MapReduce的执行过程 参考：https://www.cnblogs.com/ahu-lichang/p/6645074.html 官网的介绍： A MapReduce job usually splits the input data-set into independent chunks which are processed by the map tasks in a completely parallel manner. The framework sorts the outputs of the maps, which are then input to the reduce tasks. Typically both the input and the output of the job are stored in a file-system. The framework takes care of scheduling tasks, monitoring them and re-executes the failed tasks. Input –&gt; Spliting 一个文件被分成很多个块（默认情况下一个split对应HDFS中的一个block，用户可以进行修改） Spliting –&gt; Mapping 一个块交由一个Map任务处理，处理完的结果写到本地 Mapping –&gt; Shuffling –&gt; Reducing写到本地的文件通过Shuffle后进行传输，把相同的key写到一个Reduce中,在Reduce中进行统计 Reducing统计的结果最终写到文件系统上 看看官网的解释： The MapReduce framework operates exclusively on pairs, that is, the framework views the input to the job as a set of pairs and produces a set of pairs as the output of the job, conceivably of different types. The key and value classes have to be serializable by the framework and hence need to implement the Writable interface. Additionally, the key classes have to implement the WritableComparable interface to facilitate sorting by the framework. Input and Output types of a MapReduce job: (input) -&gt; map -&gt; -&gt; combine -&gt; -&gt; reduce -&gt; (output) 关于Writable接口在上面的介绍中看到，key和value需要实现Writable接口，并且key还需要实现WritableComparable接口 这个接口需要反复阅读 关于Writable接口的介绍：http://hadoop.apache.org/docs/current/api/org/apache/hadoop/io/Writable.html 关于WritableComparable接口的介绍：http://hadoop.apache.org/docs/current/api/org/apache/hadoop/io/WritableComparable.html 在Writable接口中主要实现write和readFields方法。 再看上文面的wordcount的图和(input) &lt;k1, v1&gt; -&gt; map -&gt; &lt;k2, v2&gt; -&gt; combine -&gt; &lt;k2, v2&gt; -&gt; reduce -&gt; &lt;k3, v3&gt; (output)这个过程： 其中k1就是偏移量。第一行的第一个字符从0开始，v1就是这一行的数据Deer Bear River。那么第二行的偏移量就是第一行的字符的长度相加，值就是Car Car River。以此类推 经过一层转换k2就是上面每一行的单词,每个单词相当于是从v1中拆分出来(Split(“ “))，是一个Text类型，每个单词就是一个1。v2就是一个IntWritable或LongWritable类型。 reduce输出的就是每个单词输出的总和。k3就是每个单词，v3就是单词出现的总和。 JAVA API的简单介绍 看上图，首先读取文件使用InputFormat类，它是一个接口，在源码中描述为 InputFormat describes the input-specification for a Map-Reduce job. InputFormat的实现类中，用的比较多的是FileInputFormat类.这是一个读取文件系统的基本的类.但是FileInputFormat类仍然是个抽象类。那么继续找它的子类可以看到TextInputFormat类.这时候它就是一个实现的类了 官方文档的介绍： An {@link InputFormat} for plain text files. Files are broken into lines. Either linefeed or carriage-return are used to signal end of line. Keys are the position in the file, and values are the line of text.. 其中InputFormat中有几个关键的方法： 1) InputSplit[] getSplits(JobConf job, int numSplits) throws IOException;即将一个输入文件分成很多Split，每一个Split交给一个MapTask处理的方法。它的返回值是一个数组，可见一个输入文件可能会的到好几个InputSplit 2) RecordReader&lt;K, V&gt; getRecordReader(InputSplit split,JobConf job,Reporter reporter) throws IOException;它是一个记录读取的方法，从参数可以看到，它从InputSplit[]数组中读进数据，可以知道每一行的数据是什么。 在InputFormat读进数据后(对于文本就是使用TextInputFormat），从图中可以看出，被拆分成好多个Split。拿到Split后，使用RR(RecordReader)把每个Split中的数据读取出来,一行一行的读，每读一行，交由一个map处理.Partitioner将相同的key交到同一个Reduce上，从图中可以看出，key可能会被发送到node1或者node2.中间有一个shuffle的过程，结果交由reduce处理。处理完的结果交给OutputFormat。 OutputFormat OutputFormat describes the output-specification for a Map-Reduce job. 其中getRecordWriter方法就对应InputFormat的getRecordReader方法 继续寻找它的实现类-&gt;FileOutputFormat-&gt;TextOutputFormat An {@link OutputFormat} that writes plain text files.将数据以文本的方式写出去 几个核心概念 Split Split是交由MapReduce作业来处理的数据块，是MapReduce中最小的计算单元 HDFS：blocksize是HDFS中最小的存储单元 128M（或者自己设定） 默认情况下：它们两个一一对应（也可以手动设置） InputFormatInputFormat将输入数据进行分片(split)：InputSplit[] getSplits(JobConf job, int numSplits) throws IOException默认实用比较多的是TextInputFormat,处理文本格式的数据 OutputFormat 和InputFormat对应 Combiner Partitioner 在通过一张图对以上内容进行梳理： MapReduce架构MapReduce1.x架构 1) JobTracker:JT 作业的管理者 将作业分解成一堆任务：Task（包括MapTask和ReduceTask) 将任务分派给TaskTracker运行 作业的监控、容错处理（task作业挂了，重启task） 在一定时间间隔内，JT没有收到TT的心跳信息,则将任务分配到其他TT上执行2) TaskTracker：TT 任务的执行者 在TT上执行Task（MapTask和ReduceTask） 会与JT进行交互：执行/启动/停止作业，发送心跳信息给JT3) MapTask 自己开发的map任务交由该Task来处理 解析每条记录的数据，交给自己的map方法处理 将map的输出结果写到本地磁盘（有些作业只有Map，则写到HDFS）4） ReduceTadk 将MapTask输出的数据进行读取 按照数据进行分组传给自己编写的reduce方法处理 处理结果输出 MapReduce2.x架构 和yarn中的流程类似,MapReduce可以在YARN上跑。 下一篇实战。]]></content>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IDEA+JAVA编写HDFS代码]]></title>
    <url>%2F2018%2F07%2F25%2FIDEA%2BJAVA%E7%BC%96%E5%86%99HDFS%E4%BB%A3%E7%A0%81%2F</url>
    <content type="text"><![CDATA[使用JAVA API操作HDFS文件 IDEA+Maven创建Java工程 添加HDFS相关依赖 开发JAVA API操作HDFS文件 IDEA环境配置首先打开IDEA,选择Maven项目，选择quickstart接着下一步 输入一些基本信息 这里IDEA有自己集成的Maven版本,在User settings file可以使用系统自带的xml文件，也可以选择自己下载的Maven。可以在网上下载一个Maven压缩包，解压到指定的文件夹，在使用的时候选择这个文件夹下的settings.xml就行了。 同样的Local repository也可以自己选择一个本地目录。 下一步填一些基本信息 然后等待Maven初始化完成，可以看到下面console窗口的SUCCESS信息。初始的项目集成了单元测试junit包。 接着配置开发hadoop需要的依赖。如图，添加12345&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt;&lt;/dependency&gt; 这里我配置的hadoop依赖版本是2.9.0,根据自己的实际版本进行修改另外，这里的${hadoop.version}在上面的&lt;properties&gt;&lt;/properties&gt;标签中进行了配置，其实和上面的代码的效果是一样的。 然后Maven会自己下载相应的依赖，可以看到在右侧已经下载成功所需的依赖。然后在右侧栏的hadoop.hdfs中新加一个包，然后新建一个类进行测试 以下是单元测试代码，其中的路径参数是我自己系统上的路径，在开发时自行替换为自己的开发测试路径。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148package hadoop.hdfs.hdfs;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.*;import org.apache.hadoop.io.IOUtils;import org.apache.hadoop.util.Progressable;import org.junit.After;import org.junit.Before;import org.junit.Test;import java.io.BufferedInputStream;import java.io.File;import java.io.FileInputStream;import java.io.InputStream;import java.net.URI;/** * hadoop HDFS java API 操作 */public class HDFSApp &#123; public static final String HDFS_PATH = "hdfs://localhost:9000"; FileSystem fileSystem = null; Configuration configuration = null; /** *创建hdfs目录 */ @Test public void mkdir() throws Exception &#123; fileSystem.mkdirs(new Path("/hdfsapi/test")); &#125; @Test public void createFile() throws Exception &#123; FSDataOutputStream out = fileSystem.create(new Path("/hdfsapi/test/a.txt")); out.write("hello hdfs".getBytes()); out.flush(); out.close(); &#125; /** * 查看hdfs文件的内容 */ @Test public void catFile() throws Exception &#123; FSDataInputStream in = fileSystem.open(new Path("/hdfsapi/test/a.txt")); IOUtils.copyBytes(in,System.out,1024); System.out.println(); in.close(); &#125; /** *重命名文件名 */ @Test public void renameFile() throws Exception &#123; Path oldPath = new Path("/hdfsapi/test/a.txt"); Path newPath = new Path("/hdfsapi/test/b.txt"); if(fileSystem.rename(oldPath,newPath))&#123; System.out.println("rename success!"); &#125; &#125; /** * 从本地拷贝一个文件 */ @Test public void copyFromLocalFile() throws Exception &#123; Path localPath = new Path("/home/hadoop/study/data/hello.txt"); Path hdfsPath = new Path("/hdfsapi/test"); fileSystem.copyFromLocalFile(localPath,hdfsPath); &#125; /** * 从本地拷贝一个大文件，并且带进度条 */ @Test public void copyFromLocalFileWithProgress() throws Exception &#123; InputStream in = new BufferedInputStream( new FileInputStream( new File("/home/hadoop/下载/hadoop-2.9.0.tar.gz"))); FSDataOutputStream out = fileSystem.create( new Path("/hdfsapi/test/hadoop-2.9.0.tar.gz"), new Progressable() &#123; @Override public void progress() &#123; System.out.print("."); &#125; &#125;); IOUtils.copyBytes(in,out,4096); &#125; /** * 下载HDFS文件到本地 */ @Test public void copyToLocalFile() throws Exception &#123; Path localpath = new Path("/home/hadoop/study/data/b.txt"); Path hdfspath = new Path("/hdfsapi/test/b.txt"); fileSystem.copyToLocalFile(hdfspath,localpath); &#125; /** * 获取目录下的所有文件和文件夹 */ @Test public void listFiles() throws Exception &#123; FileStatus[] fileStatus = fileSystem.listStatus(new Path("/")); for(FileStatus filestatus : fileStatus)&#123; String Status = filestatus.isDirectory() ? "文件夹" : "文件"; short replication = filestatus.getReplication(); //副本数 long len = filestatus.getLen(); //文件大小 String path = filestatus.getPath().toString(); System.out.println(Status + " " + replication+ " " +len+ " " +path); &#125; &#125; /** * 删除文件 */ @Test public void deleteFile() throws Exception &#123; fileSystem.delete(new Path(""),true); &#125; /** * 在所有的单元测试之前执行的 */ @Before public void setUp() throws Exception &#123; System.out.println("HDFS setUp"); configuration = new Configuration(); fileSystem = FileSystem.newInstance(new URI(HDFS_PATH),configuration); &#125; /** * 在所有的单元测试执行完之后执行的,释放资源 */ @After public void tearDown() throws Exception &#123; configuration = null; fileSystem = null; System.out.println("HDFS tearDown"); &#125;&#125; HDFS工作流程下面是在网上看到的漫画版的解释HDFS的工作流程，值得反复阅读，这里贴上来感谢作者！]]></content>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop遇到的一些坑]]></title>
    <url>%2F2018%2F07%2F24%2Fhadoop%E9%81%87%E5%88%B0%E7%9A%84%E4%B8%80%E4%BA%9B%E5%9D%91%2F</url>
    <content type="text"><![CDATA[最近开始了大数据的基础学习，刚开始就踩了一些坑。在这里做一下简要记录。 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform…由于好久没有打开ubuntu系统使用hadoop，不知道是什么原因，今天运行hadoop的一些命令的时候有警告： WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable百思不得其解，在开始安装好hadoop的时候是没有任何问题的，之后也没有动过。搜索了一圈问题后，都说是电脑上的本地库和hadoop的需求不一致，但是我改了一圈也还是没有解决问题。后在这里找到方法并且尝试后解决了。https://blog.csdn.net/znb769525443/article/details/51507283 这里根据上面博客的内容简要摘录： 增加调试信息，执行命令12export HADOOP_ROOT_LOGGER=DEBUG,consolehadoop fs -text est/data/origz/access.log.gz &gt; 这样就能够看到报错的信息解决方法：修改/HADOOP_HOME/etc/hadoop/中的hadoop_env.sh在头部添加12export HADOOP_COMMON_LIB_NATIVE_DIR=&quot;/usr/local/hadoop/lib/native/&quot;export HADOOP_OPTS=&quot;$HADOOP_OPTS -Djava.library.path=/usr/local/hadoop/lib/native/&quot; 再次执行./start-dfs.sh 问题解决。 当然每个人的错误虽然相似，但是错误原因可能不同，这里学会了一招看报错的log信息hadoop开启关闭调试信息： 开启：export HADOOP_ROOT_LOGGER=DEBUG,console 关闭：export HADOOP_ROOT_LOGGER=INFO,console hadoop集群启动之后dataNode节点没有启动今天遇到的另一个问题是，在hadoop操作的时候，向某个文件夹复制文件的时候报错：ARN hdfs.DataStreamer: DataStreamer Exception org.apache.hadoop.ipc.RemoteException然后锁定问题是dataNode节点没有启动，使用了命令stop-all.sh start-all.sh重启后仍然没有用。 使用了https://blog.csdn.net/qq_20124743/article/details/78668130 的方法得到了解决。 启动Hadoop集群之后slave机器的dataNode节点没有启动 master机器的nameNode节点启动了 1、在集群/usr/local/src/hadoop/bin目录下./stop-all.sh暂停所有服务 2、将/usr/local/src/hadoop/目录下的 logs、tmp文件夹删除(DataNode存放数据块的位置) 然后重新建立tmp logs文件夹 3、重新格式化: （同样是在bin目录下）./hadoop namenode -format 4、重新启动集群：./start-all.sh 5、通过jps查看进程 就好了]]></content>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二分搜索树]]></title>
    <url>%2F2018%2F07%2F18%2F%E4%BA%8C%E5%88%86%E6%90%9C%E7%B4%A2%E6%A0%91%2F</url>
    <content type="text"><![CDATA[二分搜索树（或者说二叉搜索树）是一种树形结构。 二分搜索树是二叉树 二分搜索树的每个节点的值：大于其左子树的所有节点的值小于其右子树的所有节点的值 每一棵子数也是二分搜索树 存储的元素必须有可比性(如果是想存储一个自定义的类型，那么要定义好这个数据类型的两个数据之间时如何比较的) 新建BST类，因为其节点值之间需要进行比较，所以继承Comparable类 基本代码12345678910111213141516171819202122232425262728public class BST&lt;E extends Comparable&lt;E&gt;&gt;&#123; //声明节点类 private class Node&#123; public E e; public Node left,right; public Node(E e)&#123; this.e = e; left = null; right = null; &#125; &#125; private Node root; //根节点 private int size; //记录存储的元素个数 //默认构造函数 public BST()&#123; root = null; size = 0; &#125; //获取节点数量 public int size()&#123; return size; &#125; //判断是否为空 public boolean isEmpty()&#123; return size == 0; &#125;&#125; 向二分搜索树添加元素采用递归方式向二分搜索树添加新元素，以下代码放入BST类中。根据二分搜索树的性质，根节点和每个子树的节点的左子树都比当前节点的值小，其右子树比当前节点的值都大。本文中的二分搜索树不包括重复元素。如果要插入的值等于当前根节点的值，则跳过。123456789101112131415161718192021222324252627/** * 向二分搜索树添加新的元素 */public void add(E e)&#123; root = add(root,e);&#125;/** * 返回插入新节点后二分搜索树的根 * @param node 要比较的节点 * @param e 要比较的值 * @return 新节点的根 */private Node add(Node node,E e)&#123; if(node == null)&#123; size ++; return new Node(e); &#125; if(e.compareTo(node.e) &lt; 0 )&#123; node.left = add(node.left,e); &#125; else if(e.compareTo(node.e) &gt; 0)&#123; node.right = add(node.right,e); &#125; return node;&#125; 二分搜索树的包含方法contains12345678910111213141516171819202122232425262728/** * 看二分搜索树种是否包含元素e,从根节点开始比较 * @param e 要比较的值 * @return true or false */public boolean contains(E e)&#123; return contains(root,e);&#125;/** * 看以node为根的二分搜索树中是否包含元素e,递归算法 * @param node 节点 * @param e 元素e * @return true or false */private boolean contains(Node node,E e)&#123; if(node==null)&#123; return false; &#125; if(e.compareTo(node.e) == 0)&#123; return true; &#125; else if(e.compareTo(node.e) &lt; 0)&#123; return contains(node.left,e); &#125; else //e.compareTo(node.e) &gt; 0 return contains(node.right,e);&#125; 二分搜索树的前序遍历从根节点访问左右子树，自上而下，所以称为前序遍历。 前序遍历的递归实现1234567891011121314151617181920/** * 二分搜索树的前序遍历 */public void preOrder()&#123; preOrder(root);&#125;/** * 前序遍历以node为根的二分搜索树，递归算法 * @param node 以node为根 */private void preOrder(Node node)&#123; //终止条件 if(node == null) return; System.out.println(node.e); preOrder(node.left); preOrder(node.right);&#125; 前序遍历的非递归实现12345678910111213141516/** * 前序遍历的非递归实现 */public void preOrderNR()&#123; Stack&lt;Node&gt; stack = new Stack&lt;&gt;(); stack.push(root); while(!stack.isEmpty())&#123; Node cur = stack.pop(); System.out.println(cur.e); if(cur.right != null) stack.push(cur.right); if(cur.left != null) stack.push(cur.left); &#125;&#125; 二分搜索树的中序遍历和前序遍历不同的是，中序遍历先访问左子树，再访问根节点，最后访问右子树。因为访问根节点在中间顺序，所以称为中序遍历。 1234567891011121314/** * 二分搜索树的中序遍历 */public void inOrder()&#123; inOrder(root);&#125;private void inOrder(Node node)&#123; if(node == null) return; inOrder(node.left); System.out.println(node.e); inOrder(node.right);&#125; 二分搜索树的中序遍历打印出的即是二分搜索树排序后的结果。 二分搜索树的后序遍历后序遍历的一个应用:为二分搜索树释放内存时，需要先从叶子节点开始释放，然后释放其父亲节点（JAVA有内存回收机制，不需考虑，但是c c++ 等语言需要考虑）。123456789101112131415/** * 二分搜索树的后序遍历 */public void postOrder()&#123; postOrder(root);&#125;private void postOrder(Node node)&#123; if(node == null) return; postOrder(node.left); postOrder(node.right); System.out.println(node.e);&#125; 二分搜索树的层序遍历二分搜索树的层序遍历采用队列的思想，首先把根节点入队，然后循环判断队列是否为空，如果不为空，则将队首元素出队，接着将这个元素的左右子树入队，如果为空则不进行入队。循环结束后也就实现了层序遍历。12345678910111213141516171819import java.util.Queue;import java.util.LinkedList; /** * 二分搜索树的层序遍历,使用队列 */ public void levelOrder()&#123; Queue&lt;Node&gt; q = new LinkedList&lt;&gt;(); //这里定义Queue底层采用链表进行实现 q.add(root); while(!q.isEmpty())&#123; Node cur = q.remove(); //队首元素出队 System.out.println(cur.e); if(cur.left != null) //队首元素的左子树 q.add(cur.left); if(cur.right != null) //队首元素的右子树 q.add(cur.right); &#125; &#125; 广度优先遍历的意义：可以更快找到要查询的元素。 寻找二分搜索树的最大最小值其实，寻找二分搜索树的最大最小值可以看成寻找一个链表的最后一个不为空的节点。这里给出递归写法。123456789101112131415161718192021222324252627282930313233343536373839404142/** * 寻找二分搜索树的最小元素 * @return */public E minimum()&#123; if(size == 0) throw new IllegalArgumentException("BST is empty"); return minimum(root).e;&#125;/** * 返回以node为根的二分搜索树的最小值所在的节点 * @param node * @return */private Node minimum(Node node)&#123; if(node.left == null) return node; return minimum(node.left);&#125;/** * 寻找二分搜索树的最大元素 * @return */public E maximum()&#123; if(size == 0) throw new IllegalArgumentException("BST is empty"); return maximum(root).e;&#125;/** * 返回以node为根的二分搜索树的最大值所在的节点 * @param node * @return */private Node maximum(Node node)&#123; if(node.right == null) return node; return maximum(node.right);&#125; 删除二分搜索树的最大最小节点12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * 从二分搜索树种删除最小值所在的节点，并返回最小值 * @return */public E removeMin()&#123; E min = minimum(); //先找到最小值是多少 root = removeMin(root); //进行删除操作 return min; //返回最小值&#125;/** * 删掉以node为根的二分搜索树中的最小节点 * @param node 以node为根 * @return 返回删除节点后新的二分搜索树的根 */private Node removeMin(Node node)&#123; if(node.left == null)&#123; Node rightNode = node.right; // 保存一下节点的右子树，即使为空也不违反逻辑 node.right = null; size --; //维护size return rightNode; //返回删除节点后新的二分搜索树的根,也即刚才保存的右子树 &#125; node.left = removeMin(node.left); return node;&#125;/** * 从二分搜索树种删除最大值所在的节点，并返回最大值 * @return */public E removeMax()&#123; E max = maximum(); root = removeMax(root); return max;&#125;/** * 删掉以node为根的二分搜索树中的最大节点 * @param node 以node为根 * @return 返回删除节点后新的二分搜索树的根 */private Node removeMax(Node node)&#123; if(node.right == null)&#123; Node leftNode = node.left; node.left = null; size --; return leftNode; &#125; node.right = removeMin(node.right); return node;&#125; 删除二分搜索树的任意节点 删除只有左子树或只有右子树的节点，即将其左子树或其右子树代替当前的节点位置即可 删除叶子节点可以将其看做只有左子树或只有右子树的节点，只是其左子树和右子树为空 删除一个既有左子树又有右子树的节点，是删除任意节点的难点。使用Hibbard Deletion方法：1) 找到d待删除节点d的右子树中最小的节点s = min(d-&gt;right),这个节点就是与d节点值相差最小的节点 2) 找到这个节点后，在d的右子树中删除这个节点,并让这个节点的右子树指向原来d节点的右子树，s-&gt;right = removeMin(d-&gt;right) 3) 让s节点的左子树等于d节点原来的左子树s-&gt;left = d-&gt;left,这时候s节点的左右子树就是d节点删除s节点后的左右子树了 4) 删除d,s称为新的子树的根，让其上一级的根指向此节点即可 或者与上述方法相同，只是在找替代节点时找d节点的左子树中最大值的节点作为s,其也符合逻辑 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** * 删除二分搜索树的任意元素 * @param e 要删除的元素的值 */public void remove(E e)&#123; root = remove(root,e);&#125;/** * 删除函数的递归调用 * @param node 当前传入的根节点 * @param e 要删除的元素的值 * @return 删除后的节点 */private Node remove(Node node,E e)&#123; if(node == null) return null; if(e.compareTo(node.e) &lt; 0)&#123; node.left = remove(node.left,e); return node; &#125; else if(e.compareTo(node.e) &gt; 0)&#123; node.right = remove(node.right,e); return node; &#125; else&#123; //e.compareTo(node.e) == 0 //待删除节点左子树为空的情况 if(node.left == null)&#123; Node rightNode = node.right; node.right = null; size --; return rightNode; &#125; //待删除节点右子树为空的情况 if(node.right == null)&#123; Node leftNode = node.left; node.left = null; size --; return leftNode; &#125; //待删除节点的左右子树均不为空的情况 //首先找到比待删除节点大的最小节点，即待删除节点右子树的最小节点 //用这个节点代替删除节点的位置 Node cur = minimum(node.right); cur.right = removeMin(node.right); cur.left = node.left; node.left = node.right = null; return cur; &#125;]]></content>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git笔记]]></title>
    <url>%2F2018%2F07%2F18%2Fgit%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[因为hexo博客之前出过一些问题，导致之前做的git笔记被覆盖掉丢失了。所以这次再一次整理一下git的一些常用操作并记录下来。 创建仓库进入到要创建仓库的文件夹,输入命令 git init这时候会有提示 然后目录下会多一个.git的隐藏文件夹。 另外一种方式 git init demo 初始化到一个叫demo的自定义文件夹 另外可以从远程仓库初始化 git clone https://github.com/xxxxxxxxx.git 克隆项目 git clone https://github.com/xxxxxxxxx.git demo 克隆项目到一个叫demo的自定义文件夹 基本用法 git status 查看仓库状态 No commits yet 是指没有提交记录Untracked files 是指未跟踪到的文件，指有文件更改了但是没有commit git add . 将所有修改添加至暂存区 git commit -m &quot;描述&quot; 提交版本(即在这个历史节点下修改和做了什么) git add . &amp;&amp; git commit -m &quot;描述&quot; 同时执行两次操作这时候再查看仓库状态，可以看到nothing to commit git log 查看版本记录黄色的一串字母数字组合表示唯一标识 下面则是描述说明的内容 git log -p可以看到修改的具体信息 git log --oneline 一行显示 git checkout xxx 穿越到指定的历史节点,xxx表示上面的唯一标识,可以不用输入全部() git checkout - 回到原来的节点 三种状态 modefied 已修改 staged 已暂存(缓冲阶段) commited 已提交 git diff 比对当前内容和暂存区内容。 git diff HEAD 比对当前内容和最近一次提交。 git diff HEAD^ 比对当前内容和倒数第二次提交。 git diff HEAD^ HEAD 比对最近两次提交。标签tag 项目开发中，在版本提交时会有很多小版本，但是其中有一些节点很重要，比如完成某些重要的功能，可以在这个重要的节点使用标签做标记。 git tag -a 标签名 -m &quot;备注&quot; 添加tag (a=annotated有注释的) 默认加在最近的节点上面 git tag -a 标签名 -m &quot;备注&quot; 版本号 在历史节点添加标签，最后加上历史版本号 git tag 罗列所有tag command1 &amp;&amp; command2 ：组合命令 添加 -a 属性，才可以后接-m属性 git show 标签名 显示tag信息 git checkout 标签名 回到tag所在的提交 分支branch git branch $branchName 在当前节点创建分支 git checkout $branchName 切换到分支(可以跳转到不同分支) git log --all --graph 图形化显示 分支的作用主要是利用分支进行当前问题的处理（比如在某个版本分支出来进行调试bug，但是在master继续版本的推进，在bug修复后合并分支，这样bug得到了解决版本也进行了推进）。 合并分支 git checkout -b $branchName 创建并切换至分支 git merge $branchName 合并分支 合并一个文件要用人工进行处理orz 远程仓库 git remote add $remoteName $giturl添加远程仓库 git push -u $remoteName $branchNamepush到远程分支输入github的用户名密码就自动开始push了 git remote -v 打印远程仓库信息 fetch是下载地址，push是上传地址 git clone $giturl 克隆 git pull = git fetch &amp;&amp; git merge 根据表严肃老师的视频进行整理 http://biaoyansu.com/i/6593023230131]]></content>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实现栈和队列]]></title>
    <url>%2F2018%2F07%2F17%2F%E5%AE%9E%E7%8E%B0%E6%A0%88%E5%92%8C%E9%98%9F%E5%88%97%2F</url>
    <content type="text"><![CDATA[栈栈在计算机中有很广泛的应用，比如括号的匹配用到了栈，系统栈，撤销操作。 栈也是一种线性结构。相比数组，栈对应的操作是数组的子集。只能从一端添加元素，从同一端取出元素，即栈顶。是一种后进先出的数据结构(Last In First Out (LIFO)) 栈的实现栈的实际底层实现有多种方式，如动态数组，链表等。 这里只实现栈的几个基本操作：入栈，出栈，查看栈顶元素，查看栈元素数量，查看栈是否为空。 定义Stack接口，基于动态数组实现栈，动态数组的实现见 https://homxuwang.github.io/2018/07/17/%E6%95%B0%E7%BB%84/ 首先在程序中创建上面链接中的Array类然后创建Stack接口Interface Stack&lt;E&gt;1234567public interface Stack&lt;E&gt; &#123; int getSize(); //获取元素个数 boolean isEmpty(); //判断是否为空 void push(E e); //进栈 E pop(); //出栈 E peek(); //查看栈顶元素&#125; ArrayStack基于动态数组实现的栈类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public class ArrayStack&lt;E&gt; implements Stack&lt;E&gt; &#123; Array&lt;E&gt; array; /** * 构造函数 * @param capacity 定义容量 */ public ArrayStack(int capacity)&#123; array = new Array&lt;&gt;(capacity); &#125; /** * 无参构造函数 */ public ArrayStack()&#123; array = new Array&lt;&gt;(); &#125; @Override public int getSize()&#123; return array.getSize(); &#125; @Override public boolean isEmpty() &#123; return array.isEmpty(); &#125; //获得栈的容量,这个方法与栈的接口无关,只有在使用动态数组的时候才有这个方法 public int getCapacity()&#123; return array.getCapacity(); &#125; @Override public void push(E e)&#123; array.addLast(e); &#125; @Override public E pop()&#123; return array.removeLast(); &#125; @Override public E peek()&#123; return array.getLast(); &#125; @Override public String toString()&#123; StringBuilder res = new StringBuilder(); res.append("Stack:"); res.append("["); for(int i = 0 ; i &lt; array.getSize(); i++)&#123; res.append(array.get(i)); if(i != array.getSize() - 1)&#123; res.append(","); &#125; &#125; res.append("] top"); return res.toString(); &#125;&#125; 在Main函数中进行测试：1234567891011public class Main &#123; public static void main(String[] args)&#123; ArrayStack&lt;Integer&gt; stack = new ArrayStack&lt;&gt;(); for(int i = 0 ; i &lt; 4 ; i++)&#123; stack.push(i); System.out.println(stack); &#125; stack.pop(); System.out.println(stack); &#125;&#125; 结果如下： 栈的复杂度分析ArrayStack void push(e) O(1) 均摊E pop() O(1) 均摊E peek() O(1)int getSize() O(1)boolean isEmpty() O(1) 队列队列也是一种线性结构。先进先出(First In First Out(FIFO)) 一般队列的实现定义Queue接口，基于动态数组实现队里，动态数组的实现见 https://homxuwang.github.io/2018/07/17/%E6%95%B0%E7%BB%84/ 基于Array类创建Queue接口 Interface Queue&lt;E&gt;1234567public interface Queue&lt;E&gt;&#123; int getSize(); //获取元素个数 boolean isEmpty; //判断队列是否为空 void enqueue(E e); //入队 E dequeue(); //出队 E getFront(); //查看队首元素&#125; 底层采用动态数组Array类来实现队列,创建队列类ArrayQueue：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class ArrayQueue&lt;E&gt; implements Queue&lt;E&gt; &#123; private Array&lt;E&gt; array; public ArrayQueue(int capacity)&#123; array = new Array&lt;&gt;(capacity); &#125; public ArrayQueue()&#123; array = new Array&lt;&gt;(); &#125; @Override public int getSize()&#123; return array.getSize(); &#125; @Override public boolean isEmpty()&#123; return array.isEmpty(); &#125; public int getCapacity()&#123; return array.getCapacity(); &#125; @Override public void enqueue(E e)&#123; array.addLast(e); &#125; @Override public E dequeue()&#123; return array.removeFirst(); &#125; @Override public E getFront()&#123; return array.getFirst(); &#125; @Override public String toString()&#123; StringBuilder res = new StringBuilder(); res.append("Queue:"); res.append("front["); for(int i = 0 ; i &lt; array.getSize() ; i ++)&#123; res.append(array.get(i)); if(i != array.getSize() - 1) res.append(","); &#125; res.append("]end"); return res.toString(); &#125;&#125; 简单测试：1234567891011public static void main(String[] args)&#123; ArrayQueue&lt;Integer&gt; queue = new ArrayQueue&lt;&gt;(); for(int i = 0 ; i &lt; 10 ; i++)&#123; queue.enqueue(i); System.out.println(queue); if(i % 3 == 2)&#123; queue.dequeue(); System.out.println(queue); &#125; &#125;&#125; 一般队列的复杂度分析void enqueue(E) O(1) 均摊E dequeue() O(n)E getFront() O(1)int getSize() O(1)boolean isEmpty() O(1) 可以看到一般队列的出队过程因为要往前挪动一个元素，导致其时间复杂度是O(n) 循环队列的实现在一般队列的基础上，引入front指向队列头,tail指向队列尾。front == tail 队列为空。在出队后，不用挪动元素，而是让front的指向后移，同时，添加元素时tail向后挪，指向第一个未添加元素的地方。在队列满的情况下，索引+1后对数组长度求余得到tail的位置。但是若tail指向的事最后一个未添加元素的位置时，再向其中添加元素，此时tail后挪后就导致tail == front而当front == tail时表示队列为空。所以用(tail + 1) % 数组容量 == front(tail + 1 % 数组容量 是因为如图，如果front == 0 ,tail == 7时也是满的) 表示队列已满，这时候进行扩容。即浪费一个空间。 循环队列底层不再使用上文将到的动态数组1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192public class LoopQueue&lt;E&gt; implements Queue&lt;E&gt; &#123; private E[] data; private int front, tail; //front表示队首所指的索引，tail指向队列最后一个元素的下一个位置所在的索引，也就是入队新的元素所存放的位置对应的索引 private int size; //队列中有多少元素(也可以用tail和front进行控制) //构造函数 public LoopQueue(int capacity)&#123; data = (E[])new Object[capacity + 1]; //因为上文提到要预留一个空间，所以这里+1 front = 0; tail = 0; size = 0; &#125; //无参构造函数 public LoopQueue()&#123; this(10); &#125; public int getCapacity()&#123; return data.length - 1; //真正能够存储的数据大小是length - 1，因为预留了一个空间 &#125; @Override public boolean isEmpty()&#123; return front == tail; &#125; @Override public int getSize()&#123; return size; &#125; @Override public void enqueue(E e)&#123; if((tail + 1) % data.length == front)&#123; //如果队列满了,则调用resize()函数 resize(getCapacity() * 2); &#125; data[tail] = e; tail = (tail + 1) % data.length; &#125; @Override public E dequeue()&#123; if(isEmpty())&#123; throw new IllegalArgumentException("Canno dequeuq from an empty queue"); &#125; E temp = data[front]; data[front] = null; front = (front + 1) % data.length; size --; if(size == getCapacity() / 4 &amp;&amp; getCapacity() / 2 != 0)&#123; //如果队列元素个数小于容量的1/4则进行缩容 resize(getCapacity() / 2); &#125; return temp; &#125; @Override public E getFront()&#123; if(isEmpty())&#123; throw new IllegalArgumentException("Queue is empty"); &#125; return data[front]; &#125; private void resize(int newCapacity)&#123; E[] newData = (E[])new Object[newCapacity + 1]; for(int i = 0 ; i &lt; size ; i++)&#123; newData[i] = data[(front + i) % data.length]; &#125; data = newData; front = 0; tail = size; &#125; /** *便于打印输出 */ @Override public String toString()&#123; StringBuilder res = new StringBuilder(); res.append(String.format("Queue: Size = %d , capacity = %d \n",size,getCapacity())); res.append("front["); for(int i = front ; i != tail ; i = (i + 1 ) % data.length)&#123; res.append(data[i]); if((i + 1) % data.length != tail) res.append(", "); &#125; res.append("] tail"); return res.toString(); &#125;&#125; 测试代码：1234567891011public static void main(String[] args)&#123; LoopQueue&lt;Integer&gt; queue = new LoopQueue&lt;&gt;(); for(int i = 0 ; i &lt; 10 ; i++)&#123; queue.enqueue(i); System.out.println(queue); if(i % 3 == 2)&#123; queue.dequeue(); System.out.println(queue); &#125; &#125;&#125; 代码结果：]]></content>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实现动态数组]]></title>
    <url>%2F2018%2F07%2F17%2F%E6%95%B0%E7%BB%84%2F</url>
    <content type="text"><![CDATA[因为数组开辟时是静态的。在这里底层使用静态数组二次封装一个属于自己的动态数组类Array Array类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245public class Array&lt;E&gt;&#123; private E[] data; //开辟一个数组 private int size; //指向第一个没有元素的索引，也代表数组中有多少个有效的元素 /** * 有参构造函数 * @param capacity 数组的容量 */ public Array(int capacity)&#123; data = (E[])new Object[capacity]; size = 0; &#125; /** * 默认构造函数,默认容量给10 */ public Array()&#123; this(10); &#125; /** * 获取数组中的元素个数 * @return元素个数 */ public int getSize()&#123; return size; &#125; /** * 获取数组的容量 * @return数组的大小 */ public int getCapacity()&#123; return data.length; &#125; /** * 返回数组是否为空 * @return 数组是否为空 */ public boolean isEmpty()&#123; return size == 0; &#125; /** * 插入新元素 * @param index 在index插入 * @param e 新元素的值 */ public void add(int index, E e)&#123; //index不能为负,index如果大于size说明数组不是紧密排列的,则不合法 if(index &lt; 0 || index &gt; size)&#123; throw new IllegalArgumentException("add() failed. Require index &gt;= 0 &amp;&amp; index &lt;= size"); &#125; if(size == data.length)&#123; //如果此时元素个数等于数组长度，则进行动态扩容 resize(2 * data.length); &#125; //将元素往后挪动 for(int i = size-1 ; i &gt;= index ; i-- )&#123; data[i+1] = data[i]; &#125; data[index] = e; size ++; &#125; /** * 向所有元素后添加一个新元素 * @param e 要插入的元素 */ public void addLast(E e)&#123; add(size,e); &#125; /** * 获取index位置的元素 * @param index * @return 元素位置 */ public void addFirst(E e)&#123; add(0,e); &#125; /** * 获取index位置的元素 * @param index 要获取的元素的索引 * @return 元素值 */ public E get(int index)&#123; if(index &lt; 0 || index &gt;= size)&#123; throw new IllegalArgumentException("Set failed,Index is illegal"); &#125; return data[index]; &#125; /** * 获取最后一个元素 * @return 最后一个元素 */ public E getLast()&#123; return get(size - 1); &#125; /** * 获取第一个元素 * @return 第一个元素 */ public E getFirst()&#123; return get(0)); &#125; /** * 修改Index索引位置的元素为e * @param index 要修改的元素的索引 * @param e 要修改的元素的值 */ void set(int index,E e)&#123; if(index &lt; 0 || index &gt;= size)&#123; throw new IllegalArgumentException("Get failed,Index is illegal"); &#125; data[index] = e; &#125; /** *查找数组中是否含有元素e * @param e 要查找的元素的值 * @return true or false */ public boolean contains(E e)&#123; for(int i = 0 ; i &lt; size ; i++)&#123; if(data[i].equals(e)) return true; &#125; return false; &#125; /** * 查找数组中元素e所在的索引,如果不存在则返回-1 * @param e 要查找的元素的值 * @return 返回索引或者-1 */ public int find(E e)&#123; for(int i = 0 ; i &lt; size ; i++)&#123; if(data[i].equals(e)) return i; &#125; return -1; &#125; /** * 从数组中删除第一个元素elem * @param elem * @return true or false */ public boolean removeElement(E elem)&#123; int index = find(elem); if(index != -1)&#123; remove(index); return true; &#125; return false; &#125; /** * 删除Index位置的元素,返回index位置的元素 * @param index 要删除的索引位置的值 * @return 返回删除的元素的值 */ public E remove(int index)&#123; if(index &lt; 0 || index &gt;= size)&#123; throw new IllegalArgumentException("Remove failed.Index is illegal"); &#125; E temp = data[index]; for(int i = index + 1 ; i &lt; size ; i++)&#123; data[i - 1] = data[i]; &#125; size --; data[size] = null; // loitering objects. 释放size位置的内容(非必须，因为访问不到) if(size == data.length / 4 &amp;&amp; data.length / 2 != 0)&#123; //如果删除元素过多，少于四分之一则数组缩小容量 resize(data.length / 2); &#125; return temp; &#125; /** * 删除第一个元素 * @return */ public E removeFirst()&#123; return remove(0); &#125; /** * 删除数组中最后一个元素 * @return */ public E removeLast()&#123; return remove(size-1); &#125; /** * 覆盖父类的方法,定义本类在打印输出时打印的信息 * @return 打印输出数组字符串 */ @Override public String toString()&#123; StringBuilder res = new StringBuilder(); res.append(String.format("Array: size = %d,capacity = %d\n",size,data.length)); res.append('['); for(int i = 0 ; i &lt; size ; i ++)&#123; res.append(data[i]); if(i != size - 1) res.append(","); &#125; res.append(']'); return res.toString(); &#125; /** * 扩容数组 * @param newCapacity 新的数组的大小 */ private void resize(int newCapacity)&#123; E[] newData =(E[]) new Object[newCapacity]; for(int i = 0 ;i &lt; size ; i++) &#123; newData[i] = data[i]; &#125; data = newData; &#125;&#125; 说明：1.实现动态数组的关键函数是类中的resize()函数，动态数组的底层实现有多种方式，比如本文讲到的使用静态数组进行扩容缩容，或者使用链表。动态数组的实现原理其实非常简单，假设数组的已经满了，那么如果继续添加元素，则会报出异常。实现动态数组的思路是，如果这时候数组满了，那么开辟一个新的数组，这个新数组的容量比原数组大(比如是原数组的1.5倍或者2倍,本文采用2倍)，然后将原数组的数据拷贝到新开辟的大数组中(这里需要循环),原来的数组data实际上是一个引用，在拷贝之后将data指向新的数组newData,这时候data这个成员变量和newData指向的是同一个空间，因为newData是封装在函数中，在函数执行结束后就失效了,而data是在整个类中的，它的生命周期是和类一样的。而原来的数组，因为没有东西指向它，java的垃圾回收机制会自动回收。这样扩容过程就结束了。 2.在remove()函数中，有一个缩容的操作，即当前的元素个数小到一定程度的时候，调用resize()函数，缩小数组的容量为当前数组容量的一半。这里是小于四分之一，并且缩小一半后的容量不能为0(所以在条件中判断data.length / 2 != 0)，这时再执行缩小操作。缩容的界限之所以选取1/4是因为如果removeLast()后刚好是数组的一半了，这时候进行缩容，然后又进行添加的时候，这时候又要进行扩容，这样如果有频繁的增删操作，在resize()函数中就会耗费大量的时间，所以采取Lazy解决方案。在删除元素后元素是1/2时，不着急进行缩容操作，而是等到元素为容量的1/4之一时再进行缩容，只缩容1/2，还留出1/4的空间。这样就防止了复杂度的震荡。 简单的复杂度分析·添加操作 addLast(e) O(1) addFirst(e) O(n) add(index,e) O(n/2) = O(n) //这里是简单的计算，严格计算需要使用概率论的知识，使用期望 resize() O(n) 所以添加操作的整体时间复杂度是O(n) ·删除操作 removeLast(e) O(1) removeFirst(e) O(n) remove(index,e) O(n/2) = O(n) resize() O(n) 整体时间复杂度是O(n) ·修改操作 set(index,e) O(1) ·查找操作 get(index) O(1) contains(e) O(n) find(e) O(n) 总结: ·增 O(n) ·删 O(n) ·改 已知索引O(1);未知索引O(n) ·查 已知索引O(1);未知索引O(n)]]></content>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法]]></title>
    <url>%2F2018%2F06%2F09%2F%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[最近在学习数据结构和算法，这里总结一下学习的排序算法。 选择排序 - SelectionSort基本思路:假设有一个数组（如图所示），进行从小到大的排序。首先在整个数组范围里，找出要放在第一个位置的数，也就是最小的数：1，然后将1和现在的第一名的位置8进行换位，经过交换以后，1所处的位置就是最终排序所在的位置，这样就继续在剩下的部分找此时最小的数，也就是2，然后把2和相应的第二个位置所在的元素进行交换，此时1和2两个元素也已经是最终排好序的结果。整个过程以此类推，继续在剩下的部分中找此时最小的元素，然后进行交换位置。。。。。 代码实现： 12345678910111213141516//arr为要进行排序的数组，//n为数组的元素个数，即数组大小template&lt;typename T&gt;void selectionSort(T arr[],int n )&#123; for(int i=0;i&lt;n;i++)&#123; //寻找[i,n)区间里的最小值 int minIndex = i; //记录当前所找的最小值所处索引的位置，初始化在位置i for( int j= i+1;j&lt;n;j++)&#123; if(arr[j] &lt; arr[minIndex]) //比较j位置的元素是否小于minIndex位置的元素，如果小于则更新当前的minIndex minIndex = j; &#125; //此时已经找到了[i,n)区间里的最小值，并且记录其位置已经记录下来 swap(arr[i] , arr[minIndex]); //如果使用std标准库不行，有可能需要引入algorithm标准库 &#125;&#125; 插入排序 - InsertionSort基本思路：开始只考虑8这个元素的时候，它就已经排好序了。接着看6这个元素,接下来的步骤是把6与它前面的数组进行比较，放在合适的位置，当6与8比较时，6&lt;8，所以6在8前面位置。接着看2这个元素，2与它前面的数组进行比较，2&lt;8，所以2和8交换一次位置，2继续和6比较，2&lt;6，所以2和6交换位置，此时2在最前面的位置。接着看3这个元素，3比8小，所以交换一次位置，3又比6小，所以交换一次位置，3比2大所以不进行交换操作，3插入在2和6中间，这时前面的4个元素排序完成。以此类推。 代码实现： 1234567891011121314151617template&lt;typename T&gt;void insertionSort (T arr[] , int n)&#123; //从i=1开始，因为对插入排序来说，第0个位置不用考虑 for(int i = 1 ; i &lt; n ; i++)&#123; //寻找arr[i]合适的插入位置 for(int j = i ; j &gt; 0 ; j-- )&#123; if ( arr[j] &lt; arr[j-1] ) swap(arr[j] , arr[j-1]); else //如果arr[i]元素已经在合适的位置，则可以直接进入下一个循环 break; &#125; &#125; //下面是简化后的代码 // for(int j = i ; j &gt; 0 &amp;&amp; arr[j] &lt; arr[j-1] ; j-- )&#123; // swap(arr[j] , arr[j-1]); // &#125;&#125; 插入排序和选择排序相比，如果满足了条件就有机会提前结束，所以它的排序效率理论上要比选择排序高但是实际上它的运行时间比选择排序要慢，这是因为其swap操作较多，浪费了时间，所以针对这个地方进行改进 改进代码思路:首先,对位置0处的元素不作处理。 接着，看位置1处的元素6，首先对元素6做一个副本保存起来，然后看元素6是否应该在当前位置，即让他与前面的元素进行对比，如果小于前面的元素，则当前元素位置的值赋值为前一个元素的值，前一个元素位置的值赋值为刚才保存起来的副本（即当前元素的值）。（其实这也是相当于一个交换操作，在满足前一个元素大于当前元素值的情况下，进行交换值操作） 接着，看位置2处的元素2，首先对元素2做一个副本保存起来，然后元素2与前一个元素8比较，2&lt;8，则将元素2处的值赋值为8；接着再比较位置1处的元素8与位置1处的元素6的大小，将位置1处的值赋值为元素6，接着将元素2放在第一个位置。这样就少进行了交换的操作。 接着，看元素3，首先对元素3做一个副本保存起来，然后看元素3该不该放在当前位置，发现3&lt;8，所以这个位置赋值为8；然后看3是不是该放在刚才元素8的位置，发现元素3比元素6小，所以元素6放在刚才元素8的位置；然后看元素3是不是该放在刚才元素6的位置，发现元素3比元素2大，所以元素3应该放在这个位置。 这样很多交换操作就通过赋值进行取代了，所以性能更好。 改进代码：12345678910111213//改进代码template&lt;typename T&gt;void insertionSort (T arr[] , int n)&#123; //从i=1开始 for(int i = 1 ; i &lt; n ; i++)&#123; T e = arr[i]; //用e暂存i位置的数 int j; //保存元素e应该插入的位置 for(j = i ; j &gt; 0 &amp;&amp; arr[j-1] &gt; e ; j--)&#123; arr[j] = arr[j-1]; &#125; arr[j] = e; &#125;&#125; 归并排序 - MergeSort自顶向下的归并排序基本思路：所谓归并排序，有两大步，一步是归，一步是并。 当对一个数组进行排序的时候,先把这个数组分成一半，然后分别把左边的数组和右边的数组排序,之后再归并起来。在对左边数组和右边的数组进行排序的时候，再次分别把左边的数组和右边的数组分成一半，然后对每一个部分进行排序。一样的，对这每一个部分进行排序的时候，再次把他们分成一半，直到它们只含有一个元素的时候，已经是有序了。（蓝色部分） 这时候就对上面最终分割完成的各个部分进行归并。在归并到上一个层级之后，继续进行归并,逐层上升，进行归并，直到归并到最后一层的时候，整个数组就有序了。（红色部分） 可以看到，对图中的8各元素进行归并的时候，分成3级，第三级就可以把数组分成单个元素了，这样每次二分，就是log2 8 = 3，如果是N个元素，则有log2 N的层级。每一层要处理的元素个数是一样的，则整个归并过程是N*log(N)的时间复杂度。 那么问题是假设左半部分和右半部分已经排好了序，怎么把他们合并成一个有序的数组。这个过程需要为数组开辟一个相同大小的临时空间进行辅助,如下图。在合并成一个数组的过程中，需要3个索引：k是最终在归并过程中要跟踪的位置，i和j分别表示两个排好序的数组，当前要考虑的元素项。首先看1和2两个元素，谁应该放在k位置，进行对比后，较小的元素1放在k位置。这时候k++，j++。紧接着，元素2和元素4进行对比，较小的元素2放在当前的k位置，这时候k++,i++。继续元素3和元素4进行对比，较小的元素3放在当前的k位置，这时候k++,i++。继续元素6和元素4进行对比，较小的元素4放在当前的k位置，这时候k++,j++。。。。。算法过程中，需要维护k,i,j满足算法的定义。并且需要跟踪i,j的越界情况。这里定义l(left),r(right),和m(middle)分别为整个数组的最左边的元素，数组最右边的元素和中间位置的元素(这里指定为第一个数组的最后一个元素)。这里再做一个说明，这里的定义整个算法的数组是前闭后闭的数组。 代码实现：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859//将arr[l...mid]和arr[mid+1...r]两部分进行归并template&lt;typename T&gt;void __merge(T arr[] , int l , int mid , int r )&#123; //开辟一段临时空间，大小和arr[]的空间一样大 T aux[r-l+1]; for( int i = l ; i &lt;= r ; i++ ) //因为是闭区间,所以用i&lt;=r aux[i-l] = arr[i]; // aux[]是从0开始的，arr[]是从l开始的，所以有个i-l的偏移量 //设置两个索引，指向两部分的开头 int i = l , j = mid + 1; for( int k = l ; k &lt;= r ; k++)&#123; //使用k索引进行遍历，每一次决定arr[k]的位置应该是谁 //先判断i和j是否越界 //这时候说明左半部分已经遍历完,k没有遍历完，j索引所指数组的元素还没有归并回去,则剩下的k索引所指的元素就对应j-l所指的元素 if(i &gt; mid )&#123; arr[k] = aux[j-l]; j++; &#125; else if(j&gt;r)&#123;//同上 arr[k] = aux[i-l]; i++; &#125; else if(aux[i-l] &lt; aux[j-l])&#123; arr[k] = aux[i-l]; i++; &#125; else&#123; arr[k] = aux[j-l]; j++; &#125; &#125;&#125;//递归使用归并排序，对arr[l...r]的范围进行排序（r是最后一个元素的位置）template&lt;typename T&gt;void __mergeSort(T arr[] , int l , int r )&#123; if(l &gt;= r) return; int mid = (l+r)/2; //对左右两部分进行归并排序 __mergeSort(arr,l,mid); __mergeSort(arr,mid+1,r); //将归并排序好的两部分[l,mid]和[mid+1,r]进行merge操作 if( arr[mid] &gt; arr[mid+1] ) __merge(arr, l , mid , r);&#125;template&lt;typename T&gt;void mergeSort(T arr[],int n)&#123; //arr:当前要处理的数据，0:要处理的数据的开始位置，n-1:要处理的数据的结束位置 __mergeSort(arr , 0 , n-1 );&#125; 自底向上的归并排序基本思路：自底向上的归并排序的基本原理是，给定一个数组，从左向右将数组依次划分为小段，如两个元素一个小段，然后进行归并排序。当这一轮归并排序完成之后，再四个元素一个小段进行归并排序，最后八个元素一个小段进行归并排序。以上面的数组为例，这时候就排序完成了。这种方法并不需要递归，只需要迭代就可以完成排序操作。 代码实现：123456789101112131415161718192021222324252627282930313233343536373839404142434445template&lt;typename T&gt;void __merge(T arr[] , int l , int mid , int r )&#123; //开辟一段临时空间，大小和arr[]的空间一样大 T aux[r-l+1]; for( int i = l ; i &lt;= r ; i++ ) //因为是闭区间,所以用i&lt;=r aux[i-l] = arr[i]; // aux[]是从0开始的，arr[]是从l开始的，所以有个i-l的偏移量 //设置两个索引，指向两部分的开头 int i = l , j = mid + 1; for( int k = l ; k &lt;= r ; k++)&#123; //使用k索引进行遍历，每一次决定arr[k]的位置应该是谁 //先判断i和j是否越界 if(i &gt; mid )&#123; //这时候说明左半部分已经遍历完,k没有遍历完，j索引所指数组的元素还没有归并回去,则剩下的k索引所指的元素就对应j-l所指的元素 arr[k] = aux[j-l]; j++; &#125; else if(j&gt;r)&#123;//同上 arr[k] = aux[i-l]; i++; &#125; else if(aux[i-l] &lt; aux[j-l])&#123; arr[k] = aux[i-l]; i++; &#125; else&#123; arr[k] = aux[j-l]; j++; &#125; &#125;&#125;//自底向上的归并排序template&lt;typename T&gt;void mergeSortBU(T arr[] , int n)&#123; for(int sz = 1 ; sz &lt;= n ; sz += sz) //size从1开始，每次加一个size的大小，直到是n大小 //第二层循环是每次归并过程中起始的元素的位置 // i+sz&lt;n 保证了第二部分的存在，也保证了i+sz-1是不会越界的 for(int i = 0; i + sz &lt; n ; i += sz + sz ) //对 arr[i...i+sz-1]和arr[i+sz...i+2*sz-1]进行归并 //min()保证i+sz+sz-1越界的时候取n-1 __merge(arr , i , i + sz - 1 , min(i + sz + sz -1, n - 1); //自底向上的归并排序方法没有用到数组的索引，所以可以很好的对链表结构进行排序&#125; 快速排序 - QuickSort基本思路：以下图的数组为例，首先选定一个元素4，把它挪到当它在排好序的时候，应该处的位置，当它处在这个位置之后，使得这个数组有了一个性质：在4之前的数都小于4，在4之后的数都大于4。接下来，对小于4的子数组和大于4的子数组分别继续进行快速排序。这样逐渐递归，完成整个排序过程。 那么关键问题是怎么把4这个元素放在正确的改放的位置，这个过程称为Partition。通常使用数组的第一个元素作为分界的标志点，第一个位置记作l，然后逐渐遍历右边没有被访问的元素，在遍历的过程中，逐步整理让元素一部分是&gt;v一部分是&lt;v的,将大于v和小于v的分界点的索引记为j，当前访问的元素e的索引记作i。下面讨论i位置的e该如何处理，如图。如果e&gt;v则将它放在这个位置不变，i++继续讨论下一个元素。否则如果e&lt;v则将i位置的e与j+1位置的元素交换，j++,i++，继续考察下一个元素。依次继续进行操作，直至遍历完数组的所有元素。这时候要将v放在数组中合适的位置，即将索引l位置的v与索引j位置的元素进行交换。最终完成此次操作。 代码实现：1234567891011121314151617181920212223242526272829303132333435363738394041//对arr[l...r]部分进行partition操作//返回p,使得arr[l...p-1] &lt; arr[p] ; arr[p+1...r] &gt; arr[p]template&lt;typename T&gt;int __partition(T arr[] , int l , int r )&#123; //先取第一个元素作为标准 T v = arr[l]; //arr[l+1...j]&lt;v ; arr[j+1...i) &gt; v ，后面是开区间是因为i是当前要考察的元素 int j = l ; for( int i = l + 1 ;i &lt;= r ; i++ )&#123; if( arr[i] &lt; v )&#123; swap( arr[j+1] , arr[i] ); j++; &#125; &#125; swap( arr[l] , arr[j] ); return j;&#125;//对arr[l...r]部分进行快速排序template&lt;typename T&gt;void __quickSort(T arr[] , int l , int r )&#123; if(l &gt;= r) return; int p = __partition(arr, l , r ); __quickSort(arr , l , p-1 ); __quickSort(arr , p + 1 , r );&#125;template&lt;typename T&gt;void quickSort(T arr[] , int n )&#123; //调用递归函数 __quickSort(arr, 0 , n - 1);&#125;]]></content>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vue-cli 目录结构笔记 及 一个简单电商项目的网页架构思路]]></title>
    <url>%2F2018%2F06%2F05%2FVue-cli-%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84%E7%AC%94%E8%AE%B0-%E5%8F%8A-%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%94%B5%E5%95%86%E9%A1%B9%E7%9B%AE%E7%9A%84%E7%BD%91%E9%A1%B5%E6%9E%B6%E6%9E%84%E6%80%9D%E8%B7%AF%2F</url>
    <content type="text"><![CDATA[学习了vue有一小段时间，期间中断去学习了java并且补了一下数据结构的基础，有点断层。跟着视频用vue2.0做了一个电商的小项目。思路稍微清晰了一些，但是因为中途转学其他的缘故，有一些东西还是忘掉了，这里总结一下使用vue-cli搭建项目的一些经验和教训。 首先是vue-cli的目录结构，这个是基于webpack的脚手架目录： 12345678910111213141516171819202122232425262728293031.|-- build // 项目构建(webpack)相关代码| |-- build.js // 生产环境构建代码| |-- check-version.js // 检查node、npm等版本| |-- utils.js // 构建工具相关| |-- vue-loader.conf.js // css加载器配置| |-- webpack.base.conf.js // webpack基础配置| |-- webpack.dev.conf.js // webpack开发环境配置| |-- webpack.prod.conf.js // webpack生产环境配置|-- config // 项目开发环境配置| |-- dev.env.js // 开发环境变量| |-- index.js // 项目一些配置变量(包括监听变量，打包路径等)| |-- prod.env.js // 生产环境变量| |-- test.env.js // 测试环境变量|-- node_modules //存放依赖的目录|-- src // 源码目录| |-- assets // 静态资源（css文件，外部js文件）| |-- components // vue公共组件| |-- router // 路由配置| |-- App.vue // 根组件| |-- main.js // 入口文件，加载各种公共组件|-- static // 静态文件，比如一些图片，json数据等|-- test // 测试文件目录|-- .babelrc // ES6语法编译配置|-- .editorconfig // 定义代码格式|-- .gitignore // git上传需要忽略的文件格式|-- .postcssrc.js|-- README.md // 项目说明|-- index.html // 入口页面|-- package.json // 项目基本信息. 当然不同版本的项目目录或者文件大同小异，基本都包括在上面了。 接下来讲一个平时用的比较多的网页排版及vue的大体配置。在此之前先介绍几个文件： index.html一般只定义一个空的根节点，在main.js里面定义的实例将挂载在根节点下，内容都通过vue组件来填充。 App.vueApp.vue 是个根组件。一个vue文件包括template,script,style三部分。vue通常用es6来写，用export default导出。&lt;style&gt;&lt;/style&gt;默认是影响全局的，如需定义作用域只在该组件下起作用，需在标签上加scoped。如要引入外部css文件，首先需给项目安装css-loader依赖包。使用import引入，比如： 12345&lt;style&gt; import &apos;./assets/css/bootstrap.css&apos;&lt;/style&gt; main.jsmain.js是个入口文件。这里:template: &#39;&lt;App/&gt;&#39;表示用&lt;app&gt;&lt;/app&gt;替换index.html里面的&lt;div id=&quot;app&quot;&gt;&lt;/div&gt;。这么做的目的很简单，&lt;App /&gt;他就是App.vue，template就是选择vue实例要加载哪个模板。最新的vue-cli脚手架模板现在是这个形式。App.vue是主程序，其他所有的.vue都是放在App.vue中，所以只需要加载App.vue就完全可以把其他的东西加载出来。 routerrouter目录下的index.js即是路由配置文件 router中可以设置多个路由，但是这里要先引入相应的组件，在进行设置： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152//引入Vue框架import Vue from &apos;vue&apos;//引入路由依赖import Router from &apos;vue-router&apos;//引入各个页面组件import IndexPage from &apos;./components/index&apos;import DetailPage from &apos;./components/detail.vue&apos;import DetailAnaPage from &apos;./components/detail/analysis&apos;import DetailPubPage from &apos;./components/detail/publish&apos;import DetailCouPage from &apos;./components/detail/count&apos;import DetailForPage from &apos;./components/detail/forecast&apos;import OrderListPage from &apos;./components/orderList&apos;Vue.use(Router)export default new Router(&#123; mode: &apos;history&apos;, routes: [ &#123; path: &apos;/&apos;, component: IndexPage &#125;, &#123; path: &apos;/orderList&apos;, component: OrderListPage &#125;, &#123; path: &apos;/detail&apos;, component: DetailPage, redirect: &apos;detail/analysis&apos;, children: [ &#123; path: &apos;forecast&apos;, component: DetailForPage &#125;, &#123; path: &apos;analysis&apos;, component: DetailAnaPage &#125;, &#123; path: &apos;publish&apos;, component: DetailPubPage &#125;, &#123; path: &apos;count&apos;, component: DetailCouPage &#125; ] &#125; ]&#125;) 这里介绍一个基础的vue模板构建思路： App.vue如下，其中的router的配置可以参见上面的代码 当然这里只设置了简单的内容，具体的方法和数据及样式根据不同的需求进行补充即可。 当然这只是一种简单的设计思路，做项目时可以用这个做为参考，但是不要被限制。]]></content>
      <tags>
        <tag>前端</tag>
        <tag>Vue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据基础学习笔记（十）——图计算]]></title>
    <url>%2F2018%2F05%2F28%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%EF%BC%89%E2%80%94%E2%80%94%E5%9B%BE%E8%AE%A1%E7%AE%97%2F</url>
    <content type="text"><![CDATA[图结构数据•许多大数据都是以大规模图或网络的形式呈现•许多非图结构的大数据，也常常会被转换为图模型后进行分析•图数据结构很好地表达了数据之间的关联性•关联性计算是大数据计算的核心——通过获得数据的关联性，可以从噪音很多的海量数据中抽取有用的信息 传统图计算解决方案的不足之处很多传统的图计算算法都存在以下几个典型问题：（1）常常表现出比较差的内存访问局部性（2）针对单个顶点的处理工作过少（3）计算过程中伴随着并行度的改变 针对大型图（比如社交网络和网络图）的计算问题，可能的解决方案及其不足之处具体如下：• （1 ）为特定的图应用定制相应的分布式实现• （2 ）基于现有的分布式计算平台进行图计算• （3 ）使用单机的图算法库：比如BGL、LEAD、NetworkX、JDSL、Standford GraphBase和FGL等• （4 ）使用已有的并行图计算系统：比如，ParallelBGL和CGM Graph，实现了很多并行图算法 图计算通用软件• 针对大型图的计算，目前通用的图计算软件主要包括两种：– 第一种主要是 基于遍历算法 的、 实时的图数据库，如Neo4j、OrientDB、DEX和 Infinite Graph– 第二种则是 以图顶点为中心的、基于消息传递批处理的并行引擎，如GoldenOrb、Giraph、Pregel和Hama，这些图处理软件主要是基于BSP模型实现的并行图处理系统 一次BSP(Bulk Synchronous Parallel Computing Model，又称“大同步”模型)计算过程包括一系列全局超步（所谓的超步就是计算中的一次迭代），每个超步主要包括三个组件：• 局部计算：每个参与的处理器都有自身的计算任务• 通讯：处理器群相互交换数据• 栅栏同步(Barrier Synchronization)：当一个处理器遇到“路障”（或栅栏），会等到其他所有处理器完成它们的计算步骤 Pregel•谷歌公司在2003年到2004年公布了GFS、MapReduce和BigTable•谷歌在后Hadoop时代的新“三驾马车”•Caffeine•Dremel•Pregel•Pregel是一种基于BSP模型实现的并行图处理系统•为了解决大型图的分布式计算问题，Pregel搭建了一套可扩展的、有容错机制的平台，该平台提供了一套非常灵活的API，可以描述各种各样的图计算•Pregel作为分布式图计算的计算框架，主要用于图遍历、最短路径、PageRank计算等等 Pregel图计算模型有向图和顶点•Pregel计算模型以有向图作为输入•有向图的每个顶点都有一个String类型的顶点ID•每个顶点都有一个可修改的用户自定义值与之关联•每条有向边都和其源顶点关联，并记录了其目标顶点ID•边上有一个可修改的用户自定义值与之关联 •在每个超步S中，图中的所有顶点都会并行执行相同的用户自定义函数•每个顶点可以接收前一个超步(S-1)中发送给它的消息，修改其自身及其出射边的状态，并发送消息给其他顶点，甚至是修改整个图的拓扑结构•在这种计算模式中，“边”并不是核心对象，在边上面不会运行相应的计算，只有顶点才会执行用户自定义函数进行相应计算 顶点之间的消息传递采用消息传递模型主要基于以下两个原因：（1）消息传递具有足够的表达能力，没有必要使用远程读取或共享内存的方式（2）有助于提升系统整体性能 Pregel的计算过程•Pregel的计算过程是由一系列被称为“超步”的迭代组成的•在每个超步中，每个顶点上面都会并行执行用户自定义的函数，该函数描述了一个顶点V在一个超步S中需要执行的操作•该函数可以读取前一个超步(S-1)中其他顶点发送给顶点V的消息，执行相应计算后，修改顶点V及其出射边的状态，然后沿着顶点V的出射边发送消息给其他顶点，而且，一个消息可能经过多条边的传递后被发送到任意已知ID的目标顶点上去•这些消息将会在下一个超步(S+1)中被目标顶点接收，然后象上述过程一样开始下一个超步(S+1)的迭代过程•在Pregel计算过程中，一个算法什么时候可以结束，是由所有顶点的状态决定的•在第0个超步，所有顶点处于活跃状态•当一个顶点不需要继续执行进一步的计算时，就会把自己的状态设置为“停机”，进入非活跃状态•当一个处于非活跃状态的顶点收到来自其他顶点的消息时，Pregel计算框架必须根据条件判断来决定是否将其显式唤醒进入活跃状态•当图中所有的顶点都已经标识其自身达到“非活跃（inactive）”状态，并且没有消息在传送的时候，算法就可以停止运行 实例 Pregel的C++ APIPregel已经预先定义好一个基类——Vertex类：123456789101112template &lt;typename VertexValue, typename EdgeValue, typename MessageValue&gt;class Vertex &#123; public: virtual void Compute(MessageIterator* msgs) = 0; const string&amp; vertex_id() const; int64 superstep() const; const VertexValue&amp; GetValue(); VertexValue* MutableValue(); OutEdgeIterator GetOutEdgeIterator(); void SendMessageTo(const string&amp; dest_vertex, const MessageValue&amp; message); void VoteToHalt();&#125;; •在Vetex类中，定义了三个值类型参数，分别表示顶点、边和消息。每一个顶点都有一个给定类型的值与之对应•编写Pregel程序时，需要继承Vertex类，并且覆写Vertex类的虚函数Compute() 消息传递机制• 顶点之间的通讯是借助于消息传递机制来实现的，每条消息都包含了消息值和需要到达的目标顶点ID。用户可以通过Vertex类的模板参数来设定消息值的数据类型• 在一个超步S中，一个顶点可以发送任意数量的消息，这些消息将在下一个超步（S+1）中被其他顶点接收• 一个顶点V通过与之关联的出射边向外发送消息，并且，消息要到达的目标顶点并不一定是与顶点V相邻的顶点，一个消息可以连续经过多条连通的边到达某个与顶点V不相邻的顶点U，U可以从接收的消息中获取到与其不相邻的顶点V的ID Combiner• Pregel计算框架在消息发出去之前，Combiner可以将发往同一个顶点的多个整型值进行求和得到一个值，只需向外发送这个“求和结果”，从而实现了由多个消息合并成一个消息，大大减少了传输和缓存的开销• 在默认情况下，Pregel计算框架并不会开启Combiner功能• 当用户打算开启Combiner功能时，可以继承Combiner类并覆写虚函数Combine()• 此外，通常只对那些满足交换律和结合律的操作才可以去开启Combiner功能 Aggregator• Aggregator提供了一种全局通信、监控和数据查看的机制• 在一个超步S中，每一个顶点都可以向一个Aggregator提供一个数据，Pregel计算框架会对这些值进行聚合操作产生一个值，在下一个超步（S+1）中，图中的所有顶点都可以看见这个值• Aggregator的聚合功能，允许在整型和字符串类型上执行最大值、最小值、求和操作，比如，可以定义一个“Sum”Aggregator来统计每个顶点的出射边数量，最后相加可以得到整个图的边的数量• Aggregator还可以实现全局协同的功能，比如，可以设计“and”Aggregator来决定在某个超步中Compute()函数是否执行某些逻辑分支，只有当“and” Aggregator显示所有顶点都满足了某条件时，才去执行这些逻辑分支 拓扑改变• Pregel计算框架允许用户在自定义函数Compute()中定义操作，修改图的拓扑结构，比如在图中增加（或删除）边或顶点• 对于全局拓扑改变，Pregel采用了惰性协调机制• 对于本地的局部拓扑改变，是不会引发冲突的，顶点或边的本地增减能够立即生效，很大程度上简化了分布式编程 输入和输出• 在Pregel计算框架中，图的保存格式多种多样，包括文本文件、关系数据库或键值数据库等• 在Pregel中，“从输入文件生成得到图结构”和“执行图计算”这两个过程是分离的，从而不会限制输入文件的格式• 对于输出，Pregel也采用了灵活的方式，可以以多种方式进行输出 Pregel的体系结构Pregel的执行过程•在Pregel计算框架中，一个大型图会被划分成许多个分区，每个分区都包含了一部分顶点以及以其为起点的边•一个顶点应该被分配到哪个分区上，是由一个函数决定的，系统默认函数为hash(ID) mod N，其中，N为所有分区总数，ID是这个顶点的标识符；当然，用户也可以自己定义这个函数•这样，无论在哪台机器上，都可以简单根据顶点ID判断出该顶点属于哪个分区，即使该顶点可能已经不存在了 在理想的情况下（不发生任何错误），一个Pregel用户程序的执行过程如下：（1）选择集群中的多台机器执行图计算任务，有一台机器会被选为Master，其他机器作为Worker（2）Master把一个图分成多个分区，并把分区分配到多个Worker。一个Worker会领到一个或多个分区，每个Worker知道所有其他Worker所分配到的分区情况 （3）Master会把用户输入划分成多个部分。然后，Master会为每个Worker分配用户输入的一部分。如果一个Worker从输入内容中加载到的顶点，刚好是自己所分配到的分区中的顶点，就会立即更新相应的数据结构。否则，该Worker会根据加载到的顶点的ID，把它发送到其所属的分区所在的Worker上。当所有的输入都被加载后，图中的所有顶点都会被标记为“活跃”状态。 （4）Master向每个Worker发送指令，Worker收到指令后，开始运行一个超步。当一个超步中的所有工作都完成以后，Worker会通知Master，并把自己在下一个超步还处于“活跃”状态的顶点的数量报告给Master。上述步骤会被不断重复，直到所有顶点都不再活跃并且系统中不会有任何消息在传输，这时，执行过程才会结束。（5）计算过程结束后，Master会给所有的Worker发送指令，通知每个Worker对自己的计算结果进行持久化存储 容错性• Pregel采用检查点机制来实现容错。在每个超步的开始，Master会通知所有的Worker把自己管辖的分区的状态写入到持久化存储设备• Master会周期性地向每个Worker发送ping消息，Worker收到ping消息后会给Master发送反馈消息• 每个Worker上都保存了一个或多个分区的状态信息，当一个Worker发生故障时，它所负责维护的分区的当前状态信息就会丢失。Master监测到一个Worker发生故障“失效”后，会把失效Worker所分配到的分区，重新分配到其他处于正常工作状态的Worker集合上，然后，所有这些分区会从最近的某超步S开始时写出的检查点中，重新加载状态信息 Worker在一个Worker中，它所管辖的分区的状态信息是保存在内存中的。分区中的顶点的状态信息包括：•顶点的当前值•以该顶点为起点的出射边列表，每条出射边包含了目标顶点ID和边的值•消息队列，包含了所有接收到的、发送给该顶点的消息•标志位，用来标记顶点是否处于活跃状态在每个超步中，Worker会对自己所管辖的分区中的每个顶点进行遍历，并调用顶点上的Compute()函数，在调用时，会把以下三个参数传递进去：•该顶点的当前值•一个接收到的消息的迭代器•一个出射边的迭代器 •在Pregel中，为了获得更好的性能，“标志位”和输入消息队列是分开保存的•对于每个顶点而言，Pregel只保存一份顶点值和边值，但是，会保存两份“标志位”和输入消息队列，分别用于当前超步和下一个超步•如果一个顶点V在超步S接收到消息，那么，它表示V将会在下一个超步S+1中（而不是当前超步S中）处于“活跃”状态 •当一个Worker上的一个顶点V需要发送消息到其他顶点U时，该Worker会首先判断目标顶点U是否位于自己机器上•如果目标顶点U在自己的机器上，就直接把消息放入到与目标顶点U对应的输入消息队列中•如果发现目标顶点U在远程机器上，这个消息就会被暂时缓存到本地，当缓存中的消息数目达到一个事先设定的阈值时，这些缓存消息会被批量异步发送出去，传输到目标顶点所在的Worker上 Master•Master主要负责协调各个Worker执行任务，每个Worker会借助于名称服务系统定位到Master的位置，并向Master发送自己的注册信息，Master会为每个Worker分配一个唯一的ID•Master维护着关于当前处于“有效”状态的所有Worker的各种信息，包括每个Worker的ID和地址信息，以及每个Worker被分配到的分区信息•Master中保存这些信息的数据结构的大小，只与分区的数量有关，而与顶点和边的数量无关 •一个大规模图计算任务会被Master分解到多个Worker去执行，在每个超步开始时，Master都会向所有处于“有效”状态的Worker发送相同的指令，然后等待这些Worker的回应•如果在指定时间内收不到某个Worker的反馈，Master就认为这个Worker失效•如果参与任务执行的多个Worker中的任意一个发生了故障失效，Master就会进入恢复模式•在每个超步中，图计算的各种工作，比如输入、输出、计算、保存和从检查点中恢复，都会在“路障（barrier）”之前结束 •Master在内部运行了一个HTTP服务器来显示图计算过程的各种信息•用户可以通过网页随时监控图计算执行过程各个细节 •图的大小 •关于出度分布的柱状图 •处于活跃状态的顶点数量 •在当前超步的时间信息和消息流量 •所有用户自定义Aggregator的值 Aggregator• 每个用户自定义的Aggregator都会采用聚合函数对一个值集合进行聚合计算得到一个全局值• 每个Worker都保存了一个Aggregator的实例集，其中的每个实例都是由类型名称和实例名称来标识的• 在执行图计算过程的某个超步S中，每个Worker会利用一个Aggregator对当前本地分区中包含的所有顶点的值进行归约，得到一个本地的局部归约值• 在超步S结束时，所有Worker会将所有包含局部归约值的Aggregator的值进行最后的汇总，得到全局值，然后提交给Master• 在下一个超步S+1开始时，Master就会将Aggregator的全局值发送给每个Worker Pregel的应用实例——单源最短路径Dijkstra算法是解决单源最短路径问题的贪婪算法 Pregel非常适合用来解决单源最短路径问题，实现代码如下：12345678910111213141516class ShortestPathVertex : public Vertex&lt;int, int, int&gt; &#123; void Compute(MessageIterator* msgs) &#123; int mindist = IsSource(vertex_id()) ? 0 : INF; for (; !msgs-&gt;Done(); msgs-&gt;Next()) mindist = min(mindist, msgs-&gt;Value()); if (mindist &lt; GetValue()) &#123; *MutableValue() = mindist; OutEdgeIterator iter = GetOutEdgeIterator(); for (; !iter.Done(); iter.Next()) SendMessageTo(iter.Target(), mindist + iter.GetValue()); &#125; VoteToHalt(); &#125; &#125;; 超步1：•顶点0：没有收到消息，依然非活跃•顶点1：收到消息100（唯一消息），被显式唤醒，执行计算，mindist变为100，小于顶点值INF，顶点值修改为100，没有出射边，不需要发送消息，最后变为非活跃•顶点2：收到消息30，被显式唤醒，执行计算，mindist变为30，小于顶点值ZNF，顶点值修改为30，有两条出射边，向顶点3发送消息90（即：30+60），向顶点1发送消息90（即：30+60），最后变为非活跃•顶点3：没有收到消息，依然非活跃•顶点4：收到消息10，被显式唤醒，执行计算，mindist变为10，小于顶点值INF，顶点值修改为10，向顶点3发送消息60（即：10+50），最后变为非活跃剩余超步省略……当所有顶点非活跃，并且没有消息传递，就结束]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据基础学习笔记（九）——流计算]]></title>
    <url>%2F2018%2F05%2F22%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B9%9D%EF%BC%89%E2%80%94%E2%80%94%E6%B5%81%E8%AE%A1%E7%AE%97%2F</url>
    <content type="text"><![CDATA[什么是流数据• 近年来，在Web应用、网络监控、传感监测等领域，兴起了一种新的数据密集型应用——流数据，即数据以大量、快速、时变的流形式持续到达• 实例：PM2.5检测、电子商务网站用户点击流• 流数据具有如下特征：– 数据快速持续到达，潜在大小也许是无穷无尽的– 数据来源众多，格式复杂– 数据量大，但是不十分关注存储，一旦经过处理，要么被丢弃，要么被归档存储– 注重数据的整体价值，不过分关注个别数据– 数据顺序颠倒，或者不完整，系统无法控制将要处理的新到达的数据元素的顺序• 对静态数据和流数据的处理，对应着两种截然不同的计算模式：批量计算和实时计算•批量计算：充裕时间处理静态数据，如Hadoop•流数据不适合采用批量计算，因为流数据不适合用传统的关系模型建模•流数据必须采用实时计算，响应时间为秒级•在大数据时代，数据格式复杂、来源众多、数据量巨大，对实时计算提出了很大的挑战。因此，针对流数据的实时计算——流计算，应运而生 流计算的概念• 流计算秉承一个基本理念，即 数据的价值随着时间的流逝而降低，如用户点击流。因此，当事件出现时就应该立即进行处理，而不是缓存起来进行批量处理。为了及时处理流数据，就需要一个低延迟、可扩展、高可靠的处理引擎• 对于一个流计算系统来说，它应达到如下需求：– 高性能– 海量式– 实时性– 分布式– 易用性– 可靠性 流计算与Hadoop• Hadoop设计的初衷是面向大规模数据的批量处理• MapReduce是专门面向静态数据的批量处理的，内部各种实现机制都为批处理做了高度优化，不适合用于处理持续到达的动态数据• 可能会想到一种“变通”的方案来降低批处理的时间延迟——将基于MapReduce的批量处理转为小批量处理，将输入数据切成小的片段，每隔一个周期就启动一次MapReduce作业。但这种方式也无法有效处理流数据– 切分成小片段，可以降低延迟，但是也增加了附加开销，还要处理片段之间依赖关系– 需要改造MapReduce以支持流式处理结论：鱼和熊掌不可兼得，Hadoop擅长批处理，但是不适合流计算• 当前业界诞生了许多专门的流数据实时计算系统来满足各自需求：• 商业级：IBM InfoSphere Streams和IBM StreamBase• 开源流计算框架：– Twitter Storm：免费、开源的分布式实时计算系统，可简单、高效、可靠地处理大量的流数据– Yahoo! S4（Simple Scalable Streaming System）：开源流计算平台，是通用的、分布式的、可扩展的、分区容错的、可插拔的流式系统• 公司为支持自身业务开发的流计算框架：– Facebook Puma– Dstream（百度）– 银河流数据处理平台（淘宝） 流计算处理流程• 传统的数据处理流程，需要先采集数据并存储在关系数据库等数据管理系统中，之后由用户通过查询操作和数据管理系统进行交互• 传统的数据处理流程隐含了两个前提：– 存储的数据是旧的。存储的静态数据是过去某一时刻的快照，这些数据在查询时可能已不具备时效性了– 需要用户主动发出查询 • 流计算的处理流程一般包含三个阶段：数据实时采集、数据实时计算、实时查询服务 数据实时采集• 数据实时采集阶段通常采集多个数据源的海量数据，需要保证实时性、低延迟与稳定可靠• 以日志数据为例，由于分布式集群的广泛应用，数据分散存储在不同的机器上，因此需要实时汇总来自不同机器上的日志数据• 目前有许多互联网公司发布的开源分布式日志采集系统均可满足每秒数百MB的数据采集和传输需求，如： – Facebook的Scribe – LinkedIn的Kafka – 淘宝的Time Tunnel – 基于Hadoop的Chukwa和Flume • 数据采集系统的基本架构一般有以下三个部分：– Agent：主动采集数据，并把数据推送到Collector部分– Collector：接收多个Agent的数据，并实现有序、可靠、高性能的转发– Store：存储Collector转发过来的数据（对于流计算不存储数据） 数据实时计算• 数据实时计算阶段对采集的数据进行实时的分析和计算，并反馈实时结果• 经流处理系统处理后的数据，可视情况进行存储，以便之后再进行分析计算。在时效性要求较高的场景中，处理之后的数据也可以直接丢弃 实时查询服务• 实时查询服务：经由流计算框架得出的结果可供用户进行实时查询、展示或储存• 传统的数据处理流程，用户需要主动发出查询才能获得想要的结果。而在流处理流程中，实时查询服务可以不断更新结果，并将用户所需的结果实时推送给用户• 虽然通过对传统的数据处理系统进行定时查询，也可以实现不断地更新结果和结果推送，但通过这样的方式获取的结果，仍然是根据过去某一时刻的数据得到的结果，与实时结果有着本质的区别• 可见，流处理系统与传统的数据处理系统有如下不同：– 流处理系统处理的是实时的数据，而传统的数据处理系统处理的是预先存储好的静态数据– 用户通过流处理系统获取的是实时结果，而通过传统的数据处理系统，获取的是过去某一时刻的结果– 流处理系统无需用户主动发出查询，实时查询服务可以主动将实时结果推送给用户 开源流计算框架Storm• Twitter Storm是一个免费、开源的分布式实时计算系统，Storm对于实时计算的意义类似于Hadoop对于批处理的意义，Storm可以简单、高效、可靠地处理流数据，并支持多种编程语言 • Storm框架可以方便地与数据库系统进行整合，从而开发出强大的实时计算系统 • Twitter是全球访问量最大的社交网站之一，Twitter开发Storm流处理框架也是为了应对其不断增长的流数据实时处理需求 Storm的特点• Storm可用于许多领域中，如实时分析、在线机器学习、持续计算、远程RPC、数据提取加载转换等 • Storm具有以下主要特点：– 整合性– 简易的API– 可扩展性– 可靠的消息处理– 支持各种编程语言– 快速部署– 免费、开源 Storm设计思想• Storm主要术语包括Streams、Spouts、Bolts、Topology和Stream Groupings • Streams ：Storm将流数据Stream描述成一个无限的Tuple序列，这些Tuple序列会以分布式的方式并行地创建和处理 •每个tuple是一堆值，每个值有一个名字，并且每个值可以是任何类型•Tuple本来应该是一个Key-Value的Map，由于各个组件间传递的tuple的字段名称已经事先定义好了，所以Tuple只需要按序填入各个Value，所以就是一个Value List（值列表） • Spout：Storm认为每个Stream都有一个源头，并把这个源头抽象为Spout • 通常Spout会从外部数据源（队列、数据库等）读取数据，然后封装成Tuple形式，发送到Stream中。Spout是一个主动的角色，在接口内部有个nextTuple函，Storm框架会不停的调用该函数 • Bolt ：Storm将Streams的状态转换过程抽象为Bolt。Bolt即可以处理Tuple，也可以将处理后的Tuple作为新的Streams发送给其他Bolt • Bolt可以执行过滤、函数操作、Join、操作数据库等任何操作• Bolt是一个被动的角色，其接口中有一个execute(Tuple input)方法，在接收到消息之后会调用此函数，用户可以在此方法中执行自己的处理逻辑 • Topology ：Storm将Spouts和Bolts组成的网络抽象成Topology，它可以被提交到Storm集群执行。Topology可视为流转换图，图中节点是一个Spout或Bolt，边则表示Bolt订阅了哪个Stream。当Spout或者Bolt发送元组时，它会把元组发送到每个订阅了该Stream的Bolt上进行处理• Topology里面的每个处理组件（Spout或Bolt）都包含处理逻辑， 而组件之间的连接则表示数据流动的方向• Topology里面的每一个组件都是并行运行的•在Topology里面可以指定每个组件的并行度，Storm会在集群里面分配那么多的线程来同时计算•在Topology的具体实现上，Storm中的Topology定义仅仅是一些Thrift结构体（二进制高性能的通信中间件），支持各种编程语言进行定义 • Stream Groupings ：Storm中的Stream Groupings用于告知Topology如何在两个组件间（如Spout和Bolt之间，或者不同的Bolt之间）进行Tuple的传送。每一个Spout和Bolt都可以有多个分布式任务，一个任务在什么时候、以什么方式发送Tuple就是由Stream Groupings来决定的 目前，Storm中的Stream Groupings有如下几种方式： (1)ShuffleGrouping：随机分组，随机分发Stream中的Tuple，保证每个Bolt的Task接收Tuple数量大致一致(2)FieldsGrouping：按照字段分组，保证相同字段的Tuple分配到同一个Task中(3)AllGrouping：广播发送，每一个Task都会收到所有的Tuple(4)GlobalGrouping：全局分组，所有的Tuple都发送到同一个Task中(5)NonGrouping：不分组，和ShuffleGrouping类似，当前Task的执行会和它的被订阅者在同一个线程中执行(6)DirectGrouping：直接分组，直接指定由某个Task来执行Tuple的处理 Storm框架设计•Storm运行任务的方式与Hadoop类似：Hadoop运行的是MapReduce作业，而Storm运行的是“Topology”•但两者的任务大不相同，主要的不同是：MapReduce作业最终会完成计算并结束运行，而Topology将持续处理消息（直到人为终止） • Storm集群采用“Master—Worker”的节点方式：– Master节点运行名为“Nimbus”的后台程序（类似Hadoop中的“JobTracker”），负责在集群范围内分发代码、为Worker分配任务和监测故障– Worker节点运行名为“Supervisor”的后台程序，负责监听分配给它所在机器的工作，即根据Nimbus分配的任务来决定启动或停止Worker进程，一个Worker节点上同时运行若干个Worker进程• Storm使用Zookeeper来作为分布式协调组件，负责Nimbus和多个Supervisor之间的所有协调工作。借助于Zookeeper，若Nimbus进程或Supervisor进程意外终止，重启时也能读取、恢复之前的状态并继续工作，使得Storm极其稳定 worker进程 (1)Worker进程:每个worker进程都属于一个特定的Topology，每个Supervisor节点的worker可以有多个，每个worker对Topology中的每个组件（Spout或Bolt）运行一个或者多个executor线程来提供task的运行服务(2)Executor：executor是产生于worker进程内部的线程，会执行同一个组件的一个或者多个task。(3)Task:实际的数据处理由task完成Worker、Executor和Task的关系 • 基于这样的架构设计，Storm的工作流程如下图所示：•所有Topology任务的提交必须在Storm客户端节点上进行，提交后，由Nimbus节点分配给其他Supervisor节点进行处理•Nimbus节点首先将提交的Topology进行分片，分成一个个Task，分配给相应的Supervisor，并将Task和Supervisor相关的信息提交到Zookeeper集群上•Supervisor会去Zookeeper集群上认领自己的Task，通知自己的Worker进程进行Task的处理 Spark StreamingSpark Streaming设计•Spark Streaming可整合多种输入数据源，如Kafka、Flume、HDFS，甚至是普通的TCP套接字。经处理后的数据可存储至文件系统、数据库，或显示在仪表盘里 Spark Streaming的基本原理是将实时输入数据流以时间片（秒级）为单位进行拆分，然后经Spark引擎以类似批处理的方式处理每个时间片数据 Spark Streaming最主要的抽象是DStream（Discretized Stream，离散化数据流），表示连续不断的数据流。在内部实现上，Spark Streaming的输入数据按照时间片（如1秒）分成一段一段的DStream，每一段数据转换为Spark中的RDD，并且对DStream的操作都最终转变为对相应的RDD的操作 Spark Streaming与Storm的对比•Spark Streaming和Storm最大的区别在于，Spark Streaming无法实现毫秒级的流计算，而Storm可以实现毫秒级响应 •Spark Streaming构建在Spark上，一方面是因为Spark的低延迟执行引擎（100ms+）可以用于实时计算，另一方面，相比于Storm，RDD数据集更容易做高效的容错处理 •Spark Streaming采用的小批量处理的方式使得它可以同时兼容批量和实时数据处理的逻辑和算法，因此，方便了一些需要历史数据和实时数据联合分析的特定应用场合 Samza1.作业 一个作业（Job）是对一组输入流进行处理转化成输出流的程序。 2.分区 •Samza的流数据单位既不是Storm中的元组，也不是Spark Streaming中的DStream，而是一条条消息•Samza中的每个流都被分割成一个或多个分区，对于流里的每一个分区而言，都是一个有序的消息序列，后续到达的消息会根据一定规则被追加到其中一个分区里 3.任务 •一个作业会被进一步分割成多个任务（Task）来执行，其中，每个任务负责处理作业中的一个分区•分区之间没有定义顺序，从而允许每一个任务独立执行•YARN调度器负责把任务分发给各个机器，最终，一个工作中的多个任务会被分发到多个机器进行分布式并行处理 4.数据流图 •一个数据流图是由多个作业构成的，其中，图中的每个节点表示包含数据的流，每条边表示数据传输•多个作业串联起来就完成了流式的数据处理流程•由于采用了异步的消息订阅分发机制，不同任务之间可以独立运行 系统架构•Samza系统架构主要包括•流数据层（Kafka）•执行层（YARN）•处理层（Samza API）•流处理层和执行层都被设计成可插拔的，开发人员可以使用其他框架来替代YARN和Kafka 处理分析过程如下： •Samza客户端需要执行一个Samza作业时，它会向YARN的ResouceManager提交作业请求 •ResouceManager通过与NodeManager沟通为该作业分配容器（包含了CPU、内存等资源）来运行Samza ApplicationMaster •Samza ApplicationMaster进一步向ResourceManager申请运行任务的容器 •获得容器后，Samza ApplicationMaster与容器所在的NodeManager沟通，启动该容器，并在其中运行Samza Task Runner •Samza Task Runner负责执行具体的Samza任务，完成流数据处理分析 Storm、Spark Streaming和Samza的应用场景•从编程的灵活性来讲，Storm是比较理想的选择，它使用Apache Thrift，可以用任何编程语言来编写拓扑结构（Topology） •当需要在一个集群中把流计算和图计算、机器学习、SQL查询分析等进行结合时，可以选择Spark Streaming，因为，在Spark上可以统一部署Spark SQL，Spark Streaming、MLlib，GraphX等组件，提供便捷的一体化编程模型 •当有大量的状态需要处理时，比如每个分区都有数十亿个元组，则可以选择Samza。当应用场景需要毫秒级响应时，可以选择Storm和Samza，因为Spark Streaming无法实现毫秒级的流计算]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据基础学习笔记（八）——Spark]]></title>
    <url>%2F2018%2F05%2F17%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AB%EF%BC%89%E2%80%94%E2%80%94Spark%2F</url>
    <content type="text"><![CDATA[Spark的特点•运行速度快：使用DAG执行引擎以支持循环数据流与内存计算 •容易使用：支持使用Scala、Java、Python和R语言进行编程，可以通过Spark Shell进行交互式编程 •通用性：Spark提供了完整而强大的技术栈，包括SQL查询、流式计算、机器学习和图算法组件 •运行模式多样：可运行于独立的集群模式中，可运行于Hadoop中，也可运行于Amazon EC2等云环境中，并且可以访问HDFS、Cassandra、HBase、Hive等多种数据源 Scala简介Scala是一门现代的多范式编程语言，运行于Java平台（JVM，Java 虚拟机），并兼容现有的Java程序 Scala的特性： •Scala具备强大的并发性，支持函数式编程，可以更好地支持分布式系统 •Scala语法简洁，能提供优雅的API Scala兼容Java，运行速度快，且能融合到Hadoop生态圈中 Scala是Spark的主要编程语言，但Spark还支持Java、Python、R作为编程语言 Scala的优势是提供了REPL（Read-Eval-Print Loop，交互式解释器），提高程序开发效率 Spark与Hadoop的对比Hadoop存在如下一些缺点： •表达能力有限 •磁盘IO开销大 •延迟高 •任务之间的衔接涉及IO开销 •在前一个任务执行完成之前，其他任务就无法开始，难以胜任复杂、多阶段的计算任务 Spark在借鉴Hadoop MapReduce优点的同时，很好地解决了MapReduce所面临的问题 相比于Hadoop MapReduce，Spark主要具有如下优点： •Spark的计算模式也属于MapReduce，但不局限于Map和Reduce操作，还提供了多种数据集操作类型，编程模型比Hadoop MapReduce更灵活 •Spark提供了内存计算，可将中间结果放到内存中，对于迭代运算效率更高 Spark基于DAG的任务调度执行机制，要优于Hadoop MapReduce的迭代执行机制 •使用Hadoop进行迭代计算非常耗资源 •Spark将数据载入内存后，之后的迭代计算都可以直接使用内存中的中间结果作运算，避免了从磁盘中频繁读取数据 Spark生态系统在实际应用中，大数据处理主要包括以下三个类型： •复杂的批量数据处理：通常时间跨度在数十分钟到数小时之间 •基于历史数据的交互式查询：通常时间跨度在数十秒到数分钟之间 •基于实时数据流的数据处理：通常时间跨度在数百毫秒到数秒之间 当同时存在以上三种场景时，就需要同时部署三种不同的软件 •比如: MapReduce / Impala / Storm这样做难免会带来一些问题： •不同场景之间输入输出数据无法做到无缝共享，通常需要进行数据格式的转换 •不同的软件需要不同的开发和维护团队，带来了较高的使用成本 •比较难以对同一个集群中的各个系统进行统一的资源协调和分配 •Spark的设计遵循“一个软件栈满足不同应用场景”的理念，逐渐形成了一套完整的生态系统 •既能够提供内存计算框架，也可以支持SQL即席查询、实时流式计算、机器学习和图计算等 •Spark可以部署在资源管理器YARN之上，提供一站式的大数据解决方案 •因此，Spark所提供的生态系统足以应对上述三种场景，即同时支持批处理、交互式查询和流数据处理 Spark生态系统已经成为伯克利数据分析软件栈BDAS（Berkeley Data Analytics Stack）的重要组成部分 Spark的生态系统主要包含了Spark Core、Spark SQL、Spark Streaming、MLLib和GraphX 等组件 Spark运行架构•RDD：是Resillient Distributed Dataset（弹性分布式数据集）的简称，是分布式内存的一个抽象概念，提供了一种高度受限的共享内存模型 •DAG：是Directed Acyclic Graph（有向无环图）的简称，反映RDD之间的依赖关系 •Executor：是运行在工作节点（WorkerNode）的一个进程，负责运行Task •Application：用户编写的Spark应用程序 •Task：运行在Executor上的工作单元 •Job：一个Job包含多个RDD及作用于相应RDD上的各种操作 •Stage：是Job的基本调度单位，一个Job会分为多组Task，每组Task被称为Stage，或者也被称为TaskSet，代表了一组关联的、相互之间没有Shuffle依赖关系的任务组成的任务集 •Spark运行架构包括集群资源管理器（Cluster Manager）、运行作业任务的工作节点（Worker Node）、每个应用的任务控制节点（Driver）和每个工作节点上负责具体任务的执行进程（Executor） •资源管理器可以自带或Mesos或YARN 与Hadoop MapReduce计算框架相比，Spark所采用的Executor有两个优点： •一是利用多线程来执行具体的任务，减少任务的启动开销 •二是Executor中有一个BlockManager存储模块，会将内存和磁盘共同作为存储设备，有效减少IO开销 •一个Application由一个Driver和若干个Job构成，一个Job由多个Stage构成，一个Stage由多个没有Shuffle关系的Task组成 •当执行一个Application时，Driver会向集群管理器申请资源，启动xecutor，并向Executor发送应用程序代码和文件，然后在Executor上执行Task，运行结束后，执行结果会返回给Driver，或者写到HDFS或者其他数据库中 Spark运行基本流程（1）首先为应用构建起基本的运行环境，即由Driver创建一个SparkContext，进行资源的申请、任务的分配和监控 （2）资源管理器为Executor分配资源，并启动Executor进程 （3）SparkContext根据RDD的依赖关系构建DAG图，DAG图提交给DAGScheduler解析成Stage，然后把一个个TaskSet提交给底层调度器TaskScheduler处理；Executor向SparkContext申请Task，Task Scheduler将Task发放给Executor运行，并提供应用程序代码 （4）Task在Executor上运行，把执行结果反馈给TaskScheduler，然后反馈给DAGScheduler，运行完毕后写入数据并释放所有资源 总体而言，Spark运行架构具有以下特点： （1）每个Application都有自己专属的Executor进程，并且该进程在Application运行期间一直驻留。Executor进程以多线程的方式运行Task （2）Spark运行过程与资源管理器无关，只要能够获取Executor进程并保持通信即可 （3）Task采用了数据本地性和推测执行等优化机制 RDD1.设计背景 •许多迭代式算法（比如机器学习、图算法等）和交互式数据挖掘工具，共同之处是，不同计算阶段之间会重用中间结果 •目前的MapReduce框架都是把中间结果写入到HDFS中，带来了大量的数据复制、磁盘IO和序列化开销 •RDD就是为了满足这种需求而出现的，它提供了一个抽象的数据架构，我们不必担心底层数据的分布式特性，只需将具体的应用逻辑表达为一系列转换处理，不同RDD之间的转换操作形成依赖关系，可以实现管道化，避免中间数据存储 2.RDD 概念 •一个RDD就是一个分布式对象集合，本质上是一个只读的分区记录集合，每个RDD可分成多个分区，每个分区就是一个数据集片段，并且一个RDD的不同分区可以被保存到集群中不同的节点上，从而可以在集群中的不同节点上进行并行计算 •RDD提供了一种高度受限的共享内存模型，即RDD是只读的记录分区的集合，不能直接修改，只能基于稳定的物理存储中的数据集创建RDD，或者通过在其他RDD上执行确定的转换操作（如map、join和group by）而创建得到新的RDD •RDD提供了一组丰富的操作以支持常见的数据运算，分为“动作”（Action）和“转换”（Transformation）两种类型 •RDD提供的转换接口都非常简单，都是类似map、filter、groupBy、join等粗粒度的数据转换操作，而不是针对某个数据项的细粒度修改（不适合网页爬虫） •表面上RDD的功能很受限、不够强大，实际上RDD已经被实践证明可以高效地表达许多框架的编程模型（比如MapReduce、SQL、Pregel） •Spark用Scala语言实现了RDD的API，程序员可以通过调用API实现对RDD的各种操作 RDD典型的执行过程如下： •RDD读入外部数据源进行创建 •RDD经过一系列的转换（Transformation）操作，每一次都会产生不同的RDD，供给下一个转换操作使用 •最后一个RDD经过“动作”操作进行转换，并输出到外部数据源这一系列处理称为一个Lineage（血缘关系），即DAG拓扑排序的结果 优点：惰性调用、管道化、避免同步等待、不需要保存中间结果、每次操作变得简单 3.RDD特性 Spark采用RDD以后能够实现高效计算的原因主要在于： （1）高效的容错性 •现有容错机制：数据复制或者记录日志 •RDD：血缘关系、重新计算丢失分区、无需回滚系统、重算过程在不同节点之间并行、只记录粗粒度的操作 （2）中间结果持久化到内存，数据在内存中的多个RDD操作之间进行传递，避免了不必要的读写磁盘开销 （3）存放的数据可以是Java对象，避免了不必要的对象序列化和反序列化 4.RDD之间的依赖关系 窄依赖表现为一个父RDD的分区对应于一个子RDD的分区或多个父RDD的分区对应于一个子RDD的分区 •宽依赖则表现为存在一个父RDD的一个分区对应一个子RDD的多个分区 5.Stage的划分 Spark通过分析各个RDD的依赖关系生成了DAG，再通过分析各个RDD中的分区之间的依赖关系来决定如何划分Stage，具体划分方法是： •在DAG中进行反向解析，遇到宽依赖就断开 •遇到窄依赖就把当前的RDD加入到Stage中 •将窄依赖尽量划分在同一个Stage中，可以实现流水线计算被分成三个Stage，在Stage2中，从map到union都是窄依赖，这两步操作可以形成一个流水线操作 流水线操作实例分区7通过map操作生成的分区9，可以不用等待分区8到分区10这个map操作的计算结束，而是继续进行union操作，得到分区13，这样流水线执行大大提高了计算的效率 Stage的类型包括两种：ShuffleMapStage和ResultStage，具体如下： （1）ShuffleMapStage：不是最终的Stage，在它之后还有其他Stage，所以，它的输出一定需要经过Shuffle过程，并作为后续Stage的输入；这种Stage是以Shuffle为输出边界，其输入边界可以是从外部获取数据，也可以是另一个ShuffleMapStage的输出，其输出可以是另一个Stage的开始；在一个Job里可能有该类型的Stage，也可能没有该类型Stage； （2）ResultStage：最终的Stage，没有输出，而是直接产生结果或存储。这种Stage是直接输出结果，其输入边界可以是从外部获取数据，也可以是另一个ShuffleMapStage的输出。在一个Job里必定有该类型Stage。因此，一个Job含有一个或多个Stage，其中至少含有一个ResultStage。 6.RDD运行过程 通过上述对RDD概念、依赖关系和Stage划分的介绍，结合之前介绍的Spark运行基本流程，再总结一下RDD在Spark架构中的运行过程： （1）创建RDD对象； （2）SparkContext负责计算RDD之间的依赖关系，构建DAG； （3）DAGScheduler负责把DAG图分解成多个Stage，每个Stage中包含了多个Task，每个Task会被TaskScheduler分发给各个WorkerNode上的Executor去执行。 Spark SQL设计Spark SQL在Hive兼容层面仅依赖HiveQL解析、Hive元数据，也就是说，从HQL被解析成抽象语法树（AST）起，就全部由Spark SQL接管了。Spark SQL执行计划生成和优化都由Catalyst（函数式关系查询优化框架）负责 •Spark SQL增加了SchemaRDD（即带有Schema信息的RDD），使用户可以在Spark SQL中执行SQL语句，数据既可以来自RDD，也可以是Hive、HDFS、Cassandra等外部数据源，还可以是JSON格式的数据 •Spark SQL目前支持Scala、Java、Python三种语言，支持SQL-92规范]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据基础学习笔记（七）——数据仓库Hive]]></title>
    <url>%2F2018%2F05%2F01%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%83%EF%BC%89%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93Hive%2F</url>
    <content type="text"><![CDATA[数据仓库概念数据仓库（Data Warehouse）是一个面向主题的（Subject Oriented）、集成的（Integrated）、相对稳定的（Non-Volatile）、反映历史变化（Time Variant）的数据集合，用于支持管理决策。 Hive简介•Hive是一个构建于Hadoop顶层的数据仓库工具 •支持大规模数据存储、分析，具有良好的可扩展性 •某种程度上可以看作是用户编程接口，本身不存储和处理数据 •依赖分布式文件系统HDFS存储数据 •依赖分布式并行计算模型MapReduce处理数据 •定义了简单的类似SQL 的查询语言——HiveQL •用户可以通过编写的HiveQL语句运行MapReduce任务 •可以很容易把原来构建在关系数据库上的数据仓库应用程序移植到Hadoop平台上 •是一个可以提供有效、合理、直观组织和使用数据的分析工具 Hive具有的特点非常适用于数据仓库 1 采用批处理方式处理海量数据 •Hive需要把HiveQL语句转换成MapReduce任务进行运行 •数据仓库存储的是静态数据，对静态数据的分析适合采用批处理方式，不需要快速响应给出结果，而且数据本身也不会频繁变化 2 提供适合数据仓库操作的工具 •Hive本身提供了一系列对数据进行提取、转换、加载（ETL）的工具，可以存储、查询和分析存储在Hadoop中的大规模数据 •这些工具能够很好地满足数据仓库各种应用场景 Hive与Hadoop生态系统中其他组件的关系•Hive 依赖于HDFS 存储数据 •Hive 依赖于MapReduce 处理数据 • 在某些场景下Pig 可以作为Hive 的替代工具 •HBase 提供数据的实时访问 Hive 与传统数据库的对比分析Hive在很多方面和传统的关系数据库类似，但是它的底层依赖的是HDFS和MapReduce，所以在很多方面又有别于传统数据库 Hive 在企业中的部署和应用 Hive在企业大数据分析平台中的应用 Hive 在Facebook 公司中的应用 •基于Oracle的数据仓库系统已经无法满足激增的业务需求 •Facebook公司开发了数据仓库工具Hive，并在企业内部进行了大量部署 Hive系统架构•用户接口模块包括CLI、HWI、JDBC、ODBC、Thrift Server •驱动模块（Driver）包括编译器、优化器、执行器等，负责把HiveSQL语句转换成一系列MapReduce作业 •元数据存储模块（Metastore）是一个独立的关系型数据库（自带derby数据库，或MySQL数据库） Hive HA基本原理问题：在实际应用中，Hive也暴露出不稳定的问题 解决方案：Hive HA（High Availability） •由多个Hive实例进行管理的，这些Hive实例被纳入到一个资源池中，并由HAProxy提供一个统一的对外接口 •对于程序开发人员来说，可以把它认为是一台超强“Hive” Hive工作原理SQL语句转换成MapReduce作业的基本原理 join的实现原理 group by 的实现原理 存在一个分组（Group By）操作，其功能是把表Score的不同片段按照rank和level的组合值进行合并，计算不同rank和level的组合值分别有几条记录：select rank, level ,count(*) as value from score group by rank, level Hive中SQL查询转换成MapReduce作业的过程•当用户向Hive输入一段命令或查询时，Hive需要与Hadoop交互工作来完成该操作： •驱动模块接收该命令或查询编译器 •对该命令或查询进行解析编译 •由优化器对该命令或查询进行优化计算 •该命令或查询通过执行器进行执行 第1步：由Hive驱动模块中的编译器对用户输入的SQL语言进行词法和语法解析，将SQL语句转化为抽象语法树的形式 第2步：抽象语法树的结构仍很复杂，不方便直接翻译为MapReduce算法程序，因此，把抽象语法书转化为查询块 第3步：把查询块转换成逻辑查询计划，里面包含了许多逻辑操作符 第4步：重写逻辑查询计划，进行优化，合并多余操作，减少MapReduce任务数量 第5步：将逻辑操作符转换成需要执行的具体MapReduce任务 第6步：对生成的MapReduce任务进行优化，生成最终的MapReduce任务执行计划 第7步：由Hive驱动模块中的执行器，对最终的MapReduce任务进行执行输出 几点说明： • 当启动MapReduce程序时，Hive本身是不会生成MapReduce算法程序的 • 需要通过一个表示“Job执行计划”的XML文件驱动执行内置的、原生的Mapper和Reducer模块 • Hive通过和JobTracker通信来初始化MapReduce任务，不必直接部署在JobTracker所在的管理节点上执行 • 通常在大型集群上，会有专门的网关机来部署Hive工具。网关机的作用主要是远程操作和管理节点上的JobTracker通信来执行任务 • 数据文件通常存储在HDFS上，HDFS由名称节点管理 ImpalaImpala简介• Impala是由Cloudera公司开发的新型查询系统，它提供SQL语义，能查询存储在Hadoop的HDFS和HBase上的PB级大数据，在性能上比Hive高出3~30倍 • Impala的运行需要依赖于Hive的元数据 • Impala是参照 Dremel系统进行设计的 • Impala采用了与商用并行关系数据库类似的分布式查询引擎，可以直接与HDFS和HBase进行交互查询 • Impala和Hive采用相同的SQL语法、ODBC驱动程序和用户接口 Impala系统架构Impala和Hive、HDFS、HBase等工具是统一部署在一个Hadoop平台上的Impala主要由Impalad，State Store和CLI三部分组成 图中虚线组件是Impala的组件 Impala主要由Impalad，State Store和CLI三部分组成 Impalad • 负责协调客户端提交的查询的执行 • 包含Query Planner、Query Coordinator和Query Exec Engine三个模块 • 与HDFS的数据节点（HDFS DN）运行在同一节点上 • 给其他Impalad分配任务以及收集其他Impalad的执行结果进行汇总 • Impalad也会执行其他Impalad给其分配的任务，主要就是对本地HDFS和HBase里的部分数据进行操作 State Store • 会创建一个statestored进程 • 负责收集分布在集群中各个Impalad进程的资源信息，用于查询调度 CLI • 给用户提供查询使用的命令行工具 • 还提供了Hue、JDBC及ODBC的使用接口 说明：Impala中的元数据直接存储在Hive中。Impala采用与Hive相同的元数据、SQL语法、ODBC驱动程序和用户接口，从而使得在一个Hadoop平台上，可以统一部署Hive和Impala等分析工具，同时支持批处理和实时查询 Impala查询执行过程 Impala执行查询的具体过程： • 第0步，当用户提交查询前，Impala先创建一个负责协调客户端提交的查询的Impalad进程，该进程会向Impala State Store提交注册订阅信息，State Store会创建一个statestored进程，statestored进程通过创建多个线程来处理Impalad的注册订阅信息。 • 第1步，用户通过CLI客户端提交一个查询到impalad进程，Impalad的Query Planner对SQL语句进行解析，生成解析树；然后，Planner把这个查询的解析树变成若干PlanFragment，发送到Query Coordinator • 第2步，Coordinator通过从MySQL元数据库中获取元数据，从HDFS的名称节点中获取数据地址，以得到存储这个查询相关数据的所有数据节点。 • 第3步，Coordinator初始化相应impalad上的任务执行，即把查询任务分配给所有存储这个查询相关数据的数据节点。 • 第4步，Query Executor通过流式交换中间输出，并由Query Coordinator汇聚来自各个impalad的结果。 • 第5步，Coordinator把汇总后的结果返回给CLI客户端。 Impala与Hive的比较 Hive与Impala的 不同点总结如下： Hive适合于长时间的批处理查询分析，而Impala适合于实时交互式SQL查询 Hive依赖于MapReduce计算框架，Impala把执行计划表现为一棵完整的执行计划树，直接分发执行计划到各个Impalad执行查询 Hive在执行过程中，如果内存放不下所有数据，则会使用外存，以保证查询能顺序执行完成，而Impala在遇到内存放不下数据时，不会利用外存，所以Impala目前处理查询时会受到一定的限制 Hive与Impala的 相同点总结如下： Hive与Impala使用相同的存储数据池，都支持把数据存储于HDFS和HBase中 Hive与Impala使用相同的元数据 Hive与Impala中对SQL的解释处理比较相似，都是通过词法分析生成执行计划 总结 •Impala的目的不在于替换现有的MapReduce工具 •把Hive与Impala配合使用效果最佳 •可以先使用Hive进行数据转换处理，之后再使用Impala在Hive处理后的结果数据集上进行快速的数据分析 转自林子雨老师的公开课 视频地址：http://www.icourse163.org/learn/XMU-1002335004#/learn/content?type=detail&amp;id=1003836807&amp;cid=1004616536&amp;replay=true]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据基础学习笔记（六）——MapReduce]]></title>
    <url>%2F2018%2F04%2F24%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AD%EF%BC%89%E2%80%94%E2%80%94MapReduce%2F</url>
    <content type="text"><![CDATA[MapReduce体系结构MapReduce主要有以下4个部分组成： 1 ）Client •用户编写的MapReduce程序通过Client提交到JobTracker端 •用户可通过Client提供的一些接口查看作业运行状态 2 ）JobTracker •JobTracker负责资源监控和作业调度 •JobTracker 监控所有TaskTracker与Job的健康状况，一旦发现失败，就将相应的任务转移到其他节点 •JobTracker 会跟踪任务的执行进度、资源使用量等信息，并将这些信息告诉任务调度器（TaskScheduler），而调度器会在资源出现空闲时，选择合适的任务去使用这些资源 3 ）TaskTracker •TaskTracker 会周期性地通过“心跳”将本节点上资源的使用情况和任务的运行进度汇报给JobTracker，同时接收JobTracker 发送过来的命令并执行相应的操作（如启动新任务、杀死任务等） •TaskTracker 使用“slot”等量划分本节点上的资源量（CPU、内存等）。一个Task 获取到一个slot 后才有机会运行，而Hadoop调度器的作用就是将各个TaskTracker上的空闲slot分配给Task使用。slot 分为Map slot 和Reduce slot 两种，分别供MapTask 和Reduce Task使用 4 ）Task Task 分为Map Task 和Reduce Task 两种，均由TaskTracker 启动 MapReduce工作流程概述MapReduce把一个大的数据集拆分成多个小数据块在多台机器上并行处理，也就是说，一个大的MapReduce作业，首先会被拆分成许多个Map任务在多台机器上并行执行，每个Map任务通常运行在数据存储的节点上，这样，计算和数据就可以放在一起运行，不需要额外的数据传输开销。当Map任务结束后，会生成以形式表示的许多中间结果。然后，这些中间结果会被分发到多个Reduce任务在多台机器上并行执行，具有相同Key的会被发送到同一个Reduce任务那里，Reduce任务会对中间结果进行会中计算得到最后的结果，并输出到分布式文件系统中。 •不同的Map任务之间不会进行通信 •不同的Reduce任务之间也不会发生任何信息交换 •用户不能显式地从一台机器向另一台机器发送消息 •所有的数据交换都是通过MapReduce框架自身去实现的 MapReduce各个执行阶段MapReduce的算法执行过程： 1）MapReduce框架使用InputFormat模块做Map前的预处理，比如验证输入的格式是否符合输入定义，然后，将输入文件切分为逻辑上的多个InputSplit，这是MapReduce对文件进行处理和运算的输入单位，只是一个逻辑概念，每个InputSplit并没有对文件进行实际切割，只是记录了要处理的数据的位置和长度。 2）因为InuptSplit是逻辑切分而非物理切分，所以还需要通过RecordReader（RR）根据InputSplit中的信息来处理InputSplit中的具体记录，加载数据并转换为适合Map任务读取的键值对，输入给Map任务。 3）Map任务会根据用户自定义的映射规则，输出一系列的作为中间结果。 4）为了让Reduce可以并行处理Map的结果，需要对Map的输出进行一定的分区（Portition）、排序（Sort）、合并（Combine）、归并（Merge）等操作，得到形式的中间结果，在交给对应的Reduce进行处理，这个过程称为Shuffle。从无序的到有序的，这个过程用Shuffle（洗牌）来称呼是非常形象的。 5）Reduce以一系列中间结果作为输入，执行用户定义的逻辑，输出结果给OutputFormat模块。 6）OutputFormat模块会验证输出目录是否已经存在以及输出结果类型是否符合配置文件中的配置类型，如果都满足，就输出Reduce的结果到分布式文件系统。 HDFS 以固定大小的block 为基本单位存储数据，而对于MapReduce 而言，其处理单位是split。split 是一个逻辑概念，它只包含一些元数据信息，比如数据起始位置、数据长度、数据所在节点等。它的划分方法完全由用户自己决定。 Map 任务的数量 •Hadoop为每个split创建一个Map任务，split 的多少决定了Map任务的数目。大多数情况下，理想的分片大小是一个HDFS块 Reduce 任务的数量 •最优的Reduce任务个数取决于集群中可用的reduce任务槽(slot)的数目 •通常设置比reduce任务槽数目稍微小一些的Reduce任务个数（这样可以预留一些系统资源处理可能发生的错误 Shuffle过程原理1.Shuffle 过程简介 2.Map 端的Shuffle 过程 Map的输出结果首先被写入缓存，当缓存满时，就启动溢写操作，把缓存中的数据写入磁盘文件，并清空缓存。当启动溢写操作时，首先需要把缓存中的数据进行分区，然后对每个分区的数据进行排序（Sort）和合并（Combine），之后再写入磁盘文件。每次溢写操作会生成一个新的磁盘文件，随着Map任务的执行，磁盘中就会生成多个溢写文件。在Map任务全部结束之前，这些溢写文件会被归并（Merge）成一个大的磁盘文件，然后通知相应的Reduce任务来领取属于自己处理的数据。 •每个Map任务分配一个缓存 •MapReduce默认100MB缓存 •设置溢写比例0.8 •分区默认采用哈希函数 •排序是默认的操作 •排序后可以合并（Combine） •合并不能改变最终结果 •在Map任务全部结束之前进行归并 •归并得到一个大的文件，放在本地磁盘 •文件归并时，如果溢写文件数量大于预定值（默认是3）则可以再次启动Combiner，少于3不需要 •JobTracker会一直监测Map任务的执行，并通知Reduce任务来领取数据 合并（Combine）和归并（Merge）的区别：两个键值对&lt;“a”,1&gt;和&lt;“a”,1&gt;，如果合并，会得到&lt;“a”,2&gt;，如果归并，会得到&lt;“a”,&gt; 3.Reduce 端的Shuffle 过程 Reduce任务从Map端的不同Map及其领回属于自己处理的那部分数据，然后对数据进行归并（Merge）后交给Reduce处理。 •Reduce任务通过RPC向JobTracker询问Map任务是否已经完成，若完成，则领取数据 •Reduce领取数据先放入缓存，来自不同Map机器，先归并，再合并，写入磁盘 •多个溢写文件归并成一个或多个大文件，文件中的键值对是排序的 •当数据很少时，不需要溢写到磁盘，直接在缓存中归并，然后输出给Reduce MapReduce应用程序执行过程 参考资料：林子雨老师的MOOC课程：https://www.icourse163.org/learn/XMU-1002335004#/learn/content?type=detail&amp;id=1003836797&amp;cid=1004616527&amp;replay=true]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据基础学习笔记（五）——云数据库架构]]></title>
    <url>%2F2018%2F04%2F17%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89%E2%80%94%E2%80%94%E4%BA%91%E6%95%B0%E6%8D%AE%E5%BA%93%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[不同的云数据库产品采用的系统架构差异很大，这里以阿里巴巴集团核心系统数据库团队开发的UMP(Unified MySQL Platform)系统为例进行介绍。 UMP系统概述•UMP系统是低成本和高性能的MySQL云数据库方案 总的来说，UMP系统架构设计遵循了以下原则： •保持单一的系统对外入口，并且为系统内部维护单一的资源池（CPU、内存、带宽、磁盘等放在一个统一的资源池，供上部组件调用） •消除单点故障，保证服务的高可用性（设置多个管家（Controller）） •保证系统具有良好的可伸缩，能够动态地增加、删减计算与存储节点 •保证分配给用户的资源也是弹性可伸缩的，资源之间相互隔离，确保应用和数据安全（多租户之间隔离，当一个用户使用过多资源时，对其进行限制，以免影响其他用户的使用） UMP系统架构 Mnesia•Mnesia是一个分布式数据库管理系统 •Mnesia支持事务，支持透明的数据分片，利用两阶段锁实现分布式事务，可以线性扩展到至少50个节点 •Mnesia的数据库模式(schema)可在运行时动态重配置，表能被迁移或复制到多个节点来改进容错性 •Mnesia的这些特性，使其在开发云数据库时被用来提供分布式数据库服务 RabbitMQ •RabbitMQ是一个工业级的消息队列产品（功能类似于IBM公司的消息队列产品IBM Websphere MQ），作为消息传输中间件来使用，可以实现可靠的消息传送 •UMP集群中各个节点之间的通信，不需要建立专门的连接，都是通过读写队列消息来实现的 ZookeeperZookeeper是高效和可靠的协同工作系统，提供分布式锁之类的基本服务（比如统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等），用于构建分布式应用，减轻分布式应用程序所承担的协调任务 在UMP系统中，Zookeeper主要发挥三个作用： •作为全局的配置服务器。UMP系统把运行的应用系统的配置信息完全交给Zookeeper来管理，把配置信息保存在Zookeeper的某个目录节点中，然后将所有需要修改的服务器对这个目录节点设置监听，也就是监控配置信息的状态，一旦配置信息发生变化，每台服务器就会收到Zookeeper的通知，然后从Zookeeper获取新的配置信息。 •提供分布式锁（选出一个集群的“总管”）。UMP急群众部署了多个Controller服务器，为了保证系统的正确运行，对于有些操作，在某一时刻，只能由一个服务器去执行，而不能同时执行。礼物，一个MySQL实例发生故障以后，需要进行主备切换，有另一个正常的服务器来代替当前发生故障的服务器，如果这个时候所有的Controller服务器都去跟踪处理并且发起主备切换流程，那么，整个系统就会进入混乱状态。因此，在同一时间，必须从集群的多个Controller服务器中选举出一个“总管”，由这个“总管”负责发起各种系统任务。Zookeeper的分布式锁功能能够帮助选出一个“总管”，让这个“总管”来管理集群。 •监控所有MySQL实例。急群众运行MySQL实例的服务器发生故障时，必须被及时监听到，然后使用其他正常服务器来替代故障服务器。UMP系统借助Zookeeper实现对所有MySQL实例的监控。每个MySQL实例在启动时都会在Zookeeper上创建一个临时类型的目录节点，当某个MySQL实例挂掉时，这个临时类型的目录节点也随之被删除，后台监听进程可以捕获到这种变化，从而知道这个MySQL实例不再可用。 LVS•LVS(Linux Virtual Server)即Linux虚拟服务器，是一个虚拟的服务器集群系统 •UMP系统借助于LVS来实现集群内部的负载均衡 •LVS集群采用IP负载均衡技术和基于内容请求分发技术 •调度器是LVS集群系统的唯一入口点，调度器具有很好的吞吐率，将请求均衡地转移到不同的服务器上执行，且调度器自动屏蔽掉服务器的故障，从而将一组服务器构成一个高性能的、高可用的虚拟服务器 •整个服务器集群的结构对客户是透明的，而且无需修改客户端和服务器端的程序 Controller服务器•Controller服务器向UMP集群提供各种管理服务，实现集群成员管理、元数据存储、MySQL实例管理、故障恢复、备份、迁移、扩容等功能 •Controller服务器上运行了一组Mnesia分布式数据库服务，其中存储了各种系统元数据，主要包括集群成员、用户的配置和状态信息，以及用户名到后端MySQL实例地址的映射关系（或称为“路由表”）等 •当其它服务器组件需要获取用户数据时，可以向Controller服务器发送请求获取数据 •为了避免单点故障，保证系统的高可用性，UMP系统中部署了多台Controller服务器，然后，由Zookeeper的分布式锁功能来帮助选出一个“总管”，负责各种系统任务的调度和监控 Web 控制台Web控制台向用户提供系统管理界面 Proxy 服务器Proxy服务器向用户提供访问MySQL数据库的服务，它完全实现了MySQL协议，用户可以使用已有的MySQL客户端连接到Proxy服务器，Proxy服务器通过用户名获取到用户的认证信息、资源配额的限制(例如QPS、IOPS（I/O Per Second）、最大连接数等)，以及后台MySQL实例的地址，然后，用户的SQL查询请求会被转发到相应的MySQL实例上。 除了数据路由的基本功能外，Proxy服务器中还实现了很多重要的功能，主要包括屏蔽MySQL实例故障、读写分离、分库分表、资源隔离、记录用户访问日志等 Agent 服务器Agent服务器部署在运行MySQL进程的机器上，用来管理每台物理机上的MySQL实例，执行主从切换、创建、删除、备份、迁移等操作，同时，还负责收集和分析MySQL进程的统计信息、慢查询日志（Slow Query Log）和bin-log 日志分析服务器日志分析服务器存储和分析Proxy服务器传入的用户访问日志，并支持实时查询一段时间内的慢日志和统计报表 信息统计服务器信息统计服务器定期将采集到的用户的连接数、QPS数值以及MySQL实例的进程状态用RRDtool进行统计，可以在 Web界面上可视化展示统计结果，也可以把统计结果作为今后实现弹性的资源分配和自动化的MySQL实例迁移的依据 愚公系统愚公系统是一个全量复制结合bin-log分析进行增量复制的工具，可以实现在不停机的情况下动态扩容、缩容和迁移]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据基础学习笔记（四）——HBase相关知识（三）]]></title>
    <url>%2F2018%2F04%2F03%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89%E2%80%94%E2%80%94HBase%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%EF%BC%88%E4%B8%89%EF%BC%89%2F</url>
    <content type="text"><![CDATA[HBase应用方案HBase实际应用中的性能优化方法行键（Row Key ） 行键是按照 字典序存储，因此，设计行键时，要充分利用这个排序特点，将经常一起读取的数据存储到一块，将最近可能会被访问的数据放在一块。 举个例子：如果最近写入HBase表中的数据是最可能被访问的，可以考虑将时间戳作为行键的一部分，由于是字典序排序，所以可以使用Long.MAX_VALUE -timestamp作为行键，这样能保证新写入的数据在读取时可以被快速命中。 InMemory创建表的时候，可以通过HColumnDescriptor.setInMemory(true)将表放到Region服务器的缓存中，保证在读取的时候被cache命中。 Max Version创建表的时候，可以通过HColumnDescriptor.setMaxVersions(int maxVersions)设置表中数据的最大版本，如果只需要保存最新版本的数据，那么可以设置setMaxVersions(1)。 Time To Live创建表的时候，可以通过HColumnDescriptor.setTimeToLive(int timeToLive)设置表中数据的存储生命期，过期数据将自动被删除，例如如果只需要存储最近两天的数据，那么可以设置setTimeToLive(2 24 60 * 60)。 HBase性能监视Master-status(自带) •HBase Master默认基于Web的UI服务端口为60010，HBase region服务器默认基于Web的UI服务端口为60030.如果master运行在名为master.foo.com的主机中，mater的主页地址就是http://master.foo.com:60010，用户可以通过Web浏览器输入这个地址查看该页面 •可以查看HBase集群的当前状态 Ganglia Ganglia是UC Berkeley发起的一个开源集群监视项目，用于监控系统性能 OpenTSDB OpenTSDB可以从大规模的集群（包括集群中的网络设备、操作系统、应用程序）中获取相应的metrics并进行存储、索引以及服务，从而使得这些数据更容易让人理解，如web化，图形化等 Ambari Ambari 的作用就是创建、管理、监视 Hadoop 的集群 在HBase之上构建SQL引擎NoSQL区别于关系型数据库的一点就是NoSQL不使用SQL作为查询语言，至于为何在NoSQL数据存储HBase上提供SQL接口，有如下原因： 易使用。使用诸如SQL这样易于理解的语言，使人们能够更加轻松地使用HBase。 减少编码。使用诸如SQL这样更高层次的语言来编写，减少了编写的代码量。方案： 1.Hive整合HBase2.Phoenix 1.Hive 整合HBase Hive与HBase的整合功能从Hive0.6.0版本已经开始出现，利用两者对外的API接口互相通信，通信主要依靠hive_hbase-handler.jar工具包(Hive Storage Handlers)。由于HBase有一次比较大的版本变动，所以并不是每个版本的Hive都能和现有的HBase版本进行整合，所以在使用过程中特别注意的就是两者版本的一致性。 2.Phoenix Phoenix由Salesforce.com开源，是构建在Apache HBase之上的一个SQL中间层，可以让开发者在HBase上执行SQL查询。 构建HBase二级索引二级索引，又叫辅助索引 HBase只有一个针对行健的索引访问HBase表中的行，只有三种方式： •通过单个行健访问 •通过一个行健的区间来访问 •全表扫描 使用其他产品为HBase行健提供索引功能： •Hindex二级索引 •HBase+Redis •HBase+solr 原理：采用HBase0.92版本之后引入的Coprocessor特性 Coprocessor构建二级索引 •Coprocessor提供了两个实现：endpoint和observer，endpoint相当于关系型数据库的存储过程，而observer则相当于触发器 •observer允许我们在记录put前后做一些处理，因此，而我们可以在插入数据时同步写入索引表 •Coprocessor构建二级索引•缺点：每插入一条数据需要向索引表插入数据，即耗时是双倍的，对HBase的集群的压力也是双倍的 优点：非侵入性：引擎构建在HBase之上，既没有对HBase进行任何改动，也不需要上层应用做任何妥协 Hindex二级索引 Hindex 是华为公司开发的纯 Java 编写的HBase二级索引，兼容 Apache HBase 0.94.8。当前的特性如下： •多个表索引 •多个列索引 •基于部分列值的索引 HBase+Redis •Redis+HBase方案 •Coprocessor构建二级索引 •Redis做客户端缓存 •将索引实时更新到Redis等KV系统中，定时从KV更新索引到HBase的索引表中 Solr+HBase Solr是一个高性能，采用Java5开发，基于Lucene的全文搜索服务器。同时对其进行了扩展，提供了比Lucene更为丰富的查询语言，同时实现了可配置、可扩展并对查询性能进行了优化，并且提供了一个完善的功能管理界面，是一款非常优秀的全文搜索引擎。]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据基础学习笔记（三）——HBase相关知识（二）]]></title>
    <url>%2F2018%2F04%2F02%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89%E2%80%94%E2%80%94HBase%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[HBase功能组件• HBase的实现包括三个主要的功能组件：– （1）库函数：链接到每个客户端– （2）一个Master主服务器（充当管家的作用）– （3）许多个Region服务器 • 主服务器Master负责管理和维护HBase表的分区信息，维护Region服务器列表，分配Region，负载均衡 • 一个大的表会被分成很多个Region，Region服务器负责存储和维护分配给自己的Region，处理来自客户端的读写请求 • 客户端并不是直接从Master主服务器上读取数据，而是在获得Region的存储位置信息后，直接从Region服务器上读取数据 • 客户端并不依赖Master，而是通过Zookeeper来获得Region位置信息，大多数客户端甚至从来不和Master通信，这种设计方式使得Master负载很小 表和Region •开始只有一个Region，后来不断分裂 •Region拆分操作非常快，接近瞬间，因为拆分之后的Region读取的仍然是原存储文件，直到“合并”过程把存储文件异步地写到独立的文件之后，才会读取新文件 这种拆分只是逻辑上的拆分，只是数据的指向发生了变化，它的实际存储还是在原来的旧的Region中的数据。当读新的Region时，后台会有一个合并操作，会把拆分的数据进行重新操作，最终会写到新的文件中去。 一个Region只能存到一个Region服务器上。 Region的定位那么有一个问题，当一个Region被拆成很多个Region时，这些Region会把它打散，分布到不同的地方存储，那么怎么知道它被存到哪里去了呢？ •元数据表，又名.META.表，存储了Region和Region服务器的映射关系 •当HBase表很大时， .META.表也会被分裂成多个Region •根数据表，又名-ROOT-表，记录所有元数据的具体位置 •-ROOT-表只有唯一一个Region，名字是在程序中被写死的 •Zookeeper文件记录了-ROOT-表的位置 •一个-ROOT-表最多只能有一个Region，也就是最多只能有128MB，按照每行（一个映射条目）占用1KB内存计算，128MB空间可以容纳128MB/1KB=2^17 行，也就是说，一个-ROOT-表可以寻址2^17 个.META.表的Region。 •同理，每个.META.表的 Region可以寻址的用户数据表的Region个数是128MB/1KB=2^17 。 •最终，三层结构可以保存的Region数目是(128MB/1KB) × (128MB/1KB) = 2^34 个Region 所以三层架构能够满足企业的需求。 客户端访问数据时的“三级寻址”•为了加速寻址，客户端会缓存位置信息，同时，需要解决缓存失效问题 •寻址过程客户端只需要询问Zookeeper服务器，不需要连接Master服务器 这里的缓存机制采用的是惰性缓存，如果在使用缓存获取数据时，获取不到数据，那么就失效了，这时候再次进行三级寻址过程，以解决缓存失效问题。 HBase运行机制 • 1. 客户端– 客户端包含访问HBase的接口，同时在缓存中维护着已经访问过的Region位置信息，用来加快后续数据访问过程 • 2. Zookeeper服务器– Zookeeper可以帮助选举出一个Master作为集群的总管，并保证在任何时刻总有唯一一个Master在运行，这就避免了Master的“单点失效”问题Zookeeper是一个很好的集群管理工具，被大量用于分布式计算，提供配置维护、域名服务、分布式同步、组服务等。提供管家的功能，维护整个HBase集群。虽然有很多备用的Master，但是它保证只有一个Master是运行的。 • 3. Master• 主服务器Master主要负责表和Region的管理工作：– 管理用户对表的增加、删除、修改、查询等操作– 实现不同Region服务器之间的负载均衡– 在Region分裂或合并后，负责重新调整Region的分布– 对发生故障失效的Region服务器上的Region进行迁移 • 4. Region服务器– Region服务器是HBase中最核心的模块，负责维护分配给自己的Region，并响应用户的读写请求 Region服务器工作原理1.用户读写数据过程 •用户写入数据时，被分配到相应Region服务器去执行 •用户数据首先被写入到MemStore和Hlog中 •只有当操作写入Hlog之后，commit()调用才会将其返回给客户端 •当用户读取数据时，Region服务器会首先访问MemStore缓存，如果找不到，再去磁盘上面的StoreFile中寻找 2.缓存的刷新 •系统会周期性地把MemStore缓存里的内容刷写到磁盘的StoreFile文件中，清空缓存，并在Hlog里面写入一个标记 •每次刷写都生成一个新的StoreFile文件，因此，每个Store包含多个StoreFile文件 •每个Region服务器都有一个自己的HLog 文件，每次启动都检查该文件，确认最近一次执行缓存刷新操作之后是否发生新的写入操作；如果发现更新，则先写入MemStore，再刷写到StoreFile，最后删除旧的Hlog文件，开始为用户提供服务 3.StoreFile 的合并 •每次刷写都生成一个新的StoreFile，数量太多，影响查找速度 •调用Store.compact()把多个合并成一个 •合并操作比较耗费资源，只有数量达到一个阈值才启动合并 Store工作原理•Store是Region服务器的核心 •多个StoreFile合并成一个 •单个StoreFile过大时，又触发分裂操作，1个父Region被分裂成两个子Region HLog工作原理• 分布式环境必须要考虑系统出错。HBase采用HLog保证系统恢复 • HBase系统为每个Region服务器配置了一个HLog文件，它是一种预写式日志（Write Ahead Log） • 用户更新数据必须首先写入日志后，才能写入MemStore缓存，并且，直到MemStore缓存内容对应的日志已经写入磁盘，该缓存内容才能被刷写到磁盘 • Zookeeper会实时监测每个Region服务器的状态，当某个Region服务器发生故障时，Zookeeper会通知Master • Master首先会处理该故障Region服务器上面遗留的HLog文件，这个遗留的HLog文件中包含了来自多个Region对象的日志记录 • 系统会根据每条日志记录所属的Region对象对HLog数据进行拆分，分别放到相应Region对象的目录下，然后，再将失效的Region重新分配到可用的Region服务器中，并把与该Region对象相关的HLog日志记录也发送给相应的Region服务器 • Region服务器领取到分配给自己的Region对象以及与之相关的HLog日志记录以后，会重新做一遍日志记录中的各种操作，把日志记录中的数据写入到MemStore缓存中，然后，刷新到磁盘的StoreFile文件中，完成数据恢复 • 共用日志优点：提高对表的写操作性能；缺点：恢复时需要分拆日志 本笔记参考自厦门大学林子雨老师的公开课：https://www.icourse163.org/course/XMU-1002335004]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据基础学习笔记（二）——HBase相关知识（一）]]></title>
    <url>%2F2018%2F04%2F02%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94Hbase%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[Hbase简介HBase是一个高可靠、高性能、面向列、可伸缩的分布式数据库，是谷歌BigTable的开源实现，主要用来存储非结构化和半结构化的松散数据。HBase的目标是处理非常庞大的表，可以通过水平扩展的方式，利用廉价计算机集群处理由超过10亿行数据和数百万列元素组成的数据表。 底层的分布式文件系统用来存储完全非结构化的数据。 Hbase是架构在底层的分布式文件系统HDFS基础之上的同时MR可以对Hbase的数据进行处理。同时Hive和Pig等都可以访问Hbase中的数据。 从上图可以看出，BigTable和HBase的底层技术的对比。 为什么要设计HBase这个数据产品呢？•Hadoop可以很好地解决大规模数据的离线批量处理问题，但是，受限于HadoopMapReduce编程框架的高延迟数据处理机制，使得Hadoop无法满足大规模数据实时处理应用的需求 •HDFS面向批量访问模式，不是随机访问模式 •传统的通用关系型数据库无法应对在数据规模剧增时导致的系统扩展性和性能问题（分库分表也不能很好解决） •传统关系数据库在数据结构变化时一般需要停机维护；空列浪费存储空间 •因此，业界出现了一类面向半结构化数据存储和处理的高可扩展、低写入/查询延迟的系统，例如，键值数据库、文档数据库和列族数据库（如BigTable和HBase等） •HBase已经成功应用于互联网服务领域和传统行业的众多在线式数据分析处理系统中 HBase与传统关系数据库的对比分析• HBase与传统的关系数据库的区别主要体现在以下几个方面： • （1）数据类型：关系数据库采用关系模型，具有丰富的数据类型（整型，字符型等等）和存储方式，HBase则采用了更加简单的数据模型，它把数据存储为未经解释的字符串（也就是Bytes数组） • （2）数据操作：关系数据库中包含了丰富的操作（增删改查），其中会涉及复杂的多表连接。HBase操作则不存在复杂的表与表之间的关系，只有简单的插入、查询、删除、清空等，因为HBase在设计上就避免了复杂的表和表之间的关系 • （3）存储模式：关系数据库是基于行模式存储的。HBase是基于列存储的，每个列族都由几个文件保存，不同列族的文件是分离的 • （4）数据索引：关系数据库通常可以针对不同列构建复杂的多个索引，以提高数据访问性能。HBase只有一个索引——行键，通过巧妙的设计，HBase中的所有访问方法，或者通过行键访问，或者通过行键扫描，从而使得整个系统不会慢下来 • （5）数据维护：在关系数据库中，更新操作会用最新的当前值去替换记录中原来的旧值，旧值被覆盖后就不会存在。而在HBase中执行更新操作时，并不会删除数据旧的版本，而是生成一个新的版本，旧有的版本仍然保留，只有在过了设置的参数期限之后，在系统后台清理的时候才会清理掉 • （6）可伸缩性：关系数据库很难实现横向扩展，纵向扩展（如添加内存，改进CPU等等）的空间也比较有限。相反，HBase和BigTable这些分布式数据库就是为了实现灵活的水平扩展而开发的，能够轻易地通过在集群中增加或者减少硬件数量来实现性能的伸缩 HBase的访问接口以后在使用Hbase的时候，可以通过哪些方式访问HBase数据库？见下图： HBase数据模型• HBase是一个稀疏、多维度、排序的映射表，这张表的索引是行键、列族、列限定符和时间戳 • 每个值是一个未经解释的字符串，没有数据类型 • 用户在表中存储数据，每一行都有一个可排序的行键和任意多的列 • 表在水平方向由一个或者多个列族组成，一个列族中可以包含任意多个列，同一个列族里面的数据存储在一起 • 列族支持动态扩展，可以很轻松地添加一个列族或列，无需预先定义列的数量以及类型，所有列均以字符串形式存储，用户需要自行进行数据类型转换 • HBase中执行更新操作时，并不会删除数据旧的版本，而是生成一个新的版本，旧有的版本仍然保留（这是和HDFS只允许追加不允许修改的特性相关的） • 表：HBase采用表来组织数据，表由行和列组成，列划分为若干个列族 • 行：每个HBase表都由若干行组成，每个行由行键（row key）来标识。 • 列族：一个HBase表被分组成许多“列族”（Column Family）的集合，它是基本的访问控制单元（支持动态拓展） • 列限定符：列族里的数据通过列限定符（或列）来定位 • 单元格：在HBase表中，通过行、列族和列限定符确定一个“单元格”（cell），单元格中存储的数据没有数据类型，总被视为字节数组byte[] • 时间戳：每个单元格都保存着同一份数据的多个版本，这些版本采用时间戳进行索引 HBase的数据坐标HBase中需要根据行键、列族、列限定符和时间戳来确定一个单元格，因此，可以视为一个“四维坐标”，即[行键, 列族, 列限定符, 时间戳] 概念视图HBase在概念上和实际的底层存储是有区分的，在概念上HBase只是一个表，如下面只给了一个行键： 如这一个行键给了两个列族，第一个列族contents中冒号前面的contents是列族的名称，冒号后面的html是列的名称，引号中的内容就是这一列的数据。一个时间戳并不一定会在所有列族插入数据，从图中就可以看出。所以这就导致了HBase的稀疏表的特性。这只是在概念上的视图。 物理视图实际上在实际存储中，并不是按上述的方式去存的。在底层存储时，是按列族为单位进行存储的。 上图是在实际存储时，存储在底层的实际的表。并没有像概念视图中存储了很多的空数据。所以概念视图和物理视图上是有区分的。 面向列的存储 传统的数据库，以行为单位进行存储，一行包括ID,姓名，年龄，性别，IP，操作等。但是按列存储，里面的姓名、年龄等进行单独存储。 它们各自的优缺点： 另外，使用列式存储，数据可以达到很高的数据压缩率。而行式存储，很难压缩。 本笔记参考自厦门大学林子雨老师的公开课：https://www.icourse163.org/course/XMU-1002335004]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据基础学习笔记（一）——Hadoop相关知识]]></title>
    <url>%2F2018%2F04%2F01%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[HadoopHadoop的应用现状和构成简介下图为Hadoop在企业中的 应用架构 访问层不用多说，满足企业的数据分析、数据挖掘和数据实时查询功能。为了满足访问层的需求，大数据层的各个技术对其进行支撑。（1）离线分析：大量数据拿过来之后进行批量处理。其中MR是MapReduce的简称，Hive数据仓库和Pig也可以进行离线数据分析。（2）实时查询：其中Hbase是一个可以支持几十亿行数据的非常好的分布式数据库。（3）BI分析：Mahout是Hadoop平台上的一款数据挖掘应用。可以把各种数据挖掘，机器学习和商务智能的算法用MapReduce实现。否则开发人员要自己用MapReduce写决策树算法。 下图为一些大数据计算模式及其代表产品 下图为Hadoop项目结构 YARNz专门负责调度内存，CPU，带宽等计算资源。而上面的事完成具体的计算工作的。 Tez会把很多的MapReduce作业进行分析优化，构建成一个有向无环图，保证获得最好的处理效率。 Spark与MapReduce类似，也是进行相应的计算。但是Spark是基于内存的，而MapReduce是基于磁盘的计算。MR在计算时，先把数据写到磁盘中，然后c处理结束后再写到分布式文件系统中。所以Spark的性能要高。 Pig实现流数据处理，较MR属于轻量级。它也支持类似于SQL的语句。是一种轻量级的脚本语言。 Oozie是一个工作流管理系统，可以把一个工作分成不同的工作环节。 Zookeeper提供分布式协调一致性服务。 Hbase是一个非关系型数据库，可以支持随机读写。 Flume是专门负责日志收集的，分析一些实时生成的数据流。 Sqoopy用于在Hadoop与传统数据库之间进行数据传递（导入导出等）。可以把之前存到关系型数据库（如Oracle）中的数据导入到HDFS、Hive或者Hbase中，反之亦可。 Ambari是一个安装部署工具，可以在一个集群上面智能化的管理一整套Hadoop上的各个套件。 Hadoop各组件的功能如下： Hadoop集群的节点类型Hadoop框架中最核心的设计是为海量数据提供存储的HDFS和对数据进行计算的MapReduce MapReduce的作业主要包括：（1）从磁盘或从网络读取数据，即IO密集工作；（2）计算数据，即CPU密集工作 •Hadoop集群的整体性能取决于CPU、内存、网络以及存储之间的性能平衡。因此运营团队在选择机器配置时要针对不同的工作节点选择合适硬件类型•一个基本的Hadoop集群中的节点主要有: •NameNode：负责协调集群中的数据存储 •DataNode：存储被拆分的数据块 •JobTracker：协调数据计算任务 •TaskTracker：负责执行由JobTracker指派的任务 •SecondaryNameNode：帮助NameNode收集文件系统运行的状态信息 HDFS全称：Hadoop Distributed File System.解决海量数据的分布式存储问题。 分布式文件系统的结构分布式文件系统在物理结构上是由计算机集群中的多个节点构成的，这些节点分为两类，一类叫“主节点”(Master Node)或者也被称为“名称结点”(NameNode)，另一类叫“从节点”（Slave Node）或者也被称为“数据节点”(DataNode) HDFS的三个节点：Namenode，Datanode，Secondary Namenode Namenode：HDFS的守护进程，用来管理文件系统的命名空间，负责记录文件是如何分割成数据块，以及这些数据块分别被存储到那些数据节点上，它的主要功能是对内存及IO进行集中管理。 Datanode：文件系统的工作节点，根据需要存储和检索数据块，并且定期向namenode发送他们所存储的块的列表。 Secondary Namenode：辅助后台程序，与NameNode进行通信，以便定期保存HDFS元数据的快照。 HDFS采用了主从（Master/Slave）结构模型，一个HDFS集群包括一个名称节点（NameNode）和若干个数据节点（DataNode）（如图所示）。名称节点作为中心服务器，负责管理文件系统的命名空间及客户端对文件的访问。集群中的数据节点一般是一个节点运行一个数据节点进程，负责处理文件系统客户端的读/写请求，在名称节点的统一调度下进行数据块的创建、删除和复制等操作。每个数据节点的数据实际上是保存在本地Linux文件系统中的。 HDFS的缺点： 1.不适合低延迟的数据访问2.无法高效存储大量小文件3.不支持多用户写入及任意修改文件 名称节点和数据节点 在HDFS中，名称节点（NameNode）负责管理分布式文件系统的命名空间（Namespace），保存了两个核心的数据结构，即FsImage和EditLog。 •FsImage用于维护文件系统树以及文件树中所有的文件和文件夹的元数据 •操作日志文件EditLog中记录了所有针对文件的创建、删除、重命名等操作 •名称节点记录了每个文件中各个块所在的数据节点的位置信息。 客户端在访问数据时，先通过名称节点，获取元数据信息，从而知道被访问的数据存到哪些数据节点，获得数据块具体存储位置的信息之后，客户端就会到各个机器上去获取它所需要的数据。写入操作类似，客户端先访问名称节点，一个大文件（如1TB,2TB）要怎么写，然后名称节点会告诉它，把文件分成多少块，每个块放到哪个数据节点上。 FsImage 文件•FsImage文件包含文件系统中所有目录和文件inode的序列化形式。每个inode是一个文件或目录的元数据的内部表示，并包含此类信息：文件的复制等级、修改和访问时间、访问权限、块大小以及组成文件的块。对于目录，则存储修改时间、权限和配额元数据 •FsImage文件没有记录块存储在哪个数据节点。而是由名称节点把这些映射保留在内存中，当数据节点加入HDFS集群时，数据节点会把自己所包含的块列表告知给名称节点，此后会定期执行这种告知操作，以确保名称节点的块映射是最新的。 在名称节点启动的时候，它会将FsImage文件中的内容加载到内存中，之后再执行EditLog文件中的各项操作，使得内存中的元数据和实际的同步，存在内存中的元数据支持客户端的读操作。 •一旦在内存中成功建立文件系统元数据的映射，则创建一个新的FsImage文件和一个空的EditLog文件 •名称节点起来之后，HDFS中的更新操作会重新写到EditLog文件中，因为FsImage文件一般都很大（GB级别的很常见），如果所有的更新操作都往FsImage文件中添加，这样会导致系统运行的十分缓慢，但是，如果往EditLog文件里面写就不会这样，因为EditLog 要小很多。每次执行写操作之后，且在向客户端发送成功代码之前，edits文件都需要同步更新 第二名称节点第二名称节点是HDFS架构中的一个组成部分，它是用来保存名称节点中对HDFS 元数据信息的备份，并减少名称节点重启的时间。SecondaryNameNode一般是单独运行在一台机器上。 SecondaryNameNode的工作情况： （1）SecondaryNameNode会定期和NameNode通信，请求其停止使用EditLog文件，暂时将新的写操作写到一个新的文件edit.new上来，这个操作是瞬间完成，上层写日志的函数完全感觉不到差别； （2）SecondaryNameNode通过HTTP GET方式从NameNode上获取到FsImage和EditLog文件，并下载到本地的相应目录下； （3）SecondaryNameNode将下载下来的FsImage载入到内存，然后一条一条地执行EditLog文件中的各项更新操作，使得内存中的FsImage保持最新；这个过程就是EditLog和FsImage文件合并； （4）SecondaryNameNode执行完（3）操作之后，会通过post方式将新的FsImage文件发送到NameNode节点上； （5）NameNode将从SecondaryNameNode接收到的新的FsImage替换旧的FsImage文件，同时将edit.new替换EditLog文件，通过这个过程EditLog就变小。 数据节点（DataNode）数据节点是分布式文件系统HDFS的工作节点，负责数据的存储和读取，会根据客户端或者是名称节点的调度来进行数据的存储和检索，并且向名称节点定期发送自己所存储的块的列表 •每个数据节点中的数据会被保存在各自节点的本地Linux文件系统中 HDFS存储原理冗余数据保存作为一个分布式文件系统，为了保证系统的容错性和可用性，HDFS采用了多副本方式对数据进行冗余存储，通常一个数据块的多个副本会被分布到不同的数据节点上，数据块1被分别存放到数据节点A和C上，数据块2被存放在数据节点A和B上。这种多副本方式具有以下几个优点：（1） 加快数据传输速度 （2） 容易检查数据错误 （3） 保证数据可靠性 数据存取策略数据存放•第一个副本：放置在上传文件的数据节点；如果是集群外提交，则随机挑选一台磁盘不太满、CPU不太忙的节点 •第二个副本：放置在与第一个副本不同的机架的节点上 •第三个副本：与第一个副本相同机架的其他节点上 •更多副本：随机节点 数据读取•HDFS提供了一个API可以确定一个数据节点所属的机架ID，客户端也可以调用API获取自己所属的机架ID •当客户端读取数据时，从名称节点获得数据块不同副本的存放位置列表，列表中包含了副本所在的数据节点，可以调用API来确定客户端和这些数据节点所属的机架ID，当发现某个数据块副本对应的机架ID和客户端对应的机架ID相同时，就优先选择该副本读取数据，如果没有发现，就随机选择一个副本读取数据 数据错误与恢复HDFS具有较高的容错性，可以兼容廉价的硬件，它把硬件出错看作一种常态，而不是异常，并设计了相应的机制检测数据错误和进行自动恢复，主要包括以下几种情形：名称节点出错、数据节点出错和数据出错。 名称节点出错名称节点保存了所有的元数据信息，其中，最核心的两大数据结构是FsImage和Editlog，如果这两个文件发生损坏，那么整个HDFS实例将失效。因此，HDFS设置了备份机制，把这些核心文件同步复制到备份服务器SecondaryNameNode上。当名称节点出错时，就可以根据备份服务器SecondaryNameNode中的FsImage和Editlog数据进行恢复。 数据节点出错•每个数据节点会定期向名称节点发送“心跳”信息，向名称节点报告自己的状态 •当数据节点发生故障，或者网络发生断网时，名称节点就无法收到来自一些数据节点的心跳信息，这时，这些数据节点就会被标记为“宕机”，节点上面的所有数据都会被标记为“不可读”，名称节点不会再给它们发送任何I/O请求 •这时，有可能出现一种情形，即由于一些数据节点的不可用，会导致一些数据块的副本数量小于冗余因子•名称节点会定期检查这种情况，一旦发现某个数据块的副本数量小于冗余因子，就会启动数据冗余复制，为它生成新的副本 •HDFS和其它分布式文件系统的最大区别就是可以调整冗余数据的位置 数据出错•网络传输和磁盘错误等因素，都会造成数据错误 •客户端在读取到数据后，会采用md5和sha1对数据块进行校验，以确定读取到正确的数据 •在文件被创建时，客户端就会对每一个文件块进行信息摘录，并把这些信息写入到同一个路径的隐藏文件里面 •当客户端读取文件的时候，会先读取该信息文件，然后，利用该信息文件对每个读取的数据块进行校验，如果校验出错，客户端就会请求到另外一个数据节点读取该文件块，并且向名称节点报告这个文件块有错误，名称节点会定期检查并且重新复制这个块 本笔记的来源源自林子雨老师的MOOC课程和课件，地址：https://www.icourse163.org/course/XMU-1002335004]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
</search>
