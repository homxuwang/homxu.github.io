<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[[总结]What is deadlock?How to troubleshoot deadlocks?]]></title>
    <url>%2F2019%2F06%2F12%2FWhat-is-deadlock-How-to-troubleshoot-deadlocks%2F</url>
    <content type="text"><![CDATA[什么是死锁？当两个(或多个)任务正在等待必须由另一线程释放的某个共享资源，而线程该线程又正在等待必须由前述任务之一释放的另一共享资源时，并发应用程序就出现了死锁。 当系统中同时出现如下四种条件时，就会导致这种情形。我们称其为Coffman条件(产生死锁的4个必要条件) 如果一个系统中如下4种情形同时存在，则产生死锁情形的机会就会上升 互斥条件：进程要求对所分配的资源进行排它性控制，即在一段时间内某资源仅为一进程所占用(死锁中涉及的资源必须是不可共享的。一次只有一个任务可以使用该资源) 占有并等待条件：当进程因请求资源而阻塞时，对已获得的资源保持不放(一个任务在占有某一互斥的资源时又请求另一互斥的资源。当它在等待时，不会释放任何资源) 不可剥夺条件：进程已获得的资源在未使用完之前，不能剥夺，只能在使用完时由自己释放(资源只能被那些持有它们的任务释放) 循环等待条件：在发生死锁时，必然存在一个进程–资源的环形链。例如存在进程集合{P1,P2,P3….Pn}，P1 申请P2获取的资源，P2申请P3资源….而Pn申请P1获取的资源，这样形成了一个闭环，即循环等待。 这4个条件即Coffman条件，由Edward G.Coffman, Jr先生于1971年首次提出。 如何避免死锁有一些机制可以用来避免死锁： 忽略它们：这是最常用的机制。你可以假设自己的系统绝对不会出现死锁，而如果发生死锁，结果就是你可以停止应用程序并且重新执行它。 检测与修复：系统中有一项专门分析系统状态的任务，可以检测是否发生了死锁。如果它检测到了死锁，可以采取一些措施来修复该问题，例如，结束某个任务或者强制释放某一资源。 预防：如果你想防止系统出现死锁，就必须预防Coffman条件中的一条或者多条出现。 规避：如果你可以在某一任务执行之前得到该任务所使用资源的相关信息，那么死锁是可以规避的。当一个任务要开始执行时，你可以对系统中空闲的资源和任务所需的资源进行分析，这样就可以判断任务是否能够开始执行。 忽略它们有时候也称为鸵鸟策略：把头埋在沙子里，假装根本没发生问题。 因为解决死锁问题的代价很高，因此鸵鸟策略这种不采取任务措施的方案会获得更高的性能。 当发生死锁时不会对用户造成多大影响，或发生死锁的概率很低，可以采用鸵鸟策略。 大多数操作系统，包括 Unix，Linux 和 Windows，处理死锁问题的办法仅仅是忽略它。 死锁检测与修复 每种类型一个资源的死锁检测 上图为资源分配图，其中方框表示资源，圆圈表示进程。资源指向进程表示该资源已经分配给该进程，进程指向资源表示进程请求获取该资源。图a可以抽取出环，如图b，它满足了环路等待条件，因此会发生死锁。每种类型一个资源的死锁检测算法是通过检测有向图是否存在环来实现，从一个节点出发进行深度优先搜索，对访问过的节点进行标记，如果访问了已经标记的节点，就表示有向图存在环，也就是检测到死锁的发生。 每种类型多个资源的死锁检测上图中，有三个进程四个资源，每个数据代表的含义如下： E 向量：资源总量 A 向量：资源剩余量 C 矩阵：每个进程所拥有的资源数量，每一行都代表一个进程拥有资源的数量 R 矩阵：每个进程请求的资源数量进程 P1 和 P2 所请求的资源都得不到满足，只有进程 P3 可以，让 P3 执行，之后释放 P3 拥有的资源，此时 A = (2 2 2 0)。P2 可以执行，执行后释放 P2 拥有的资源，A = (4 2 2 1) 。P1 也可以执行。所有进程都可以顺利执行，没有死锁。算法总结如下：每个进程最开始时都不被标记，执行过程有可能被标记。当算法结束时，任何没有被标记的进程都是死锁进程。 寻找一个没有标记的进程 Pi，它所请求的资源小于等于 A。 如果找到了这样一个进程，那么将 C 矩阵的第 i 行向量加到 A 中，标记该进程，并转回 1。 如果没有这样一个进程，算法终止。 死锁恢复 利用抢占恢复 利用回滚恢复 通过杀死进程恢复 死锁预防 破坏互斥条件例如假脱机打印机技术允许若干个进程同时输出，唯一真正请求物理打印机的进程是打印机守护进程。 破坏占有并等待条件一种实现方式是规定所有进程在开始执行前请求所需要的全部资源。 破坏不可抢占条件 破坏环路等待给资源统一编号，进程只能按编号顺序来请求资源。 比如: 超时放弃当使用synchronized关键词提供的内置锁时，只要线程没有获得锁，那么就会永远等待下去，然而Lock接口提供了boolean tryLock(long time, TimeUnit unit) throws InterruptedException方法，该方法可以按照固定时长等待锁，因此线程可以在获取锁超时以后，主动释放之前已经获得的所有的锁。通过这种方式，也可以很有效地避免死锁。 死锁规避(避免)在程序运行时避免发生死锁。 安全状态图 a 的第二列 Has 表示已拥有的资源数，第三列 Max 表示总共需要的资源数，Free 表示还有可以使用的资源数。从图 a 开始出发，先让 B 拥有所需的所有资源（图 b），运行结束后释放 B，此时 Free 变为 5（图 c）；接着以同样的方式运行 C 和 A，使得所有进程都能成功运行，因此可以称图 a 所示的状态时安全的。 定义：如果没有死锁发生，并且即使所有进程突然请求对资源的最大需求，也仍然存在某种调度次序能够使得每一个进程运行完毕，则称该状态是安全的。 安全状态的检测与死锁的检测类似，因为安全状态必须要求不能发生死锁。下面的银行家算法与死锁检测算法非常类似，可以结合着做参考对比。 单个资源的银行家算法一个小城镇的银行家，他向一群客户分别承诺了一定的贷款额度，算法要做的是判断对请求的满足是否会进入不安全状态，如果是，就拒绝请求；否则予以分配。上图 c 为不安全状态，因此算法会拒绝之前的请求，从而避免进入图 c 中的状态。 多个资源的银行家算法上图中有五个进程，四个资源。左边的图表示已经分配的资源，右边的图表示还需要分配的资源。最右边的 E、P 以及 A 分别表示：总资源、已分配资源以及可用资源，注意这三个为向量，而不是具体数值，例如 A=(1020)，表示 4 个资源分别还剩下 1/0/2/0。 检查一个状态是否安全的算法如下： 查找右边的矩阵是否存在一行小于等于向量 A。如果不存在这样的行，那么系统将会发生死锁，状态是不安全的。 假若找到这样一行，将该进程标记为终止，并将其已分配资源加到 A 中。 重复以上两步，直到所有进程都标记为终止，则状态时安全的。如果一个状态不是安全的，需要拒绝进入这个状态。 举个栗子http://tpcg.io/EW1UmT 1234567891011121314151617181920212223242526272829303132333435363738public static void main(String[] args) &#123; final Object a = new Object(); final Object b = new Object(); Thread threadA = new Thread(new Runnable() &#123; public void run() &#123; synchronized (a) &#123; try &#123; System.out.println("now i in threadA-locka"); Thread.sleep(1000l); synchronized (b) &#123; System.out.println("now i in threadA-lockb"); &#125; &#125; catch (Exception e) &#123; // ignore &#125; &#125; &#125; &#125;); Thread threadB = new Thread(new Runnable() &#123; public void run() &#123; synchronized (b) &#123; try &#123; System.out.println("now i in threadB-lockb"); Thread.sleep(1000l); synchronized (a) &#123; System.out.println("now i in threadB-locka"); &#125; &#125; catch (Exception e) &#123; // ignore &#125; &#125; &#125; &#125;); threadA.start(); threadB.start();&#125; 程序执行结果:12now i in threadA-lockanow i in threadB-lockb 死锁检测Jstack命令jstack是java虚拟机自带的一种堆栈跟踪工具。jstack用于打印出给定的java进程ID或core file或远程调试服务的Java堆栈信息。Jstack工具可以用于生成java虚拟机当前时刻的线程快照。线程快照是当前java虚拟机内每一条线程正在执行的方法堆栈的集合，生成线程快照的主要目的是定位线程出现长时间停顿的原因，如线程间死锁、死循环、请求外部资源导致的长时间等待等。 线程出现停顿的时候通过jstack来查看各个线程的调用堆栈，就可以知道没有响应的线程到底在后台做什么事情，或者等待什么资源。首先，我们通过jps确定当前执行任务的进程号: 首先，我们通过jps确定当前执行任务的进程号: 可以确定任务进程号是51028，然后执行jstack命令查看当前进程堆栈信息(在eclipse中运行后可能会有jstack无法连接报错的信息，所以我换成了IDEA来执行 ORZ)： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172C:\Users\homxu&gt;jps1089647284 Jps52168 DeadLockC:\Users\homxu&gt;jstack -F 52168Attaching to process ID 52168, please wait...Debugger attached successfully.Server compiler detected.JVM version is 25.144-b01Deadlock Detection:Found one Java-level deadlock:=============================&quot;Thread-0&quot;: waiting to lock Monitor@0x00000000569a3968 (Object@0x00000000e071b2f8, a java/lang/Object), which is held by &quot;Thread-1&quot;&quot;Thread-1&quot;: waiting to lock Monitor@0x00000000569a1188 (Object@0x00000000e071b2e8, a java/lang/Object), which is held by &quot;Thread-0&quot;Found a total of 1 deadlock.Thread 1: (state = BLOCKED)Thread 23: (state = BLOCKED) - com.study.DeadLock$2.run() @bci=28, line=30 (Interpreted frame) - java.lang.Thread.run() @bci=11, line=748 (Interpreted frame)Thread 22: (state = BLOCKED) - com.study.DeadLock$1.run() @bci=28, line=14 (Interpreted frame) - java.lang.Thread.run() @bci=11, line=748 (Interpreted frame)Thread 15: (state = IN_NATIVE) - java.net.SocketInputStream.socketRead0(java.io.FileDescriptor, byte[], int, int, int) @bci=0 (Interpreted frame) - java.net.SocketInputStream.socketRead(java.io.FileDescriptor, byte[], int, int, int) @bci=8, line=116 (Interpreted frame) - java.net.SocketInputStream.read(byte[], int, int, int) @bci=117, line=171 (Interpreted frame) - java.net.SocketInputStream.read(byte[], int, int) @bci=11, line=141 (Interpreted frame) - sun.nio.cs.StreamDecoder.readBytes() @bci=135, line=284 (Interpreted frame) - sun.nio.cs.StreamDecoder.implRead(char[], int, int) @bci=112, line=326 (Interpreted frame) - sun.nio.cs.StreamDecoder.read(char[], int, int) @bci=180, line=178 (Interpreted frame) - java.io.InputStreamReader.read(char[], int, int) @bci=7, line=184 (Interpreted frame) - java.io.BufferedReader.fill() @bci=145, line=161 (Interpreted frame) - java.io.BufferedReader.readLine(boolean) @bci=44, line=324 (Interpreted frame) - java.io.BufferedReader.readLine() @bci=2, line=389 (Interpreted frame) - com.intellij.rt.execution.application.AppMainV2$1.run() @bci=36, line=64 (Interpreted frame)Thread 14: (state = BLOCKED)Thread 13: (state = BLOCKED)Thread 12: (state = BLOCKED) - java.lang.Object.wait(long) @bci=0 (Interpreted frame) - java.lang.ref.ReferenceQueue.remove(long) @bci=59, line=143 (Interpreted frame) - java.lang.ref.ReferenceQueue.remove() @bci=2, line=164 (Interpreted frame) - java.lang.ref.Finalizer$FinalizerThread.run() @bci=36, line=209 (Interpreted frame)Thread 11: (state = BLOCKED) - java.lang.Object.wait(long) @bci=0 (Interpreted frame) - java.lang.Object.wait() @bci=2, line=502 (Interpreted frame) - java.lang.ref.Reference.tryHandlePending(boolean) @bci=54, line=191 (Interpreted frame) - java.lang.ref.Reference$ReferenceHandler.run() @bci=1, line=153 (Interpreted frame) 可以看到，进程的确存在死锁，两个线程分别在等待对方持有的Object对象 JConsole工具Jconsole是JDK自带的监控工具，在JDK/bin目录下可以找到。它用于连接正在运行的本地或者远程的JVM，对运行在Java应用程序的资源消耗和性能进行监控，并画出大量的图表，提供强大的可视化界面。而且本身占用的服务器内存很小，甚至可以说几乎不消耗。我们在命令行中敲入jconsole命令，会自动弹出以下对话框，选择进程52168，并点击“链接” 可以看到进程中存在死锁。以上例子我都是用synchronized关键词实现的死锁，如果读者用ReentrantLock制造一次死锁，再次使用死锁检测工具，也同样能检测到死锁，不过显示的信息将会更加丰富，有兴趣的读者可以自己尝试一下。 在我的理解当中，死锁就是“两个任务以不合理的顺序互相争夺资源”造成，因此为了规避死锁，应用程序需要妥善处理资源获取的顺序。另外有些时候，死锁并不会马上在应用程序中体现出来，在通常情况下，都是应用在生产环境运行了一段时间后，才开始慢慢显现出来，在实际测试过程中，由于死锁的隐蔽性，很难在测试过程中及时发现死锁的存在，而且在生产环境中，应用出现了死锁，往往都是在应用状况最糟糕的时候——在高负载情况下。因此，开发者在开发过程中要谨慎分析每个系统资源的使用情况，合理规避死锁，另外一旦出现了死锁，也可以尝试使用本文中提到的一些工具，仔细分析，总是能找到问题所在的。 参考 《精通JAVA并发编程(第二版)》 https://www.cnblogs.com/thomaschen750215/p/4109646.html https://cyc2018.github.io/CS-Notes/#/notes/%E8%AE%A1%E7%AE%97%E6%9C%BA%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%20-%20%E6%AD%BB%E9%94%81?id=%E6%AD%BB%E9%94%81%E6%A3%80%E6%B5%8B%E4%B8%8E%E6%AD%BB%E9%94%81%E6%81%A2%E5%A4%8D https://juejin.im/post/5aaf6ee76fb9a028d3753534#heading-1]]></content>
      <tags>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[转]乐观锁与悲观锁]]></title>
    <url>%2F2019%2F06%2F11%2F%E4%B9%90%E8%A7%82%E9%94%81%E4%B8%8E%E6%82%B2%E8%A7%82%E9%94%81%2F</url>
    <content type="text"><![CDATA[乐观并发控制(乐观锁)和悲观并发控制（悲观锁）是并发控制主要采用的技术手段。 无论是悲观锁还是乐观锁，都是人们定义出来的概念，可以认为是一种思想。网上最常见的解答是数据库管理系统(DBMS)中锁的机制的介绍。当然不仅仅是在关系型数据库系统中有乐观锁和悲观锁的概念，像memcache(一个分布式内存对象缓存系统)、hibernate、tair(与redis类似,是一个分布式key/value存储系统)等都有类似的概念。 可见，只要是涉及到并发，就很难绕开”锁”。所以不要把乐观并发控制和悲观并发控制狭义的理解为DBMS中的概念。 悲观锁(Pessimistic Lock)顾名思义，就是很悲观，每次去拿数据的时候都认为别人会修改，所以每次在拿数据/修改数据的时候都会上锁，这样别人想拿这个数据就会block直到它拿到锁(一旦加锁，不同线程同时执行时,只能有一个线程执行，其他的线程在入口处等待，直到锁被释放)。 比如: 传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。 Java的synchronized关键字 当我们要对一个数据库中的一条数据进行修改的时候，为了避免同时被其他人修改，最好的办法就是直接对该数据进行加锁以防止并发。 这种借助数据库锁机制在修改数据之前先锁定，再修改的方式被称之为悲观并发控制（又名“悲观锁”，Pessimistic Concurrency Control，缩写“PCC”）。 DMBS中悲观锁的实现，往往依靠数据库提供的锁机制 （也只有数据库层提供的锁机制才能真正保证数据访问的排他性，否则，即使在本系统中实现了加锁机制，也无法保证外部系统不会修改数据） 在数据库中，悲观锁的流程如下： 在对任意记录进行修改前，先尝试为该记录加上排他锁（exclusive locking）。如果加锁失败，说明该记录正在被修改，那么当前查询可能要等待或者抛出异常。 具体响应方式由开发者根据实际需要决定。如果成功加锁，那么就可以对记录做修改，事务完成后就会解锁了。其间如果有其他对该记录做修改或加排他锁的操作，都会等待我们解锁或直接抛出异常。 优点和缺点:悲观并发控制实际上是“先取锁再访问”的保守策略，为数据处理的安全提供了保证。但是在效率方面，处理加锁的机制会让数据库产生额外的开销，还有增加产生死锁的机会；另外，在只读型事务处理中由于不会产生冲突，也没必要使用锁，这样做只能增加系统负载；还有会降低了并行性，一个事务如果锁定了某行数据，其他事务就必须等待该事务处理完才可以处理那行数 乐观锁(Optimistic Lock)顾名思义，就是很乐观，认为数据一般情况下不会造成冲突(认为操作不会产生并发问题(不会有其他线程对数据进行修改)，因此不会上锁)，所以在数据进行提交更新的时候，才会正式对数据的冲突与否进行检测，如果发现冲突了，则返回用户错误的信息，让用户决定如何去做。 相对于悲观锁，在对数据库进行处理的时候，乐观锁并不会使用数据库提供的锁机制。但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制。 乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库如果提供类似于write_condition机制的其实都是提供的乐观锁。 优点和缺点：乐观并发控制相信事务之间的数据竞争(data race)的概率是比较小的，因此尽可能直接做下去，直到提交的时候才去锁定，所以不会产生任何锁和死锁。这样提高了效率，但是虽然事务之间数据竞争的概率是很小的，但是仍可能产生这种概率。 乐观锁的实现乐观锁一般使用版本号机制或CAS(compare and swap)算法实现 版本号机制 取出记录时，获取当前version 更新时，带上这个version 执行更新时， set version = newVersion where version = oldVersion如果version不对，就更新失败 例如: 1update table set name = 'Aron', version = version + 1 where id = #&#123;id&#125; and version = #&#123;version&#125;; CAS乐观锁的另一种技术，当多个线程尝试使用CAS同时更新同一个变量时，只有其中一个线程能更新变量的值，而其它线程都失败，失败的线程并不会被挂起，而是被告知这次竞争中失败，并可以再次尝试。 CAS操作中包含三个操作数 : 需要读写的内存位置V 进行比较的预期原值A 拟写入的新值B 如果内存位置V的值与预期原值A相匹配，那么处理器会自动将该位置值更新为新值B。否则处理器不做任何操作。无论哪种情况，它都会在 CAS 指令之前返回该位置的值（在 CAS 的一些特殊情况下将仅返回 CAS 是否成功，而不提取当前值）。CAS 有效地说明了“ 我认为位置 V 应该包含值 A；如果包含该值，则将 B 放到这个位置；否则，不要更改该位置，只告诉我这个位置现在的值即可。 ”这其实和乐观锁的冲突检查+数据更新的原理是一样的。 简单来说就是：CAS原理就是对v对象进行赋值时，先判断原来的值是否为A，如果为A，就把新值B赋值到V对象上面，如果原来的值不是A（代表V的值放生了变化），就不赋新值。 concurrent包的实现由于java的CAS同时具有 volatile读和volatile写的内存语义，因此Java线程之间的通信现在有了下面四种方式： A线程写volatile变量，随后B线程读这个volatile变量。 A线程写volatile变量，随后B线程用CAS更新这个volatile变量。 A线程用CAS更新一个volatile变量，随后B线程用CAS更新这个volatile变量。 A线程用CAS更新一个volatile变量，随后B线程读这个volatile变量。 Java的CAS会使用现代处理器上提供的高效机器级别原子指令，这些原子指令以原子方式对内存执行读-改-写操作，这是在多处理器中实现同步的关键（从本质上来说，能够支持原子性读-改-写指令的计算机器，是顺序计算图灵机的异步等价机器，因此任何现代的多处理器都会去支持某种能对内存执行原子性读-改-写操作的原子指令）。同时，volatile变量的读/写和CAS可以实现线程之间的通信。把这些特性整合在一起，就形成了整个concurrent包得以实现的基石。 仔细分析concurrent包的源代码实现，会发现一个通用化的实现模式： 首先，声明共享变量为volatile； 然后，使用CAS的原子条件更新来实现线程之间的同步； 同时，配合以volatile的读/写和CAS所具有的volatile读和写的内存语义来实现线程 缺点 ABA问题 CAS算法实现一个重要前提需要取出内存中某时刻的数据，而在下时刻比较并替换，那么在这个时间差类会导致数据的变化。比如说一个线程one从内存位置V中取出A，这时候另一个线程two也从内存中取出A，并且two进行了一些操作变成了B，然后two又将V位置的数据变成A，这时候线程one进行CAS操作发现内存中仍然是A，然后one操作成功。尽管线程one的CAS操作成功，但是不代表这个过程就是没有问题的。如果链表的头在变化了两次后恢复了原值，但是不代表链表就没有变化。因此前面提到的原子操作AtomicStampedReference/AtomicMarkaBleReference就很有用了。这允许一对变化的元素进行原子操作。 循环时间长开销大 自旋CAS（不成功，就一直循环执行，直到成功）如果长时间不成功，会给CPU带来非常大的执行开销。如果JVM能支持处理器提供的pause指令那么效率会有一定的提升，pause指令有两个作用，第一它可以延迟流水线执行指令（de-pipeline）,使CPU不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，在一些处理器上延迟时间是零。第二它可以避免在退出循环的时候因内存顺序冲突（memory order violation）而引起CPU流水线被清空（CPU pipeline flush），从而提高CPU的执行效率。 只能保证一个共享变量的原子操作 当对一个共享变量执行操作时，我们可以使用循环CAS的方式来保证原子操作，但是对多个共享变量操作时，循环CAS就无法保证操作的原子性，这个时候就可以用锁，或者有一个取巧的办法，就是把多个共享变量合并成一个共享变量来操作。比如有两个共享变量i ＝ 2,j = a，合并一下ij = 2a，然后用CAS来操作ij。从Java 1.5开始JDK提供了AtomicReference类来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行CAS操作。 总结二者选择 1、乐观锁并未真正加锁，效率高。一旦锁的粒度掌握不好，更新失败的概率就会比较高，容易发生业务失败。高并发环境下锁粒度把控是一门重要的学问，选择一个好的锁，在保证数据安全的情况下，可以大大提升吞吐率，进而提升性能。 2、悲观锁依赖数据库锁，效率低。更新失败的概率比较低。 随着互联网三高架构（高并发、高性能、高可用）的提出，悲观锁已经越来越少的被使用到生产环境中了，尤其是并发量比较大的业务场景。 悲观锁适合写多读少的场景。因为在使用的时候该线程会独占这个资源，在本文的例子来说就是某个id的文章，如果有大量的评论操作的时候，就适合用悲观锁，否则用户只是浏览文章而没什么评论的话，用悲观锁就会经常加锁，增加了加锁解锁的资源消耗。 乐观锁适合写少读多的场景。由于乐观锁在发生冲突的时候会回滚或者重试，如果写的请求量很大的话，就经常发生冲突，经常的回滚和重试，这样对系统资源消耗也是非常大。 所以悲观锁和乐观锁没有绝对的好坏，必须结合具体的业务情况来决定使用哪一种方式。另外在阿里巴巴开发手册里也有提到： 如果每次访问冲突概率小于 20%，推荐使用乐观锁，否则使用悲观锁。乐观锁的重试次 数不得小于 3 次。 阿里巴巴建议以冲突概率20%这个数值作为分界线来决定使用乐观锁和悲观锁，虽然说这个数值不是绝对的，但是作为阿里巴巴各个大佬总结出来的也是一个很好的参考。 参考https://www.hollischuang.com/archives/934 https://blog.csdn.net/hongchangfirst/article/details/26004335 https://segmentfault.com/a/1190000016611415#articleHeader0 https://www.cnblogs.com/549294286/p/3766717.html https://zzzzbw.cn/article/18#%E4%B9%90%E8%A7%82%E9%94%81%E8%A7%A3%E5%86%B3%E5%B9%B6%E5%8F%91%E9%97%AE%E9%A2%98]]></content>
      <tags>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅析IOC和DI]]></title>
    <url>%2F2019%2F06%2F06%2F%E6%B5%85%E6%9E%90IOC%E5%92%8CDI%2F</url>
    <content type="text"><![CDATA[概念 IOC(Inversion of Control): 其思想是反转资源获取的方向.传统的资源查找方式要求组件向容器发起请求查找资源.作为回应，容器适时的返回资源。而应用了IOC之后，则是容器主动地将资源推送给它所管理的组件，组件所要做的仅是选择一种合适的方式来接受资源.这种形式也被称为查找的被动形式。 控制什么？控制对象的创建及销毁（生命周期） 反转什么？对象的控制权被反转了，将对象的控制权交给IOC容器 DI(Dependency Injection)——IOC的另一种表述方式：即组件以一些预先定义好的方式(如:setter方法)接受来自肉容器的资源注入.相对于IOC而言，这种表述更直接. 为何是反转，哪些方面反转了：有反转就有正转，传统应用程序是由我们自己在对象中主动控制去直接获取依赖对象，也就是正转；而反转则是由容器来帮忙创建及注入依赖对象；为何是反转？因为由容器帮我们查找及注入依赖对象，对象只是被动的接受依赖对象，所以是反转；哪些方面反转了？依赖对象的获取被反转了。 IoC不是一种技术，只是一种思想，一个重要的面向对象编程的法则，它能指导我们如何设计出松耦合、更优良的程序。传统应用程序都是由我们在类内部主动创建依赖对象，从而导致类与类之间高耦合，难于测试；有了IoC容器后，把创建和查找依赖对象的控制权交给了容器，由容器进行注入组合对象，所以对象与对象之间是松散耦合，这样也方便测试，利于功能复用，更重要的是使得程序的整个体系结构变得非常灵活。 依赖注入：是组件之间依赖关系由容器在运行期决定，形象的说，即由容器动态的将某个依赖关系注入到组件之中。依赖注入的目的并非为软件系统带来更多功能，而是为了提升组件重用的频率，并为系统搭建一个灵活、可扩展的平台。通过依赖注入机制，我们只需要通过简单的配置，而无需任何代码就可指定目标需要的资源，完成自身的业务逻辑，而不需要关心具体的资源来自何处，由谁实现。 理解DI的关键是：“谁依赖谁，为什么需要依赖，谁注入谁，注入了什么”，那我们来深入分析一下： 谁依赖于谁：当然是某个容器管理对象依赖于IoC容器；“被注入对象的对象”依赖于“依赖对象”； 为什么需要依赖：容器管理对象需要IoC容器来提供对象需要的外部资源； 谁注入谁：很明显是IoC容器注入某个对象，也就是注入“依赖对象”； 注入了什么：就是注入某个对象所需要的外部资源（包括对象、资源、常量数据）。 IoC和DI由什么关系呢？其实它们是同一个概念的不同角度描述，由于控制反转概念比较含糊（可能只是理解为容器控制对象这一个层面，很难让人想到谁来维护对象关系），所以2004年大师级人物Martin Fowler又给出了一个新的名字：“依赖注入”，相对IoC 而言，“依赖注入”明确描述了“被注入对象依赖IoC容器配置依赖对象”。 分析实际假设有个人叫张三，他有一辆奥迪车。张三下班的时候要开车回家，那么他对奥迪便产生了一种依赖关系。 Audi和ZhangSan的实现类：1234567891011121314151617181920package com.iocstudy;public class Audi &#123; public void start() &#123; System.out.println("Audi.start"); &#125; public void turnLeft() &#123; System.out.println("Audi.turnLeft"); &#125; public void turnRight() &#123; System.out.println("Audi.turnRight"); &#125; public void stop() &#123; System.out.println("Audi.stop"); &#125; &#125; 1234567891011121314package com.iocstudy;public class ZhangSan &#123; public void goHome()&#123; Audi audi = new Audi(); audi.start(); audi.turnLeft(); audi.turnRight(); audi.turnRight(); audi.stop(); &#125;&#125; 所以，张三要回家的话，需要 创建一辆车 1Audi audi = new Audi(); 控制车辆启动、直行、左右转 12345audi.start();audi.turnLeft();audi.turnRight();audi.turnRight();audi.stop(); 那么，张三用车肯定不只是用来回家。还可以买东西、约会、飙车等等。这时候，代码就该修改为:123456789101112131415161718192021222324package com.iocstudy;public class ZhangSan &#123; public void goHome()&#123; Audi audi = new Audi(); audi.start(); audi.turnLeft(); audi.turnRight(); audi.turnRight(); audi.stop(); &#125; public void goShop()&#123; Audi audi = new Audi(); audi.start(); audi.turnLeft(); audi.turnRight(); audi.turnRight(); audi.stop(); &#125;&#125; 这时候，张三换车了，换成一辆BMW1234567891011121314151617181920package com.iocstudy;public class BMW &#123; public void start() &#123; System.out.println("Audi.start"); &#125; public void turnLeft() &#123; System.out.println("Audi.turnLeft"); &#125; public void turnRight() &#123; System.out.println("Audi.turnRight"); &#125; public void stop() &#123; System.out.println("Audi.stop"); &#125;&#125; 那么这时候要把所有ZhangSan类中的引用Audi的代码都改为BMW。这体现出来了车和张三之间的高耦合性。其次，这在设计时有一些问题： 张三所有的行为都需要自己主动创建一辆车。 更换车辆的代价是巨大的。要把所有奥迪车的引用都换为BMW。 这时候可以在张三类中进行一些改进，把他的车提到张三的属性域中，这样只需修改属性域中的车辆就可以了:12345678910111213141516171819202122package com.iocstudy;public class ZhangSan &#123; private BMW bmw = new BMW(); public void goHome()&#123; audi.start(); audi.turnLeft(); audi.turnRight(); audi.turnRight(); audi.stop(); &#125; public void goShop()&#123; audi.start(); audi.turnLeft(); audi.turnRight(); audi.turnRight(); audi.stop(); &#125;&#125; 那么接着思考： 张三需要的事一辆奥迪车？还是一辆宝马车？还是只是一辆车？张三只需要回家和买东西，只是需要一辆车而已。 张三会制造（创建）车辆吗？不会。车辆不应该由张三进行创建。 解决方法：抽象出一个车的接口,并且车辆不由张三创建，而是在构造函数中以参数的形式传入。 定义接口：123456789101112package com.iocstudy;public interface Car &#123; public void start(); public void turnLeft(); public void turnRight(); public void stop();&#125; 让奥迪和宝马实现接口：123456789101112131415161718192021package com.iocstudy;public class Audi implements Car&#123; public void start() &#123; System.out.println("Audi.start"); &#125; public void turnLeft() &#123; System.out.println("Audi.turnLeft"); &#125; public void turnRight() &#123; System.out.println("Audi.turnRight"); &#125; public void stop() &#123; System.out.println("Audi.stop"); &#125; &#125; 1234567891011121314151617181920package com.iocstudy;public class BMW implements Car&#123; public void start() &#123; System.out.println("Audi.start"); &#125; public void turnLeft() &#123; System.out.println("Audi.turnLeft"); &#125; public void turnRight() &#123; System.out.println("Audi.turnRight"); &#125; public void stop() &#123; System.out.println("Audi.stop"); &#125;&#125; 接着回到张三这个类，张三只是需要一辆车，车不应该由张三创建。1234567891011121314151617181920212223242526package com.iocstudy;public class ZhangSan &#123; private Car car; ZhangSan(Car car)&#123; this.car = car; &#125; public void goHome()&#123; audi.start(); audi.turnLeft(); audi.turnRight(); audi.turnRight(); audi.stop(); &#125; public void goShop()&#123; audi.start(); audi.turnLeft(); audi.turnRight(); audi.turnRight(); audi.stop(); &#125;&#125; 那么接着思考一下：既然车不应该由张三创建，那么应该由谁创建呢？那就是今天的主角：IOC容器 实现一个自己的IOC容器场景模拟延续上一节中场景的例子，新增一个Human接口，和HumanWithCar类(有车一族类)，张三和李四继承自有车一族类。如下图： 在使用IOC管理这个场景之前，进行一些约定,以便简化IOC的业务逻辑： 所有的Bean的声明周期交给IOC容器管理 所有被依赖的Bean通过构造方法进行注入。(其实除了构造方法注入，还有setter方法注入) 被依赖的Bean需要优先创建。（比如要创建张三，那么张三依赖的奥迪需要先创建了并且交由IOC容器管理了） 这是新的场景模拟的代码：Car接口123456789101112package com.iocstudy.car;public interface Car &#123; public void start(); public void turnLeft(); public void turnRight(); public void stop();&#125; 奥迪实现类：1234567891011121314151617181920package com.iocstudy.car;public class Audi implements Car&#123; public void start() &#123; System.out.println("Audi.start"); &#125; public void turnLeft() &#123; System.out.println("Audi.turnLeft"); &#125; public void turnRight() &#123; System.out.println("Audi.turnRight"); &#125; public void stop() &#123; System.out.println("Audi.stop"); &#125;&#125; 宝马实现类： 1234567891011121314151617181920package com.iocstudy.car;public class BMW implements Car&#123; public void start() &#123; System.out.println("Audi.start"); &#125; public void turnLeft() &#123; System.out.println("Audi.turnLeft"); &#125; public void turnRight() &#123; System.out.println("Audi.turnRight"); &#125; public void stop() &#123; System.out.println("Audi.stop"); &#125;&#125; Human接口:123456package com.iocstudy.Human;public interface Human &#123; public void goHome(); public void goShop();&#125; HumanWithCar类，它要实现接口中的goHome方法和goShop方法，但是有车一族是一个笼统的概念，张三和李四都是有车一族，他们回家和购物的路线和方式肯定不是一样的，要执行的方法的具体内容不一样。所以,这两个方法应该被声明为虚方法，HumanWithCar类也应是抽象类。12345678910111213141516package com.iocstudy.Human;import com.iocstudy.car.Car;public abstract class HumanWithCar implements Human &#123; protected Car car; HumanWithCar(Car car)&#123; this.car = car; &#125; public abstract void goHome(); public abstract void goShop();&#125; 张三和李四: 123456789101112131415161718192021222324package com.iocstudy.Human;import com.iocstudy.car.Car;public class ZhangSan extends HumanWithCar &#123; public ZhangSan(Car car) &#123; super(car); &#125; @Override public void goHome() &#123; car.start(); car.turnLeft(); car.turnRight(); car.stop(); &#125; @Override public void goShop() &#123; car.start(); car.turnRight(); car.stop(); &#125;&#125; 123456789101112131415161718192021222324package com.iocstudy.Human;import com.iocstudy.car.Car;public class LiSi extends HumanWithCar &#123; public LiSi(Car car) &#123; super(car); &#125; @Override public void goHome() &#123; car.start(); car.turnLeft(); car.stop(); &#125; @Override public void goShop() &#123; car.start(); car.turnRight(); car.turnLeft(); car.stop(); &#125;&#125; 然后要书写IoC容器，按照约定， 所有的Bean的声明周期交给IOC容器管理 所有被依赖的Bean通过构造方法进行注入。 被依赖的Bean需要优先创建。 所以， 容器要能实例化bean 实例化后要保存bean 要能够提供bean 每个bean要产生一个id与之相对应 那么， 保存bean：要有私有域来存储IoC创建好的bean(使用Map存储) 提供bean:提供一个getBean()方法 实例化bean：提供一个setBean()方法，向Map中增加bean 创建容器类:123456789101112131415161718192021222324252627package com.iocstudy;import java.util.Map;import java.util.concurrent.ConcurrentHashMap;public class IoCContainer &#123; private Map&lt;String,Object&gt; beans = new ConcurrentHashMap&lt;String, Object&gt;(); /** * 根据beanId获取一个bean * @param beanId beanId * @return 返回bean */ public Object getBean(String beanId)&#123; return beans.get(beanId); &#125; /** * 委托ioc容器创建一个bean * @param clazz 要创建的bean的class * @param beanId beanId * @param paramBeanIds 要创建的bean的class的构造方法所需要的参数的beanId(即要创建的bean所需要的依赖bean) */ public void setBeans(Class&lt;?&gt; clazz,String beanId,String... paramBeanIds)&#123; &#125;&#125; 接下来就是实现setBeans方法： 因为是根据构造方法创建一个bean,构造方法是需要参数值的，所以要组装构造方法所需要的参数值 调用构造方法，实例化bean 将实例化的bean放到Map中 1234567891011121314151617181920212223242526public void setBeans(Class&lt;?&gt; clazz,String beanId,String... paramBeanIds)&#123; // 1 组装构造方法所需要的参数值 Object[] paramValues = new Object[paramBeanIds.length]; //因为约定中所有的被依赖的bean需要被优先创建,所以需要的bean从beans中取就可以了 for (int i = 0; i &lt; paramBeanIds.length; i++) &#123; paramValues[i] = beans.get(paramBeanIds[i]); &#125; // 2 调用构造方法，实例化bean Object bean = null; //先定义好最重要实例化的bean //循环使用要创建的bean的构造方法 for (Constructor&lt;?&gt; constructor : clazz.getConstructors()) &#123; try &#123; bean = constructor.newInstance(paramValues); //这里不处理异常。如果最终所有的构造方法都不能完成实例化，则bean为null &#125; catch (InstantiationException e) &#123; &#125; catch (IllegalAccessException e) &#123; &#125; catch (InvocationTargetException e) &#123; &#125; &#125; //如果最终没有实例化，则抛出错误 if(bean == null)&#123; throw new RuntimeException("找不到合适的构造方法实例化bean"); &#125; // 3 将实例化的bean放到Map中 beans.put(beanId,bean); &#125; 使用单元测试类进行测试:123456789101112131415161718192021222324252627282930package com.iocstudy;import com.iocstudy.Human.Human;import com.iocstudy.Human.LiSi;import com.iocstudy.Human.ZhangSan;import com.iocstudy.car.Audi;import com.iocstudy.car.BMW;import org.junit.Before;import org.junit.Test;public class springIocTest &#123; private IoCContainer ioCContainer = new IoCContainer(); @Before //向容器中注册bean public void before()&#123; ioCContainer.setBeans(Audi.class,"audi"); ioCContainer.setBeans(BMW.class,"BMW"); ioCContainer.setBeans(ZhangSan.class,"zhangsan","audi"); ioCContainer.setBeans(LiSi.class,"lisi","BMW"); &#125; @Test public void test() &#123; Human zhangsan = (Human) ioCContainer.getBean("zhangsan"); zhangsan.goHome(); Human lisi = (Human) ioCContainer.getBean("lisi"); lisi.goHome(); &#125;&#125; 打印结果:1234567Audi.startAudi.turnLeftAudi.turnRightAudi.stopAudi.startAudi.turnLeftAudi.stop 这样就实现了一个最简单的IoC容器。 这样做的好处: 所有的依赖关系被集中统一的管理起来，清晰明了。在Before中将所有的依赖关系先集中管理起来。 每个类只需要关注于自己的业务逻辑。例如在张三的代码中，张三只需要关注goHome()的方法，而不需要关心他的车是什么，只需要IoC容器提供一辆就可以了。 修改依赖关系是很简单的事情。例如，张三现在依赖的是奥迪车，那么在setBean中只需修改为BMW，他就依赖于宝马了。 那么再回过头来看看IOC和DI的理解： 在没有使用Spring的时候，每个对象在需要使用他的合作对象时，自己均要使用像new object() 这样的语法来将合作对象创建出来，这个合作对象是由自己主动创建出来的，创建合作对象的主动权在自己手上，自己需要哪个合作对象，就主动去创建，创建合作对象的主动权和创建时机是由自己把控的，而这样就会使得对象间的耦合度高了，A对象需要使用合作对象B来共同完成一件事，A要使用B，那么A就对B产生了依赖，也就是A和B之间存在一种耦合关系，并且是紧密耦合在一起，而使用了Spring之后就不一样了，创建合作对象B的工作是由Spring来做的，Spring创建好B对象，然后存储到一个容器里面，当A对象需要使用B对象时，Spring就从存放对象的那个容器里面取出A要使用的那个B对象，然后交给A对象使用，至于Spring是如何创建那个对象，以及什么时候创建好对象的，A对象不需要关心这些细节问题(你是什么时候生的，怎么生出来的我可不关心，能帮我干活就行)，A得到Spring给我们的对象之后，两个人一起协作完成要完成的工作即可。所以控制反转IoC(Inversion of Control)是说创建对象的控制权进行转移，以前创建对象的主动权和创建时机是由自己把控的，而现在这种权力转移到第三方，比如转移交给了IoC容器，它就是一个专门用来创建对象的工厂，你要什么对象，它就给你什么对象，有了 IoC容器，依赖关系就变了，原先的依赖关系就没了，它们都依赖IoC容器了，通过IoC容器来建立它们之间的关系。这是我对Spring的IoC(控制反转)的理解。DI(依赖注入)其实就是IOC的另外一种说法，DI是由Martin Fowler 在2004年初的一篇论文中首次提出的。他总结：控制的什么被反转了？就是：获得依赖对象的方式反转了。 参考 https://www.imooc.com/video/19046 https://blog.csdn.net/bestone0213/article/details/47424255 http://sishuok.com/forum/blogPost/list/2427.html https://blog.csdn.net/yqj2065/article/details/80450929]]></content>
      <tags>
        <tag>SPRING</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL中使用trigger(触发器)]]></title>
    <url>%2F2019%2F05%2F24%2FMySQL%E4%B8%AD%E4%BD%BF%E7%94%A8trigger-%E8%A7%A6%E5%8F%91%E5%99%A8%2F</url>
    <content type="text"><![CDATA[最近有一个项目提出新需求,有一个表由之前的一对一变成一对多,为了不影响使用原表的视图(为了进行统计汇总等),所以打算为这个表拓展子表,在子表上定义触发器,在子表改变的时候对应改变母表.所以这里记录一下定义使用trigger的过程. 在此之前提醒一下： ！！尽量少使用触发器,不建议使用。 假设触发器触发每次执行1s,insert table 500条数据,那么就需要触发500次触发器,光是触发器执行的时间就花费了500s,而insert 500条数据一共是1s,那么这个insert的效率就非常低了。因此我们特别需要注意的一点是触发器的begin end;之间的语句的执行效率一定要高,资源消耗要小。触发器尽量少的使用,因为不管如何,它还是很消耗资源,如果使用的话要谨慎的使用,确定它是非常高效的：触发器是针对每一行的；对增删改非常频繁的表上切记不要使用触发器,因为它会非常消耗资源。 MySQL触发器创建1234567CREATE [DEFINER = &#123; 'user' | CURRENT_USER &#125;]TRIGGER trigger_nametrigger_time trigger_eventON table_nameFOR EACH ROW[trigger_order]trigger_body 其中一些字段含义:|字段|含义|可能的值|| – | – | – ||DEFINER=|可选参数,指定创建者,默认为当前登录用户(CURRENT_USER);该触发器将以此参数指定的用户执行,所以需要考虑权限问题；|DEFINER=’root@%’DEFINER=CURRENT_USER||trigger_name|触发器名称,最好由表名+触发事件关键词+触发时间关键词组成；|||trigger_time|触发时间,在某个事件之前还是之后|BEFORE、AFTER||trigger_event|触发事件,如插入时触发、删除时触发；INSERT：插入操作触发器,INSERT、LOAD DATA、REPLACE时触发;UPDATE：更新操作触发器,UPDATE操作时触发;DELETE：删除操作触发器,DELETE、REPLACE操作时触发;|INSERT、UPDATE、DELETE||table_name |触发操作事件的表名|||trigger_order|可选参数,如果定义了多个具有相同触发事件和触法时间的触发器时(如：BEFORE UPDATE),默认触发顺序与触发器的创建顺序一致,可以使用此参数来改变它们触发顺序。mysql 5.7.2起开始支持此参数。FOLLOWS：当前创建触发器在现有触发器之后激活；PRECEDES：当前创建触发器在现有触发器之前激活|FOLLOWS、PRECEDES||trigger_body|触发执行的SQL语句内容,一般以begin开头,end结尾|begin .. end| 在trigger_body中,我们可以使用NEW表示将要插入的新行(相当于MS SQL的INSERTED),OLD表示将要删除的旧行(相当于MS SQL的DELETED)。通过OLD,NEW中获取它们的字段内容,方便在触发操作中使用,下面是对应事件是否支持OLD、NEW的对应关系：|事件|OLD|NEW||– | – | – ||INSERT|×|√||DELETE|√|×||UPDATE|√|√|由于UPDATE相当于删除旧行(OLD),然后插入新行(NEW),所以UPDATE同时支持OLD、NEW； MySQL分隔符(DELIMITER): MySQL默认使用”;”作为分隔符，SQL语句遇到”;”就会提交。而我们的触发器中可能会有多个”;”符，为了防止触发器创建语句过早的提交，我们需要临时修改MySQL分隔符，创建完后，再将分隔符改回来。使用DELIMITER可以修改分隔符，如下： 1234DELIMITER $... --触发器创建语句;$ --提交创建语句;DELIMITER ; MySQL触发器中使用变量：MySQL触发器中变量变量前面加’@’，无需定义，可以直接使用： 12345-- 变量直接赋值set @num=999; -- 使用select语句查询出来的数据方式赋值，需要加括号：set @name =(select name from table); MySQL触发器中使用if语做条件判断： 123456789-- 简单的if语句：set sex = if (new.sex=1, &apos;男&apos;, &apos;女&apos;); -- 多条件if语句：if old.type=1 then update table ...;elseif old.type=2 then update table ...;end if; 案例由于项目中的表过于复杂,这里就只用比较简单的表作为例子进行记录。 创建母表:12345678CREATE TABLE `employ` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` char(20) NOT NULL, `email` char(40) DEFAULT NULL, `salary` int(11) NOT NULL, `salary2` int(11) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8 创建子表:123456789CREATE TABLE `employ_copy` ( `id` int(11) NOT NULL AUTO_INCREMENT, `employ_id` int(11) NOT NULL, `name` char(20) NOT NULL, `email` char(40) DEFAULT NULL, `salary` int(11) NOT NULL, `salary2` int(11) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=7 DEFAULT CHARSET=utf8 母表自表之间没有用外键连接,但是子表(employ_copy)根据employ_id与母表(employ)的id进行匹配. 为了更直观,这里使用Navicat进行操作：首先插入一条数据: 在employ_copy中定义插入时的触发器: 在Navicat中直接这样写就可以了: 现在在employ_copy表中插入一条数据,然后看employ表中数据的对应改变:插入的数据:数据改变: 当向employ_copy中插入数据时,让employ中的salary和salary2字段等于原来的字段加上新插入的值.由于原来都是0,插入的两个值为100,插入后employ中的两个值变为100 然后定义更新时的触发器: 将刚才employ_copy中的数据进行修改: 然后查看新值: 思路是employ中的值等于减去update之前的旧值然后再加上update之后的新值. 最后定义删除时的触发器: 将刚才employ_copy中的数据进行删除: 然后查看新值: employ_copy中删除数据后,employ中对应数据也进行了删除 这次只是简单的trigger使用,做一个简单的记录 参考 https://www.cnblogs.com/geaozhang/p/6819648.html https://aiezu.com/article/mysql_trigger_syntax.html]]></content>
      <tags>
        <tag>mysql</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[事务隔离级别]]></title>
    <url>%2F2019%2F05%2F16%2F%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%2F</url>
    <content type="text"><![CDATA[本文大部分转载自事务隔离级别(图文详解).md) 事务什么是事务事务是逻辑上的一组操作，要么都执行，要么都不执行。 事务最经典也经常被拿出来说例子就是转账了。假如小明要给小红转账1000元，这个转账会涉及到两个关键操作就是：将小明的余额减少1000元，将小红的余额增加1000元。万一在这两个操作之间突然出现错误比如银行系统崩溃，导致小明余额减少而小红的余额没有增加，这样就不对了。事务就是保证这两个关键操作要么都成功，要么都要失败。 事务的特性(ACID) 原子性： 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用； 一致性： 执行事务前后，数据保持一致，多个事务对同一个数据读取的结果是相同的； 隔离性： 并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的； 持久性： 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。、 并发事务带来的问题在典型的应用程序中，多个事务并发运行，经常会操作相同的数据来完成各自的任务（多个用户对统一数据进行操作）。并发虽然是必须的，但可能会导致以下的问题。 丢失修改(Lost to modify)：或者称为更新遗失(Lost Updata).指在一个事务读取一个数据时，另外一个事务也访问了该数据，那么在第一个事务中修改了这个数据后，第二个事务也修改了这个数据。这样第一个事务内的修改结果就被丢失，因此称为丢失修改。基本上就是某个事务对字段进行更新的信息，因为另一个事务的介入而遗失了更新的内容。例如：甲售票点（甲事务）读出某航班的机票余额A,设A=16.乙售票点（乙事务）读出同一航班的机票余额A,也为16.甲售票点卖出一张机票,修改余额A←A-1.所以A为15,把A写回数据库.乙售票点也卖出一张机票,修改余额A←A-1.所以A为15,把A写回数据库.结果明明卖出两张机票，数据库中机票余额只减少1。乙事务没有读到甲事务修改后的值。 脏读(Dirty Read):当一个事务正在访问数据并且对数据进行了修改，而这种修改还没有提交到数据库中，这时另外一个事务也访问了这个数据，然后使用了这个数据。因为这个数据是还没有提交的数据，那么另外一个事务读到的这个数据是“脏数据”(即不干净,不正确的数据)，依据“脏数据”所做的操作可能是不正确的。 不可重复读（Unrepeatable Read）:指在一个事务内多次读同一数据。在这个事务还没有结束时，另一个事务也访问该数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改导致第一个事务两次读取的数据可能不太一样。这就发生了在一个事务内两次读到的数据是不一样的情况，因此称为不可重复读。例如，事务A在事务B更新前后进行数据的读取，事务A得到了不同的结果。 幻读（Phantom Read）:幻读与不可重复读类似。它发生在一个事务（T1）读取了几行数据，接着另一个并发事务（T2）插入了一些数据时。在随后的查询中，第一个事务（T1）就会发现多了一些原本不存在的记录，就好像发生了幻觉一样，所以称为幻读。即同一事务期间，读取到的数据笔数不一致。例如，事务A第一次读取得到5笔数据，此时事务B增加了一条数据，导致事务A再次读取得到6笔数据。 不可重复度和幻读区别： 不可重复读的重点是修改，幻读的重点在于新增或者删除。 例1（同样的条件, 你读取过的数据, 再次读取出来发现值不一样了 ）：事务1中的A先生读取自己的工资为 1000的操作还没完成，事务2中的B先生就修改了A的工资为2000，导 致A再读自己的工资时工资变为 2000；这就是不可重复读。 例2（同样的条件, 第1次和第2次读出来的记录数不一样 ）：假某工资单表中工资大于3000的有4人，事务1读取了所有工资大于3000的人，共查到4条记录，这时事务2 又插入了一条工资大于3000的记录，事务1再次读取时查到的记录就变为了5条，这样就导致了幻读。 事务隔离级别SQL 标准定义了四个隔离级别： READ-UNCOMMITTED(读取未提交)： 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读。——即对应解决丢失修改 READ-COMMITTED(读取已提交)： 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生。——对应解决脏读 REPEATABLE-READ(可重复读)： 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。——对应解决不可重复读、脏读 SERIALIZABLE(可串行化)： 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。——对应解决脏读、不可重复读、幻读 下表为各个隔离级别可以预防的问题: 隔离行为(级别) 丢失修改 脏读 不可重复读 幻读 READ-UNCOMMITTED √ READ-COMMITTED √ √ REPEATABLE-READ √ √ √ SERIALIZABLE √ √ √ √ MySQL InnoDB 存储引擎的默认支持的隔离级别是REPEATABLE-READ（可重读）。我们可以通过SELECT @@tx_isolation;命令来查看1234567mysql&gt; SELECT @@tx_isolation;+-----------------+| @@tx_isolation |+-----------------+| REPEATABLE-READ |+-----------------+1 row in set (0.00 sec) 这里需要注意的是：与 SQL 标准不同的地方在于InnoDB 存储引擎在 REPEATABLE-READ（可重读） 事务隔离级别下使用的是Next-Key Lock 锁算法，因此可以避免幻读的产生，这与其他数据库系统(如 SQL Server)是不同的。所以说InnoDB 存储引擎的默认支持的隔离级别是 REPEATABLE-READ（可重读） 已经可以完全保证事务的隔离性要求，即达到了SQL标准的SERIALIZABLE(可串行化)隔离级别。 因为隔离级别越低，事务请求的锁越少，所以大部分数据库系统的隔离级别都是READ-COMMITTED(读取提交内容):，但是你要知道的是InnoDB 存储引擎默认使用 REPEATABLE-READ（可重读）并不会有任何性能损失。 InnoDB 存储引擎在分布式事务 的情况下一般会用到SERIALIZABLE(可串行化) 隔离级别。 实际情况演示在下面我会使用 2 个命令行mysql ，模拟多线程（多事务）对同一份数据的脏读问题。 MySQL 命令行的默认配置中事务都是自动提交的，即执行SQL语句后就会马上执行 COMMIT 操作。如果要显式地开启一个事务需要使用命令：START TARNSACTION。 我们可以通过下面的命令来设置隔离级别。 1SET [SESSION|GLOBAL] TRANSACTION ISOLATION LEVEL [READ UNCOMMITTED|READ COMMITTED|REPEATABLE READ|SERIALIZABLE] 我们再来看一下我们在下面实际操作中使用到的一些并发控制语句: START TARNSACTION |BEGIN：显式地开启一个事务。 COMMIT：提交事务，使得对数据库做的所有修改成为永久性。 ROLLBACK：回滚会结束用户的事务，并撤销正在进行的所有未提交的修改。 脏读(读未提交) 避免脏读(读已提交) 不可重复读还是刚才上面的读已提交的图，虽然避免了读未提交，但是却出现了，一个事务还没有结束，就发生了 不可重复读问题。 可重复读 防止幻读(可重复读)一个事务对数据库进行操作，这种操作的范围是数据库的全部行，然后第二个事务也在对这个数据库操作，这种操作可以是插入一行记录或删除一行记录，那么第一个是事务就会觉得自己出现了幻觉，怎么还有没有处理的记录呢? 或者 怎么多处理了一行记录呢? 幻读和不可重复读有些相似之处 ，但是不可重复读的重点是修改，幻读的重点在于新增或者删除。 参考 https://github.com/Snailclimb/JavaGuide/blob/master/docs/database/%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB(%E5%9B%BE%E6%96%87%E8%AF%A6%E8%A7%A3).md 《JAVA JDK8 学习笔记》——林信良 著]]></content>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈Java代理(Proxy)]]></title>
    <url>%2F2019%2F05%2F15%2F%E6%B5%85%E8%B0%88Java%E4%BB%A3%E7%90%86-Proxy%2F</url>
    <content type="text"><![CDATA[代理模式 利用代理可以在运行时创建一个实现了一组给定接口的新类。这种功能只有在编译时无法确定需要实现那个接口时才有必要使用。 ——摘自《JAVA核心技术卷I》 何时使用代理？ 假设有一个表示接口的Class对象(有可能只包含一个接口),它的确切类型在编译时无法知道。这确实有些难度。想要构造一个实现这些接口的类,就需要使用newInstance方法或反射找出这个类的构造器。但是，不能实例化一个接口，需要在程序处于运行状态时定义一个新类。为了解决这个问题,有些程序将会生成代码；将这西代码放置在一个文件中；调用编译器；然后再加载结果类文件。很自然，这样做的速度会比较慢，并且需要将编译器与程序放在一起。而代理机制则是一种更好的解决方案。代理类可以在运行时创建全新的类。这样的代理类能够实现指定的接口。 ——摘自《JAVA核心技术卷I》 代理模式中主要有三个要素: Subject(被代理的接口).声明了代理对象和真实对象的公共接口.即定义了程序要实现哪些行为. RealSubject(真正被代理的对象）.即真实对象,是代理对象所表示的真实对象,也就是最终被引用的对象.真实对象继承于接口,真正提供一些服务. Proxy(代理).可以看做代理对象,当访问真实对象时,需要通过代理去访问。 也即: 用户只关心接口功能，而不在乎谁提供了功能。上图中接口是 Subject。 接口真正实现者是上图的 RealSubject，但是它不与用户直接接触，而是通过代理。 代理就是上图中的 Proxy，由于它实现了 Subject 接口，所以它能够直接与用户接触。 用户调用 Proxy 的时候，Proxy 内部调用了 RealSubject。所以，Proxy 是中介者，它可以增强 RealSubject 操作。 代理和真实对象必须继承同一个接口. 代理对象必须包含真实的对象. 如果难于理解的话，我用事例说明好了。值得注意的是，代理可以分为静态代理和动态代理两种。先从静态代理讲起。 静态代理电影开始前会播放一些广告,电影是电影公司委托给影院进行播放的，但是影院可以在播放电影的时候，产生一些自己的经济收益，比如卖爆米花、可乐等，然后在影片开始结束时播放一些广告。 现在用代码来进行模拟。根据第一节中介绍的,代理模式要有三个要素:Subject、RealSubject、Proxy. 首先得有一个接口，通用的接口是代理模式实现的基础。这个接口我们命名为 Movie，代表电影播放的能力。123public interface Movie &#123; void play();&#125; 然后要有一个真正的实现这个 Movie 接口的类.这个类表示真正的影片。它实现了 Movie 接口，当play() 方法调用时，影片就开始播放。123456public class RealMovie implements Movie &#123; @Override public void play() &#123; System.out.println("You are watching Avengers4"); &#125;&#125; 然后需要一个实现接口的代理类。Cinema 就是 Proxy 代理对象，前面提到代理对象必须包含真实的对象，Cinema对象中就有一个RealMovie对象。它有一个 play() 方法。不过调用 play() 方法时，它进行了一些相关利益的处理，那就是广告。现在，我们编写测试代码。123456789101112131415161718192021222324252627public class Cinema implements Movie &#123; private RealMovie movie; public Cinema(RealMovie movie)&#123; super(); this.movie = movie; &#125; @Override public void play() &#123; playAdvertisement(true); movie.play(); playAdvertisement(false); &#125; private void playAdvertisement(boolean isStart) &#123; if( isStart )&#123; System.out.println("The film will be on at once.This is an advertisement!"); &#125; else&#123; System.out.println("The film is over.This is an advertisement!"); &#125; &#125;&#125; 测试代码:123456789public class ProxyTest &#123; public static void main(String[] args)&#123; RealMovie realmovie = new RealMovie(); Movie movie = new Cinema(realmovie); movie.play(); &#125;&#125; 测试结果:123The film will be on at once.This is an advertisement!You are watching Avengers4The film is over.This is an advertisement! 代理模式可以在不修改被代理对象的基础上，通过扩展代理类，进行一些功能的附加与增强。值得注意的是，代理类和被代理类应该共同实现一个接口，或者是共同继承某个类。 上面介绍的是静态代理的一个示例，为什么叫做静态呢？因为它的类型是事先预定好的，比如上面代码中的 Cinema 这个类。下面要介绍的内容就是动态代理。 动态代理动态代理的代理类并不需要在Java代码中定义，而是在运行时动态生成的。相比于静态代理， 动态代理的优势在于可以很方便的对代理类的函数进行统一的处理，而不用修改每个代理类的函数。通过使用动态代理，可以做一个“统一指示”，对所有代理类的方法进行统一处理，而不用逐一修改每个方法。 与静态代理相比，抽象角色、真实角色都没有变化。变化的只有代理类。 上一节代码中 Cinema 类是代理，需要手动编写代码让 Cinema 实现 Movie 接口，而在动态代理中，我们可以让程序在运行的时候自动在内存中创建一个实现 Movie 接口的代理，而不需要去定义 Cinema 这个类。这就是它被称为动态的原因。 还是使用上面的例子 Subject:123public interface Movie &#123; void play();&#125; RealSubject,我们播放的电影为《复仇者联盟4》:123456public class Avengers4 implements Movie &#123; @Override public void play() &#123; System.out.println("You are watching Avengers4"); &#125;&#125; 关键的不同在于代理类的实现,在使用动态代理时，需要定义一个位于代理类与委托类之间的中介类，也叫动态代理类，这个类被要求实现InvocationHandler接口.:12345678910111213141516171819import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;public class Cinema implements InvocationHandler &#123; private Object moive; public Cinema(Object moive)&#123; this.moive = moive; &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println("This is "+ this.getClass().getSimpleName() +".The film will be on at once."); method.invoke(moive,args); System.out.println("The film is over."); return null; &#125;&#125; 当调用代理类对象的方法时，这个“调用”会转送到中介类的invoke方法中，参数method标识了具体调用的是代理类的哪个方法，args为这个方法的参数。 进行测试:1234567891011121314151617import java.lang.reflect.InvocationHandler;import java.lang.reflect.Proxy;public class ProxyTest &#123; public static void main(String[] args)&#123; //要代理的真实对象 RealMovie realmovie = new RealMovie(); //创建中介类实例 InvocationHandler cinema = new Cinema(realmovie); //动态产生一个代理类 Movie dynamicProxyOfMovie = (Movie) Proxy.newProxyInstance(realmovie.getClass().getClassLoader(), realmovie.getClass().getInterfaces(), cinema); //通过代理类，执行doSomething方法 dynamicProxyOfMovie.play(); &#125;&#125; 测试结果:123This is Cinema.The film will be on at once.You are watching Avengers4The film is over. 在上面动态代理的例子中,并没有像静态代理那样为 Moive 接口实现一个代理类，但最终它仍然实现了相同的功能，这其中的差别，就是之前讨论的动态代理所谓“动态”的原因。一个典型的动态代理可分为以下四个步骤： 创建抽象角色(Movie) 创建真实角色(Avengers4) 通过实现InvocationHandler接口创建中介类(Cinema) 通过场景类，动态生成代理类 动态代理语法Proxy动态代理对象,使用Proxy的静态方法newProxyInstance方法。 1234public static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) throws IllegalArgumentException 这个方法有三个参数: 一个类加载器(class loader). 一个Class对象数组,每个元素都是需要实现的接口. 一个调用处理器. InvocationHandlerInvocationHandler 是一个接口，官方文档解释说，每个代理的实例都有一个与之关联的 InvocationHandler 实现类，如果代理的方法被调用，那么代理便会通知和转发给内部的 InvocationHandler 实现类，由它决定处理。 12345public interface InvocationHandler &#123; public Object invoke(Object proxy, Method method, Object[] args) throws Throwable;&#125; invoke方法中的三个参数: proxy 代理对象.即我们上面的RealMovie method 代理对象调用的方法 args 调用的方法中的参数 无论何时调用代理对象的方法,调用处理器的invoke方法都会被调用,并向其传递Method对象和原始的调用参数。调用处理器必须给出处理调用的方式。 ——摘自《JAVA核心技术卷I》 正是这个方法决定了怎么样处理代理传递过来的方法调用。因为 Proxy 动态产生的代理会调用 InvocationHandler 实现类，所以 InvocationHandler 是实际执行者。 那么实际生活中,电影院中不仅播放《复仇者联盟4》,还会播放其他电影。比如我们播放《黑客帝国(The Matrix)》 创建类TheMatrix:123456public class TheMatrix implements Movie &#123; @Override public void play() &#123; System.out.println("You are watching The Matrix."); &#125;&#125; 那么电影院同样播放电影《黑客帝国》,它也在同一个电影院播放。1234567891011121314151617181920212223import java.lang.reflect.InvocationHandler;import java.lang.reflect.Proxy;public class ProxyTest &#123; public static void main(String[] args)&#123; Avengers4 avengers4 = new Avengers4(); TheMatrix theMatrix = new TheMatrix(); InvocationHandler cinemaWanDa = new Cinema(avengers4); InvocationHandler cinemaWanDa1 = new Cinema(theMatrix); Movie dynamicProxyAvengers = (Movie) Proxy.newProxyInstance(avengers4.getClass().getClassLoader(), avengers4.getClass().getInterfaces(), cinemaWanDa); Movie dynamicProxyTheMatrix = (Movie) Proxy.newProxyInstance(theMatrix.getClass().getClassLoader(), theMatrix.getClass().getInterfaces(), cinemaWanDa1); dynamicProxyAvengers.play(); dynamicProxyTheMatrix.play(); &#125;&#125; 运行结果:123456This is Cinema.The film will be on at once.You are watching Avengers4The film is over.This is Cinema.The film will be on at once.You are watching The Matrix.The film is over. 假设此时正值世界杯期间，电影院开通了观看球赛的服务。首先和电影一样,创建一个球赛的接口(Subject).123public interface WorldCup &#123; void watchTheMatch();&#125; 这时候需要一个RealSubject，假设我们观看法国和西班牙的比赛:123456public class FrenchVSSpain implements WorldCup &#123; @Override public void watchTheMatch() &#123; System.out.println("You are watching French VS Spain"); &#125;&#125; 然后进行测试:1234567891011121314151617181920212223242526272829303132import java.lang.reflect.InvocationHandler;import java.lang.reflect.Proxy;public class ProxyTest &#123; public static void main(String[] args)&#123; Avengers4 avengers4 = new Avengers4(); TheMatrix theMatrix = new TheMatrix(); FrenchVSSpain frenchVSSpain = new FrenchVSSpain(); InvocationHandler cinemaWanDa = new Cinema(avengers4); InvocationHandler cinemaWanDa1 = new Cinema(theMatrix); InvocationHandler cinemaWanda2 = new Cinema(frenchVSSpain); Movie dynamicProxyAvengers = (Movie) Proxy.newProxyInstance(avengers4.getClass().getClassLoader(), avengers4.getClass().getInterfaces(), cinemaWanDa); Movie dynamicProxyTheMatrix = (Movie) Proxy.newProxyInstance(theMatrix.getClass().getClassLoader(), theMatrix.getClass().getInterfaces(), cinemaWanDa1); dynamicProxyAvengers.play(); dynamicProxyTheMatrix.play(); WorldCup dynamicProxyFVSS = (WorldCup) Proxy.newProxyInstance(frenchVSSpain.getClass().getClassLoader(), frenchVSSpain.getClass().getInterfaces(), cinemaWanda2); dynamicProxyFVSS.watchTheMatch(); &#125;&#125; 测试结果:123456789This is Cinema.The film will be on at once.You are watching Avengers4The film is over.This is Cinema.The film will be on at once.You are watching The Matrix.The film is over.This is Cinema.The film will be on at once.You are watching French VS SpainThe film is over. 同样是通过 Proxy.newProxyInstance() 方法，却产生了 Movie 和 WorldCup 两种接口的实现类代理，这就是动态代理的强大之处。 动态代理的秘密Proxy 能够动态产生不同接口类型的代理，是通过传递的接口，然后通过反射动态生成了一个接口实例。 Proxy的全部源码查看点击这里 newProxyInstance()方法:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) throws IllegalArgumentException &#123; Objects.requireNonNull(h); final Class&lt;?&gt;[] intfs = interfaces.clone(); final SecurityManager sm = System.getSecurityManager(); if (sm != null) &#123; checkProxyAccess(Reflection.getCallerClass(), loader, intfs); &#125; /* * Look up or generate the designated proxy class. */ Class&lt;?&gt; cl = getProxyClass0(loader, intfs); /* * Invoke its constructor with the designated invocation handler. */ try &#123; if (sm != null) &#123; checkNewProxyPermission(Reflection.getCallerClass(), cl); &#125; final Constructor&lt;?&gt; cons = cl.getConstructor(constructorParams); final InvocationHandler ih = h; if (!Modifier.isPublic(cl.getModifiers())) &#123; AccessController.doPrivileged(new PrivilegedAction&lt;Void&gt;() &#123; public Void run() &#123; cons.setAccessible(true); return null; &#125; &#125;); &#125; return cons.newInstance(new Object[]&#123;h&#125;); &#125; catch (IllegalAccessException|InstantiationException e) &#123; throw new InternalError(e.toString(), e); &#125; catch (InvocationTargetException e) &#123; Throwable t = e.getCause(); if (t instanceof RuntimeException) &#123; throw (RuntimeException) t; &#125; else &#123; throw new InternalError(t.toString(), t); &#125; &#125; catch (NoSuchMethodException e) &#123; throw new InternalError(e.toString(), e); &#125; &#125; Proxy类的newProxyInstance方法，主要业务逻辑如下：123456//生成代理类class，并加载到jvm中Class&lt;?&gt; cl = getProxyClass0(loader, interfaces);//获取代理类参数为InvocationHandler的构造函数final Constructor&lt;?&gt; cons = cl.getConstructor(constructorParams);//生成代理类，并返回return cons.newInstance(new Object[]&#123;h&#125;); 上面代码做了三件事： 根据传入的参数interfaces动态生成一个类，它实现interfaces中的接口，该例中即Movie接口的play方法。假设动态生成的类为$Proxy0。 通过传入的classloder,将刚生成的$Proxy0类加载到jvm中。 利用中介类，调用 $Proxy0的$Proxy0(InvocationHandler)构造函数，创建$Proxy0类的实例，其InvocationHandler属性，为我们创建的中介类。 newProxyInstance 方法创建了一个实例，它是通过 cl 这个 Class 文件的构造方法反射生成。cl 由 getProxyClass0() 方法获取。上面的核心，就在于getProxyClass0方法：1234567891011private static Class&lt;?&gt; getProxyClass0(ClassLoader loader, Class&lt;?&gt;... interfaces) &#123; if (interfaces.length &gt; 65535) &#123; throw new IllegalArgumentException("interface limit exceeded"); &#125; // If the proxy class defined by the given loader implementing // the given interfaces exists, this will simply return the cached copy; // otherwise, it will create the proxy class via the ProxyClassFactory return proxyClassCache.get(loader, interfaces); &#125; 在Proxy类中有个属性proxyClassCache，这是一个WeakCache类型的静态变量。它指示了类加载器和代理类之间的映射。所以proxyClassCache的get方法用于根据类加载器来获取Proxy类，如果已经存在则直接从cache中返回，如果没有则创建一个映射并更新cache表。 直接通过缓存获取，如果获取不到，注释说会通过 ProxyClassFactory 生成。(这里就不贴ProxyClassFactory方法的全部源码,可以去上面的连接查看) ProxyClassFactory的部分源码:123456789101112131415161718192021222324252627282930313233343536373839/** * A factory function that generates, defines and returns the proxy class given * the ClassLoader and array of interfaces. */ private static final class ProxyClassFactory implements BiFunction&lt;ClassLoader, Class&lt;?&gt;[], Class&lt;?&gt;&gt; &#123; // prefix for all proxy class names private static final String proxyClassNamePrefix = "$Proxy"; ... /* * Choose a name for the proxy class to generate. */ long num = nextUniqueNumber.getAndIncrement(); String proxyName = proxyPkg + proxyClassNamePrefix + num; ... /* * Generate the specified proxy class. */ byte[] proxyClassFile = ProxyGenerator.generateProxyClass( proxyName, interfaces, accessFlags); try &#123; return defineClass0(loader, proxyName, proxyClassFile, 0, proxyClassFile.length); &#125; catch (ClassFormatError e) &#123; /* * A ClassFormatError here means that (barring bugs in the * proxy class generation code) there was some other * invalid aspect of the arguments supplied to the proxy * class creation (such as virtual machine limitations * exceeded). */ throw new IllegalArgumentException(e.toString()); &#125; &#125; 这个类的注释说，通过指定的 ClassLoader 和 接口数组 用工厂方法生成 proxy class。 然后这个 proxy class 的名字是：包名+$Proxy+id序号 跟一下代理类的创建流程：调用Factory类的get方法，而它又调用了ProxyClassFactory类的apply方法，最终找到下面一行代码：12byte[] proxyClassFile = ProxyGenerator.generateProxyClass( proxyName, interfaces, accessFlags); 就是它，生成了代理类。 下面检测检测一下动态生成的代理类的名字是不是包名+$Proxy+id序号,在测试类中添加如下代码:123456789101112import java.lang.reflect.InvocationHandler;import java.lang.reflect.Proxy;public class ProxyTest &#123; public static void main(String[] args)&#123; ... System.out.println("dynamicProxyAvengers class name:"+dynamicProxyAvengers.getClass().getName()); System.out.println("dynamicProxyTheMatrix class name:"+dynamicProxyTheMatrix.getClass().getName()); System.out.println("dynamicProxyFVSS class name:"+dynamicProxyFVSS.getClass().getName()); &#125;&#125; 测试结果:123456789101112This is Cinema.The film will be on at once.You are watching Avengers4The film is over.This is Cinema.The film will be on at once.You are watching The Matrix.The film is over.This is Cinema.The film will be on at once.You are watching French VS SpainThe film is over.dynamicProxyAvengers class name:com.sun.proxy.$Proxy0dynamicProxyTheMatrix class name:com.sun.proxy.$Proxy0dynamicProxyFVSS class name:com.sun.proxy.$Proxy1 Movie 接口的代理类名是：com.sun.proxy.$Proxy0WorldCup 接口的代理类名是：com.sun.proxy.$Proxy1这说明动态生成的 proxy class 与 Proxy 这个类同一个包。 图片来自 https://blog.csdn.net/briblue/article/details/73928350 红框中 $Proxy0就是通过 Proxy 动态生成的。$Proxy0实现了要代理的接口。$Proxy0通过调用 InvocationHandler来执行任务。 代理的作用主要作用，还是在不修改被代理对象的源码上，进行功能的增强。 这在 AOP 面向切面编程领域经常见。 代理类是在程序运行过程中创建。然而一旦被创建,就变成了常规类，与虚拟机中的任何其他类没有什么区别。所有的代理类都拓展于Proxy类。一个代理类只有一个实例域——调用处理器，它定义在Proxy的超类中。为了履行代理对象的职责，所需要的任何附加数据都必须存储在调用处理器中。 例如上面我们代理Avengers4对象时,Cinema包装了实际的对象。(因为电影要在电影院播放)。 没有定义代理类的名字,Sun虚拟机中的Proxy类将生成一个以字符串$Proxy开头的类名。对于特定的类加载器和预设的一组接口来说,只能有一个代理类。也就是说,如果使用同一个类加载器和接口数组调用两次newProxyInstance方法的话,那么只能够得到同一个类的两个对象,也可以利用getProxyClass方法获得这个类:1Class proxyClass = Proxy.getProxyClass(null,interfaces); 代理类一定是public和final。如果代理类实现的所有接口都是pubic,代理类就不属于某个特定的包;否则,所有非公有的接口都必须属于同一个包,同时,代理类也属于这个包。 总结上面的例子可以用下图来概括。 代理分为静态代理和动态代理两种。 静态代理，代理类需要自己编写代码写成。 动态代理，代理类通过 Proxy.newInstance() 方法生成。不管是静态代理还是动态代理，代理与被代理者都要实现两样接口，它们的实质是面向接口编程。 静态代理和动态代理的区别是在于要不要开发者自己定义 Proxy 类。 动态代理通过 Proxy 动态生成 proxy class，但是它也指定了一个 InvocationHandler 的实现类。 代理模式本质上的目的是为了增强现有代码的功能。 参考 《JAVA核心技术卷I》 轻松学，Java 中的代理模式及动态代理 Java8-Source-Code Java三种代理模式：静态代理、动态代理和cglib代理 说说JAVA代理模式]]></content>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初识Maven]]></title>
    <url>%2F2019%2F05%2F12%2F%E5%88%9D%E8%AF%86Maven%2F</url>
    <content type="text"><![CDATA[Maven项目对象模型(POM)，可以通过一小段描述信息来管理项目的构建，报告和文档的项目管理工具软件。Maven 除了以程序构建能力为特色之外，还提供高级项目管理工具。由于 Maven 的缺省构建规则有较高的可重用性，所以常常用两三行 Maven 构建脚本就可以构建简单的项目。由于 Maven 的面向项目的方法，许多 Apache Jakarta 项目发文时使用 Maven，而且公司项目采用 Maven 的比例在持续增长。 安装Maven首先去官网下载对应的版本. 然后设置环境变量 先定义Maven的路径: 在path中添加环境变量 检验是否添加成功: 使用MavenMaven 目录结构Maven定义了一个标准的目录结构。12345678910- src - main - java - resources - webapp - test - java - resources- target 并不是所有的maven都要使用上面的目录,上面的目录中有一些事可选的.一个最简单的目录应该是这样的:12345- src - main - java - test - java 下面以构建一个简单的maven项目为例,我构建的一个名为maven01的项目目录结构如下: 然后创建两个类,两个类代码如下:1234567package io.github.homxuwang.maven01.model;public class HelloWorld &#123; public String sayHello() &#123; return "Hello World!"; &#125;&#125; 1234567891011package io.github.homxuwang.maven01.model;import org.junit.*;import org.junit.Assert.*;public class HelloWorldTest &#123; @Test public void testHello() &#123; Assert.assertEquals("Hello World!",new HelloWorld().sayHello()); &#125;&#125; 并根据其package创建对应目录: 编写pom.xml文件为maven01项目编写pom.xml文件,pom.xml文件与src文件夹同级. 1234567891011121314151617181920&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;!-- groupId是项目的包名 --&gt; &lt;groupId&gt;io.github.homxuwang.maven01&lt;/groupId&gt; &lt;!-- 模块名 --&gt; &lt;artifactId&gt;maven01-model&lt;/artifactId&gt; &lt;version&gt;0.0.1SNAPSHOT&lt;/version&gt;&lt;!-- 项目的依赖 --&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.10&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 这便是最基本的pom.xml文件.&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;是固定的.&lt;groupId&gt;&lt;/groupId&gt;是项目的包名.&lt;artifactId&gt;&lt;/artifactId&gt;为模块名.&lt;version&gt;&lt;/version&gt;则是定义发布的版本.&lt;dependencies&gt;&lt;/dependencies&gt;中存放相关的依赖包,因为上面的例子中使用了junit依赖,所以这里进行相关设置,junit的版本为4.10. 可以根据上面的例子查看. 使用mvn命令进行编译等操作进入到项目目录(我的为maven01),输入1mvn compile 进行编译,第一次编译时要下载相应依赖,所以有可能会很慢: 使用1mvn test 进行测试. 可以看到测试结果中,Tests run:1,即成功执行了一个测试. 在src目录下,生成了一个target文件夹, target文件夹内容如下:classes文件夹内就是编译后生成的字节码文件.surefire-reports是相应的测试报告. 接着使用1mvn package 对程序进行打包 可以看到target文件夹下生成了一个jar包,其文件名就是在pom.xml中定义的 &lt;artifactId&gt;maven01-model&lt;/artifactId&gt;+&lt;version&gt;0.0.1SNAPSHOT&lt;/version&gt;的组合. 这便是maven最基本的操作 maven拓展学习maven常用命令 mvn -v 查看版本 mvn compile 编译 mvn test 测试 mvn package 打包 mvn clean 删除target文件夹 mvn install 安装jar包到本地仓库中如,再别的项目中如果想要使用maven01的话,那么就得先将maven01安装到本地仓库.然后再新项目的dependencies标签中添加maven01的相关信息.如:12345678910111213141516...&lt;!-- 项目的依赖 --&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.10&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.github.homxuwang.maven01&lt;/groupId&gt; &lt;artifactId&gt;maven01-model&lt;/artifactId&gt; &lt;version&gt;0.0.1SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;... 其实在加载依赖时,maven首先会扫描本地仓库是否有符合的依赖,如果有则加载,如果没有会到网上的maven中央仓库中进行下载,并放到本地仓库中以供使用. 使用archetype插件自动建立目录骨架在最开始的例子中,我们要手动创建相应的符合maven开发的目录.而使用archetype插件可以自动生成一个符合maven开发的目录结构.maven规定主代码的目录为:src:main:java:主代码测试目录为:src:test:测试代码 在相应的项目目录下使用1mvn archetype:generate 然后按照提示进行选择,之后会自动创建对应目录. 第一次使用时会下载一些文件,可能时间会较长. 当然也可以一次性输入命令:1mvn archetype:generate -DgroupId=[groupId] -DartifactId=[artifactId] -Dversion=[version] -Dpackage=[代码所在的包名] maven中的坐标和仓库maven中的123&lt;groupId&gt;xxx&lt;/groupId&gt;&lt;artifactId&gt;xxx&lt;/artifactId&gt;&lt;version&gt;xxx&lt;/version&gt; 可以组成项目的一个基本坐标,任何的构建使用这个坐标进行唯一标识一般groupId标签使用[公司网址的反写.项目名]artifactId使用[项目名-模块名]version则是对应的版本号 而很多的构件都是存放在仓库中的.仓库分为本地仓库和远程仓库.首先maven会在本地仓库找相应的构件,如果找不到则会到远程仓库进行寻找.如果在找不到则会报错. 默认的全球中央仓库的地址是https://repo.maven.apache.org/maven2这里包含了大多数的开源项目. 可以在https://search.maven.org/搜索相应的包. 当然国内也有很多镜像仓库.通过修改[你的maven目录]\conf\settings.xml文件,找到mirrors标签,进行相关镜像仓库的设置。其中给出了设置镜像仓库的格式:123456&lt;mirror&gt; &lt;id&gt;mirrorId&lt;/id&gt; &lt;mirrorOf&gt;repositoryId&lt;/mirrorOf&gt; &lt;name&gt;Human Readable Name for this Mirror.&lt;/name&gt; &lt;url&gt;http://my.repository.com/repo/path&lt;/url&gt;&lt;/mirror&gt; 国内的两个常用的Maven仓库地址: 开源中国 123456&lt;mirror&gt; &lt;id&gt;nexus-osc&lt;/id&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt; &lt;name&gt;Nexus osc&lt;/name&gt; &lt;url&gt;http://maven.oschina.net/content/groups/public/&lt;/url&gt;&lt;/mirror&gt; 阿里 123456&lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt;&lt;/mirror&gt; 更改仓库的位置:maven从远程仓库下载的构件,默认存放在C:\Users\[用户名]\.m2\repository中 更改本地仓库的默认路径,依然要修改[你的maven目录]\conf\settings.xml文件,找到localRepository标签进行修改,注意目录间要使用/1&lt;localRepository&gt;/path/to/local/repo&lt;/localRepository&gt; 可以将settings.xml文件复制一份到新的仓库文件夹中,这样如果以后修改maven版本就不用再修改settings文件了. maven的生命周期完整的项目构建过程包括:清理-&gt;编译-&gt;测试-&gt;打包-&gt;集成测试-&gt;验证-&gt;部署 maven的三套生命周期: clean 清理项目 default 构建项目 site 生成项目站点 三套生命周期之间的关系：较之于生命周期阶段的前后依赖关系，三套生命周期本身是相互独立的，用户可以仅仅调用clean生命周期的某个阶段，或者仅仅调用default生命周期的某个阶段，而不会对其他生命周期产生任何影响。例如，当用户调用clean生命周期的clean阶段的时候，不会触发default生命周期的任何阶段，反之亦然，当用户调用default生命周期的compile阶段的时候，也不会触发clean生命周期的任何阶段。而当我们执行package时，会自动执行compile和test. clean生命周期 clean生命周期的目的是清理项目，它包含三个阶段： pre-clean执行一些清理前需要完成的工作。 clean清理上一次构建生成的文件。 post-clean执行一些清理后需要完成的工作。 default生命周期(核心)default 生命周期定义了真正构建时所需要执行的所有步骤，它是所有生命周期中最核心的部分，常用的几个阶段有compile,test,package,install.其包含的阶段如下，这里只对重要的阶段进行解释： validate initialize generate-sources process-sources处理项目主资源文件。一般来说，是对src/main/resources目录的内容进行变量替换等工作后，复制到项目输出的主classpath目录中。 generate-resources process-resourcescompile编译项目的主源码。一般来说，是编译src/main/java目录下的Java文件至项目输出的主classpath目录中。 process-classes generate-test-sources process-test-sources处理项目测试资源文件。一般来说，是对src/test/resources目录的内容进行变量替换等工作后，复制到项目输出的测试classpath目录中。 generate-test-resources process-test-resources test-compile编译项目的测试代码。一般来说，是编译src/test/java目录下的Java文件至项目输出的测试classpath目录中。 process-test-classestest使用单元测试框架运行测试，测试代码不会被打包或部署。 prepare-packagepackage接受编译好的代码，打包成可发布的格式，如JAR。 pre-integration-test integration-test post-integration-test verify install将包安装到Maven本地仓库，供本地其他Maven项目使用。 deploy将最终的包复制到远程仓库，供其他开发人员和Maven项目使用。 site生命周期site生命周期的目的是建立和发布项目站点，Maven能够基于POM所包含的信息，自动生成一个友好的站点，方便团队交流和发布项目信息。该生命周期包含如下阶段： pre-site执行一些在生成项目站点之前需要完成的工作。 site生成项目站点文档。 post-site执行一些在生成项目站点之后需要完成的工作。 site-deploy将生成的项目站点发布到服务器上。 maven的插件http://maven.apache.org/plugins 可以查看maven提供的插件以使用source插件为例.这个插件可以将项目源码进行打包. 在pom.xml中绑定插件1234567891011121314151617181920212223242526&lt;project&gt; ... &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;!-- groupId artifactId version 是maven的坐标 --&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-source-plugin&lt;/artifactId&gt; &lt;version&gt;2.4&lt;/version&gt; &lt;!-- 绑定source到default生命周期的package阶段 这样在执行package命令时就可以将源码进行打包 --&gt; &lt;executions&gt; &lt;execution&gt; &lt;!-- 指定阶段 --&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;jar-no-fork&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; ...&lt;/project&gt; 注：指定source的目标(即)&lt;goals&gt;&lt;goal&gt;jar-no-fork&lt;/goal&gt;&lt;/goals&gt;时,可在 http://maven.apache.org/plugins/maven-source-plugin/ 进行查看 pom.xml解析project为根标签,不用多说modelVersion指定了当前pom的版本 坐标信息基本坐标信息由groupId artifactId version packaging构成 groupId标签定义当前的Maven属于哪个实际的项目。maven项目和实际的项目不是一一对应关系,maven项目提现的是模块化概念,因此一个实际项目往往被划分为很多的模块. artifactId标签表示模块的标识,表示实际项目中的一个模块.一般用[项目名+模块名]进行标识 version标签标识版本号.第一个0表示大版本号,第二个0表示分支版本号,第三个0表示小版本号.snapshot-快照,alpha-内部测试,beta-公测,Release-稳定,GA-正式发布 packaging标签表示maven项目的打包方式.默认为jar,也可以指定为war,zip,pom等 其他的一些描述信息: name标签表示项目描述名 url标签表示项目地址 description标签表示项目的描述信息 developers标签表示开发人员列表 licenses标签表示许可信息 organization标签表示组织信息 依赖列表dependencies依赖列表放在dependencies标签中，其下可以包含多个依赖项dependency.依赖项里面如何确定一个依赖所在的位置呢？其实也是使用坐标,即groupId artifactId version这三个在上面的坐标信息中有介绍,还可以有: type标签 scope标签表示依赖范围.比如junit依赖,scope标签其中填写test，表示junit只在test的依赖范围内有用. optional标签表示设置以来是否可选,有两个值true,false,默认是false.如果为false则子项默认是继承的,否则要显示引入该依赖 exclusions标签表示排除依赖传递列表。其子标签为exclusion表示不需要使用到的依赖. 依赖管理dependencyManagement依赖管理放在dependencyManagement标签中.其中也可以声明依赖列表,包含多个依赖,但她不会被引用到依赖中，不会实际运行. 12345&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt;&lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 为构建行为提供支持build在上面介绍过build,为构建项目提供一些支持.其根标签为build,常用的一个字标签为plugins即插件列表,在其下定义plugin,plugin依然要提供插件的坐标. 123456789&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;&lt;/groupId&gt; &lt;artifactId&gt;&lt;/artifactId&gt; &lt;version&gt;&lt;/version&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 继承父模块parentparent通常表示在子模块中对父模块pom的继承 对多个模块进行编译modules可以在modules下指定多个模块module. 依赖范围在上一节中简单介绍了scope标签的一些内容.在开发时,如果要用到某个jar包,就要将其引入到项目的classpath构件中,这样项目就能用该框架.maven中提供了三种classpath:编译、测试、运行.所以这里的依赖范围就是用来控制依赖与三种classpath的关系的.主要有6个Dependency Scope compile provided runtime test system import 在这里查看提供的scope值,并有具体描述. 依赖传递比如C依赖B,而B依赖A.在A打包并install到本地仓库后(保证B添加依赖后能正确编译),在B中添加对A的依赖,即在B的pom.xml中添加依赖:12345678910&lt;project&gt;... &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;A&lt;/groupId&gt; &lt;artifactId&gt;A&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 对B进行编译和打包(使得C可以添加对B的依赖,否则会报错). 然后让C依赖于B,即在C的pom.xml中添加依赖:12345678910&lt;project&gt;... &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;B&lt;/groupId&gt; &lt;artifactId&gt;B&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 然后对C进行编译,可以编译成功. 在对C编译成功后,C的maven依赖有A和B,虽然在对C进行添加依赖时只添加了B,但是由于依赖的传递性,A也会成为C的依赖. 排除依赖如果C想排除A的依赖,要用到exclusions标签,如下:12345678910111213141516&lt;project&gt;... &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;B&lt;/groupId&gt; &lt;artifactId&gt;B&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;!-- declare the exclusion here --&gt; &lt;groupId&gt;A&lt;/groupId&gt; &lt;artifactId&gt;A/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 这样就可以去除A的依赖了 依赖冲突假设A和B依赖了不同版本的相同构件,对于依赖于A、B的C来说,它究竟依赖A和B的哪一个版本的构件.这里有两个原则： 短路优先假设A-&gt;B-&gt;C-&gt;X(jar)A-&gt;D-&gt;X(jar)那么A会依赖路径较短的版本 先声明路径优先如果路径长度相同,则谁先声明,先解析谁 聚合和继承 聚合在依赖传递中看到,如果要使用A和B，需要分别对A和B打包编译install，有一种方法可以将多个项目进行install,这个方式称为聚合.进行聚合需要新建一个maven项目,这个项目与几个要打包的项目在同一目录(当然也可以不在同一目录,只是在module中要寻找相应的路径).在pom.xml中,将坐标信息的packaging标签修改为pom.可以将这个pom看成一个pom容器. 然后使用modules标签中的module找到要打包的多个项目:12345678910111213&lt;project&gt;... &lt;groupId&gt;io.github.homxuwang&lt;/groupId&gt; &lt;artifactId&gt;mavenStudy-model&lt;/artifactId&gt; &lt;version&gt;0.0.1SNAPSHOT&lt;/version&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;modules&gt; &lt;module&gt;../A&lt;/module&gt; &lt;module&gt;../B&lt;/module&gt; &lt;module&gt;../C&lt;/module&gt; &lt;/modules&gt;&lt;/project&gt; 然后对这个项目进行install时,会构建modules中的三个项目. 继承假设A,B,C三个项目都用到了junit依赖,那么它们就有了重复的配置.在maven中可以将共同的特性封装成一个父类，使用dependencyManagement(使用这个标签,它的内容不会在项目中运行).在父类pom.xml文件中,(这里假设新建一个名为Parent的新的maven项目,让ABC三个项目进行继承). 1234567891011121314151617181920&lt;project&gt;... &lt;groupId&gt;io.github.homxuwang&lt;/groupId&gt; &lt;artifactId&gt;mavenStudy-model&lt;/artifactId&gt; &lt;version&gt;0.0.1SNAPSHOT&lt;/version&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;properties&gt; &lt;junit.version&gt;3.8.1&lt;/junit.version&gt; &lt;/properties&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;$&#123;junit.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt;&lt;/project&gt; 上面的pom.xml中使用了&lt;properties&gt;标签,定义了junit的版本,然后在dependency中的version标签使用${}对其进行引用. 对于父亲pom，它的main目录和test目录是没有作用的,可以删除. 在子类pom.xml中继承父类的pom,以C为例:1234567891011121314151617181920&lt;project&gt;... &lt;groupId&gt;C&lt;/groupId&gt; &lt;artifactId&gt;C&lt;/artifactId&gt; &lt;version&gt;0.0.1&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;!-- 使用parent标签,其中写父亲pom的坐标 --&gt; &lt;parent&gt; &lt;!-- 父pom的坐标 --&gt; &lt;groupId&gt;io.github.homxuwang&lt;/groupId&gt; &lt;artifactId&gt;mavenStudy-model&lt;/artifactId&gt; &lt;version&gt;0.0.1SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/project&gt; 可以看到,dependency中junit的版本号(version标签)已经删除了,但是由于它集成了父pom,其中定义了junit的版本,所以是正确的. 参考https://www.imooc.com/video/8644 https://blog.csdn.net/wangdong5678999/article/details/72848044]]></content>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅析Socket编程]]></title>
    <url>%2F2019%2F05%2F11%2F%E6%B5%85%E6%9E%90Socket%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[在浅谈TCP协议中提到过：在本地两个进程间进行通信时，可以使用唯一标识符PID。但是如果是两台不同计算机之间的进程进行通信时,只用PID是不够的,这时就在传输层中使用协议端口号,简称端口。IP可以唯一标识主机，而TCP协议和端口号可以唯一标识一个主机中的进程。这样可以用三元组(IP地址+协议+端口号)唯一标识一个网络中的进程。 在唯一标识了一个网络中的进程后,便可以用socket进行编程了。 Socket简介Socket与TCP/IP协议没有必然的联系,Socket是对TCP/IP协议的封装，Socket本身并不是协议，而是一个调用接口(API)。 Socket通常也称作”套接字”，用于描述IP地址和端口，是一个通信链的句柄。网络上的两个程序通过一个双向的通讯连接实现数据的交换，这个双向链路的一端称为一个Socket，一个Socket由一个IP地址和一个端口号唯一确定。应用程序通常通过”套接字”向网络发出请求或者应答网络请求。 从设计模式的角度看来，Socket其实就是一个门面模式，它把复杂的TCP/IP协议族隐藏在Socket接口后面，对用户来说，一组简单的接口就是全部，让Socket去组织数据，以符合指定的协议。 Socket 起源于 Unix ，Unix/Linux 基本哲学之一就是“一切皆文件”，都可以用“打开(open) –&gt; 读写(write/read) –&gt; 关闭(close)”模式来进行操作。因此 Socket 也被处理为一种特殊的文件。 使用TCP协议通信的Socket的通信流程如下图： 服务器根据地址类型（ipv4,ipv6）、socket类型、协议创建socket 服务器为socket绑定ip地址和端口号 服务器socket监听端口号请求，随时准备接收客户端发来的连接，这时候服务器的socket并没有被打开 客户端创建socket 客户端打开socket，根据服务器ip地址和端口号试图连接服务器socket 服务器socket接收到客户端socket请求，被动打开，开始接收客户端请求，直到客户端返回连接信息。这时候socket进入阻塞状态，所谓阻塞即接收方法一直到客户端返回连接信息后才返回，开始接收下一个客户端请求。 客户端连接成功，向服务器发送连接状态信息 服务器接收方法返回，连接成功 客户端向socket写入信息 服务器读取信息 客户端关闭 服务器端关闭 简单的Socket编程客户端想服务器发送一个字符串,服务器接收到字符串后打印,然后向客户端返回字符串的长度,最后,客户端输出服务器端返回的该字符串的长度,分别用TCP和UDP两种方式实现. 结合上一节内容,流程概括为: socket() 创建套接字 bind() 分配套接字地址 listen() 等待连接请求 accept() 允许连接请求 read()/write() 数据交换 close() 关闭连接 在项目中创建以下结构: TCP先以TCP为例.TCPClient表示TCP客户端,TCPServer表示TCP服务端,LengthCaculator是一个单独的处理线程,对数据的业务逻辑处理交给它来实现. TCPServer:123456789101112131415161718package io.github.homxuwang.socket;import java.net.ServerSocket;import java.net.Socket;public class TCPServer &#123; public static void main(String[] args) throws Exception &#123; //创建Socket,并将socket绑定到5202端口 ServerSocket ss = new ServerSocket(5202); //死循环,使得socket一直等待并处理客户端发送过来的请求 while(true) &#123; //监听5202端口,知道客户端返回信息后才返回 Socket socket = ss.accept(); //获取客户端的请求信息后,执行相关业务逻辑 new LengthCalculator(socket).start(); &#125; &#125;&#125; TCPClient:1234567891011121314151617181920212223242526272829package io.github.homxuwang.socket;import java.io.InputStream;import java.io.OutputStream;import java.net.Socket;public class TCPClient &#123; public static void main(String[] args) throws Exception&#123; //创建socket,并指定连接的是本机的端口号为5202的服务器socket Socket socket = new Socket("127.0.0.1",5202); //获取输出流 OutputStream os = socket.getOutputStream(); //获取输出流 InputStream is = socket.getInputStream(); //将要传递给server的字符串参数转换成byte数组,并将数组写入到输出流中 os.write(new String("Hello Socket").getBytes()); int ch = 0; byte[] buff = new byte[1024]; //buff 主要用来读取输入的内容,存成byte数组,ch主要用来获取读取数组的长度 ch = is.read(buff); //将接收流的byte数组转换成字符串,这里是从服务端发回来的字符串参数的长度 String content = new String (buff,0,ch); System.out.println(content); //关闭流和socket os.close(); is.close(); socket.close(); &#125;&#125; LengthCaculator:123456789101112131415161718192021222324252627282930313233343536373839package io.github.homxuwang.socket;import java.io.InputStream;import java.io.OutputStream;import java.net.Socket;public class LengthCalculator extends Thread &#123; //以socket为成员变量 private Socket socket; public LengthCalculator(Socket socket) &#123; this.socket = socket; &#125; @Override public void run() &#123; try&#123; //获取socket的输出流 OutputStream os = socket.getOutputStream(); //获取socket的输入流 InputStream is = socket.getInputStream(); int ch = 0; byte[] buff = new byte[1024]; //buff主要用来读取输入的内容,存成byte数组,主要用来获取读取数组的长度 ch = is.read(buff); //将接收流的byte数组转换成字符串,这里获取的内容是客户端发送来的字符串参数 String content = new String(buff,0,ch); System.out.println(content); //向输出流里写入获得的字符串长度,回发给客户端 os.write(String.valueOf(content.length()).getBytes()); //关闭输入输出流和socket os.close(); is.close(); socket.close(); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; &#125;&#125; 进行测试时,先运行Server,使得其能够进行监听,然后运行Client,打印结果如下: UDPUDPServer.java:123456789101112131415161718192021222324252627package io.github.homxuwang.socket;import java.net.DatagramPacket;import java.net.DatagramSocket;public class UDPServer &#123; public static void main(String[] args) throws Exception &#123; //服务端接受客户端发送的数据报 DatagramSocket socket = new DatagramSocket(52021); //监听的端口号 byte[] buff = new byte[100]; //存储从客户端接受到的内容 DatagramPacket packet = new DatagramPacket(buff,buff.length); //接受客户端发送过来的内容,并将内容封装进DatagramPacket对象中 socket.receive(packet); byte[] data = packet.getData(); //从DatagramPacket对象中获取到真正存储的数据 //将数据从二进制转换成字符串形式 String content = new String(buff,0,packet.getLength()); System.out.println(content); //将要发送给客户端的数据转换成二进制 byte[] sendedContent = String.valueOf(content.length()).getBytes(); //服务端给客户端发送数据报 //从DatagramPacket对象中获取到数据的来源地址与端口号 DatagramPacket packetToClient = new DatagramPacket(sendedContent, sendedContent.length,packet.getAddress(),packet.getPort()); socket.send(packetToClient); //发送数据给客户端 &#125;&#125; UDPServer.java:123456789101112131415161718192021222324252627282930package io.github.homxuwang.socket;import java.net.DatagramPacket;import java.net.DatagramSocket;import java.net.InetAddress;public class UDPClient &#123; public static void main(String[] args) throws Exception&#123; //客户端发送数据报给服务端 DatagramSocket socket = new DatagramSocket(); //要发给服务端的数据 byte[] buf = "Hello Socket".getBytes(); //将IP封装成InetAddress对象 InetAddress address = InetAddress.getByName("127.0.0.1"); //将要发送给服务端的数据封装成DatagramPacket对象 需要填写上ip地址与端口号 DatagramPacket packet = new DatagramPacket(buf,buf.length,address,52021); //发送数据给服务端 socket.send(packet); //客户端接受服务端发送来的数据报文 byte[] data = new byte[100]; //创建DatagramPacket对象用来存储服务端发送来的数据 DatagramPacket receivedPacket = new DatagramPacket(data,data.length); //将收到的数据存储到DatagramPacket对象中 socket.receive(receivedPacket); //将服务端发送来的数据取出并打印 String content = new String(receivedPacket.getData(),0,receivedPacket.getLength()); System.out.println(content); &#125;&#125; 进行测试时,先运行Server,使得其能够进行监听,然后运行Client,打印结果如下: 参考https://hit-alibaba.github.io/interview/basic/network/Socket-Programming-Basic.html https://github.com/halfrost/Halfrost-Field/blob/master/contents/iOS/WebSocket/iOS_WebSocket.md https://broqiang.com/posts/waht-is-socket https://blog.csdn.net/ns_code/article/details/14105457]]></content>
      <tags>
        <tag>java基础</tag>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[转]JavaScript原型和原型链]]></title>
    <url>%2F2019%2F05%2F10%2F%E8%BD%AC-JavaScript%E5%8E%9F%E5%9E%8B%E5%92%8C%E5%8E%9F%E5%9E%8B%E9%93%BE%2F</url>
    <content type="text"><![CDATA[本文转自: https://github.com/mqyqingfeng/Blog/issues/2 构造函数创建对象我们先使用构造函数创建一个对象：123456function Person() &#123;&#125;var person = new Person();person.name = 'Kevin';console.log(person.name) // Kevin 在这个例子中，Person 就是一个构造函数，我们使用 new 创建了一个实例对象 person。 很简单吧，接下来进入正题： prototype每个函数都有一个 prototype 属性，就是我们经常在各种例子中看到的那个 prototype ，比如：12345678910function Person() &#123;&#125;// 虽然写在注释里，但是你要注意：// prototype是函数才会有的属性Person.prototype.name = 'Kevin';var person1 = new Person();var person2 = new Person();console.log(person1.name) // Kevinconsole.log(person2.name) // Kevin 那这个函数的 prototype 属性到底指向的是什么呢？是这个函数的原型吗？ 其实，函数的 prototype 属性指向了一个对象，这个对象正是调用该构造函数而创建的实例的原型，也就是这个例子中的 person1 和 person2 的原型。 那什么是原型呢？你可以这样理解：每一个JavaScript对象(null除外)在创建的时候就会与之关联另一个对象，这个对象就是我们所说的原型，每一个对象都会从原型”继承”属性。 让我们用一张图表示构造函数和实例原型之间的关系： 在这张图中我们用 Object.prototype 表示实例原型。 那么我们该怎么表示实例与实例原型，也就是 person 和 Person.prototype 之间的关系呢，这时候我们就要讲到第二个属性： __proto__这是每一个JavaScript对象(除了 null )都具有的一个属性，叫proto，这个属性会指向该对象的原型。 为了证明这一点,我们可以在火狐或者谷歌中输入：12345function Person() &#123;&#125;var person = new Person();console.log(person.__proto__ === Person.prototype); // true 于是我们更新下关系图：既然实例对象和构造函数都可以指向原型，那么原型是否有属性指向构造函数或者实例呢？ constructor指向实例倒是没有，因为一个构造函数可以生成多个实例，但是原型指向构造函数倒是有的，这就要讲到第三个属性：constructor，每个原型都有一个 constructor 属性指向关联的构造函数。 为了验证这一点，我们可以尝试：1234function Person() &#123;&#125;console.log(Person === Person.prototype.constructor); // true 所以再更新下关系图：综上我们已经得出：12345678910function Person() &#123;&#125;var person = new Person();console.log(person.__proto__ == Person.prototype) // trueconsole.log(Person.prototype.constructor == Person) // true// 顺便学习一个ES5的方法,可以获得对象的原型console.log(Object.getPrototypeOf(person) === Person.prototype) // true 了解了构造函数、实例原型、和实例之间的关系，接下来我们讲讲实例和原型的关系： 实例与原型当读取实例的属性时，如果找不到，就会查找与对象关联的原型中的属性，如果还查不到，就去找原型的原型，一直找到最顶层为止。 举个例子：12345678910111213function Person() &#123;&#125;Person.prototype.name = 'Kevin';var person = new Person();person.name = 'Daisy';console.log(person.name) // Daisydelete person.name;console.log(person.name) // Kevin 在这个例子中，我们给实例对象 person 添加了 name 属性，当我们打印 person.name 的时候，结果自然为 Daisy。 但是当我们删除了 person 的 name 属性时，读取 person.name，从 person 对象中找不到 name 属性就会从 person 的原型也就是 person.proto ，也就是 Person.prototype中查找，幸运的是我们找到了 name 属性，结果为 Kevin。 但是万一还没有找到呢？原型的原型又是什么呢？ 原型的原型在前面，我们已经讲了原型也是一个对象，既然是对象，我们就可以用最原始的方式创建它，那就是：123var obj = new Object();obj.name = 'Kevin'console.log(obj.name) // Kevin 其实原型对象就是通过 Object 构造函数生成的，结合之前所讲，实例的 proto 指向构造函数的 prototype ，所以我们再更新下关系图： 原型链那 Object.prototype 的原型呢？ null，我们可以打印：1console.log(Object.prototype.__proto__ === null) // true 然而 null 究竟代表了什么呢？ 引用阮一峰老师的 《undefined与null的区别》 就是： null 表示“没有对象”，即该处不应该有值。 所以 Object.prototype.proto 的值为 null 跟 Object.prototype 没有原型，其实表达了一个意思。 所以查找属性的时候查到 Object.prototype 就可以停止查找了。 最后一张关系图也可以更新为：顺便还要说一下，图中由相互关联的原型组成的链状结构就是原型链，也就是蓝色的这条线。 补充最后，补充三点大家可能不会注意的地方： constructor首先是 constructor 属性，我们看个例子：12345function Person() &#123;&#125;var person = new Person();console.log(person.constructor === Person); // true 当获取 person.constructor 时，其实 person 中并没有 constructor 属性,当不能读取到constructor 属性时，会从 person 的原型也就是 Person.prototype 中读取，正好原型中有该属性，所以：1person.constructor === Person.prototype.constructor __proto__其次是 proto ，绝大部分浏览器都支持这个非标准的方法访问原型，然而它并不存在于 Person.prototype 中，实际上，它是来自于 Object.prototype ，与其说是一个属性，不如说是一个 getter/setter，当使用 obj.proto 时，可以理解成返回了 Object.getPrototypeOf(obj)。 真的是继承吗？最后是关于继承，前面我们讲到“每一个对象都会从原型‘继承’属性”，实际上，继承是一个十分具有迷惑性的说法，引用《你不知道的JavaScript》中的话，就是： 继承意味着复制操作，然而 JavaScript 默认并不会复制对象的属性，相反，JavaScript 只是在两个对象之间创建一个关联，这样，一个对象就可以通过委托访问另一个对象的属性和函数，所以与其叫继承，委托的说法反而更准确些。 总结构造函数的prototype属性指向原型,实例的proto属性指向原型,原型链通过proto链接起来 参考 本文转自 https://github.com/mqyqingfeng/Blog/issues/2]]></content>
      <tags>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows下开启和使用mysql的binlog]]></title>
    <url>%2F2019%2F05%2F09%2Fwindows%E4%B8%8B%E5%BC%80%E5%90%AF%E5%92%8C%E4%BD%BF%E7%94%A8mysql%E7%9A%84binlog%2F</url>
    <content type="text"><![CDATA[Mysql Binlog是二进制格式的日志文件。Binlog是用来记录Mysql内部对数据库的改动（只记录对数据的修改操作），主要用于数据库的主从复制以及增量恢复。 binlog作用MySQL的作用类似于Oracle的归档日志，可以用来查看数据库的变更历史（具体的时间点所有的SQL操作）、数据库增量备份和恢复（增量备份和基于时间点的恢复）、Mysql的复制（主主数据库的复制、主从数据库的复制）。 开启binlog首先找到my.ini文件. 在# Binary Logging部分添加以下内容:12log-bin=mysql-binbinlog-format=Row 如下图所示: 修改并保存,然后重启mysql服务: 查看是否已经开始binlog,分别使用命令:12show variables like &apos;log_bin&apos;; show binary logs; binlog文件的默认目录(相对于my.ini)为./Data: 可以在配置中使用log-bin=路径名改变binlog的位置,如1log-bin=D:\logbin logbin的使用首先创建测试数据库:12mysql&gt; create database logbintest;Query OK, 1 row affected (0.01 sec) 切换数据库:12mysql&gt; use logbintest;Database changed 创建表:12mysql&gt; create table t1(id int,name varchar(20));Query OK, 0 rows affected (0.03 sec) 插入数据:12345mysql&gt; insert into t1 values(1,'hm');Query OK, 1 row affected (0.02 sec)mysql&gt; insert into t1 values(2,'xx');Query OK, 1 row affected (0.00 sec) 执行flush logs;命令:12mysql&gt; flush logs;Query OK, 0 rows affected (0.02 sec) 这时目录下多了一个log-bin文件: 可以使用show master status查看当前数据库binary log的位置1234567mysql&gt; show master status;+------------------+----------+--------------+------------------+-------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+------------------+----------+--------------+------------------+-------------------+| mysql-bin.000002 | 154 | | | |+------------------+----------+--------------+------------------+-------------------+1 row in set (0.00 sec) 继续输入sql命令:12345mysql&gt; insert into t1 values(3,'test');Query OK, 1 row affected (0.01 sec)mysql&gt; insert into t1 values(4,'hello');Query OK, 1 row affected (0.01 sec) 再次执行flush logs;命令和show master status命令:12345678910mysql&gt; flush logs;Query OK, 0 rows affected (0.02 sec)mysql&gt; show master status;+------------------+----------+--------------+------------------+-------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+------------------+----------+--------------+------------------+-------------------+| mysql-bin.000003 | 154 | | | |+------------------+----------+--------------+------------------+-------------------+1 row in set (0.00 sec) 接着输入sql命令:12345678mysql&gt; insert into t1 values(5,'hey');Query OK, 1 row affected (0.01 sec)mysql&gt; drop table t1;Query OK, 0 rows affected (0.01 sec)mysql&gt; drop database logbintest;Query OK, 0 rows affected (0.01 sec) 经过以上操作后,文件夹中有了3个binlog文件,*.index是索引文件. 可以用mysqlbinlog 工具来恢复数据。为了下面讲解的方便，我们先将binlog文件解析成txt文件，在mysql-bin文件目录下执行命令,如下：12345C:\ProgramData\MySQL\MySQL Server 5.7\Data&gt;mysqlbinlog mysql-bin.000001 &gt; D:/1.txtC:\ProgramData\MySQL\MySQL Server 5.7\Data&gt;mysqlbinlog mysql-bin.000002 &gt; D:/2.txtC:\ProgramData\MySQL\MySQL Server 5.7\Data&gt;mysqlbinlog mysql-bin.000003 &gt; D:/3.txt 通过这三个命令，可以在D盘下生成3个文件，里面分别记录了日志文件的内容，也就是用户操作的步骤。 三个文件内容见附录。 下面开始恢复binlog日志到Mysql数据库，这里只将第1个日志文件恢复。12C:\ProgramData\MySQL\MySQL Server 5.7\Data&gt;mysqlbinlog mysql-bin.000001 | mysql -u[数据库用户名] -p[数据库密码]mysql: [Warning] Using a password on the command line interface can be insecure. 然后查看是否恢复成功:12345678mysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || logbintest |+--------------------+2 rows in set (0.01 sec) 123456789101112131415161718mysql&gt; use logbintest;Database changedmysql&gt; show tables;+----------------------+| Tables_in_logbintest |+----------------------+| t1 |+----------------------+1 row in set (0.00 sec)mysql&gt; select * from t1;+------+------+| id | name |+------+------+| 1 | hm || 2 | xx |+------+------+2 rows in set (0.01 sec) 这时候恢复到相应的备份位置了. 接着恢复第三个备份:12C:\ProgramData\MySQL\MySQL Server 5.7\Data&gt;mysqlbinlog mysql-bin.000003 | mysql -u[用户名] -p[密码]mysql: [Warning] Using a password on the command line interface can be insecure. 查看数据:12345678910mysql&gt; select * from t1;+------+-------+| id | name |+------+-------+| 1 | hm || 2 | xx || 3 | test || 4 | hello |+------+-------+4 rows in set (0.00 sec) 注意:直接恢复mysql-bin.000002会提示Table &#39;logbintest.t1&#39; doesn&#39;t exist.我猜想因为mysql-bin.000002文件中不包含创建数据库和表的命令,所以只能在恢复mysql-bin.000001,即恢复了相应数据库和表格的情况下,才能继续恢复数据.123C:\ProgramData\MySQL\MySQL Server 5.7\Data&gt;mysqlbinlog mysql-bin.000002 | mysql -u[用户名] -p[密码]mysql: [Warning] Using a password on the command line interface can be insecure.ERROR 1146 (42S02) at line 35: Table &apos;logbintest.t1&apos; doesn&apos;t exist 参考https://baike.baidu.com/item/Mysql%20Binlog https://www.cnblogs.com/wangwust/p/6433453.html https://blog.csdn.net/king_kgh/article/details/74890381 https://www.iteblog.com/mysql-binlog_basic_usage/ 附录1.txt12345678/*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=1*/;/*!50003 SET @OLD_COMPLETION_TYPE=@@COMPLETION_TYPE,COMPLETION_TYPE=0*/;DELIMITER /*!*/;SET @@SESSION.GTID_NEXT= &apos;AUTOMATIC&apos; /* added by mysqlbinlog */ /*!*/;DELIMITER ;# End of log file/*!50003 SET COMPLETION_TYPE=@OLD_COMPLETION_TYPE*/;/*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=0*/; 2.txt1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768/*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=1*/;/*!50003 SET @OLD_COMPLETION_TYPE=@@COMPLETION_TYPE,COMPLETION_TYPE=0*/;DELIMITER /*!*/;# at 4#190509 16:40:28 server id 1 end_log_pos 123 CRC32 0xd5ca9838 Start: binlog v 4, server v 5.7.17-log created 190509 16:40:28BINLOG &apos;fOfTXA8BAAAAdwAAAHsAAAAAAAQANS43LjE3LWxvZwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEzgNAAgAEgAEBAQEEgAAXwAEGggAAAAICAgCAAAACgoKKioAEjQAATiYytU=&apos;/*!*/;# at 123#190509 16:40:28 server id 1 end_log_pos 154 CRC32 0xcbc8683a Previous-GTIDs# [empty]# at 154#190509 16:48:03 server id 1 end_log_pos 219 CRC32 0x66c5aab9 Anonymous_GTID last_committed=0 sequence_number=1SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;/*!*/;# at 219#190509 16:48:03 server id 1 end_log_pos 297 CRC32 0x7c1a0b04 Query thread_id=3 exec_time=0 error_code=0SET TIMESTAMP=1557391683/*!*/;SET @@session.pseudo_thread_id=3/*!*/;SET @@session.foreign_key_checks=1, @@session.sql_auto_is_null=0, @@session.unique_checks=1, @@session.autocommit=1/*!*/;SET @@session.sql_mode=1344274432/*!*/;SET @@session.auto_increment_increment=1, @@session.auto_increment_offset=1/*!*/;/*!\C gbk *//*!*/;SET @@session.character_set_client=28,@@session.collation_connection=28,@@session.collation_server=33/*!*/;SET @@session.lc_time_names=0/*!*/;SET @@session.collation_database=DEFAULT/*!*/;BEGIN/*!*/;# at 297#190509 16:48:03 server id 1 end_log_pos 351 CRC32 0x6c102375 Table_map: `logbintest`.`t1` mapped to number 328# at 351#190509 16:48:03 server id 1 end_log_pos 396 CRC32 0xec3029be Write_rows: table id 328 flags: STMT_END_FBINLOG &apos;Q+nTXBMBAAAANgAAAF8BAAAAAEgBAAAAAAEACmxvZ2JpbnRlc3QAAnQxAAIDDwI8AAN1IxBsQ+nTXB4BAAAALQAAAIwBAAAAAEgBAAAAAAEAAgAC//wDAAAABHRlc3S+KTDs&apos;/*!*/;# at 396#190509 16:48:03 server id 1 end_log_pos 427 CRC32 0x57760240 Xid = 16COMMIT/*!*/;# at 427#190509 16:48:19 server id 1 end_log_pos 492 CRC32 0x985b3888 Anonymous_GTID last_committed=1 sequence_number=2SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;/*!*/;# at 492#190509 16:48:19 server id 1 end_log_pos 570 CRC32 0x602ea4d9 Query thread_id=3 exec_time=0 error_code=0SET TIMESTAMP=1557391699/*!*/;BEGIN/*!*/;# at 570#190509 16:48:19 server id 1 end_log_pos 624 CRC32 0x0a30970d Table_map: `logbintest`.`t1` mapped to number 328# at 624#190509 16:48:19 server id 1 end_log_pos 670 CRC32 0x30f313bc Write_rows: table id 328 flags: STMT_END_FBINLOG &apos;U+nTXBMBAAAANgAAAHACAAAAAEgBAAAAAAEACmxvZ2JpbnRlc3QAAnQxAAIDDwI8AAMNlzAKU+nTXB4BAAAALgAAAJ4CAAAAAEgBAAAAAAEAAgAC//wEAAAABWhlbGxvvBPzMA==&apos;/*!*/;# at 670#190509 16:48:19 server id 1 end_log_pos 701 CRC32 0xe612b29f Xid = 17COMMIT/*!*/;# at 701#190509 16:49:37 server id 1 end_log_pos 748 CRC32 0x8503b6e4 Rotate to mysql-bin.000003 pos: 4SET @@SESSION.GTID_NEXT= &apos;AUTOMATIC&apos; /* added by mysqlbinlog */ /*!*/;DELIMITER ;# End of log file/*!50003 SET COMPLETION_TYPE=@OLD_COMPLETION_TYPE*/;/*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=0*/; 3.txt12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364/*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=1*/;/*!50003 SET @OLD_COMPLETION_TYPE=@@COMPLETION_TYPE,COMPLETION_TYPE=0*/;DELIMITER /*!*/;# at 4#190509 16:49:37 server id 1 end_log_pos 123 CRC32 0x67bdb5fc Start: binlog v 4, server v 5.7.17-log created 190509 16:49:37# Warning: this binlog is either in use or was not closed properly.BINLOG &apos;oenTXA8BAAAAdwAAAHsAAAABAAQANS43LjE3LWxvZwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEzgNAAgAEgAEBAQEEgAAXwAEGggAAAAICAgCAAAACgoKKioAEjQAAfy1vWc=&apos;/*!*/;# at 123#190509 16:49:37 server id 1 end_log_pos 154 CRC32 0x0e916e92 Previous-GTIDs# [empty]# at 154#190509 16:50:51 server id 1 end_log_pos 219 CRC32 0x0fcc180d Anonymous_GTID last_committed=0 sequence_number=1SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;/*!*/;# at 219#190509 16:50:51 server id 1 end_log_pos 297 CRC32 0xd5da7fee Query thread_id=3 exec_time=0 error_code=0SET TIMESTAMP=1557391851/*!*/;SET @@session.pseudo_thread_id=3/*!*/;SET @@session.foreign_key_checks=1, @@session.sql_auto_is_null=0, @@session.unique_checks=1, @@session.autocommit=1/*!*/;SET @@session.sql_mode=1344274432/*!*/;SET @@session.auto_increment_increment=1, @@session.auto_increment_offset=1/*!*/;/*!\C gbk *//*!*/;SET @@session.character_set_client=28,@@session.collation_connection=28,@@session.collation_server=33/*!*/;SET @@session.lc_time_names=0/*!*/;SET @@session.collation_database=DEFAULT/*!*/;BEGIN/*!*/;# at 297#190509 16:50:51 server id 1 end_log_pos 351 CRC32 0x60f30968 Table_map: `logbintest`.`t1` mapped to number 328# at 351#190509 16:50:51 server id 1 end_log_pos 395 CRC32 0xace24246 Write_rows: table id 328 flags: STMT_END_FBINLOG &apos;6+nTXBMBAAAANgAAAF8BAAAAAEgBAAAAAAEACmxvZ2JpbnRlc3QAAnQxAAIDDwI8AANoCfNg6+nTXB4BAAAALAAAAIsBAAAAAEgBAAAAAAEAAgAC//wFAAAAA2hleUZC4qw=&apos;/*!*/;# at 395#190509 16:50:51 server id 1 end_log_pos 426 CRC32 0xae6e3d87 Xid = 20COMMIT/*!*/;# at 426#190509 16:51:36 server id 1 end_log_pos 491 CRC32 0xdb44b864 Anonymous_GTID last_committed=1 sequence_number=2SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;/*!*/;# at 491#190509 16:51:36 server id 1 end_log_pos 618 CRC32 0x6dec15c8 Query thread_id=3 exec_time=0 error_code=0use `logbintest`/*!*/;SET TIMESTAMP=1557391896/*!*/;DROP TABLE `t1` /* generated by server *//*!*/;# at 618#190509 16:52:34 server id 1 end_log_pos 683 CRC32 0xe3edccf9 Anonymous_GTID last_committed=2 sequence_number=3SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;/*!*/;# at 683#190509 16:52:34 server id 1 end_log_pos 780 CRC32 0xb8287261 Query thread_id=3 exec_time=0 error_code=0SET TIMESTAMP=1557391954/*!*/;drop database logbintest/*!*/;SET @@SESSION.GTID_NEXT= &apos;AUTOMATIC&apos; /* added by mysqlbinlog */ /*!*/;DELIMITER ;# End of log file/*!50003 SET COMPLETION_TYPE=@OLD_COMPLETION_TYPE*/;/*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=0*/;]]></content>
      <tags>
        <tag>数据库</tag>
        <tag>系统开发记录</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈HTTP/HTTPS协议]]></title>
    <url>%2F2019%2F05%2F08%2F%E6%B5%85%E8%B0%88HTTP%E5%8D%8F%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[HTTP即超文本传输协议 HTTP的特性 HTTP 协议构建于 TCP/IP 协议之上，是一个应用层协议，默认端口号是 80 HTTP 是基于请求与相应模式的无连接无状态的协议.无连接的目的是限制每次连接只处理一个请求,服务器处理完客户请求并受到客户应答后即断开连接,采用这种方式可以节省时间.HTTP1.1起默认使用长连接,即服务器要等待一定时间后才断开连接,以保证连接特性,虽然目前的一些技术(如keepAlive)使用长连接进行优化,但这些是属于HTTP请求之外的.也就是说,在每个独立的HTTP请求之中,用户无法知道当前的HTTP是否处于长连接的状态,所以用户始终都要认为HTTP在请求结束后就会关闭.至于下层实现是否在HTTP请求结束后关闭连接,都不会改变这个特性.长连接可以理解为下层实现对上层透明.无状态是指协议对于事务处理没有记忆能力,缺少状态意味着如果后续处理需要前面的信息,则必须被重传.可能导致每次连接传送的数据量增大,不过再服务器不需要先前信息时,它的应答就较快. HTTP 支持客户服务器模式.HTTP工作在Client、Server架构之上,浏览器作为HTTP客户端,通过URL向HTTP服务端(Web服务器)发送请求.Web服务器根据请求向客户端发送响应信息. HTTP 简单快速.客户端发送请求时只需发送请求方法和路径.由于HTTP协议较为简单，使HTTP程序规模小,因而通信速度很快 HTTP 较为灵活.HTTP可以传输任意类型的数据对象(使用ContentType标记). HTTP请求/响应HTTP请求结构 HTTP的请求报文主要由请求行、请求头部、空行、请求数据组成. 请求行主要包括:请求方法: 主要有GET、POST、PUT、DELETE等URL: 全称是资源描述符,即请求路径.一个URL地址，用于描述一个网络上的资源协议版本: HTTP版本(1.0、1.1) 请求头部主要包括:请求头部由若干个报头组成,每个报头的格式为头部字段名:值用来设置HTTP请求的一些参数.例如HOST,被请求数据的主机和端口号;ContentType等. 请求数据:请求数据就是数据体,它一般只在POST请求中用到,表示上传的数据 注意:请求头部后面的空行是必须的,用它来标识已经结束了头部信息的发送. 使用抓包工具查看一段HTTP请求的信息. 可以看到第一行请求方法为GET;接着空格;然后是URL;接着空格;然后是协议版本,为HTTP1.1;最后回车换行.第二部分就是一些请求头,包括Host、Connection等.请求头部分一般就是Host到Cookie部分.再往下就空出一行. GET请求和POST请求的区别 GET POST HTTP报文层面 请求信息放在URL 请求信息放在报文体 数据库层面 符合幂等性和安全性 不符合幂等性和安全性 其他层面 可以被缓存、被存储 不可以被缓存、被存取 GET的请求信息和URL之间以?隔开,请求信息的格式为键值对,可参考上图GET请求的路径.因为是通过URL,所以是有长度限制的(HTTP 协议规范没有对 URL 长度进行限制。这个限制是特定的浏览器及服务器对它的限制).POST将请求信息放在报文体中，想获得请求信息要解析报文,所以安全性较GET方式要高一些(其实要获得报文信息也很简单,所以二者安全性没有太大区别,如果要提高安全性最好使用HTTPS). 安全性意味着该操作用于获取信息而非修改信息。换句话说，GET请求一般不应产生副作用。就是说，它仅仅是获取资源信息，就像数据库查询一样，不会修改，增加数据，不会影响资源的状态。幂等性意味着对同一URL的多个请求应该返回同样的结果。 大部分的GET请求通常都被CDN缓存了,这样可以减小对Web服务器的压力.而POST是非幂等的操作,每次都要交由Web服务器处理. HTTP响应结构 HTTP 响应与 HTTP 请求相似，HTTP响应也由3个部分构成，分别是： 状态行 响应头(Response Header) 响应正文状态行由协议版本、数字形式的状态代码、及相应的状态描述，各元素之间以空格分隔。响应头是客户端要使用的一些附加信息.比如Date是响应生成的日期时间;Content-Type是对应的数据格式.响应体则是具体的数据.即Content的内容 常见的状态码有如下几种： 1xx 指示信息 表示请求已接收,继续处理 2xx 成功 表示请求已被成功接收、理解 200 OK 客户端请求成功 3xx 重定向 要完成请求必须进行更进一步操作 301 Moved Permanently 请求永久重定向 302 Moved Temporarily 请求临时重定向 304 Not Modified 文件未修改，可以直接使用缓存的文件。 4xx 客户端错误 请求有语法错误或者请求无法实现 400 Bad Request 由于客户端请求有语法错误，不能被服务器所理解。 401 Unauthorized 请求未经授权。这个状态代码必须和WWW-Authenticate报头域一起使用 403 Forbidden 服务器收到请求，但是拒绝提供服务。服务器通常会在响应正文中给出不提供服务的原因 404 Not Found 请求的资源不存在，例如，输入了错误的URL 5xx 服务器端错误 服务器未能实现合法的请求 500 Internal Server Error 服务器发生不可预期的错误，导致无法完成客户端的请求。 503 Service Unavailable 服务器当前不能够处理客户端的请求，在一段时间之后，服务器可能会恢复正常。 HTTP请求/响应的步骤 客户端连接到Web服务器.通常是浏览器客户端与Web服务器的HTTP端口(默认为80端口)建立TCP套接字(socket)连接. 发送HTTP请求.即客户端通过TCP套接字向服务器发送文本请求报文. 服务器接收请求并返回HTTP响应.Web服务器解析请求并定位请求资源,服务器将资源副本写入TCP套接字,由客户端读取. 释放TCP连接.若连接模式为CLOSE,则服务器主动关闭TCP连接,客户端被动关闭连接.若为KeepAlive,则该连接会保持一段时间,在该时间内可以继续接收请求. 客户端浏览器解析内容.客户端拿到数据后首先解析状态行,查看表明请求是否成功,然后解析每个响应头,对内容进行解析(HTML、json等) 浏览器键入URL,按下回车后经历的流程 1 DNS解析.浏览器根据URL逐层查询DNS服务器缓存,解析URL中域名对应的IP地址.DNS解析是一个递归查询的过程，从哪个缓存找到对应的IP后就直接返回,不再查询后面的缓存。DNS域名解析过程域名解析过程： . -&gt; .com -&gt; google.com. -&gt; www.google.com.DNS缓存从近到远依次是浏览器缓存:浏览器缓存DNS记录一段时间，操作系统并没有告诉浏览器每个DNS记录的生存时间，因此浏览器会将其缓存一段固定的时间（一般在2到30分钟之间）。系统缓存:如果浏览器缓存不包含所需的记录，则浏览器进行调用操作系统的缓存。路由器缓存:请求继续到路由器，路由器通常具有自己的DNS缓存。ips服务器缓存:检查的下一个位置是缓存ISP的DNS服务器。根域名服务器缓存顶级域名服务器缓存 2 TCP连接找到了IP地址后,会根据IP地址和对应端口(默认为80)与服务器建立TCP连接(三次握手). 3 发送HTTP请求浏览器向服务器发送HTTP请求 4 服务器处理请求并返回HTTP报文服务器收到浏览器的请求后返回对应的HTTP报文 5 浏览器解析返回的数据浏览器收到服务器返回的数据后,进行进一步操作(渲染页面等) 6 连接结束释放TCP连接(四次挥手) 其中5 6可以认为是同时发生的. Cookie和SessionCookieCookie是服务器发送给客户端的特殊信息,以文本的形式保存在客户端.当浏览器再请求该网站时，浏览器把请求的网址连同该Cookie一同提交给服务器,而这一次Cookie信息则存放在HTTP请求头中.服务器接收到后,解析该Cookie,得到客户端特有的信息,动态生成与客户端相对应的内容。服务器还可以根据需要修改Cookie的内容。 1、首先，客户端会发送一个http请求到服务器端。 2、 服务器端接受客户端请求后，发送一个http响应到客户端，这个响应头，其中就包含Set-Cookie头部。 3、在客户端发起的第二次请求（注意：如果服务器需要我们带上Cookie，我们就需要在B步骤上面拿到这个Cookie然后作为请求头一起发起第二次请求），提供给了服务器端可以用来唯一标识客户端身份的信息。这时，服务器端也就可以判断客户端是否启用了cookies。尽管，用户可能在和应用程序交互的过程中突然禁用cookies的使用，但是，这个情况基本是不太可能发生的，所以可以不加以考虑，这在实践中也被证明是对的。 客户端可以采用两种方式来保存这个 Cookie 对象，一种方式是保存在客户端内存中，称为临时 Cookie，浏览器关闭后这个 Cookie 对象将消失。另外一种方式是保存在客户机的磁盘上，称为永久 Cookie。以后客户端只要访问该网站，就会将这个 Cookie 再次发送到服务器上，前提是这个 Cookie 在有效期内，这样就实现了对客户的跟踪。 Cookie 是可以被客户端禁用的。 SessionSession是对于服务端来说的,服务器使用一种类似于散列表的结构来保存信息.当程序需要为某个客户端的请求创建一个Session时,服务器首先检查这个请求是否已包含一个Session标识,称为session id,如果已经包含此session id,说明以前为此客户端创建过session,服务器就根据session id将这个session检索出来进行使用.如果客户端请求没有session id,就会新建一个session,并且生成一个与此session相关的session id,session id将会在本次响应中回发给客户端进行保存. Session的实现方式 1.使用Cookie实现.服务器为每个Session提供一个唯一的JSESSIONID,并通过Cookie发送给客户端.当客户端发起新的请求时,将在Cookie头中携带这个JSESSIONID,这样服务器能够找到客户端对应的Session. 2.使用URL回写来实现.这种方法是指服务器在发送给浏览器页面的所有链接中,都携带JSESSIONID的参数,这样客户端点击任何一个链接都会把JSESSIONID带回服务器,如果直接在地址栏输入URL来请求资源,那么会匹配不到Session. 这两种方式的实现都和JSESSIONID有关.JSESSIONID维护了客户端和服务器之间请求和响应的映射关系. Cookie和Session的区别 Cookie Session 存放位置 客户浏览器 服务器 安全性 不是很安全 更安全 服务器压力 对服务器压力小 对服务器压力大 cookie不是很安全，别人可以分析存放在本地的cookie并进行cookie欺骗,考虑到安全应当使用session。 session会在一定时间内保存在服务器上。当访问增多，会比较占用你服务器的性能,考虑到减轻服务器性能方面，应当使用cookie。 单个cookie保存的数据不能超过4K，很多浏览器都限制一个站点最多保存20个cookie。 建议： 将登陆信息等重要信息存放为session 其他信息如果需要保留，可以放在cookie中 TokenToken 也称作令牌，由uid+time+sign[+固定参数]Token 的认证方式类似于临时的证书签名, 并且是一种服务端无状态的认证方式, 非常适合于 REST API 的场景. 所谓无状态就是服务端并不会保存身份认证相关的数据。 Token和Session有一定的类似，但是服务器不保存状态，而是生成一个Token保存在客户端，这个Token是加密并确保完整性和不变性的，也就是修改后无效的，所以是安全的，可以保存在客户端。 token的组成:uid: 用户唯一身份标识time: 当前时间的时间戳sign: 签名, 使用 hash/encrypt 压缩成定长的十六进制字符串，以防止第三方恶意拼接固定参数(可选): 将一些常用的固定参数加入到 token 中是为了避免重复查库 Token的认证流程:Token 的认证流程与cookie很相似 用户登录，成功后服务器返回Token给客户端 客户端收到数据后保存在客户端 客户端再次访问服务器，将token放入headers中 服务器端采用filter过滤器校验。校验成功则返回请求数据，校验失败则返回错误码 对于3,Token可以用url传参，也可以用post提交，也可以夹在http的header中。 token与session的流程对比:session:注册登录-&gt;服务端将user存入session-&gt;将sessionid存入浏览器的cookie-&gt;再次访问时根据cookie里的sessionid找到session里的usertoken:注册登录-&gt;服务端将生成一个token，并将token与user加密生成一个密文-&gt;将token+user+密文数据 返回给浏览器-&gt;再次访问时传递token+user+密文数据，后台会再次使用token+user生成新密文，与传递过来的密文比较，一致则正确。注：上文中得token里保存的用户信息，一般不会包含敏感信息。 Session与Token的异同点 Session的状态是存储在服务器端，客户端只有session id,session依赖cookie；而Token的状态是存储在客户端; Token和Session其实都是为了身份验证，Session一般翻译为会话，而token更多的时候是翻译为令牌； cookie + session在跨域场景表现并不好 作为身份认证 token 安全性比session好，因为每个请求都有签名还能防止监听以及重放攻击，而session就必须靠链路层来保障通讯安全了。基于 cookie 的机制很容易被 CSRF,Token可以抵抗CSRF 注:CSRF（Cross-site request forgery，跨站请求伪造）CSRF(XSRF) 顾名思义，是伪造请求，冒充用户在站内的正常操作。例如，一论坛网站的发贴是通过 GET 请求访问，点击发贴之后 JS 把发贴内容拼接成目标 URL 并访问： http://example.com/bbs/create_post.php?title=标题&amp;content=内容那么，我们只需要在论坛中发一帖，包含一链接： http://example.com/bbs/create_post.php?title=你好&amp;content=哈哈只要有用户点击了这个链接，那么他们的帐户就会在不知情的情况下发布了这一帖子。可能这只是个恶作剧，但是既然发贴的请求可以伪造，那么删帖、转帐、改密码、发邮件全都可以伪造。 HTTPS HTTPS(超文本传输安全协议) 即 HyperText Transfer Protocol Secure；常称为HTTP over TLS、HTTP over SSL或HTTP Secure，是一种在加密信道进行 HTTP 内容传输的协议。 TLS 的早期版本叫做 SSL(Security Sockets Layer,安全套接层)。SSL 的 1.0, 2.0, 3.0 版本均已经被废弃，出于安全问题考虑广大浏览器也不再对老旧的 SSL 版本进行支持了，因此这里我们就统一使用 TLS 名称了。 为网络通信提供安全及数据完整性的一种安全协议 是操作系统对外的API 采用身份认证和数据加密保证网络通信的安全和数据的完整性 加密方式: 对称加密：加密和解密都使用同一个密钥.该算法性能较非对称加密要高,但是安全性相对较弱. 非对称加密:加密使用的密钥和解密使用的密钥是不相同的.分别称为公钥和私钥,公钥和算法都是公开的,但是私钥是保密的.该算法性能较低,但是安全性强. 哈希算法:将任意长度的信息转换为固定长度的值,算法不可逆.常用的有MD5加密算法. 数字签名:在信息后面加上一段内容,证明某个消息或者文件是某人发出的实际应用中,仅使用其中一种加密方式不能满足生产要求,要么非对称加密性能过低,要么加密密钥容易泄露.因此HTTPS使用证书配合各种加密手段进行加密. TLS的基本过程如下:(参考what-happens-when-zh_CN) 客户端发送一个 ClientHello 消息到服务器端，消息中同时包含了它的 Transport Layer Security (TLS) 版本，可用的加密算法和压缩算法。 服务器端向客户端返回一个 ServerHello 消息，消息中包含了服务器端的 TLS 版本，服务器所选择的加密和压缩算法，以及数字证书认证机构（Certificate Authority，缩写 CA）签发的服务器公开证书，证书中包含了公钥。客户端会使用这个公钥加密接下来的握手过程，直到协商生成一个新的对称密钥。证书中还包含了该证书所应用的域名范围（Common Name，简称 CN），用于客户端验证身份。 客户端根据自己的信任 CA 列表，验证服务器端的证书是否可信。如果认为可信（具体的验证过程在下一节讲解），客户端会生成一串伪随机数，使用服务器的公钥加密它。这串随机数会被用于生成新的对称密钥 服务器端使用自己的私钥解密上面提到的随机数，然后使用这串随机数生成自己的对称主密钥 客户端发送一个 Finished 消息给服务器端，使用对称密钥加密这次通讯的一个散列值 服务器端生成自己的 hash 值，然后解密客户端发送来的信息，检查这两个值是否对应。如果对应，就向客户端发送一个 Finished 消息，也使用协商好的对称密钥加密 从现在开始，接下来整个 TLS 会话都使用对称秘钥进行加密，传输应用层（HTTP）内容从上面的过程可以看到，TLS 的完整过程需要三个算法（协议），密钥交互算法，对称加密算法，和消息认证算法（TLS 的传输会使用 MAC(message authentication code) 进行完整性检查）。 HTTPS在进行网络传输之前,会与网站服务器和Web浏览器进行一次握手,在握手时确认双方的加密密码信息.概括来说，HTTPS数据传输流程: 浏览器将支持的加密算法信息发送给服务器 服务器选择一套浏览器支持的加密算法和哈希算法,将验证身份的信息以证书形式回发给浏览器.证书信息中包含了证书发布的CA机构,证书的有效期,公钥,证书所有者,签名等. 浏览器验证证书合法性,并结合证书公钥加密信息发送给服务器 服务器接收到客户端的信息后,使用私钥解密信息,验证哈希值是否与Web浏览器一致,然后服务器加密响应消息回发给浏览器 最后,浏览器解密响应消息,并对消息进行验真,之后浏览器和服务器进行加密交换数据 使用抓包工具可以看到,HTTPS抓取到的数据都是经过加密的,而HTTP的数据是明文. HTTP和HTTPS的区别： HTTPS需要到CA申请证书,HTTP不需要 HTTPS密文传输,HTTP明文传输 连接方式不同,HTTPS默认使用443端口,HTTP默认使用80端口 HTTPS=HTTP+加密+认证+完整性保护,较HTTP安全 中间人攻击：HTTPS 的过程并不是密不透风的，HTTPS 有若干漏洞，给中间人攻击（Man In The Middle Attack，简称 MITM）提供了可能。 所谓中间人攻击，指攻击者与通讯的两端分别建立独立的联系，并交换其所收到的数据，使通讯的两端认为他们正在通过一个私密的连接与对方直接对话，但事实上整个会话都被攻击者完全控制。在中间人攻击中，攻击者可以拦截通讯双方的通话并插入新的内容。 HTTPS并不是真的安全:SSL剥离SSL 剥离即阻止用户使用 HTTPS 访问网站.浏览器会默认填充http://,请求需要进行跳转,这个访问完全是明文的，这就给了攻击者可乘之机,有被劫持的风险.通过攻击 DNS 响应，攻击者可以将自己变成中间人。 HSTS为了防止上面说的这种情况，一种叫做 HSTS 的技术被引入了。HSTS（HTTP Strict Transport Security）是用于强制浏览器使用 HTTPS 访问网站的一种机制。它的基本机制是在服务器返回的响应中，加上一个特殊的头部，指示浏览器对于此网站，强制使用 HTTPS 进行访问：1Strict-Transport-Security: max-age=31536000; includeSubdomains; preload 可以看到如果这个过期时间非常长，就是导致在很长一段时间内，浏览器都会强制使用 HTTPS 访问该网站。 HSTS 有一个很明显的缺点，是需要等待第一个服务器的影响中的头部才能生效，但如果第一次访问该网站就被攻击呢？为了解决这个问题，浏览器中会带上一些网站的域名，被称为 HSTS preload list。对于在这个 list 的网站来说，直接强制使用 HTTPS。 伪造证书攻击HSTS 只解决了 SSL 剥离的问题，然而即使在全程使用 HTTPS 的情况下，我们仍然有可能被监听。 假设我们想访问 www.google.com，但我们的 DNS 服务器被攻击了，指向的 IP 地址并非 Google 的服务器，而是攻击者的 IP。当攻击者的服务器也有合法的证书的时候，我们的浏览器就会认为对方是 Google 服务器，从而信任对方。这样，攻击者便可以监听我们和谷歌之前的所有通信了。 可以看到攻击者有两步需要操作，第一步是需要攻击 DNS 服务器。第二步是攻击者自己的证书需要被用户信任，这一步对于用户来说是很难控制的，需要证书颁发机构能够控制自己不滥发证书。 参考https://hit-alibaba.github.io/interview/basic/network/HTTP.html https://hit-alibaba.github.io/interview/basic/network/HTTPS.html https://zhuanlan.zhihu.com/p/33781119 http://blog.wizun.cn/2019/02/19/cookie-session/ https://juejin.im/post/59d1f59bf265da06700b0934 https://segmentfault.com/a/1190000017831088 https://www.zhihu.com/question/51759560/answer/332128488 http://www.intelligentunit.com/authorization-with-session-token-principle-and-compare/ https://blog.csdn.net/qq_35891226/article/details/79931210 https://juejin.im/post/5a437441f265da43294e54c3]]></content>
      <tags>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈TCP协议]]></title>
    <url>%2F2019%2F05%2F06%2F%E6%B5%85%E8%B0%88TCP%E5%8D%8F%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[TCP协议比较复杂，这里主要记录学习TCP过程中的一些要点。 目前有四层、五层、七层协议。 OSI开放式互联参考模型(概念化七层协议) 第一层(物理层)要解决两台物理机之间的通讯需求,机器A向机器B发送比特流,机器B能收到比特流。这是物理层要解决的问题。物理层定义了物理设备的标准,比如网线的类型、光纤接口类型、各种传输介质的传输速率。它的主要作用是传输比特流(0101)数据,将其转化为电流强弱进行传输,到达目的地后再转化为0101的机器码,也就是数模转换和模数转化,这一层的数据是比特。网卡工作在这一层。 第二层(数据链路层)物理寻址，同时将原始比特流转为逻辑传输线路。在传输比特流的过程中，可能会产生错传和数据不完整的情况。这一层定义了如何格式化数据以进行传输,以及控制对物理介质的访问。这一层还提供错误检测和纠正，以确保数据传输的可靠性。该层将比特数据组成了帧,其中交换机工作在该层,对帧解码，并根据帧中包含的信息发送到接收方。 第三层(网络层)随着网络节点的增加，点对点的通讯是要经过多个节点的,如何找到目标节点、选择最佳路径成为首要需求，此时便有了网络层。网络层的主要功能是将网络地址翻译为对应的物理地址,并决定如何将数据从发送方路由到接收方。网络层通过综合考虑发送优先权、网络拥塞程度、服务质量、可选路由的花费来决定从一个网络中节点A到另一个网络中节点B的最佳路径。由于网络层处理并智能指导数据传送,路由器连接网络各个节点,所以路由器属于网络层。此层的数据被称为数据包,此层需要关注的协议是TCP/IP协议中的IP协议。 第四层(传输层)随着网络通讯需求的进一步扩大，通讯过程中需要发送大量数据,如海量文件传输等,可能需要很长的时间,而网络在通讯过程中会中断好多次,此时为了保证传输大量文件的准确性,需要对数据进行切分,切割为一个个的段落,如果其中一个段落丢失了需不需要重新传输,每个段落是否要按照顺序到达,这便是传输层要考虑的问题。传输层解决了主机间的数据传输(可以是不同网络),同时解决了传输质量的问题。该层是OSI模型中最重要的一层:传输协议同时进行流量控制,或是基于接收方可接受数据的快慢程度规定适当的发送速率。除此之外,传输层按照网络处理的最大尺寸,将较长的数据包进行强制分割,将文件处理为数据片并为每一个数据片安排一个序列号,以便数据片到达接收方时能够正确重组。该过程即称为排序。传输层中需要关注的协议有TCP/IP协议中的TCP协议和UDP协议。 第五层(会话层)第四层保证了给正确的计算机发送正确的封装过后的信息了,但是用户的体验好不好(难道每次都要调用TCP去打包,然后调用IP协议去找路由),当然不可以。我们要建立一个自动收发包，自动寻址的功能，于是发明了会话层。会话层的作用就是管理和应用程序之间的通讯。 第六层(表示层)在第五层的基础上，可以保证应用程序自动收发包和寻址了。但是如果是两个不同的系统之间通讯，往往有阻碍。就像shell不能在windows下直接执行一样。于是需要表示层帮用户解决不同系统之间的通讯语法的问题。 第七层(应用层)在表示层，数据将按照网络能理解的方案进行格式化。这种格式化也因所使用的网络类型的不同而不同，此时发送方虽然知道自己发送的是什么，转化为字节数组后有多长，但接收方不知道。所以应用层的网络协议诞生了。它规定发送方和接收方必须使用一个固定长度的消息头，消息头必须使用某种固定的组成，而且消息头必须记录消息体的长度等信息，以方便接收方正确解析发送方发送的数据。应用层旨在让用户更方便的应用从网络中接收到的数据，至于数据传递，没有该层用户也可以在两台电脑间传递信息。只不过穿来穿去都是01组成的字节数组。该层需要重点关注的是TCP/IP协议中的HTTP协议。 以上就是OSI各层次的划分。先自上而下从应用层开始都要对传输的数据头部进行处理，加上本层的一些信息。最后由物理层通过电缆、以太网等介质将数据解析为比特流在网络中进行传输。数据传递到目标地址，再自底而上的将先前对应层的头部进行解析分离。这就是网络处理的整个流程。(先自上而下，后自下而上处理数据头部) 图片引自 TCP/IPOSI并不是一个标准，只是一个在指定标准时使用的概念框架。其标准是TCP/IP，即TCP/IP是OSI的实现。 虽然TCP/IP并不完全符合OSI的七层参考模型。但是它依然是OSI的一种实现。 图片引自 TCP/IP很多情况下是用IP进行通信时所必须用到的协议群的统称。TCP/IP在分层模块上与OSI略有缺别，可参照上图。 OSI注重通讯协议必要的功能是什么，而TCP/IP则更强调实现协议要开发哪种程序。 TCPTCP(Transmission Control Protocol)简介TCP是属于传输层的协议，是传输控制协议。 是一种面向连接的，可靠的，基于字节流的传输层通信协议 将应用层的数据流分割成报文段并发送给目标节点的TCP层 TCP为了不丢失包,每个数据包都有序号,对方收到则发送ACK确认,未收到则重新传送 使用校验和函数来检验数据再传输过程中是否有误，发送和接收时都要计算校验和。即使用校验和,确认和重传机制来保证可靠传输 在一个 TCP 连接中，仅有两方进行彼此通信。广播和多播不能用于 TCP TCP 给数据分节进行排序，并使用累积确认保证数据的顺序不变和非重复 TCP 使用滑动窗口机制来实现流量控制，通过动态改变窗口的大小进行拥塞控制 注意：TCP 并不能保证数据一定会被对方接收到，因为这是不可能的。TCP 能够做到的是，如果有可能，就把数据递送到接收方，否则就（通过放弃重传并且中断连接这一手段）通知用户。因此准确说 TCP 也不是 100% 可靠的协议，它所能提供的是数据的可靠递送或故障的可靠通知。 TCP报文头 TCP连接中传送的字节流中的每个字节都按顺序编号。例如一短报文的序列号字段值是107,而携带的字段为100个字段。如果有下一个报文段，则从207开始。 Source Port 代表源端口 占2字节 Destination Port 代表目的端口 占2字节 Sequence Number 包的序号Seq，用于解决网络包乱序（reordering） 占4个字节 Acknowledgment Number 确认字符 占4个字节 代表期望收到对方下一段报文的下一个字节的序号 用于确认收到，用来解决不丢包的问题(本文图片中的ACKnum 有的则写成小写ack) 例如B收到了A发来的报文，其序列号字段是301,而数据长度为200字节。这表明B收到了到序号到500(301+200-1)为止的数据。因此B期望收到A的下一个数据序号是501.所以B发送给A的确认报文段中把ACK置为501. Offset 数据偏移由于头部有可选字段，长度不固定，因此它指出TCP报文的数据距离TCP报文的起始处有多远 Reserved 保留域 TCP Flags 控制位 由八个标志位组成 每一个位置表示一个控制功能TCP Flags中常见的6个: URG: 紧急指针标志。为1时紧急指针有效，为0则忽略紧急指针 ACK(Acknowledgement): 确认序号标志。为1时表示确认号有效，为0表示报文中不含确认信息。上面的确认号是否有效就是由该控制位控制的。 PSH: push标志。为1时表示是带有PUSH标志的数据，指示接收方接收到后尽快将报文段交给应用程序，而不是在缓冲区排队。 RST: 重置连接标志。用于重置由于主机崩溃或其他原因而出现错误的连接，或者用于拒绝非法的报文段或拒绝连接请求。 SYN(Synchronize Sequence Numbers): 同步序号,用于建立连接过程。在连接请求中,SYN=1和ACK=0表示该数据段没有使用捎带的确认域,而连接应答捎带一个确认即SYN=1和ACK=1. FIN(finish): finish标志，用于释放连接。为1时表示发送方已经没有数据发送了，即关闭本方数据流。 Window 又叫Advertised Window，可以近似理解为滑动窗口（Sliding Window）的大小，用于流控。以此控制发送端发送数据的速率，从而达到流量控制。详细解析可见本文的滑动窗口部分. Checksum 校验和。此校验和是对TCP报文段,包括TCP头部,和TCP数据以16位进行计算所得。由发送端计算和存储并由接收端进行验证 Urgent Pointer 紧急指针。只有当URG为1时有效，指出本报文段中紧急数据的字节数。 TCP Options 可选项。长度可变，定义一些可选参数。 注意: TCP和UDP不包含IP地址信息,但是它们包含源端口和目的端口。即端口是属于传输层范畴的，IP地址是IP层上的事。 在本地两个进程间进行通信时，可以使用唯一标识符PID。但是如果是两台不同计算机之间的进程进行通信时,只用PID是不够的,这时就在传输层中使用协议端口号,简称端口。IP可以唯一标识主机，而TCP协议和端口号可以唯一标识一个主机中的进程。这样可以用IP地址+协议+端口号唯一标识一个网络中的进程。一些场合中，这种唯一标识模式也称为套接字(socket)。虽然通信的重点是应用进程，但是只要把要传送的报文交给目的主机的正确端口，剩下的工作就由TCP来完成。 一个TCP连接需要四个元组来表示是同一个连接（src_ip, src_port, dst_ip, dst_port）（准确说是五元组，还有一个是协议，但因为这里只是说TCP协议，所以，这里我只说四元组）。 其他字段参考下图: TCP三次握手当应用程序希望通过TCP与另一个应用程序通信时，它会发送一个通信请求，这个请求必须被送到一个确切的地址。在双方握手之后，TCP将会在两个应用程序之间建立一个全双工的通信。这个全双工的通信将占用两个计算机之间的通信线路，直到它被一方或双方关闭为止。注:全双工是指允许数据在两个方向上进行传输,即在同一时间,服务器可以发送数据给客户端,客户端也可以发送数据给服务器。 所谓三次握手(Three-way Handshake)，是指建立一个 TCP 连接时，需要客户端和服务器总共发送3个包。 三次握手的目的是连接服务器指定端口，建立 TCP 连接，并同步连接双方的序列号和确认号，交换 TCP 窗口大小信息。在 socket 编程中，客户端执行 connect() 时。将触发三次握手。 第一次握手(SYN=1, seq=x): 客户端发送一个 TCP 的 SYN 标志位置1的包，指明客户端打算连接的服务器的端口，以及初始序号 X,保存在包头的序列号(Sequence Number)字段里。 发送完毕后，客户端进入 SYN_SEND 状态。这个报文不携带数据，并消耗一个序号。 第二次握手(SYN=1, ACK=1, seq=y, ACKnum=x+1): 服务器发回确认包(ACK)应答。即 SYN 标志位和 ACK 标志位均为1。服务器端选择自己 ISN(Initial Sequence Number) 序列号，放到 Seq 域里，同时将确认序号(Acknowledgement Number)设置为客户的 ISN 加1，即ACKnum=X+1。 发送完毕后，服务器端进入 SYN_RCVD 状态。这个报文不携带数据，并消耗一个序号。 第三次握手(ACK=1，ACKnum=y+1) 客户端再次发送确认包(ACK)，确认报文的 ACK = 1，并且把服务器发来 ACK 的序号字段+1，放在确定字段中发送给对方，并且在数据段放写ISN的+1 发送完毕后，TCP连接建立，客户端进入 ESTABLISHED 状态，当服务器端接收到这个包时，也进入 ESTABLISHED 状态，这时双方就可以开始进行通信了。TCP 握手结束。 TCP规定，这个ACK报文段可以携带数据(也可以不携带).如果不携带数据就不会消耗序号。 三次握手主要是要初始化Sequence Number 的初始值。双方需要发送自己的ISN（初始化序列号，Inital Sequence Number）给对方，并收到对方的回复——最简单的方式也是两个一来一回，四次握手，但此处进行了优化，将server端回复的ACK同server端自己的SYN合并在一个报文中发送给client，所以减少为三次。 通信的双方要同步对方ISN——所以叫SYN（全称Synchronize Sequence Numbers）。也就是上图中的 x 和 y。这个号在以后的数据通信中，在client端按发送顺序递增，在server端按递增顺序重新组织，以保证应用层接收到的数据不会因为网络问题乱序。 抓包使用抓包工具,对三次握手进行查看: 图中9087-&gt;80是客户端的9087端口号发送到服务器端的80端口. (1)中,客户端发送了一个SYN包,在SYN包中首次握手，标明了Seq = x = 0。客户端进入 SYN_SEND 状态。(2)中，服务器收到了SYN包,再回送SYN = 1, Ack = 1的确认包。此时服务器端的Seq = 0.Ack = 1是因为客户端最开始发送的Seq = x = 0,所以消耗一个序号后Ack = x + 1 = 0 + 1 = 1.将Seq = y = 0 ,Ack = 1 回发给客户端.服务器端进入 SYN_RCVD 状态(3)中，浏览器接收到服务器的ACK确认后，回发一个ACK,因为服务器的Seq = y = 0 ,需要消耗掉一个序号，所以此时的Ack = y + 1 = 0 + 1 = 1.Seq = 1.服务器和客户端进入ESTABLISHED状态.双方建立起连接，可以开始通信了. 其中 Win = xxxx ,Win的值就是作为滑动窗口进行流量控制的。 总结：在TCP/IP协议中，TCP协议提供可靠的连接服务，采用三次握手建立一个连接。 第一次握手:建立连接时,客户端发送SYN包(syn=j)到服务器，并进入SYN_SEND状态，等待服务器确认；第二次握手:服务器收到SYN包，必须确认用户的SYN(ack = j + 1),同时自己也发送一个SYN包(syn = k),即SYN+ACK包,此时服务器进入SYN_RECV状态;第三次握手:客户端收到服务器的SYN+ACK包,向服务器发送确认包ACK(ack = k + 1),此包发送完毕，客户端和服务器进入ESTABLISHED状态,完成三次握手。 相关问题 为什么要三次挥手建立连接？ 为了初始化Sequence Number的初始值.通信的双方要互相通知对方自己初始化的Sequence Number，也就是上图的x和y,这个号要作为以后数据通讯的序号，以保证传输的数据不会因为网络上的传输问题而乱序,即TCP用这个序号进行拼接数据。因此在服务器回发Sequence Number即第二次握手之后，还需要发送确认报文给服务器，告诉服务器客户端已经收到初始化的Sequence Number了。 首次握手的隐患—SYN超时 问题起因分析:Server收到Client的SYN,回复SYN-ACK的时候未收到ACK确认(比如客户端掉线).那么连接就会处于中间状态即半连接(half-open connect)，没有成功也没有失败。 Server不断重试(重新发送SYN-ACK)直至超时，Linux默认最多发送5次,每次发送时间翻倍。默认等待63秒才判定超时，TCP才断开连接。这样可能会产生针对SYN-Flood攻击的风险。 SYN攻击 SYN 攻击指的是，攻击客户端在短时间内伪造大量不存在的IP地址，向服务器不断地发送SYN包，服务器回复确认包，并等待客户的确认。由于源地址是不存在的，服务器需要不断的重发直至超时，这些伪造的SYN包将长时间占用未连接队列，正常的SYN请求被丢弃，导致目标系统运行缓慢，严重者会引起网络堵塞甚至系统瘫痪。SYN 攻击是一种典型的 DoS/DDoS 攻击。 针对SYN-Flood的防护措施： ①检测 SYN 攻击非常的方便，当你在服务器上看到大量的半连接状态时，特别是源IP地址是随机的，基本上可以断定这是一次SYN攻击。在 Linux/Unix 上可以使用系统自带的 netstats 命令来检测 SYN 攻击。②SYN cookies技术 SYN队列满后,TCP通过源地址端口，目标地址端口和时间戳，打造一个特别的Sequence Number回发回去.(通过tcp_syncookies参数回发SYN Cookie.) 若是攻击者则没有响应。如果是正常用户，则Client会回发SYN Cookie,直接建立连接 ③缩短超时（SYN Timeout）时间④过滤网关防护⑤增加最大半连接数 建立连接后,Client出现故障怎么办？保活机制:在一段时间处于保活时间(KeepAlive Time)，在这段时间连接处于非活动状态。开启保活功能的一段将向对方发送保活探测报文，如果发送端未收到响应报文，经过一个提前配置好的保活时间间隔,继续发送保活探测报文。直到发送次数达到保活探测数仍未收到响应，则对方主机则确认为不可达，此时中断连接。 KeepAlive 的局限首先 TCP KeepAlive 监测的方式是发送一个 probe 包，会给网络带来额外的流量，另外 TCP KeepAlive 只能在内核层级监测连接的存活与否，而连接的存活不一定代表服务的可用。例如当一个服务器 CPU 进程服务器占用达到 100%，已经卡死不能响应请求了，此时 TCP KeepAlive 依然会认为连接是存活的。因此 TCP KeepAlive 对于应用层程序的价值是相对较小的。需要做连接保活的应用层程序，例如 QQ，往往会在应用层实现自己的心跳功能。 TCP四次挥手是为了终止连接，客户端和服务器总共要发送4个包，来确认连接的断开,因此称为四次挥手(Four-way handshake)。客户端或服务器均可主动发起挥手动作,任何一方执行 close() 操作即可产生挥手操作. 第一次挥手(FIN=1,seq=x)假设客户端想要关闭连接，客户端发送一个 FIN 标志位置为1的包，表示自己已经没有数据可以发送了，但是仍然可以接受数据。此时的序列号seq = ESTABLISHED状态下数据最后一次发送的时候已经传送过来的数据的最后一个字节的序号+1。 发送完毕后，客户端进入 FIN_WAIT_1 状态。 第二次挥手(ACK=1,ACKnum=x+1)服务器端确认客户端的 FIN 包，发送一个确认包，表明自己接受到了客户端关闭连接的请求，但还没有准备好关闭连接。 发送完毕后，服务器端进入 CLOSE_WAIT 状态，客户端接收到这个确认包之后，进入 FIN_WAIT_2 状态，等待服务器发送释放连接报文。 第三次挥手(FIN=1,ACK=1,seq=y)服务器端准备好关闭连接时，向客户端发送结束连接请求，FIN = 1,ACK = 1。 发送完毕后，服务器端进入 LAST_ACK 状态，等待来自客户端的最后一个ACK。 第四次挥手(ACK=1，ACKnum=y+1)客户端接收到来自服务器端的关闭请求，发送一个确认包，并进入 TIME_WAIT 状态，等待可能出现的要求重传的 ACK 包。 服务器端接收到这个确认包之后，关闭连接，进入 CLOSED 状态。 客户端等待了某个固定时间（两个最大段生命周期，2MSL，2 Maximum Segment Lifetime）之后，没有收到服务器端的 ACK ，认为服务器端已经正常关闭连接，于是自己也关闭连接，进入 CLOSED 状态。 总结:第一次挥手：Client发送一个FIN,用来关闭Client到Server的数据传送,Client进入FIN_WAIT_1状态;第二次挥手: Server收到FIN后,发送一个ACK给Client,确认序号为收到序号+1(与SYN相同,一个FIN占用一个序号),Server进入CLOSE_WAIT状态;第三次挥手: Server发送一个FIN,用来关闭Server到Client的数据传送,Server进入LAST_ACK状态;第四次挥手: Client收到FIN后,Client进入TIME_WAIT状态,接着发送一个ACK给Server,确认序号为收到序号+1,Server进入CLOSED状态,客户端等待2MSL时间后,进入CLOSED状态.完成四次挥手。 相关问题 为什么要设置2MSL时间后客户端才关闭？①:确保有足够的时间让对方收到ACK包或主动方收到了被动发超时重传的FIN。即，如果被动方没有收到Ack，就会触发被动方重传FIN，发送Ack+接收FIN正好2个MSL，TIME_WAIT状态的连接收到重传的FIN后，重传Ack，再等待2 MSL时间。②:避免新旧连接混淆.确保有足够的时间让“迷途的重复分组”过期丢弃。这只需要1 MSL即可，超过MSL的分组将被丢弃，否则很容易同新连接的数据混在一起（仅仅依靠ISN是不行的）。因为有些路由器会缓存IP数据包,如果连接被重用了,那么延迟收到的包就有可能跟新连接混在一起. 为什么要进行四次才能完成挥手操作？参照三次握手的过程，其实是双方各自关闭资源进行2次挥手。握手的目的是同步双方的ISN，不是耗时操作，因此可以“将server端回复的ACK同server端自己的SYN合并在一个报文中发送给client”；但挥手要回收大量资源，是耗时操作，因此，不能强制“将server端回复的ACK同server端自己的FIN合并在一个报文中发送给client”，所以通常认为挥手需要四次。对于挥手，因为TCP是全双工的，client与server都占用各自的资源发送segment（同一通道，同时双向传输seq和ack），所以，双方都需要关闭自己的资源（向对方发送FIN）并确认对方资源已关闭（回复对方Ack）,也就是双方都需要FIN报文和ACK报文；而双方可以同时主动关闭，也可以由一方主动关闭带动另一方被动关闭。只不过，通常以一方主动另一方被动举例（如图，client主动server被动），所以看上去是所谓的4次挥手。 服务器出现大量CLOSE_WAIT状态的原因对方关闭socket连接,我方忙于读或写,没有及时关闭连接.需要检查代码,特别是释放资源的代码.检查配置,特别是处理请求的线程配置.如果CLOSE_WAIT状态的连接过多的话,就需要去排查问题。如果CLOSE_WAIT一直保持，意味着对应数目的通道一直被占用,一旦达到了上限,则新的请求就无法被处理，甚至会让服务器崩溃。 UDPUDP报文结构相比TCP要简单,主要包括:Source Port: 源端口Destination Port: 目标端口Length: 数据包长度Checksum: 校验值最后一部分为用户数据 因为UDP的报文头较TCP简单,所以它不像TCP一样会支持错误重传、滑动窗口等精细控制. UDP的特点: 面向非连接的协议,传输数据之前,源端与终端不建立连接。UDP 客户和服务器之前不必存在长期的关系。UDP 发送数据报之前也不需要经过握手创建连接的过程。当想传送时,就简单抓取来自应用程序的数据,并尽可能快的放到网络上.发送端,UDP的传送速度是应用程序生成数据的速度、计算机的能力、传送带宽的限制.在接收端，UDP把每个消息段放在队列中,应用程序每次从队列中读取一个消息段. 由于不建立连接,所以不需要维护连接状态,因此UDP 支持多播和广播,即支持向多个客户端传送相同的消息。 UDP数据包报头只有8个字节(TCP有20个字节),额外开销较小 吞吐量不受拥挤控制算法的调节，只受限于数据生成速率、传输带宽及机器性能 UDP 缺乏可靠性。UDP 本身不提供确认，序列号，超时重传等机制。UDP 数据报可能在网络中被复制，被重新排序。即 UDP 不保证数据报会到达其最终目的地，也不保证各个数据报的先后顺序，也不保证每个数据报只到达一次。(UDP尽最大努力交付,不保证可靠交付,不需要维持复杂的链接状态表) UDP 数据报是有长度的。每个 UDP 数据报都有长度，如果一个数据报正确地到达目的地，那么该数据报的长度将随数据一起传递给接收方。而 TCP 是一个字节流协议，没有任何（协议上的）记录边界。 UDP面向报文,不对应用程序提交的报文信息进行拆分或者合并。因此应用程序要选择合适的大小。 TCP和UDP的区别:||TCP|UDP|| – | – | – ||面向连接 vs 无连接|面向连接|无连接|||点对点传输|单个点向多个点传输||可靠性|可靠性保证(握手、挥手)|不提供可靠性保证(可能会丢失,不知道是否被接收)||有序性|有序性保证(到达可能无序,但是会进行排序保证有序)|不具备有序性||速度|速度慢(要创建连接、保证可靠性、有序性等)|较快(比较适合对速度敏感的应用:在线视频等)||量级(体现在元数据头的大小)|重量级(20个字节)|轻量级(8个字节)| TCP的滑窗RTT和RTO从前面的TCP重传机制我们知道Timeout的设置对于重传非常重要。 设长了，重发就慢，丢了老半天才重发，没有效率，性能差； 设短了，会导致可能并没有丢就重发。于是重发的就快，会增加网络拥塞，导致更多的超时，更多的超时导致更多的重发。 而且，这个超时时间在不同的网络的情况下，根本没有办法设置一个死的值。只能动态地设置。 为了动态地设置，TCP引入了RTT——Round Trip Time，也就是一个数据包从发出去到回来的时间。这样发送端就大约知道需要多少的时间，从而可以方便地设置Timeout——RTO（Retransmission TimeOut），以让我们的重传机制更高效,即TCP在发送一个数据包之后,会启动一个重传定时器,RTO就是这个定时器的重传时间. 听起来似乎很简单，好像就是在发送端发包时记下t0，然后接收端再把这个ack回来时再记一个t1，于是RTT = t1 – t0。没那么简单，这只是一个采样，不能代表普遍情况。所以简单的说,RTT就是发送一个数据包(记录时间为t0)到收到对应的ACK(记录时间为t1)所花费的时间(RTT=t1-t0)；RTO(Retransmission TimeOut)是重传时间间隔,最开始预先算一个定时器时间,如果回复了ACK那么重传定时器就失效,即不用重传了.如果没有回复ACK,而RTO定时器的时间又到了,则就重传.由于RTO是本次发送当前数据包所预估的超时时间,则RTO就需要一个很好的算法来统计,更好的预测这次的超时时间,所以RTO不是写死的配置,而是根据RTT计算出来的.有了RTT才能计算出RTO.关于具体计算方法可以参考本文的参考部分的链接. 基于RTO才有了重传机制,进一步有了滑动窗口. 滑动窗口计算过程TCP在传送时会将数据拆分成段,出于效率和传输速度的考虑,不可能将数据一段一段的发送,而是要实现对数据的批量发送,所以TCP要解决可靠传输和包乱序的问题.所以TCP要知道网络实际的数据处理带宽或数据处理速度,这样才不会引起网络拥塞,导致丢包.TCP使用滑动窗口(Sliding Window)做流量控制与乱序重排即它的两个作用是 保证TCP的可靠性 保证TCP的流控特性 在本文前面TCP的报文字段中,有一个Window值,也叫Advertised-Window,用于接收方通知发送方自己还有多少缓冲区可以接收数据,发送方根据接收方的处理能力来发送数据,这样不会导致接收方处理不过来,这就是流量控制.同时窗口控制还体现了TCP面向字节流的设计思路. 上图中左半部分是TCP协议的发送端缓冲区,右图是接收端缓冲区. LastByteAcked指向了被接收端Ack过的位置（表示成功发送确认）,即从左端算起连续已经被接收端的程序发送ACK回执确认，已收到了Sequence Number. LastByteSent指向已发送的最后一个字节的位置,该位置只是发送出去,但是还没有收到ACK的回应. LastByteWritten指向上层应用已写完的最后一个字节的位置.即当前程序已经准备好的需要发送的最后一个数据段. 所以LastByteAcked之前的数据是已经发送并确认的数据,LastByteAcked和LastByteSent之间的数据是已经发送出去但还没有确认的数据,LastByteWritten指向的是上层应用正在写的地方。 LastByteRead指向上层应用已经读完的最后一个字节的位置(TCP缓冲区中读到的位置),即收到了发送方的数据,并且已经处理和回执了的数据的最后一个位置. NextByteExpected指向收到的连续最大的Sequence的位置(收到的连续包的最后一个位置)，即这段数据已经收到了但是还没有给发送端发送回执信息. LastByteRcved指向已收到包的最后一个字节的位置,中间有些数据还没有到达，所以有数据空白区. 接收端在给发送端回ACK中会汇报自己还能处理的数据AdvertisedWindow = MaxRcvBuffer – (LastByteRcvd – LastByteRead);MaxRcvBuffer:接收方能接收的最大数据量,可以理解为接收端缓存池的大小;LastByteRcvd – LastByteRead: 表示当前接收方已为接收到的数据或为还没有接收到的预定的数据留出来的空间.这样做减法，用最大缓存减去不能用的缓存,就是还能接收的数据量。然后将接收端在给发送端回ACK中会汇报自己还能处理的数据AdvertisedWindow告知发送方.发送方要根据接收端在给发送端回ACK中会汇报自己还能处理的数据AdvertisedWindow的值,保证LastByteSent - LastByteAcked &lt;= AdvertisedWindow 窗口内剩余可发送数据的大小EffectiveWindow = AdvertisedWindow - (LastByteSent - LastByteAcked).即LastByteSent - LastByteAcked为已经发送了但待确认的,那么还能发送的内容就用接收方能承受的数据量AdvertisedWindow为基准,减去这部分大小.这样才能保证接收方能处理这些数据. 滑动窗口的基本原理TCP会话的发送方 上图中分成了四个部分，分别是：（其中那个黑模型就是滑动窗口） 1已发送并收到端ack回应的数据 2已经发送但还没收到ack的 3在窗口中未发送但还允许发送的 4未发送且由于达到了window的大小,对端不允许发送的数据 则2 3部分就组成了一个发送窗口 原滑动窗口的边界为32-51,已发送但未被确认的序号为32-45,此时如果32和33都没有被确认,即使34被确认了,窗口也不会向右滑动,只有等到32也被确认后(即连续确认后)窗口才会移动,在移动之前,大于51的数据是不能被发送的.滑动后32-36都被确认了,则滑动窗口向右移动了5位,进而后面能继续发送52-56的数据了.滑动后收到36的ack，并发出了46-51的字节 TCP会话的接收方 1 2已接收和已发送回执的状态 3未接收但可以接收的数据(准备接收的状态) 4未接收并不能接收的状态(因为达到窗口阈值) 由于ACK直接由TCP栈回复,默认没有应用延迟,所以不存在已接收但是未回复ACK的状态 其中绿色部分(未接收但准备接收)就是接收窗口.其滑动机制和传送方的机制是一样的. 总结:TCP的最基本的传输可靠性来源于确认重传机制,TCP的滑动窗口的可靠性也是建立在确认重传基础上.发送窗口只有收到接收端对于本段发送窗口内字节的ack确认,才会移动发送窗口的左边界,接收窗口只有在前面所有的段都确认的情况下,才会移动左边界.当前面还有字节未接收,但收到后面字节的情况下窗口是不会移动的,不会对后续的字节确认,以此确保对端会对数据重传.滑动窗口的大小可以依据一定的策略进行动态调整,应用会根据自身处理能力的变化,通过本端TCP接收窗口大小的控制实现对端的发送窗口进行流量限制. 参考https://coolshell.cn/articles/11609.html https://hit-alibaba.github.io/interview/basic/network/TCP.html https://monkeysayhi.github.io/2018/03/07/%E6%B5%85%E8%B0%88TCP%EF%BC%881%EF%BC%89%EF%BC%9A%E7%8A%B6%E6%80%81%E6%9C%BA%E4%B8%8E%E9%87%8D%E4%BC%A0%E6%9C%BA%E5%88%B6/]]></content>
      <tags>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最短路径问题]]></title>
    <url>%2F2019%2F04%2F23%2F%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[最短路径问题(Shortest Path)本文的最短路径问题对于有向图也无线图都是成立的。常见应用:路径规划、工作任务规划。 在图的遍历-广度优先遍历中介绍的广度优先遍历的结果就是求出了一个最短路径。这种方法求的是从一个节点开始，到其他所有节点的最短路径，这样形成的是一个节点连接其他节点的树，即最短路径树(Shortest Path Tree).这棵树也是这个图的生成树，但是不是最小生成树，最小生成树是所有的边权值总和是最小的，而最短路径树是所有的节点距离起始节点的权值是最小的，这个求得最短路径树的过程，称为单源最短路径问题(Single Source Shortest Path).单源即是一个起始点的意思。这里是求出了从一个点到其他所有可以抵达的点的最短路径，不是两点间的最短路径。 下图中从0-&gt;2-&gt;1的操作称为松弛操作,即想计算到一个点的最短路径，尝试通过其他点”绕一下”,然后比较是否比直接到达要短。松弛操作是最短路径求解的核心。 Dijkstra 单源最短路径算法Dijkstra算法的前提: 图中不能有值为负的权重边 其时间复杂度为O(E log(V)) 以下面的有向加权图为例，介绍Dijkstra单源最短路径算法. 假设从点0开始，寻求0到其他所有点的最短路径。因为起点是0，所以0到0的最短路径是0.step1:首先对起点进行标识，然后对它所有的临边进行访问,同时更新数组.此时找到没有访问的节点中，能够以最短的方式抵达的节点(此时为节点2,它只需用权值为2的费用就可以抵达),则可以说,从源点到节点2的最短路径就是权值2.(因为2已经是所有能抵达的相邻的节点中最小的权值了)。 图中不能有负权边保证了这一前提的成立，因为没有负权边，所以即使通过别的边进行松弛操作，那么它们的权重之和也肯定也会大于该边 step2:找到了从源点到节点2的最短路径以后，下面就要进行松弛操作.即验证从源点通过节点2到其他节点的路径是否比源点直接到达其他节点的路径要短。 开始看节点2的所有临边。 第一条临边为2-&gt;1的边，从源点到节点2权值为2,从节点2到节点1权值为1,总权重为31的最短路径为0-&gt;2-&gt;1 = 3.所以可以在数组中更新对应的最短路径。 接下来看节点2的下一个相邻节点节点4.因为节点4还没被访问过，所以从源点到节点4找到了一条权重为7的路径,在数组中进行记录。 然后接着看2-&gt;3.从2-&gt;3的权值为3,从0-&gt;2-&gt;3的权重为5.小于从0-&gt;3的权重6.这时候完成了Dijkstra算法的一轮循环. step3:接着找到此时还没有找到最短路径的节点中,现在存的最短的路径能抵达的节点是谁，此时为节点1(权值为3).则可以说0-&gt;2-&gt;1就是从0到1的最短路径. 然后根据节点1的临边，进行松弛操作。此时为1-&gt;4的临边,其权值为1.由于0-&gt;1的最短路径为3,1-&gt;4的路径为1,此时经过1到达4的路径总长度为4.小于7.所以可以将从源点到节点4的路径长度更新为4.之前指向4的边就可以废弃不用了，它一定不是一条最短路径。此时和1相邻的边的松弛操作就做完了。 下面只有节点3和节点4没有访问到，此时最短的路径是到4,它的权值是4,从1-&gt;4的最短路径就是4. 然后对针对节点4,对节点4的所有邻边进行松弛操作.但是4无法到其他任何节点,所以此时不需要进行操作. step4:最后只剩下节点3没有访问过，此时没有其他选择,即现在的0-&gt;2-&gt;3即为0-&gt;3的最短路径. 此时就完成了Dijkstra算法.找到了一个以节点0为根的最短路径树. 算法总结和实现细节: 对路径的记录都是在右侧的列表操作.第一个操作是找到没有访问过的节点中的最短的路径相应的节点.第二个操作是更新列表.可以借用IndexMinHeap实现该操作.开辟节点个数的空间.每次的插入和更新操作都是O(logV)的时间复杂度.Dijkstra的整个操作对所有的边进行遍历,最终的时间复杂度为O(ElogV). Dijkstra算法实现程序的目录结构 相关参考代码:堆和堆排序图的表达最小索引堆ReadWeightedGraph.java有实现代码 Dijkstra算法的具体实现代码见附录1 处理负权边如图所示，如果引入了负权边,那么在没有负权边的情况下进行松弛操作之前就能确定一个最短边,现在则是有可能“绕路”的情况要比直接到达的情况效率更高。可以看出,它计算0-&gt;1-&gt;2的操作仍然依赖了松弛操作。 当然，负权边会产生一个问题,如图。下图多了一条从2-&gt;0的负权边,这样就形成了一个负权环.如果一个图中出现了负权环,那么寻找一点到另一点的最短路径,只要经过了负权环,那么它就会在环中不断“循环”,因为在环中每转一次,那么最短路径都会减少.所以这样就无法计算出最短路径了(或最短路径为负无穷). 那么可以得出一个结论：拥有负权环的图没有最短路径. 两个结点也可以构成负权环,如下图的节点1和2: Bellman-Ford 单源最短路径算法该算法的前提是,图中可以有负权边,但是不能有负权环 Bellman-Ford可以判断图中是否有负权环,不过因此它的时间复杂度为O(EV). Bellman-Ford的算法思想,如果一个图没有负权环,那么从一个点(起始点)到另一个点的最短路径,最多经过所有的顶点,有V-1条边.否则,则存在一个顶点经过了两次,说明存在负权环. 如下图,存在两个负权边. step1:找到所有节点0的临边,那么暂时找到了从0到其相邻顶点的最短路径.从另一个角度来看待节点0的所有临边,相当于对0的相应的所有的节点进行了一次松弛操作,这次操作的结果得到了从原点开始,经过1条边到达其他节点的最短路径. step2:对所有的点进行第2次松弛操作(这里记住松弛操作的核心思想就是尝试从一个点出发中间经过一个点到达目的点的路径是否会更短.即找一个两条边的路径,看其权值是否会比只有一条边的路径权值更小)。结合本例，第二次松弛操作就找到了从节点0到节点2更短的路径. 对一个点的一次松弛操作,就是找到经过这个点的另外一条路径,多一条边,权值会更小. 前面提到过,如果一个图没有负权环,那么从一个点(起始点)到另一个点的最短路径,最多经过所有的顶点,有V-1条边.否则,则存在一个顶点经过了两次,说明存在负权环. 如果对所有的点都进行V-1次松弛操作,理论上就找到了从源点到其他所有点的最短路径.也即Bellman-Ford的主体思想就是对所有的点进行V-1次松弛操作. 如果还可以进行松弛操作(也就是进行第V轮松弛操作),那么说明原图中有负权环. 对所有点进行松弛操作,每个边都要遍历一遍,时间复杂度为O(E).这样的操作要进行V轮,所以其时间复杂度为O(EV). Bellman-Ford 代码实现 BellmanFord的实现代码和两个图的表达见附录2 Bellman-Ford代码的优化和其他算法可以使用队列进行相应优化,比如queue-based bellman-ford算法. 算法 要求 要求 复杂度 dijkstra 无负权边 有向无向图均可 O(ElogV) Bellman-Ford 无负权环 有向图 O(VE) 利用拓扑排序 有向无环图DAG 有向图 O(V+E) 所有对最短路径算法,可以回答任何一个点到其他任何点的最短路径.可以用Floyed算法实现,处理无负权环的图O(V^3) 对于最长路径问题,最长路径问题不能有正权环。无权图的最长路径问题是指数级难度的。对于有权图,不能使用Dijkstra求最长路径问题,但是可以使用Bellman-Ford算法. 附录1-Dijkstra算法实现Dijkstra算法实现代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135import java.util.Vector;import java.util.Stack;// Dijkstra算法求最短路径public class Dijkstra&lt;Weight extends Number &amp; Comparable&gt; &#123; private WeightedGraph G; // 图的引用 private int s; // 起始点 private Number[] distTo; // distTo[i]存储从起始点s到i的最短路径长度 private boolean[] marked; // 标记数组,在算法运行过程中标记节点i是否被访问 private Edge&lt;Weight&gt;[] from; // from[i]记录最短路径中,到达i节点的边是哪一条,可以用来恢复整个最短路径 //构造函数,使用Dijkstra算法求最短路径 public Dijkstra(WeightedGraph graph, int s)&#123; //算法初始化 G = graph; if(s &lt; 0 || s &gt; G.V())&#123; throw new IllegalArgumentException("s is out of bounds!"); &#125; this.s = s; distTo = new Number[G.V()]; marked = new boolean[G.V()]; from = new Edge[G.V()]; for (int i = 0 ; i &lt; G.V() ; i ++) &#123; distTo[i] = 0.0; marked[i] = false; from[i] = null; &#125; // 使用索引堆记录当前找到的到达每个顶点的最短距离 IndexMinHeap&lt;Weight&gt; ipq = new IndexMinHeap&lt;Weight&gt;(G.V()); //对于起始点s进行初始化 distTo[s] = 0.0; from[s] = new Edge&lt;Weight&gt;(s, s, (Weight)(Number)0.0); ipq.insert(s,(Weight)distTo[s]); marked[s] = true; while (!ipq.isEmpty())&#123; int v = ipq.extractMinIndex(); // distTo[v]就是s到v的距离 marked[v] = true; // 对v的所有相邻节点进行更新,即松弛操作 for( Object item : G.adj(v) )&#123; Edge&lt;Weight&gt; e = (Edge&lt;Weight&gt;) item; int w = e.other(v); //如果从s点到w点的最短路径还没有找到 if( !marked[w] )&#123; //如果w点以前没有访问过 //或者访问过,但是通过当前的v点到w点距离更短,则进行更新 if( from[w] == null || distTo[v].doubleValue() + e.wt().doubleValue() &lt; distTo[w].doubleValue() )&#123; //长度更新为较小的值 distTo[w] = distTo[v].doubleValue() + e.wt().doubleValue(); //对from进行更新 from[w] = e; //对最小索引堆进行判断 if( ipq.contain(w) )&#123; ipq.change(w,(Weight)distTo[w]); &#125; else&#123; ipq.insert(w,(Weight)distTo[w]); &#125; &#125; &#125; &#125; &#125; &#125; // 返回从s点到w点的最短路径长度 Number shortestPathTo( int w )&#123; if(w &lt; 0 || w &gt;= G.V() )&#123; throw new IllegalArgumentException("w is out of bounds!"); &#125; if(!hasPathTo(w))&#123; throw new IllegalArgumentException("these two have no path!"); &#125; return distTo[w]; &#125; //判断从s点到w点是否联通 boolean hasPathTo( int w )&#123; if(w &lt; 0 || w &gt;= G.V() )&#123; throw new IllegalArgumentException("w is out of bounds!"); &#125; return marked[w]; &#125; //寻找从s到w的最短路径,将整个路径经过的边存放在vec中 Vector&lt;Edge&lt;Weight&gt;&gt; shortestPath( int w )&#123; if(w &lt; 0 || w &gt;= G.V() )&#123; throw new IllegalArgumentException("w is out of bounds!"); &#125; if(!hasPathTo(w))&#123; throw new IllegalArgumentException("these two have no path!"); &#125; //通过from数组逆向查找到从s到w的路径,存放到栈中 Stack&lt;Edge&lt;Weight&gt;&gt; s = new Stack&lt;&gt;(); Edge&lt;Weight&gt; e = from[w]; while( e.v() != this.s )&#123; s.push(e); e = from[e.v()]; &#125; s.push(e); //从栈中依次取出元素,获得顺序的从s到w的路径 Vector&lt;Edge&lt;Weight&gt;&gt; res = new Vector&lt;&gt;(); while (!s.empty())&#123; e = s.pop(); res.add(e); &#125; return res; &#125; //打印出从s点到w点的路径 void showPath(int w)&#123; if(w &lt; 0 || w &gt;= G.V() )&#123; throw new IllegalArgumentException("w is out of bounds!"); &#125; if(!hasPathTo(w))&#123; throw new IllegalArgumentException("these two have no path!"); &#125; Vector&lt;Edge&lt;Weight&gt;&gt; path = shortestPath(w); for (int i = 0 ; i &lt; path.size(); i ++)&#123; System.out.print(path.elementAt(i).v() + "-&gt;"); if( i == path.size() - 1) System.out.println(path.elementAt(i).w()); &#125; &#125;&#125; 测试用的图文件testG1.txt1234567895 80 1 50 2 20 3 61 4 12 1 12 4 52 3 33 4 2 Main函数中测试:12345678910111213141516171819202122232425262728public class Main &#123; // 测试Dijkstra最短路径算法 public static void main(String[] args) &#123; String filename = "testG1.txt"; int V = 5; SparseWeightedGraph&lt;Integer&gt; g = new SparseWeightedGraph&lt;Integer&gt;(V, true); // Dijkstra最短路径算法同样适用于有向图 //SparseGraph&lt;int&gt; g = SparseGraph&lt;int&gt;(V, false); ReadWeightedGraph readGraph = new ReadWeightedGraph(g, filename); System.out.println("Test Dijkstra:\n"); Dijkstra&lt;Integer&gt; dij = new Dijkstra&lt;Integer&gt;(g,0); for( int i = 1 ; i &lt; V ; i ++ )&#123; if(dij.hasPathTo(i)) &#123; System.out.println("Shortest Path to " + i + " : " + dij.shortestPathTo(i)); dij.showPath(i); &#125; else System.out.println("No Path to " + i ); System.out.println("----------"); &#125; &#125;&#125; 测试结果1234567891011121314Test Dijkstra:Shortest Path to 1 : 3.00-&gt;2-&gt;1----------Shortest Path to 2 : 2.00-&gt;2----------Shortest Path to 3 : 5.00-&gt;2-&gt;3----------Shortest Path to 4 : 4.00-&gt;2-&gt;1-&gt;4---------- 附录2-Bellman-Ford 代码实现BellmanFord.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143import java.util.Vector;import java.util.Stack;// 使用BellmanFord算法求最短路径public class BellmanFord&lt;Weight extends Number &amp; Comparable&gt; &#123; private WeightedGraph G; //图的引用 private int s; //设置起始点 private Number[] distTo; //distT[i]存放从起始点s到i的最短路径长度 Edge&lt;Weight&gt;[] from; //from[i]记录最短路径中,到达i点的边是哪一条 //可以用来回复整个最短路径 boolean hasNegativeCycle; //标记图中是否有负权环 //构造函数,使用BellanFord算法求最短路径 public BellmanFord(WeightedGraph graph, int s)&#123; G = graph; this.s = s; distTo = new Number[G.V()]; from = new Edge[G.V()]; //初始化所有的节点s都不可达,由from数组来表示 for(int i = 0 ; i &lt; G.V() ; i ++)&#123; from[i] = null; &#125; //设置distTo[s]=0 ,并且让from[s]不为NULL,表示初始s节点可达且距离为0 distTo[s] = 0.0; from[s] = new Edge&lt;Weight&gt;(s,s,(Weight)(Number)0.0); //Bellman-Ford过程 //进行V-1次循环,每一次循环求出从起点到其余所有点,最多使用pass步可以到达的最短距离 for( int pass = 1 ; pass &lt; G.V() ; pass ++)&#123; //每次循环中对所有的边进行一遍松弛操作 //遍历所有边的方式是先遍历所有的顶点,然后遍历和所有顶点相邻的所有边 for( int i = 0 ; i &lt; G.V() ; i ++)&#123; //使用临边迭代器遍历和所有顶点相邻的所有边 for( Object item : G.adj(i) )&#123; Edge&lt;Weight&gt; e = (Edge&lt;Weight&gt;) item; //对于每一个边首先判断e.v()可达 //之后看如果e.w()以前没有到达过,显然可以更新distTo[e.w()] //或者e.w()以前虽然到达过,但是通过这个e可以获得一个更短的距离,即可以进行一次松弛操作,也可以更新distTo[e.w()] if(from[e.v()] != null &amp;&amp; (from[e.w()] == null || distTo[e.v()].doubleValue() + e.wt().doubleValue() &lt; distTo[e.w()].doubleValue()) )&#123; distTo[e.w()] = distTo[e.v()].doubleValue() + e.wt().doubleValue(); from[e.w()] = e; &#125; &#125; &#125; hasNegativeCycle = detectNegativeCycle(); &#125; &#125; //判断图中是否有负权环 boolean detectNegativeCycle() &#123; for(int i = 0 ; i &lt; G.V() ; i ++)&#123; for(Object item : G.adj(i) )&#123; Edge&lt;Weight&gt; e = (Edge&lt;Weight&gt;)item; if( from[e.v()] != null &amp;&amp; distTo[e.v()].doubleValue() + e.wt().doubleValue() &lt; distTo[e.w()].doubleValue()) return true; &#125; &#125; return false; &#125; //返回图中是否有负权环 boolean negativeCycle() &#123; return hasNegativeCycle; &#125; //返回从s点到w点的最短路径长度 Number shortestPathTo(int w)&#123; if(w &lt; 0 || w &gt;= G.V())&#123; throw new IllegalArgumentException("w is out of bounds!"); &#125; if (hasNegativeCycle)&#123; throw new IllegalArgumentException("this Graph hasNegativeCycle!"); &#125; if(!hasPathTo(w))&#123; throw new IllegalArgumentException(s + "to "+ w +"doesn't have path!"); &#125; return distTo[w]; &#125; //判断s点到w点是否联通 boolean hasPathTo(int w)&#123; if(w &lt; 0 || w &gt;= G.V())&#123; throw new IllegalArgumentException("w is out of bounds!"); &#125; return from[w] != null; &#125; //寻找s点到w点的最短路径,将整个路径经过的边存放在vec中 Vector&lt;Edge&lt;Weight&gt;&gt; shortestPath(int w)&#123; if(w &lt; 0 || w &gt;= G.V())&#123; throw new IllegalArgumentException("w is out of bounds!"); &#125; if (hasNegativeCycle)&#123; throw new IllegalArgumentException("this Graph hasNegativeCycle!"); &#125; if(!hasPathTo(w))&#123; throw new IllegalArgumentException(s + "to "+ w +"doesn't have path!"); &#125; //通过from数组逆向查找到从s到w的路径,存放到栈中 Stack&lt;Edge&lt;Weight&gt;&gt; s = new Stack&lt;&gt;(); Edge&lt;Weight&gt; e = from[w]; while (e.v() != this.s)&#123; s.push(e); e = from[e.v()]; &#125; s.push(e); //从栈中依次取出元素,获得顺序的从s到w的路径 Vector&lt;Edge&lt;Weight&gt;&gt; res = new Vector&lt;&gt;(); while (!s.empty())&#123; e = s.pop(); res.add(e); &#125; return res; &#125; //打印出从s点到w点的路径 void showPath(int w) &#123; if(w &lt; 0 || w &gt;= G.V())&#123; throw new IllegalArgumentException("w is out of bounds!"); &#125; if (hasNegativeCycle)&#123; throw new IllegalArgumentException("this Graph hasNegativeCycle!"); &#125; if(!hasPathTo(w))&#123; throw new IllegalArgumentException(s + "to "+ w +"doesn't have path!"); &#125; Vector&lt;Edge&lt;Weight&gt;&gt; res = shortestPath(w); for(int i = 0 ; i &lt; res.size() ; i ++)&#123; System.out.print(res.elementAt(i).v() + " -&gt; "); if( i == res.size() - 1 )&#123; System.out.println(res.elementAt(i).w()); &#125; &#125; &#125;&#125; testG2.txt: 1234567895 80 1 50 2 20 3 61 2 -41 4 22 4 52 3 34 3 -3 testG_negative_circle.txt 123456789105 90 1 50 2 20 3 61 2 -42 1 11 4 22 4 52 3 34 3 -3 Main测试函数1234567891011121314151617181920212223242526272829303132333435public class Main &#123; // 测试我们的Bellman-Ford最短路径算法 public static void main(String[] args) &#123; String filename = "testG2.txt";// String filename = "testG_negative_circle.txt"; int V = 5; SparseWeightedGraph&lt;Integer&gt; g = new SparseWeightedGraph&lt;Integer&gt;(V, true); ReadWeightedGraph readGraph = new ReadWeightedGraph(g, filename); System.out.println("Test Bellman-Ford:\n"); int s = 0; BellmanFord&lt;Integer&gt; bellmanFord = new BellmanFord&lt;Integer&gt;(g, s); if( bellmanFord.negativeCycle() ) System.out.println("The graph contain negative cycle!"); else for( int i = 0 ; i &lt; V ; i ++ )&#123; if(i == s) continue; if(bellmanFord.hasPathTo(i)) &#123; System.out.println("Shortest Path to " + i + " : " + bellmanFord.shortestPathTo(i)); bellmanFord.showPath(i); &#125; else System.out.println("No Path to " + i ); System.out.println("----------"); &#125; &#125;&#125; 对没有负权环图的测试结果testG2.txt:1234567891011121314Test Bellman-Ford:Shortest Path to 1 : 5.00 -&gt; 1----------Shortest Path to 2 : 1.00 -&gt; 1 -&gt; 2----------Shortest Path to 3 : 3.00 -&gt; 1 -&gt; 2 -&gt; 4 -&gt; 3----------Shortest Path to 4 : 6.00 -&gt; 1 -&gt; 2 -&gt; 4---------- 对有负权环图testG_negative_circle.txt的测试结果:123Test Bellman-Ford:The graph contain negative cycle!]]></content>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[堆和堆排序]]></title>
    <url>%2F2019%2F04%2F07%2F%E5%A0%86%E5%92%8C%E5%A0%86%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[在优先队列这一文章中，实现优先队列使用了最大堆的数据结构。这一次将详细讲解堆。 数据结构二叉堆能够很好地实现优先队列的基本操作。在二叉堆的数组中，每个元素都要保证大于等于另两个特定位置的元素。相应地，这些位置的元素又至少要大于等于数组中的另两个元素，以此类推。如果我们将所有元素画成一棵二叉树，将每个较大元素和两个较小的元素用边连接就可以很容易看出这种结构。 定义：当一棵二叉树的每个结点都大于等于它的两个子结点时，它被称为有序堆。 堆中某个结点的值总是不大于其父结点的值 堆总是一棵完全二叉树 层数越高并不一定值越大 堆的基本实现如果用指针来表示堆有序的二叉树，那么每个元素都需要三个指针来找到它的上下结点(父结点和两个子结点)。由于二叉堆是一个完全二叉树，因此可以使用数组来表示它。 定义。二叉堆是一组能够用堆有序的完全二叉树排序的元素，并在数组中按照层级储存(不使用数组的第一个位置)。 下面均以最大堆为例进行介绍。 下图为算法第四版中堆的表示： 在一个堆中，位置k的结点的父亲结点的位置为[k/2],而它的两个子结点的位置则分别为2k和2k+1。这样在不使用指针的情况下我们也可以通过计算数组的索引在树中上下移动:从a[k]向上一层就令k等于k/2，向下一层则令k等于2k或2k+1. 1234parent(i) = i / 2left child (i) = 2 * iright chilld (i) = 2 * i + 1 堆的算法 在有序化的过程中我们会遇到两种情况。当某个结点的优先级上升(或是在堆底加入一个新的元素)时，我们需要由下至上恢复堆的顺序。当某个结点的优先级下降(例如，将根节点替换为一个较小的元素)时，我们需要由上至下恢复堆的顺序。 由下至上的堆有序化(上浮)以下面的图为例，在堆中添加一个元素，相当于在数组末尾添加了一个元素. 此时加入的元素52打破了堆的定义，它变得比它的父结点更大，所以需要交换它和它的父结点来修复堆。交换后这个节点比它的两个子结点都大(一个是曾经的父结点，另一个比它更小，因为它是曾经父结点的子结点)，但这个节点仍可能比它现在的父结点更大。所以要一遍遍地用相同的方法进行恢复，将这个结点不断上移，直到遇到一个比它大的父结点。 只要记住位置k的结点的父结点是[k/2]。在《算法(第四版)》中，这个方法定义为swim()，意为当一个节点太大的时候它需要浮(swim)到堆的更高层也有的地方将此方法定义为shiftUp()。这里只需明白其本质含义，理解起来就不难了。swim()方法中的循环可以保证只有位置k上的节点大于它的父结点时，堆的有序状态才会被打破。因此只要该节点不再大于它的父结点，堆的有序状态就恢复了。 首先需要两个辅助函数less()和exch():1234567891011//辅助函数less():比较索引i处的值是否小于索引j处的值private boolean less(int i, int j)&#123; return data[i].compareTo(data[j]) &lt; 0;&#125;//辅助函数交换堆中索引为i和j的两个元素private void exch(int i,int j)&#123; Item t = data[i]; data[i] = data[j]; data[j] = t;&#125; 然后实现swim()函数:1234567//最大堆核心辅助函数——swim()，进行元素上浮操作private void swim(int k)&#123; while ( k &gt; 1 &amp;&amp; less(k/2,k))&#123; exch(k/2,k); k /= 2; &#125;&#125; 这些辅助函数在元素插入的时候进行相关调用:12345678910//插入函数public void insert(Item item)&#123; if(count + 1 &gt; capacity)&#123; throw new IllegalArgumentException("out of bounds!"); &#125; data[count + 1] = item; //因为索引0位置不用，所以要+1 count ++; //上面两行可以写成data[++count] = item; swim( count );&#125; 由上至下的堆有序化(下沉)与上浮操作对应的操作就是下沉(sink或叫Shift Down)操作。 如果堆的有序状态因为某个结点变得比它的两个子结点或是其中之一更小而被打破了，那么我们可以通过将它和它的两个子结点中较大者交换来恢复堆。交换可能会在子结点处继续打破堆的有序状态，因此我们需要不断地用相同的方式将其修复，将结点向下移动直到它的子结点都比它更小或是到达了堆的底部。 上浮操作对应了堆的插入操作，下沉操作则对应了堆的取出元素的操作(也就是在优先队列中出队操作时，需要调用堆的取出元素操作，进而需要使用sink来维护堆)。 在取出元素时，只能取出根结点的元素(即最大值)。在取出根结点的元素后，将数组最后的元素放到根结点，然后从根结点开始下沉。(这是根据堆是完全二叉树的性质，将最后一个元素放到根结点，整个堆仍是一个完全二叉树) 下沉操作函数sink():12345678910111213//最大堆核心辅助函数——sink(),进行元素下沉private void sink(int k)&#123; while (2 * k &lt;= count)&#123; int j = 2 * k;// 在此轮循环中,data[k]和data[j]交换位置 if( j+1 &lt;= count &amp;&amp; less(j,j+1)) //判断是否有右孩子,如果有的话得到左孩子和右孩子中较大的值 j ++; // data[j] 是 data[2*k]和data[2*k+1]中的最大值 if(!less(k,j)) //data[k].compareTo(data[j]) &gt;= 0 break; exch(k,j); k = j; &#125;&#125; 在《算法(第四版)》中，取出最大值的函数为delMax()，个人觉得”删除”不是很合适。所以使用了bobo老师的命名extractMax(). 12345678910111213//从最大堆中取出堆顶元素public Item extractMax()&#123; if(!(count &gt; 0))&#123; throw new IllegalArgumentException("count should be &gt; 0!"); &#125; Item ret = data[1]; exch(1,count); count --; sink(1); return ret;&#125; 实现最大堆的完成代码:见附录一 Heapify对于给定的一个完全二叉树，它不满足堆的性质，如图。但是，对于给定的二叉树来说，它的所有叶子节点(即没有孩子的结点)，他们本身就是一个最大堆(黄色标记的节点)，每个堆中的元素只有一个。 对于一个完全二叉树来说，第一个非叶子节点的索引是[完全二叉树的元素个数/2]得到的索引值.即对于这个完全二叉树来说，第一个非叶子结点的索引是[10/2=5]. 从后向前考虑每一个不是叶子结点的结点。上面提到了，第一个非叶子结点是索引为5的结点，它和它的子结点62不满足堆的性质，所以对其进行sink()操作；然后对索引为4的结点进行sink()操作;接着是索引为3的节点、索引为2的节点、索引为1的节点(根节点).然后操作完成。 看看算法第四版中的描述: 一个更聪明更有效的办法是从右至左用sink()函数构造子堆。数组的每个位置都已经是一个子堆的根节点了，sink()对于这些子堆也适用。如果一个根节点的两个子结点都已经是堆了，那么在该节点上调用sink()可以将它们变成一个堆。这个过程会递归的建立起堆的秩序。开始时我们只需要扫描数组中的一半元素，因为我们可以跳过大小为1的子堆。最后我们在位置1上调用sink()方法,扫描结束。在排序的第一阶段，堆的构造方法和我们的想象有所不同，因为我们的目标是构造一个堆有序的数组并使最大元素位于数组的开头(次大的元素在附近)而非构造函数结束的末尾。 新的构造函数:1234567891011121314151617// 构造函数, 通过一个给定数组创建一个最大堆// 该构造堆的过程, 时间复杂度为O(n)public MaxHeap(Item[] arr)&#123; int n = arr.length; data = (Item[]) new Comparable[n+1]; capacity = n; for(int i = 0 ; i &lt; n ; i ++)&#123; data[i+1] = arr[i]; &#125; count = n; for(int i = count / 2 ; i &gt;= 1 ; i --)&#123; //从第一个不是叶子结点的节点开始 sink(i); &#125;&#125; 这里要强调，堆排序的效率其实不如快速排序和归并排序，它在处理一些动态问题时是有优势的。 使用这种方式建堆比一个个插入元素建堆的效率是要高的。将n个元素逐个插入到一个空堆中，算法复杂度为O(nlogn).但是heapify的过程，算法复杂度为O(n). 使用这种方法初始化堆，然后进行堆排序虽然在速度上有了一定提升，但是和上面的堆排序一样，他们的空间复杂度都为O(n),即都需要重新构建n个空间的数组来进行相应操作，但是其实可以对其进行优化，使得排序在原数组上进行. 原地堆排序所谓原地堆排序就是直接在相应的数组上进行排序，而不用再额外开辟n个空间将数组复制到堆中，原地堆排序的空间复杂度为O(1).前面的知识我们知道，其实一个堆就是一个数组，只是它的索引从1开始，空间是n+1.所以我们完全可以把传入的数组看成一个堆，然后在它上面进行相应操作。 ①、②：首先，将传入的数组使用heapify操作，将其变成一个最大堆。在最大堆中，第一个元素的位置就是整个数组中最大值. ③：然后，将最大的元素V放在数组末尾的位置，即让arr[0]:V与arr[n]:W交换。这时候，前面的数组arr[0…n-1]部分不再满足最大堆的性质，因为第一个位置为W，它不是最大值。 ④：所以这时候对元素W进行下沉操作，将绿色部分的数组再次转换为最大堆。这时，整个堆中最大的元素又在arr[0]位置，堆末尾的元素在倒数第个位置arr[n-1]处。 ⑤：此时再将堆首和堆尾的元素进行交换。此时的最大元素arr[0]放在了arr[n-1]位置处。由于这次交换，又打破了最大堆的性质，前面的数组(绿色部分)不再是最大堆。 ⑥：这时候继续对索引为0的位置进行下沉操作。使得前半部分数组再次成为最大堆。 这样依次进行操作，知道堆中所有的元素都进入到蓝色部分。 整个算法直接在原数组上进行，空间复杂度为O(1). 由于数组都是从索引0开始的，所以在对其操作时，需要注意相应的索引都应-1. 这时候计算孩子和父亲时需要注意:123456789//父节点parent(i) = (i-1) / 2//孩子节点left child (i) = 2 * i + 1right child (i) = 2 * i + 2//最后一个非叶子节点的索引index = (count-1) / 2 代码实现:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273import java.util.*;// 不使用一个额外的最大堆, 直接在原数组上进行原地的堆排序public class HeapSort &#123; //不产生实例 private HeapSort()&#123;&#125; public static void sort(Comparable[] arr)&#123; int n = arr.length; //对数组进行hepify操作 //注意，此时的堆是从0开始索引的 //从(最后一个元素的索引-1)/2开始 //最后一个元素的索引 = n - 1 for(int i = (n - 1 - 1 ) / 2 ; i &gt;= 0 ; i --)&#123; sink2(arr,n,i); &#125; for(int i = n - 1 ; i &gt; 0 ; i --)&#123; //依次交换数组首个元素和索引为i的元素 exch(arr,0,i); //交换后对数组首元素进行下沉操作，使数组前部分扔为最大堆 sink(arr,i,0); &#125; &#125; //交换堆中索引为i和j的两个元素 private static void exch(Object[] arr,int i,int j)&#123; Object t = arr[i]; arr[i] = arr[j]; arr[j] = t; &#125; /** * sink下沉操作 * @param arr 用来处理的数组 * @param n 元素个数 * @param k 用于下沉的元素索引 */ private static void sink(Comparable[] arr,int n,int k)&#123; while (2 * k + 1 &lt; n)&#123; int j = 2 * k + 1; //左孩子 if( j + 1 &lt; n &amp;&amp; arr[j+1].compareTo(arr[j]) &gt; 0) j += 1; if(arr[k].compareTo(arr[j]) &gt;= 0 ) break; exch(arr,k,j); k = j; &#125; &#125; // 优化的下沉过程, 使用赋值的方式取代不断的exch, // 该优化思想和我们之前对插入排序进行优化的思路是一致的 private static void sink2(Comparable[] arr, int n, int k)&#123; Comparable e = arr[k]; while( 2*k+1 &lt; n )&#123; int j = 2*k+1; if( j+1 &lt; n &amp;&amp; arr[j+1].compareTo(arr[j]) &gt; 0 ) j += 1; if( e.compareTo(arr[j]) &gt;= 0 ) break; arr[k] = arr[j]; k = j; &#125; arr[k] = e; &#125;&#125; 索引堆 - Index Heap前面的介绍中，在进行堆排序时，实际上是在数组中进行了操作，排序的过程让元素在数组中的位置发生改变。当然这只是简单的元素。如果在实际应用中用了更复杂的结构，如字符串，让它进行交换会产生很大的性能消耗；又如本来堆排好序后是表示任务的优先级，现在要改变指定索引处任务则需要进行遍历、修改数据结果等操作，这都要消耗很大的性能。所以引入索引堆。 以最大索引堆为例,对于索引堆来说，将索引和数据分开存储，而真正表征堆的数组，是由索引构建成的，如下图，二叉堆的每个结点存的是索引号，当将数组构建成堆后，data域并没有改变，真正改变的是index域。index数组发生了改变，形成了一个堆-索引堆。 那么如何解读索引堆呢？看上图，此时堆顶的元素是10，意思是堆顶的元素是索引10指向的data[10]:62;相应的，堆顶元素的左孩子为9，即对应data数组中索引为9的元素data[9]:41,右孩子为7,即对应data数组中索引为7的元素data[7]:28…依次类推 这样做的优点是： 构建索引堆的过程只是索引的位置发生交换，而索引就只是简单的int型，如果data中存储的是很复杂的数据结构，仅交换索引的效率是很高的； 如果想对堆中的数据进行操作，比如将进程号为7的系统任务提高优先级，将data[7]:28提高为data[7]:38,进行完提高优先级操作后，这时候还要进行一系列操作维持堆的性质，这时候只需要根据新的data[]数组改变index数组就可以了。 简单的解释就是在数据比较的时候比较的是data中的数据，在进行交换的时候交换的是index的索引。 索引堆的代码实现：见附录二 索引堆的优化在索引堆的代码中，进行change操作时，维护indexes数组时对indexes数组进行了一次遍历，使得其时间复杂度变为了O(n).其实有方法将其时间复杂度提升. 改进方法的思路成为反向查找。如下图，在原来两个数组的基础上又多了一个数组rev(reverse),rev[i]表示i这个索引在数组中的位置是什么。 举例来说，如果用户把位置4处的data:13修改了，修改了之后，就要维护4这个索引在堆中的位置，即维护indexes数组，那么可以通过rev[4]找到其在堆中的位置，即rev[4] = 9,索引4在堆中的位置就是在indexes[9]处。 使用rev数组，在更新操作时就可以使用O(1)复杂度更新索引在indexes中的位置。 0 1 2 3 4 5 6 7 8 9 10 index 10 9 7 8 5 6 3 1 4 2 data 15 17 19 13 22 16 28 30 41 62 rev 8 10 7 9 5 6 3 4 2 1 reverse和indexes的关系:reverse[i] 表示索引i在indexes(堆)中的位置 如果对堆进行了改动，那么reverse也要进行改动:indexes[i] = j reverse[j] = i indexes[reverse[i]] = i reverse[indexes[i]] = i 在书写代码时需要注意，在change函数操作时，输入索引i时，即使i&gt;=1 &amp;&amp; i &lt; capacity满足，但是也不意味着i处的元素在堆中。因此需要引入辅助函数contain(i)来检查索引堆是否包含该索引。需要在getItem和change方法中进行调用. 修改后的代码请见附录三(只写出了发生改变的函数). 其他可以使用堆实现优先队列，动态选择优先级最高的任务执行。 可以借用堆实现多路归并排序。 有了二叉堆的基础，可以基于用数组表示的完全三叉树构造堆，或者构造多叉堆。 本篇的用例都是以最大堆为例进行讲解和代码编写，转换一下逻辑也可以实现最小堆。 本篇中的数组采用的是固定大小的数组，也可是使用动态数组来实现，没有了capacity的限制，动态调整堆中数组的大小。 附录一123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112import java.util.*;// 在堆的有关操作中，需要比较堆中元素的大小，所以Item需要extends Comparablepublic class MaxHeap&lt;Item extends Comparable&gt; &#123; private Item[] data; private int count; private int capacity; //能够容纳的元素数量 //构造函数,构造一个空的堆，能够容纳capacity个元素 public MaxHeap(int capacity)&#123; data = (Item[])new Comparable[capacity + 1]; count = 0; this.capacity = capacity; &#125; //返回堆中的元素个数 public int size()&#123; return count; &#125; //返回一个布尔值，表示堆是否为空 public boolean isEmpty()&#123; return count == 0; &#125; //插入函数 public void insert(Item item)&#123; if(count + 1 &gt; capacity)&#123; throw new IllegalArgumentException("out of bounds!"); &#125; data[count + 1] = item; //因为索引0位置不用，所以要+1 count ++; //上面两行可以写成data[++count] = item; swim( count ); &#125; //从最大堆中取出堆顶元素 public Item extractMax()&#123; if(!(count &gt; 0))&#123; throw new IllegalArgumentException("count should be &gt; 0!"); &#125; Item ret = data[1]; exch(1,count); count --; sink(1); return ret; &#125; // 获取最大堆中的堆顶元素 public Item getMax()&#123; if(!(count &gt; 0))&#123; throw new IllegalArgumentException("count should be &gt; 0!"); &#125; return data[1]; &#125; //最大堆核心辅助函数——swim()，进行元素上浮操作 private void swim(int k)&#123; while ( k &gt; 1 &amp;&amp; less(k/2,k))&#123; exch(k/2,k); k /= 2; &#125; &#125; //最大堆核心辅助函数——sink(),进行元素下沉 private void sink(int k)&#123; while (2 * k &lt;= count)&#123; int j = 2 * k;// 在此轮循环中,data[k]和data[j]交换位置 if( j+1 &lt;= count &amp;&amp; less(j,j+1)) //判断是否有右孩子,如果有的话得到左孩子和右孩子中较大的值 j ++; // data[j] 是 data[2*k]和data[2*k+1]中的最大值 if(!less(k,j)) //data[k].compareTo(data[j]) &gt;= 0 break; exch(k,j); k = j; &#125; &#125; //辅助函数less():比较索引i处的值是否小于索引j处的值 private boolean less(int i, int j)&#123; return data[i].compareTo(data[j]) &lt; 0; &#125; //辅助函数交换堆中索引为i和j的两个元素 private void exch(int i,int j)&#123; Item t = data[i]; data[i] = data[j]; data[j] = t; &#125; // 测试 MaxHeap public static void main(String[] args) &#123; MaxHeap&lt;Integer&gt; maxHeap = new MaxHeap&lt;Integer&gt;(100); int N = 100; // 堆中元素个数 int M = 100; // 堆中元素取值范围[0, M) for( int i = 0 ; i &lt; N ; i ++ ) maxHeap.insert( new Integer((int)(Math.random() * M)) ); Integer[] arr = new Integer[N]; // 将maxheap中的数据逐渐使用extractMax取出来 // 取出来的顺序应该是按照从大到小的顺序取出来的 for( int i = 0 ; i &lt; N ; i ++ )&#123; arr[i] = maxHeap.extractMax(); System.out.print(arr[i] + " "); &#125; System.out.println(); // 确保arr数组是从大到小排列的 for( int i = 1 ; i &lt; N ; i ++ ) assert arr[i-1] &gt;= arr[i]; &#125;&#125; 附录二123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143import java.util.*;import java.lang.*;//最大索引堆public class IndexMaxHeap&lt;Item extends Comparable&gt; &#123; protected Item[] data; //存储最大索引堆中的数据 protected int[] indexes;//存储最大索引堆中的索引 protected int count; protected int capacity; //构造函数,构造一个空堆,可容纳capacity个元素 public IndexMaxHeap(int capacity)&#123; data = (Item[])new Comparable[capacity +1 ]; indexes = new int[capacity + 1]; this.capacity = capacity; &#125; //返回索引堆中的元素个数 public int size()&#123; return count; &#125; //返回一个布尔值,表示索引堆中是否为空 public boolean isEmpty()&#123; return count == 0; &#125; // 获取最大索引堆中索引为i的元素 public Item getItem( int i )&#123; if(!(i + 1 &gt;= 1 &amp;&amp; i + 1 &lt;= capacity))&#123; throw new IllegalArgumentException("out of bounds!"); &#125; return data[i+1]; //对用户来说，索引从0开始，所以这里要+1 &#125; //向最大索引堆中插入一个新的元素,新元素的索引为i,元素为item //传入的i对用户而言,是从0索引的 public void insert(int i , Item item)&#123; if(count + 1 &gt; capacity)&#123; throw new IllegalArgumentException("out of bounds!"); &#125; if(!(i + 1 &gt;= 1 &amp;&amp; i + 1 &lt;= capacity))&#123; throw new IllegalArgumentException("out of bounds!"); &#125; //对用户而言是从索引0开始，但是内部还是从1开始 i += 1; data[i] = item; //将item插入到data[i]处 indexes[count + 1 ] = i; //之前是插入到data堆中，现在则插入到indexes堆底 count ++; swim(count); &#125; //从最大索引堆中取出堆顶元素,即索引堆中所存储的最大数据 public Item extractMax()&#123; if(count &lt;= 0)&#123; throw new IllegalArgumentException("count should be &gt; 0."); &#125; Item ret = data[indexes[1]]; exchIndexes(1,count); count --; sink(1); return ret; &#125; // 从最大索引堆中取出堆顶元素的索引 public int extractMaxIndex()&#123; if(count &lt;= 0)&#123; throw new IllegalArgumentException("count should be &gt; 0."); &#125; int ret = indexes[1] - 1; //对用户来说，索引从0开始，所以要-1 exchIndexes( 1 , count ); count --; sink(1); return ret; &#125; // 获取最大堆中的堆顶元素 public Item getMax()&#123; if(!(count &gt; 0))&#123; throw new IllegalArgumentException("count should be &gt; 0!"); &#125; return data[indexes[1]]; &#125; // 获取最大索引堆中的堆顶元素的索引 public int getMaxIndex()&#123; if(!(count &gt; 0))&#123; throw new IllegalArgumentException("count should be &gt; 0!"); &#125; return indexes[1]-1; //对用户来说，索引是从0开始的，所以要减一 &#125; //将最大索引堆中索引为i的元素修改为newItem public void change(int i , Item newItem)&#123; i += 1; data[i] = newItem; //找到indexes[j] = i ,j表示data[i]在堆中的位置 //之后进行sink(j)、swim(j) (前后顺序可变) for(int j = 1 ; j &lt; count ; j ++)&#123; if(indexes[j] == i)&#123; swim(j); sink(j); return; &#125; &#125; &#125; // 交换索引堆中的索引i和j private void exchIndexes(int i, int j)&#123; int t = indexes[i]; indexes[i] = indexes[j]; indexes[j] = t; &#125; //在索引堆中，数据之间的比较根据data的大小进行比较，但实际操作的是索引 private void swim(int k) &#123; while(k &gt; 1 &amp;&amp; data[indexes[k/2]].compareTo(data[indexes[k]])&lt; 0)&#123; //原来直接对于data数组的操作要通过indexes索引堆间接找到data中的元素 exchIndexes(k,k/2); k /= 2; &#125; &#125; // 索引堆中, 数据之间的比较根据data的大小进行比较, 但实际操作的是索引 private void sink(int k)&#123; while(2 * k &lt;= count)&#123; int j = 2 * k; if(j + 1 &lt;= count &amp;&amp; data[indexes[j+1]].compareTo(data[indexes[j]]) &gt; 0) j ++; if(data[indexes[k]].compareTo(data[indexes[j]]) &gt;= 0) break; exchIndexes(k,j); k = j; &#125; &#125;&#125; 附录三123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235import java.util.*;import java.lang.*;//最大索引堆public class IndexMaxHeap&lt;Item extends Comparable&gt; &#123; protected Item[] data; //存储最大索引堆中的数据 protected int[] indexes;// 最大索引堆中的索引, indexes[x] = i 表示索引i在x的位置 protected int[] reverse;// 最大索引堆中的反向索引, reverse[i] = x 表示索引i在x的位置 protected int count; protected int capacity; //构造函数,构造一个空堆,可容纳capacity个元素 public IndexMaxHeap(int capacity)&#123; data = (Item[])new Comparable[capacity +1 ]; indexes = new int[capacity + 1]; reverse = new int[capacity + 1]; //reverse表示i索引在堆中的位置 //取标记值,i不存在时应为0.因为索引是从1开始计数，0没有意义. for(int i = 0 ; i &lt;= capacity ; i ++) reverse[i] = 0; this.capacity = capacity; &#125; //返回索引堆中的元素个数 //... //返回一个布尔值,表示索引堆中是否为空 //... // 获取最大索引堆中索引为i的元素 public Item getItem( int i )&#123; if(!(i + 1 &gt;= 1 &amp;&amp; i + 1 &lt;= capacity))&#123; throw new IllegalArgumentException("out of bounds!"); &#125; if(!(contain(i)))&#123; throw new IllegalArgumentException("index i is not in the heap!"); &#125; return data[i+1]; //对用户来说，索引从0开始，所以这里要+1 &#125; //向最大索引堆中插入一个新的元素,新元素的索引为i,元素为item //传入的i对用户而言,是从0索引的 public void insert(int i , Item item)&#123; if(count + 1 &gt; capacity)&#123; throw new IllegalArgumentException("out of bounds!"); &#125; if(!(i + 1 &gt;= 1 &amp;&amp; i + 1 &lt;= capacity))&#123; throw new IllegalArgumentException("out of bounds!"); &#125; //在插入一个新元素前，还需要保证索引i所在的位置是没有元素的 if(contain(i))&#123; throw new IllegalArgumentException("index i is not in the heap!"); &#125; //对用户而言是从索引0开始，但是内部还是从1开始 i += 1; data[i] = item; //将item插入到data[i]处 indexes[count + 1 ] = i; //之前是插入到data堆中，现在则插入到indexes堆底 reverse[i] = count + 1; count ++; swim(count); &#125; //从最大索引堆中取出堆顶元素,即索引堆中所存储的最大数据 public Item extractMax()&#123; if(count &lt;= 0)&#123; throw new IllegalArgumentException("count should be &gt; 0."); &#125; Item ret = data[indexes[1]]; exchIndexes(1,count); reverse[indexes[count]] = 0; //因为交换后，相当于把栈顶元素放到了最后，然后用count--就相当于删除这个元素，删除这个元素那么它的reverse值指向0就可以了. count --; sink(1); return ret; &#125; // 从最大索引堆中取出堆顶元素的索引 public int extractMaxIndex()&#123; if(count &lt;= 0)&#123; throw new IllegalArgumentException("count should be &gt; 0."); &#125; int ret = indexes[1] - 1; //对用户来说，索引从0开始，所以要-1 exchIndexes( 1 , count ); reverse[indexes[1]] = 1; reverse[indexes[count]] = 0; //因为交换后，相当于把栈顶元素放到了最后，然后用count--就相当于删除这个元素，删除这个元素那么它的reverse值指向0就可以了. count --; sink(1); return ret; &#125; // 获取最大堆中的堆顶元素 public Item getMax()&#123; if(!(count &gt; 0))&#123; throw new IllegalArgumentException("count should be &gt; 0!"); &#125; return data[indexes[1]]; &#125; // 获取最大索引堆中的堆顶元素的索引 public int getMaxIndex()&#123; if(!(count &gt; 0))&#123; throw new IllegalArgumentException("count should be &gt; 0!"); &#125; return indexes[1]-1; //对用户来说，索引是从0开始的，所以要减一 &#125; //将最大索引堆中索引为i的元素修改为newItem public void change(int i , Item newItem)&#123; if(!(contain(i)))&#123; throw new IllegalArgumentException("index i is not in the heap!"); &#125; i += 1; data[i] = newItem; //找到indexes[j] = i ,j表示data[i]在堆中的位置 //之后进行sink(j)、swim(j) (前后顺序可变) // 有了 reverse 之后可以非常简单的通过reverse直接定位索引i在indexes中的位置 int j = reverse[i]; swim(j); sink(j);// for(int j = 1 ; j &lt; count ; j ++)&#123;// if(indexes[j] == i)&#123;// swim(j);// sink(j);// return;// &#125;// &#125; &#125; // 交换索引堆中的索引i和j // 由于有了反向索引reverse数组， // indexes数组发生改变以后， 相应的就需要维护reverse数组 private void exchIndexes(int i, int j)&#123; int t = indexes[i]; indexes[i] = indexes[j]; indexes[j] = t; reverse[indexes[i]] = i; reverse[indexes[j]] = j; &#125; //在索引堆中，数据之间的比较根据data的大小进行比较，但实际操作的是索引 private void swim(int k) &#123; while(k &gt; 1 &amp;&amp; data[indexes[k/2]].compareTo(data[indexes[k]])&lt; 0)&#123; //原来直接对于data数组的操作要通过indexes索引堆间接找到data中的元素 exchIndexes(k,k/2); //下面两行写在了exchIndexes中 // reverse[indexes[k/2]] = k / 2; // reverse[indexes[k]] = k; k /= 2; &#125; &#125; // 索引堆中, 数据之间的比较根据data的大小进行比较, 但实际操作的是索引 private void sink(int k)&#123; while(2 * k &lt;= count)&#123; int j = 2 * k; if(j + 1 &lt;= count &amp;&amp; data[indexes[j+1]].compareTo(data[indexes[j]]) &gt; 0) j ++; if(data[indexes[k]].compareTo(data[indexes[j]]) &gt;= 0) break; exchIndexes(k,j); //下面两行写在了exchIndexes中 // reverse[indexes[k]] = k; // reverse[indexes[j]] = j; k = j; &#125; &#125; //判断索引是否在堆中 private boolean contain( int i )&#123; if(!(i + 1 &gt;= 1 &amp;&amp; i + 1 &lt;= capacity))&#123; throw new IllegalArgumentException("i out of bounds!"); &#125; return reverse[ i + 1 ] != 0; &#125; // 测试索引堆中的索引数组index和反向数组reverse // 注意:这个测试在向堆中插入元素以后, 不进行extract操作有效 public boolean testIndexes()&#123; int[] copyIndexes = new int[count+1]; int[] copyReverseIndexes = new int[count+1]; for( int i = 0 ; i &lt;= count ; i ++ ) &#123; copyIndexes[i] = indexes[i]; copyReverseIndexes[i] = reverse[i]; &#125; copyIndexes[0] = 0; copyReverseIndexes[0] = 0; Arrays.sort(copyIndexes); Arrays.sort(copyReverseIndexes); // 在对索引堆中的索引和反向索引进行排序后, // 两个数组都应该正好是1...count这count个索引 boolean res = true; for( int i = 1 ; i &lt;= count ; i ++ ) if( copyIndexes[i-1] + 1 != copyIndexes[i] || copyReverseIndexes[i-1] + 1 != copyReverseIndexes[i] )&#123; res = false; break; &#125; if( !res )&#123; System.out.println("Error!"); return false; &#125; return true; &#125; // 测试 IndexMaxHeap public static void main(String[] args) &#123; int N = 1000000; IndexMaxHeap&lt;Integer&gt; indexMaxHeap = new IndexMaxHeap&lt;Integer&gt;(N); for( int i = 0 ; i &lt; N ; i ++ ) indexMaxHeap.insert( i , (int)(Math.random()*N) ); assert indexMaxHeap.testIndexes(); &#125;&#125;]]></content>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最小生成树]]></title>
    <url>%2F2019%2F04%2F05%2F%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%2F</url>
    <content type="text"><![CDATA[有权图有权图是指在无权图的基础上，每一条边都有一个数值，代表两个结点之间有一定的数值关系，比如图用来表示路网时，权值可以表示两点之间的距离，到达彼此所需要的时间等。 有权图的邻接矩阵表达见下图: 有权图的邻接表表达见下图:s 邻接表每个节点后面存储两个信息:to表示和它相邻的节点的索引，w表示对应的边的权值.所以可以将这两个信息封装为Edge属性，它包含to和w属性。 当然要注意一点，邻接矩阵中依然用了传统的二维数组来存储，为了统一图的接口，也可以将a[i][j]位置存储为Edge格式。 这里就直接先写java代码了 创建WeightedGraph.java接口:12345678public interface WeightedGraph&lt;Weight extends Number &amp; Comparable&gt; &#123; public int V(); //获取图的顶点数 public int E(); //获取图的边数 public void addEdge(Edge&lt;Weight&gt; e); //在v和w两个顶点间添加一条边 boolean hasEdge( int v , int w);//查看v和w两个顶点间是否有边 void show();//打印图 public Iterable&lt;Edge&lt;Weight&gt;&gt; adj(int v); //获取与v顶点连接的所有边&#125; 为了实现有权图，先编写一个Edge.java类:12345678910111213141516171819202122232425262728293031323334353637383940414243//边public class Edge&lt;Weight extends Number &amp; Comparable&gt; implements Comparable&lt;Edge&gt; &#123; private int a,b; //边的两个端点 private Weight weight; //边的权值 public Edge(int a, int b , Weight weight)&#123; this.a = a ; this.b = b ; this.weight = weight; &#125; public Edge(Edge&lt;Weight&gt; e)&#123; this.a = e.a; this.b = e.b; this.weight = e.weight; &#125; public int v() &#123;return a;&#125; //返回第一个顶点 public int w() &#123;return b;&#125; //返回第二个顶点 public Weight wt() &#123;return weight;&#125; // 返回权值 //给定一个顶点，返回另一个顶点 public int other(int x)&#123; if( x == a || x == b)&#123; return x == a ? b : a; &#125;else throw new IllegalArgumentException("x is not correct"); &#125; //输出边的信息 public String toString()&#123; return "" + a + "-" + b + ": " + weight; &#125; @Override public int compareTo(Edge that) &#123; if(weight.compareTo(that.wt()) &lt; 0) return -1; else if(weight.compareTo(that.wt()) &gt; 0) return +1; else return 0; &#125;&#125; 在DenseGraph.java的基础上修改，创建DenseGraph.java:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192import java.util.Vector;public class DenseWeightedGraph&lt;Weight extends Number &amp; Comparable&gt; implements WeightedGraph &#123; private int n; //节点数 private int m; //边数 private boolean directed; // 是否为有向图 private Edge&lt;Weight&gt;[][] g; //图的具体数据，用二维数组表达 //构造函数 public DenseWeightedGraph( int n , boolean directed)&#123; if(n &lt; 0) throw new IllegalArgumentException("the value of n should be &gt;= 0."); this.n = n; this.m = 0 ;//初始化时没有任何边 this.directed = directed; // g初始化为n*n的布尔矩阵, 每一个g[i][j]均为null, 表示没有任和边 // false为boolean型变量的默认值 g = new Edge[n][n]; for(int i = 0 ; i &lt; n ; i ++) for(int j = 0 ; j &lt; n ;j ++) g[i][j] = null; &#125; //返回节点个数 @Override public int V() &#123; return n; &#125; //返回边数 @Override public int E() &#123; return m; &#125; //向图中添加一条边 @Override public void addEdge(Edge e) &#123; if(!(e.v() &gt;= 0 &amp;&amp; e.v() &lt; n)) throw new IllegalArgumentException("you should type v &gt;= 0 &amp;&amp; v &lt; n"); if(!(e.w() &gt;= 0 &amp;&amp; e.w() &lt; n)) throw new IllegalArgumentException("you should type w &gt;= 0 &amp;&amp; w &lt; n"); if(hasEdge(e.v(),e.w())) return; g[e.v()][e.w()] = new Edge(e); if(e.v() != e.w() &amp;&amp; !directed) g[e.w()][e.v()] = new Edge(e.w(),e.v(),e.wt()); m ++; &#125; //判断图中是否有v到w的边 @Override public boolean hasEdge(int v, int w) &#123; if(!(v &gt;= 0 &amp;&amp; v &lt; n)) throw new IllegalArgumentException("the value of v is Illegal!you should type the value of v between 0 and n.(v &gt;=0 &amp;&amp; v &lt; n)."); if(!(w &gt;= 0 &amp;&amp; w &lt; n)) throw new IllegalArgumentException("the value of w is Illegal!you should type the value of w between 0 and n.(w &gt;=0 &amp;&amp; w &lt; n)."); return g[v][w] != null; &#125; //打印显示图的信息 @Override public void show() &#123; for(int i = 0 ; i &lt; n ; i ++)&#123; for( int j = 0 ; j &lt; n ; j ++) if( g[i][j] != null) System.out.println(g[i][j].wt()+"\t"); else System.out.println("NULL\t"); System.out.println(); &#125; &#125; //返回图中v顶点的所有邻边 //由于java使用引用机制，返回一个Vector不会带来额外开销 @Override public Iterable&lt;Edge&lt;Weight&gt;&gt; adj(int v) &#123; if(!(v &gt;= 0 &amp;&amp; v &lt; n)) throw new IllegalArgumentException("the value of v is Illegal!you should type the value of v between 0 and n.(v &gt;=0 &amp;&amp; v &lt; n)."); Vector&lt;Edge&lt;Weight&gt;&gt; adjV = new Vector&lt;Edge&lt;Weight&gt;&gt;(); for(int i = 0 ; i &lt; n ; i++) if(g[v][i] != null) adjV.add(g[v][i]); return adjV; &#125;&#125; 同样的修改SparseGraph.java后得到SparseWeightedGraph.java：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586import java.util.Vector;public class SparseWeightedGraph&lt;Weight extends Number &amp; Comparable&gt; implements WeightedGraph &#123; private int n; //节点数 private int m; //边数 private boolean directed; //是否为有向图 private Vector&lt;Edge&lt;Weight&gt;&gt;[] g; //图的具体数据 //构造函数 public SparseWeightedGraph(int n , boolean directed)&#123; if(n &lt; 0) throw new IllegalArgumentException("the value of n should be &gt;= 0."); this.n = n; this.m = 0; this.directed = directed; //g初始化为n个空的vector,表示每一个g[i]都为空，即没有任何边 g = (Vector&lt;Edge&lt;Weight&gt;&gt;[]) new Vector[n]; for(int i = 0 ; i &lt; n ; i ++) g[i] = new Vector&lt;Edge&lt;Weight&gt;&gt;(); &#125; //返回节点个数 @Override public int V() &#123; return n; &#125; //返回边数 @Override public int E() &#123; return m; &#125; //向图中添加一条边 @Override public void addEdge(Edge e) &#123; if(!(e.v() &gt;= 0 &amp;&amp; e.v() &lt; n)) throw new IllegalArgumentException("the value of v is Illegal!you should type the value of v between 0 and n.(v &gt;=0 &amp;&amp; v &lt; n)."); if(!(e.w() &gt;= 0 &amp;&amp; e.w() &lt; n)) throw new IllegalArgumentException("the value of w is Illegal!you should type the value of w between 0 and n.(w &gt;=0 &amp;&amp; w &lt; n)."); g[e.v()].add(new Edge&lt;&gt;(e)); if(e.v() != e.w() &amp;&amp; !directed) //如果不是自环边，并且它是无向图，则创建w到v的边 g[e.w()].add(new Edge(e.w(),e.v(),e.wt())); m ++; &#125; //验证图中是否有v到w的边 @Override public boolean hasEdge(int v, int w) &#123; if(!(v &gt;= 0 &amp;&amp; v &lt; n)) throw new IllegalArgumentException("the value of v is Illegal!you should type the value of v between 0 and n.(v &gt;=0 &amp;&amp; v &lt; n)."); if(!(w &gt;= 0 &amp;&amp; w &lt; n)) throw new IllegalArgumentException("the value of w is Illegal!you should type the value of w between 0 and n.(w &gt;=0 &amp;&amp; w &lt; n)."); for(int i = 0 ; i &lt; g[v].size() ; i++) if( g[v].elementAt(i).other(v) == w) return true; return false; &#125; //显示图的信息 @Override public void show() &#123; for(int i = 0; i &lt; n ; i ++)&#123; System.out.printf("vertex %d :\t",i); for(int j = 0 ; j &lt; g[i].size() ; j++)&#123; Edge e = g[i].elementAt(j); System.out.print("( to:" + e.other(i) + ",wt:" + e.wt() + ")\t"); &#125; System.out.println(); &#125; &#125; // 返回图中一个顶点的所有邻边 // 由于java使用引用机制，返回一个Vector不会带来额外开销, @Override public Iterable&lt;Edge&lt;Weight&gt;&gt; adj(int v) &#123; if(!(v &gt;= 0 &amp;&amp; v &lt; n)) throw new IllegalArgumentException("the value of v is Illegal!you should type the value of v between 0 and n.(v &gt;=0 &amp;&amp; v &lt; n)."); return g[v]; &#125;&#125; 修改ReadGraph.java后得到ReadWeightedGraph:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import java.io.BufferedInputStream;import java.io.File;import java.io.FileInputStream;import java.io.IOException;import java.util.InputMismatchException;import java.util.Locale;import java.util.NoSuchElementException;import java.util.Scanner;public class ReadWeightedGraph &#123; private Scanner scanner; public ReadWeightedGraph(WeightedGraph&lt;Double&gt; graph , String filename)&#123; readFile(filename); try&#123; int V = scanner.nextInt(); if(V &lt; 0 ) throw new IllegalArgumentException("number of vertices in a Graph must be nonnegative"); assert V == graph.V(); int E = scanner.nextInt(); if(E &lt; 0 ) throw new IllegalArgumentException("number of edges in a Graph must be nonnegative"); for(int i = 0 ; i &lt; E ; i ++)&#123; int v = scanner.nextInt(); int w = scanner.nextInt(); Double weight = scanner.nextDouble(); //读取权值 if(!(v &gt;= 0 &amp;&amp; v &lt; V)) throw new IllegalArgumentException("the value of v is Illegal!you should type the value of v between 0 and n.(v &gt;=0 &amp;&amp; v &lt; n)."); if(!(w &gt;= 0 &amp;&amp; w &lt; V)) throw new IllegalArgumentException("the value of w is Illegal!you should type the value of w between 0 and n.(w &gt;=0 &amp;&amp; w &lt; n)."); graph.addEdge(new Edge&lt;Double&gt;(v,w,weight) ); &#125; &#125;catch (InputMismatchException e)&#123; String token = scanner.next(); throw new InputMismatchException("attempts to read an 'int' value from input stream,but the next token is \""+token +"\""); &#125;catch(NoSuchElementException e)&#123; throw new NoSuchElementException("attemps to read an 'int' value from input stream, but there are no more tokens available"); &#125; &#125; private void readFile(String filename)&#123; if(filename == null) throw new IllegalArgumentException("filename should not be null!"); try &#123; File file = new File(filename); if (file.exists()) &#123; FileInputStream fis = new FileInputStream(file); scanner = new Scanner(new BufferedInputStream(fis), "UTF-8"); scanner.useLocale(Locale.ENGLISH); &#125;else throw new IllegalArgumentException(filename + "doesn't exists."); &#125;catch (IOException ex)&#123; throw new IllegalArgumentException("Could not open " + filename,ex); &#125; &#125;&#125; 测试文件testG1.txt:12345678910111213141516178 164 5 .354 7 .375 7 .280 7 .161 5 .320 4 .382 3 .171 7 .190 2 .261 2 .361 3 .292 7 .346 2 .403 6 .526 0 .586 4 .93 在Main.java中进行测试:12345678910111213141516171819202122public class Main &#123; // 测试通过文件读取图的信息 public static void main(String[] args) &#123; // 使用两种图的存储方式读取testG1.txt文件 String filename = "testG1.txt"; SparseWeightedGraph&lt;Double&gt; g1 = new SparseWeightedGraph&lt;Double&gt;(8, false); ReadWeightedGraph readGraph1 = new ReadWeightedGraph(g1, filename); System.out.println("test G1 in Sparse Weighted Graph:"); g1.show(); System.out.println(); DenseWeightedGraph&lt;Double&gt; g2 = new DenseWeightedGraph&lt;Double&gt;(8, false); ReadWeightedGraph readGraph2 = new ReadWeightedGraph(g2 , filename ); System.out.println("test G1 in Dense Graph:"); g2.show(); System.out.println(); &#125;&#125; 最小生成树问题算法（第四版）中的定义：最小生成树。给定一个加权无向图，找到它的一棵最小生成树（Minimum Span Tree）。 如果一个图有v个节点，那么就应该有v-1条边连接这V个节点，这就是这个图的生成树。不仅如此，这v-1条边连接了所有的v个节点，这v-1条边的权值相加也是最小的，如何找到这个生成树，就是最小生成树问题。 一些约定这里在计算最小生成树的过程中会出现各种特殊情况，所以为了行文流畅，进行约定： 只考虑连通图 针对带权无向图 所有边的权重都各不相同 所以在进行最小生成树相关算法的过程中要找V-1条边，连接V个顶点使得其总权值最小。 切分定理 - Cut Property把图中的结点分为两个部分，成为一个切分(Cut). 如下图就是一个切分: 如果一个边的两个端点，属于切分(Cut)不同的两边，这个边称为横切边(Crossing Edge). 如下图的蓝色线条就是横切边: 切分定理的定义：在一副加权图中，给定任意切分，横切边中权值最小的边必然属于图的最小生成树。 例如上图中的权重为0.4的边肯定是最小生成树的一条边: 算法(第四版)中的证明: 今e为权重最小的横切边，T为图的最小生成树。我们采用反证法:假设T不包含e。那么如果将e加入T，得到的图必然含有一条经过e的环，且这个环至少含有另一条横切边——设为f，f的权重必然大于e(因为e的权重是最小的且图中所有边的权重均不同)。那么我们删掉f而保留e就可以得到一棵权重更小的生成树。这和我们的假设T矛盾。 在假设所有的边的权重均不相同的前提下，每幅连通图都只有一棵唯一的最小生成树，切分定理也表明了对于每一种切分，权重最小的横切边必然属于最小生成树。 切分定理是解决最小生成树问题的所有算法的基础。更确切的说，这些算法都是一种贪心算法的特殊情况:使用切分定理找到最小生成树的一条边，不断重复直到找到最小生成树的所有边。这些算法相互之间的不同之处在于保存切分和判定权重最小的横切边的方式。 Lazy Prim算法 首先将一个起始节点作为切分的一部分，这里从0节点开始。将0节点作为切分的一部分，剩下的节点作为切分的另一部分。每一次找到横切边中权值最小的边。这里可以使用最小堆，将横切边放入最小堆中，作为最小生成树所包含的边的候选。这些边进入堆后，下一步只需拿出最小堆中的最短边就可以了。 这一步找出的是0-7:0.16权值为0.16的边，它一定属于最小生成树。 第二步，确定了0-7这条边为最小生成树的一条边，而结点7没有被访问过，此时就可以将结点7加入到红色节点部分，这样就形成了一个新的切分。这个新的切分就和另一部分切分形成了新的横切边。 然后将这些新的横切边推入最小堆，然后在最小堆中选出权值最小的边1-7:0.19。 第三步，确定了1-7为最小生成树的一条边，而节点1没有被访问过，此时将结点1加入到红色结点部分，这样又形成了一个新的切分。也相应的和另一部分切分形成了新的横切边。将这些新的横切边加入到最小堆。 接着看在候选的横切边中，最短的边为0-2:0.26的边。 第四步，确定了0-2为最小生成树的一条边，而结点2没有被访问过，所以将结点2加入红色结点部分，这样就又形成一个新的切分。又有新的边成为了横切边。将他们加入到最小堆中。 此时需要注意，将结点2加入到红色部分后，最小堆中蓝色的部分中，边2-7:0.34和边1-2:0.36，实际上已经不是横切边了，所以这两条边不应该成为最小生成树的边的候选了。所以这里体现出Lazy Prim算法的”懒惰”性，当前这两条边虽然不可能成为最小生成树的候选边，但是这里不需要着急将其剔除，先将其保留在最小堆中，当拿出这两条边时，发现这两条边不是横切边时，再将这两条边剔除。 接着在最小堆中寻找最小的边，最短边为2-3:0.17的边. 第五步，确定了2-3为最小生成树的一条边，而结点3没有被访问过，所以将结点3加入到红色结点部分，形成新的切分后，将新的横切边加入到最小堆中。 接着最小堆中的最小边应该是5-7:0.28的边。 第六步，确定了5-7为最小生成树的一条边，而结点5没有被访问过，所以将结点5加入到红色部分，形成新的切分并将新的横切边加入到最小堆中。 接着看在最小堆中，最小的边为1-3:0.29的边，当将其拿出后发现，结点1和结点3都是红色的结点，即这条边不是一条横切边（只有横切边中的最小边才是最小生成树的边），所以将其拿出后剔除。接着看在最小堆中的最短边为1-5:0.32的边，拿出后发现结点1和结点5都是红色的结点，所以也要将其剔除。接着看剩下的最小堆中的最短边为2-7:0.34，也不满足横切边的性质，剔除。接着看剩下的最小堆中的最短边为4-5:0.35，满足横切边的性质，所以将这条边作为最小生成树的边。 第七步，确定了5-4为最小生成树的一条边，由于结点4没有被访问过，所以将结点4加入到红色部分，行成新的切分并将新的横切边4-6:0.93加入到最小堆中。 然后在最小堆中找最短的边，为1-2:0.36这条边，但是它不是横切边，将其剔除。接着最小堆中最短的边为4-7:0.37，它不是横切边，将其剔除。继续找最小堆中最短边为0-4:0.38，不是横切边，将其剔除。继续找最小堆中的最短边为2-6:0.40，它是横切边，将其作为最小生成树的边。 由于结点6没有被访问过，将其加入到红色部分。这时所有节点都在红色部分，程序至此就可以结束。 如果以最小堆中的边为空作为判断依据的话，那么依然可以拿出最小堆中的最小边进行判断，此时所有的边都不是横切边，将剩下的边判断完后，程序结束。 此时就用Lazy Prim算法获得了最小生成树。 Lazy Prim算法代码见附录1 Prim算法的优化pq.extractMin()操作的时间复杂度为O(logE),因为pq中最多承载E条边。visit()操作时，遍历节点的所有临边，合在一起也是O(E)，如果是邻接矩阵就是O(V^2)，在稠密图的邻接矩阵中O(V^2)和O(E)是一个级别的。同时其中有个add()操作，它是O(logE)级别的。 所以，Lazy Prim算法的时间复杂度为O(ElogE). 通过优化，Prim算法的时间复杂度可以改进为O(ElogV). Lazy Prim算法的一个问题就是，所有的边都要进入最小堆，随着切分的进行，很多已经不是横切边的边仍在最小堆中。另一问题是，虽然有很多横切边，但我们只关注最短的那个横切边，尤其是和一个节点相连的很多横切边，其实只需考虑和这个点相连的最短的横切边就可以了。 基于这个思想，需要维护一个数据结构——存储和每个节点相连的那个最短的横切边。在不断增加红色节点改变切分的过程中，只要不断更新和每个节点相连的最短的横切边就可以了。即这个数据结构要满足：①它能取到最小值②能够供我们更新。所以需要使用IndexMinHeap. 第一步:以0作为起点开始，由于最小生成树有V-1个边，而IndexMinHeap有V个空间，所以肯定有一个节点不需要存储任何东西。就将初始结点作为不需要存储东西的节点。此时有了初始节点后，相当于有了切分，这时，这个节点的所有临边都是横切边，这种情况下，就将这些横切边加入到IndexMinHeap中。此时，和2相连的横切边，最小权值是0.26;和4相连的横切边，最小权值是0.38;和6相连的横切边，最小权值是0.58;和7相连的横切边，最小权值是0.16. 这时，从IndexMinHeap中找出权值最小的一条边，根据切分定理，一定是最小生成树的边，即0.16这条边。这时节点7加入红色部分。 第二步，由于结点7加入了红色部分。要考虑更多的横切边，这时遍历结点7的临边。先看1-7，此时IndexMinHeap中没有和1相连的横切边权值最小的边，所以IndexMinHeap中1的值更新为0.19;再看2-7，其值为0.34，而IndexMinHeap中和2相连的横切边权值最小的是0-2:0.26，所以2-7肯定不会是最小生成树中的一条边，可以直接剔除；接着看4-7，这个边的权值为0.37,它小于当前IndexMinHeap中存储的0.38，所以将4-7:0.37更新到最小索引堆中，与此同时，意味着将0-4:0.38这条边丢弃，因为这条边不可能会是最小生成树的边;最后，看5-7，这个边的权值为0.28，当前IndexMinHeap和5相连的边还没有，所以将其放入IndexMinHeap索引为5的位置中。这时候就完成了visit(7)操作。 然后从最小索引堆IndexMinHeap中找出权值最小的边，即和1相连的1-7:0.19这条边，可以确定它属于最小生成树。因此结点1也可以加入红色部分 第三步，看和结点1相邻的所有边。首先是1-2:0.36，在IndexMinHeap中索引为2的位置其值为0.26，所以1-2不是最小生成树的边；接着看1-5:0.32，但是在IndexMinHeap中和5相连的横切边最小的值为0.28，所以它也不是最小生成树的边;最后看1-3:0.29，由于IndexMinHeap中没有和3相连的横切边，将其放入IndexMinHeap中。 然后从最小索引堆中找出权值最小的边，即和2相连的0-2:0.26这条边，可以确定它属于最小生成树。与此同时结点2加入红色部分。 第四步，看和结点2相邻的所有边。这里注意2-7和1-2不是横切边，所以不用再看了；然后看2-3:0.17，此时最小索引堆中索引为3的位置的值为0.29，所以将其更新为0.17,同时0.29这条边可以剔除不在考虑;接着看2-6:0.40，此时最小索引堆中的索引为6的位置的值为0.58，所以将其更新为0.40，同时0.58这条边可以剔除不在考虑了. 然后从最小索引堆中取出权值最小的边，即和3相连的0.17这条边，根据切分定理，其一定属于最小生成树。同时，将接点3加入红色部分。 第五步，看和接点3相邻的所有边。1-3不是横切边，所以不用看；剩下一条3-6:0.52,其值0.52大于最小索引堆索引为6处的0.40，所以这条边剔除不再考虑。 然后从最小索引堆中找出权值最小的边，即和5相连的0.28这条边，它一定是最小生成树的边。同时将结点5加入红色部分。 第六步，看和节点5相邻的所有边。1-5、5-7不是横切边，所以不用再看；4-5:0.35其值小于最小索引堆索引为4处的值0.37，将其更新为0.35，同时将0.37这条边剔除. 然后从最小索引堆中找出最小边，即0.35这条边，它就是最小生成树的边。同时将结点4加入红色部分。 第七步，看和节点4相邻的所有横切边。此时只有4-6:0.93这一条横切边，其值大于此时最小索引堆中索引为6处的值0.40，所以将其剔除。 最后将最小索引堆中的0.40这条边取出，作为最小生成树的边。同时结点6加入红色部分。这时所有节点遍历完毕，并且找到了最小生成树。 这种方法对于不是横切边的边在判断后会扔掉，所以Prim的时间复杂度改进还是很可观的。 Prim算法代码见附录2 Kruskal算法 Kruskal算法的主要思想是按照边的权重顺序(从小到大)处理它们，将边加入最小生成树中，加入的边不会与已经加入的边构成环，知道树中含有V-1条边为止。这些边逐渐由一片森林合并为一棵树，也就是图的最小生成树。——算法第四版 使用Kruskal算法，为了方便地每次都能取出最短的边，首先对图中所有的边按照权值排序。 然后就要取出还没有考虑的边中最短的那条边，看将此边加入到图中是否会生成环（如果生成环那么就不是最小生成树或者说不是树），如果没有生成环那么它就是最小生成树中的边。 第一步：0-7:0.16,将其放入最小生成树中，不会生成环，所以将这条边作为最小生成树中的一条边 第二步：下一条最小的边为2-3:0.17，将其放入最小生成树中，也不会生成环，所以它是最小生成树的一条边。 第三步：下一条最小的边为1-7:0.19，将其放入最小生成树中，不会生成环，所以它是最小生成树的一条边。 第四步：下一条最小的边为0-2:0.26，这里需要注意，虽然节点0和节点2都是红色，但是将这条边加入最小生成树后并没有形成环，所以它可以作为最小生成树的一条边。 第五步：下一条最小的边为5-7:0.28，将其放入最小生成树中，不会生成环，所以它是最小生成树的一条边。 第六步：下一条最小的边为1-3:0.29，将其放入最小生成树中，会生成环，不能作为最小生成树的一条边，所以不再考虑它。 第七步：下一条最小的边为1-5:0.32，将其放入最小生成树中，会生成环，所以不再考虑它。 第八步：下一条最小的边为2-7:0.34，将其放入最小生成树中，会生成环，所以不再考虑它。 第九步：下一条最小的边为4-5:0.35，将其放入最小生成树中，不会生成环，所以它是最小生成树的一条边。 第十步：下一条最小的边为1-2:0.36，将其放入最小生成树中，会生成环，所以不再考虑它。 第十一步、第十二步的边4-7:0.37、0-4:0.38，将其放入最小生成树中，会生成环，所以不再考虑它们。 第十三步：下一条最小的边为2-6:0.40，将其放入最小生成树中，不会生成环，所以它是最小生成树的一条边。 至此已经有了V-1条边，并且将V个顶点都连接起来了，这时候算法就可以结束了。如果继续扫描的话，剩下的边也都会让最小生成树形成环，也都不会被考虑，直到扫描结束。 整个过程中，关键的操作是判断加入一条边后，是否会行程一个环，这个操作可以使用并查集结构进行辅助。 那么如何使用并查集进行辅助呢？在每一次加入一条边作为最小生成树的同时，对一条边的两个端点进行Union操作。比如此时要加入1-3这条边，去找并查集中1的根节点和3的根节点，它们一定属于同一个根，说明最小生成树中已经连接了节点1和节点3了，此时再将1-3连接的话，就必然形成一个环，这时就可以不考虑这条边了。 Kruskal算法代码实现见附录3 Kruskal算法的计算一副含有V个顶点和E条边的连通加权无向图的最小生成树所需的空间和E成正比，所需的时间和ElogE成正比(最坏情况)。 与Prim算法一样，这个估计是比较保守的，因为算法在找到V-1条边之后就会终止。实际的成本应该与E+E0logE成正比，其中E0是权重小于最小生成树中权重最大的边的所有边的总数。尽管拥有这个优势，Kruskal算法一般还是比Prim算法要慢，因为在处理每条边时除了两种算法都要完成的优先队列操作之外，他还要进行一次connect()操作。 总结Lazy Prim : O(ElogE)Prim : O(ElogV) 整体而言效率最高Kruskal : O(ElogE) 在算法第四版有约定，所有边的权重都各不相同。但是如果有横切边相等的边，那么根据算法的具体实现，每次选择一个边，此时存在多个最小生成树。 另外还有个Vyssotsky&#39;s Algorithm提出的算法，将边逐渐的添加到生成树中，一旦形成环，就删除环中权值最大的边。这个过程完成后可以形成一个最小生成树。 附录1首先创建最小堆MinHeap.java:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127import java.util.*;import java.lang.*;// 在堆的有关操作中，需要比较堆中元素的大小，所以Item需要extends Comparablepublic class MinHeap&lt;Item extends Comparable&gt; &#123; protected Item[] data; protected int count; protected int capacity; // 构造函数, 构造一个空堆, 可容纳capacity个元素 public MinHeap(int capacity)&#123; data = (Item[])new Comparable[capacity+1]; count = 0; this.capacity = capacity; &#125; // 构造函数, 通过一个给定数组创建一个最小堆 // 该构造堆的过程, 时间复杂度为O(n) public MinHeap(Item arr[])&#123; int n = arr.length; data = (Item[])new Comparable[n+1]; capacity = n; for( int i = 0 ; i &lt; n ; i ++ ) data[i+1] = arr[i]; count = n; for( int i = count/2 ; i &gt;= 1 ; i -- ) shiftDown(i); &#125; // 返回堆中的元素个数 public int size()&#123; return count; &#125; // 返回一个布尔值, 表示堆中是否为空 public boolean isEmpty()&#123; return count == 0; &#125; // 向最小堆中插入一个新的元素 item public void insert(Item item)&#123; assert count + 1 &lt;= capacity; data[count+1] = item; count ++; shiftUp(count); &#125; // 从最小堆中取出堆顶元素, 即堆中所存储的最小数据 public Item extractMin()&#123; assert count &gt; 0; Item ret = data[1]; swap( 1 , count ); count --; shiftDown(1); return ret; &#125; // 获取最小堆中的堆顶元素 public Item getMin()&#123; assert( count &gt; 0 ); return data[1]; &#125; // 交换堆中索引为i和j的两个元素 private void swap(int i, int j)&#123; Item t = data[i]; data[i] = data[j]; data[j] = t; &#125; //******************** //* 最小堆核心辅助函数 //******************** private void shiftUp(int k)&#123; while( k &gt; 1 &amp;&amp; data[k/2].compareTo(data[k]) &gt; 0 )&#123; swap(k, k/2); k /= 2; &#125; &#125; private void shiftDown(int k)&#123; while( 2*k &lt;= count )&#123; int j = 2*k; // 在此轮循环中,data[k]和data[j]交换位置 if( j+1 &lt;= count &amp;&amp; data[j+1].compareTo(data[j]) &lt; 0 ) j ++; // data[j] 是 data[2*k]和data[2*k+1]中的最小值 if( data[k].compareTo(data[j]) &lt;= 0 ) break; swap(k, j); k = j; &#125; &#125; // 测试 MinHeap public static void main(String[] args) &#123; MinHeap&lt;Integer&gt; minHeap = new MinHeap&lt;Integer&gt;(100); int N = 100; // 堆中元素个数 int M = 100; // 堆中元素取值范围[0, M) for( int i = 0 ; i &lt; N ; i ++ ) minHeap.insert( new Integer((int)(Math.random() * M)) ); Integer[] arr = new Integer[N]; // 将minheap中的数据逐渐使用extractMin取出来 // 取出来的顺序应该是按照从小到大的顺序取出来的 for( int i = 0 ; i &lt; N ; i ++ )&#123; arr[i] = minHeap.extractMin(); System.out.print(arr[i] + " "); &#125; System.out.println(); // 确保arr数组是从小到大排列的 for( int i = 1 ; i &lt; N ; i ++ ) assert arr[i-1] &lt;= arr[i]; &#125;&#125; 然后创建LazyPrimMST.java类进行算法的编写.12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import java.util.Vector;public class LazyPrimMST&lt;Weight extends Number &amp; Comparable&gt; &#123; private WeightedGraph&lt;Weight&gt; G; //图的引用 private MinHeap&lt;Edge&lt;Weight&gt;&gt; pq; //最小堆，算法辅助数据结构 private boolean[] marked; //标记数组，在算法运行过程中标记节点i是否被访问（即图中的蓝色部分和红色部分） private Vector&lt;Edge&lt;Weight&gt;&gt; mst; //最小生成树所包含的所有边 private Number mstWeight; //最小生成树的权值 //构造函数，使用Prim算法求图的最小生成树 public LazyPrimMST(WeightedGraph&lt;Weight&gt; graph)&#123; //算法初始化 G = graph; pq = new MinHeap&lt;Edge&lt;Weight&gt;&gt;(G.E()); marked = new boolean[G.V()]; mst = new Vector&lt;Edge&lt;Weight&gt;&gt;(); //Lazy Prim visit(0); while (!pq.isEmpty())&#123; // 使用最小堆找出已经访问的边中权值最小的边 Edge&lt;Weight&gt; e = pq.extractMin(); // 如果这条边的两端都已经访问过了, 它不是横切边,扔掉这条边 if(marked[e.v()] == marked[e.w()]) continue; //否则，这条边应该存在最小生成树中 mst.add(e); //访问和这条边连接的还没有被访问过的节点(找到蓝色一端的端点) if( !marked[e.v()] ) visit(e.v()); else visit(e.w()); &#125; //计算最小生成树的权值 mstWeight = mst.elementAt(0).wt(); for( int i = 1 ; i &lt; mst.size() ; i ++) mstWeight = mstWeight.doubleValue() + mst.elementAt(i).wt().doubleValue(); &#125; //访问节点 private void visit(int v) &#123; if( marked[v] )&#123; throw new IllegalArgumentException("this point has been visited!"); &#125; marked[v] = true; //将和节点v相连的所有未访问的边放入最小堆中 for(Edge&lt;Weight&gt; e : G.adj(v))&#123; if(!marked[e.other(v)]) //找到与其对应的另一个端点，如果是横切边 pq.insert(e); //就加入到堆中 &#125; &#125; // 返回最小生成树的所有边 Vector&lt;Edge&lt;Weight&gt;&gt; mstEdges()&#123; return mst; &#125;; // 返回最小生成树的权值 Number result()&#123; return mstWeight; &#125;;&#125; 在Main.java中进行测试:123456789101112131415161718192021import java.util.Vector;public class Main &#123; // 测试通过文件读取图的信息 public static void main(String[] args) &#123; // 使用两种图的存储方式读取testG1.txt文件 String filename = "testG1.txt"; SparseWeightedGraph&lt;Double&gt; g = new SparseWeightedGraph&lt;Double&gt;(8, false); ReadWeightedGraph readGraph1 = new ReadWeightedGraph(g, filename); // Test Lazy Prim MST System.out.println("Test Lazy Prim MST:"); LazyPrimMST&lt;Double&gt; lazyPrimMST = new LazyPrimMST&lt;Double&gt;(g); Vector&lt;Edge&lt;Double&gt;&gt; mst = lazyPrimMST.mstEdges(); for( int i = 0 ; i &lt; mst.size() ; i ++ ) System.out.println(mst.elementAt(i)); System.out.println("The MST weight is: " + lazyPrimMST.result()); System.out.println(); &#125;&#125; 附录2最小索引堆IndexMinHeap.java:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169import java.lang.reflect.Array;import java.util.*;import java.lang.*;// 最小索引堆public class IndexMinHeap&lt;Item extends Comparable&gt; &#123; protected Item[] data; // 最小索引堆中的数据 protected int[] indexes; // 最小索引堆中的索引, indexes[x] = i 表示索引i在x的位置 protected int[] reverse; // 最小索引堆中的反向索引, reverse[i] = x 表示索引i在x的位置 protected int count; protected int capacity; // 构造函数, 构造一个空堆, 可容纳capacity个元素 public IndexMinHeap(int capacity)&#123; data = (Item[])new Comparable[capacity+1]; indexes = new int[capacity+1]; reverse = new int[capacity+1]; for( int i = 0 ; i &lt;= capacity ; i ++ ) reverse[i] = 0; count = 0; this.capacity = capacity; &#125; // 返回索引堆中的元素个数 public int size()&#123; return count; &#125; // 返回一个布尔值, 表示索引堆中是否为空 public boolean isEmpty()&#123; return count == 0; &#125; // 向最小索引堆中插入一个新的元素, 新元素的索引为i, 元素为item // 传入的i对用户而言,是从0索引的 public void insert(int i, Item item)&#123; assert count + 1 &lt;= capacity; assert i + 1 &gt;= 1 &amp;&amp; i + 1 &lt;= capacity; // 再插入一个新元素前,还需要保证索引i所在的位置是没有元素的。 assert !contain(i); i += 1; data[i] = item; indexes[count+1] = i; reverse[i] = count + 1; count ++; shiftUp(count); &#125; // 从最小索引堆中取出堆顶元素, 即索引堆中所存储的最小数据 public Item extractMin()&#123; assert count &gt; 0; Item ret = data[indexes[1]]; swapIndexes( 1 , count ); reverse[indexes[count]] = 0; count --; shiftDown(1); return ret; &#125; // 从最小索引堆中取出堆顶元素的索引 public int extractMinIndex()&#123; assert count &gt; 0; int ret = indexes[1] - 1; swapIndexes( 1 , count ); reverse[indexes[count]] = 0; count --; shiftDown(1); return ret; &#125; // 获取最小索引堆中的堆顶元素 public Item getMin()&#123; assert count &gt; 0; return data[indexes[1]]; &#125; // 获取最小索引堆中的堆顶元素的索引 public int getMinIndex()&#123; assert count &gt; 0; return indexes[1]-1; &#125; // 看索引i所在的位置是否存在元素 boolean contain( int i )&#123; assert i + 1 &gt;= 1 &amp;&amp; i + 1 &lt;= capacity; return reverse[i+1] != 0; &#125; // 获取最小索引堆中索引为i的元素 public Item getItem( int i )&#123; assert contain(i); return data[i+1]; &#125; // 将最小索引堆中索引为i的元素修改为newItem public void change( int i , Item newItem )&#123; assert contain(i); i += 1; data[i] = newItem; // 有了 reverse 之后, // 我们可以非常简单的通过reverse直接定位索引i在indexes中的位置 shiftUp( reverse[i] ); shiftDown( reverse[i] ); &#125; // 交换索引堆中的索引i和j // 由于有了反向索引reverse数组， // indexes数组发生改变以后， 相应的就需要维护reverse数组 private void swapIndexes(int i, int j)&#123; int t = indexes[i]; indexes[i] = indexes[j]; indexes[j] = t; reverse[indexes[i]] = i; reverse[indexes[j]] = j; &#125; //******************** //* 最小索引堆核心辅助函数 //******************** // 索引堆中, 数据之间的比较根据data的大小进行比较, 但实际操作的是索引 private void shiftUp(int k)&#123; while( k &gt; 1 &amp;&amp; data[indexes[k/2]].compareTo(data[indexes[k]]) &gt; 0 )&#123; swapIndexes(k, k/2); k /= 2; &#125; &#125; // 索引堆中, 数据之间的比较根据data的大小进行比较, 但实际操作的是索引 private void shiftDown(int k)&#123; while( 2*k &lt;= count )&#123; int j = 2*k; if( j+1 &lt;= count &amp;&amp; data[indexes[j+1]].compareTo(data[indexes[j]]) &lt; 0 ) j ++; if( data[indexes[k]].compareTo(data[indexes[j]]) &lt;= 0 ) break; swapIndexes(k, j); k = j; &#125; &#125; // 测试 IndexMinHeap public static void main(String[] args) &#123; int N = 1000000; IndexMinHeap&lt;Integer&gt; indexMinHeap = new IndexMinHeap&lt;Integer&gt;(N); for( int i = 0 ; i &lt; N ; i ++ ) indexMinHeap.insert( i , (int)(Math.random()*N) ); &#125;&#125; PrimMST.java：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107import java.util.Vector;public class PrimMST&lt;Weight extends Number &amp; Comparable&gt; &#123; private WeightedGraph G; //图的引用 private IndexMinHeap&lt;Weight&gt; ipq; //最小索引堆 private Edge&lt;Weight&gt;[] edgeTo; //访问的点所对应的边 private boolean[] marked; //标记数组,在算法运行过程中标记节点i是否被访问 private Vector&lt;Edge&lt;Weight&gt;&gt; mst; //最小生成树所包含的所有边 private Number mstWeight; //最小生成树的权值 //构造函数，使用Prim算法求图的最小生成树 public PrimMST(WeightedGraph graph)&#123; G = graph; if(!(graph.E() &gt;= 1))&#123; throw new IllegalArgumentException("Edges of graph should &gt;= 1"); &#125; ipq = new IndexMinHeap&lt;&gt;(graph.V()); //开辟顶点个数的空间就可以 //算法初始化 marked = new boolean[G.V()]; edgeTo = new Edge[G.V()]; for(int i = 0 ; i &lt; G.V() ; i ++)&#123; marked[i] = false; edgeTo[i] = null; &#125; mst = new Vector&lt;Edge&lt;Weight&gt;&gt;(); //Prim visit(0); while (!ipq.isEmpty())&#123; // 使用最小索引堆找出已经访问的边中权值最小的边 // 最小索引堆中存储的是点的索引, 通过点的索引找到相对应的边 int v = ipq.extractMinIndex(); if(edgeTo[v] == null)&#123; throw new IllegalArgumentException("Edge should't be null"); &#125; mst.add(edgeTo[v]); visit(v); &#125; //计算最小生成树的权值 mstWeight = mst.elementAt(0).wt(); for(int i = 1 ; i &lt; mst.size() ; i ++)&#123; mstWeight = mstWeight.doubleValue() + mst.elementAt(i).wt().doubleValue(); &#125; &#125; //访问节点v void visit(int v)&#123; if( marked[v] )&#123; throw new IllegalArgumentException("this point has been visited!"); &#125; marked[v] = true; // 将和节点v相连接的未访问的另一端点, 和与之相连接的边, 放入最小堆中 for(Object item :G.adj(v))&#123; Edge&lt;Weight&gt; e = (Edge&lt;Weight&gt;) item; int w = e.other(v); //如果边的另一端点未被访问 if(!marked[w])&#123; //如果从没有考虑过这个端点，直接将这个端点和与之相连接的边加入索引堆，即它是横切边 if(edgeTo[w] == null)&#123; edgeTo[w] = e; ipq.insert(w,e.wt()); &#125; //如果曾经考虑这个端点,但现在的边比之前考虑的边更短,则进行替换 else if(e.wt().compareTo(edgeTo[w].wt())&lt; 0 )&#123; edgeTo[w] = e; ipq.change(w,e.wt()); &#125; &#125; &#125; &#125; // 返回最小生成树的所有边 Vector&lt;Edge&lt;Weight&gt;&gt; mstEdges()&#123; return mst; &#125; // 返回最小生成树的权值 Number result()&#123; return mstWeight; &#125; // 测试 Prim public static void main(String[] args) &#123; String filename = "testG1.txt"; int V = 8; SparseWeightedGraph&lt;Double&gt; g = new SparseWeightedGraph&lt;Double&gt;(V, false); ReadWeightedGraph readGraph = new ReadWeightedGraph(g, filename); // Test Prim MST System.out.println("Test Prim MST:"); PrimMST&lt;Double&gt; primMST = new PrimMST&lt;Double&gt;(g); Vector&lt;Edge&lt;Double&gt;&gt; mst = primMST.mstEdges(); for( int i = 0 ; i &lt; mst.size() ; i ++ ) System.out.println(mst.elementAt(i)); System.out.println("The MST weight is: " + primMST.result()); System.out.println(); &#125;&#125; 附录3UnionFind.java:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566// Union-Findpublic class UnionFind &#123; // rank[i]表示以i为根的集合所表示的树的层数 // 在后续的代码中, 我们并不会维护rank的语意, 也就是rank的值在路径压缩的过程中, 有可能不在是树的层数值 // 这也是我们的rank不叫height或者depth的原因, 他只是作为比较的一个标准 private int[] rank; private int[] parent; // parent[i]表示第i个元素所指向的父节点 private int count; // 数据个数 // 构造函数 public UnionFind(int count)&#123; rank = new int[count]; parent = new int[count]; this.count = count; // 初始化, 每一个parent[i]指向自己, 表示每一个元素自己自成一个集合 for( int i = 0 ; i &lt; count ; i ++ )&#123; parent[i] = i; rank[i] = 1; &#125; &#125; // 查找过程, 查找元素p所对应的集合编号 // O(h)复杂度, h为树的高度 int find(int p)&#123; assert( p &gt;= 0 &amp;&amp; p &lt; count ); // path compression 1 while( p != parent[p] )&#123; parent[p] = parent[parent[p]]; p = parent[p]; &#125; return p; &#125; // 查看元素p和元素q是否所属一个集合 // O(h)复杂度, h为树的高度 boolean isConnected( int p , int q )&#123; return find(p) == find(q); &#125; // 合并元素p和元素q所属的集合 // O(h)复杂度, h为树的高度 void unionElements(int p, int q)&#123; int pRoot = find(p); int qRoot = find(q); if( pRoot == qRoot ) return; // 根据两个元素所在树的元素个数不同判断合并方向 // 将元素个数少的集合合并到元素个数多的集合上 if( rank[pRoot] &lt; rank[qRoot] )&#123; parent[pRoot] = qRoot; &#125; else if( rank[qRoot] &lt; rank[pRoot])&#123; parent[qRoot] = pRoot; &#125; else&#123; // rank[pRoot] == rank[qRoot] parent[pRoot] = qRoot; rank[qRoot] += 1; // 此时, 我维护rank的值 &#125; &#125;&#125; MinHeap.java:见附录1 KruskalMST.java:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374import java.awt.event.WindowEvent;import java.util.Vector;public class KruskalMST&lt;Weight extends Number &amp; Comparable&gt; &#123; private Vector&lt;Edge&lt;Weight&gt;&gt; mst; //最小生成树所包含的所有边 private Number mstWeight; //最小生成树的权值 //构造函数，使用Kruskal算法计算graph的最小生成树 public KruskalMST(WeightedGraph graph)&#123; mst = new Vector&lt;Edge&lt;Weight&gt;&gt;(); //将所有的边进行排序，使用堆排序，将所有的边放入一个最小堆中 MinHeap&lt;Edge&lt;Weight&gt;&gt; pq = new MinHeap&lt;Edge&lt;Weight&gt;&gt;(graph.E()); for(int i = 0 ; i &lt; graph.V(); i ++)&#123; for(Object item : graph.adj(i))&#123; Edge&lt;Weight&gt; e = (Edge&lt;Weight&gt;) item; if( e.v() &lt;= e.w() ) //防止存入两次同一条边(比如边1-2和边2-1，只存入边1-2) pq.insert(e); &#125; &#125; // 创建一个并查集，查看已经访问的节点的联通情况 UnionFind uf = new UnionFind(graph.V()); while (!pq.isEmpty() &amp;&amp; mst.size() &lt; graph.V() - 1)&#123;//pq不为空且最小生成树的边数小于V-1 //从最小堆中依次从小到大取出所有的边 Edge&lt;Weight&gt; e = pq.extractMin(); //如果该边的两个端点是联通的，说明加入这条边将产生环，扔掉这条边 if(uf.isConnected(e.v(),e.w())) continue; //否则，将这条边加入最小生成树，同时标记边的两个端点联通 mst.add(e); uf.unionElements(e.v(),e.w()); &#125; //计算最小生成树的权值 mstWeight = mst.elementAt(0).wt(); for(int i = 1 ; i &lt; mst.size() ; i ++)&#123; mstWeight = mstWeight.doubleValue() + mst.elementAt(i).wt().doubleValue(); &#125; &#125; // 返回最小生成树的所有边 Vector&lt;Edge&lt;Weight&gt;&gt; mstEdges()&#123; return mst; &#125; // 返回最小生成树的权值 Number result()&#123; return mstWeight; &#125; // 测试 Kruskal public static void main(String[] args) &#123; String filename = "testG1.txt"; int V = 8; SparseWeightedGraph&lt;Double&gt; g = new SparseWeightedGraph&lt;Double&gt;(V, false); ReadWeightedGraph readGraph = new ReadWeightedGraph(g, filename); // Test Kruskal System.out.println("Test Kruskal:"); KruskalMST&lt;Double&gt; kruskalMST = new KruskalMST&lt;Double&gt;(g); Vector&lt;Edge&lt;Double&gt;&gt; mst = kruskalMST.mstEdges(); for( int i = 0 ; i &lt; mst.size() ; i ++ ) System.out.println(mst.elementAt(i)); System.out.println("The MST weight is: " + kruskalMST.result()); System.out.println(); &#125;&#125; 测试结果:]]></content>
      <tags>
        <tag>数据结构</tag>
        <tag>图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图的遍历]]></title>
    <url>%2F2019%2F04%2F01%2F%E5%9B%BE%E7%9A%84%E9%81%8D%E5%8E%86%2F</url>
    <content type="text"><![CDATA[对于图的遍历，和树一样，也有深度优先遍历和广度优先遍历。对于图的深度优先遍历，是从一个点开始，不停的往下试探，知道不能继续为止。但是与树不同的是，图有可能会存在环，所以要记录一下某个点是否已经被遍历过，如果已经被遍历过，则下面的遍历就不需要继续走这个点了。 以下面的数据为例: 深度优先遍历假设从0开始遍历，看与0相连的第一个节点1，节点1没有遍历过所以遍历了1，此时为0 1；接着看和节点1相邻的节点，只有节点0，节点0已经被遍历过了，不用管，此时1没有其他相邻节点了，此时这条路就算走完了，则退回到节点0；然后看0的下一个没有被遍历的节点为节点2,节点2没有被遍历过，所以将2遍历，此时为0 1 2；接着看和节点2相邻的节点，只有节点0，节点0已经被遍历过，退回到节点0；接着看下一个和0相邻的节点，为节点5，没有被遍历过，所以对节点5进行遍历，此时遍历的元素为0 1 2 5；接着看和节点5相邻的节点，第一个节点为节点0，已经被遍历过；第二个节点为节点3，没有被遍历过，所以进行遍历，此时遍历的元素为0 1 2 5 3；接着看和节点3相邻的节点，第一个为节点4，没有被遍历过，所以进行遍历，此时遍历元素为0 1 2 5 3 4；然后看和节点4相邻的节点，节点3和节点5已经被遍历过了，不进行遍历；节点6没有被遍历过，所以进行遍历，此时遍历的元素为0 1 2 5 3 4 6；然后来到节点6，和节点6相邻的节点0和节点4已经被遍历过了，所以不进行遍历；然后退回到节点4，节点4从节点6退回来之后，没有其他节点，节点4的这条路遍历完了，所以从节点4退回到节点3；对于节点3来说，刚才将节点4遍历了，下一个节点5也已经遍历了，节点3这条路遍历完了；所以又从节点3退回到节点5，对于节点5的下一个节点——节点4也已经遍历过了，所以节点5这条路也遍历完了；退回到节点0，节点0的下一个要遍历的节点6已经被遍历过了，所以节点0这条路也已经便利完了；至此，使用深度优先的方式，将所有节点全部遍历了一遍。 整个过程需要记录一下某个节点是否已经被遍历过。 连通分量深度优先遍历的一个典型应用就是求连通分量。 对于一张有三个部分的图，在图论中，这三部分就是连通分量。连通分量之间没有任何边相连。求连通分量只需整体遍历这个图，从某个点开始遍历，先将这个连通分量遍历完；然后再找一个没有被遍历的点，继续遍历，则也可以将第二部分遍历完；最后将剩下的部分进行遍历。 以求连通分量为例，实现图的深度优先遍历。创建Component.h123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#ifndef COMPONENT_H_INCLUDED#define COMPONENT_H_INCLUDED#include &lt;iostream&gt;#include &lt;cassert&gt;using namespace std;template &lt;typename Graph&gt;class Component&#123;private: Graph &amp;G; bool *visited; //记录某个节点是否已经被访问过 int ccount; //连通分量个数 void dfs( int v )&#123; visited[v] = true; typename Graph::adjIterator adj(G, v); //加typename指定adjInterator是一个类型而不是一个成员变量 for( int i = adj.begin() ; !adj.end() ; i = adj.next() ) &#123;//从adj的第一个开始，只要不到adj的结尾，就进行循环，每次i变为adj的下一个元素 if( !visited[i] ) //如果没有被访问过，使用递归方法对它继续进行深度优先遍历， dfs(i); &#125; &#125;public: Component(Graph &amp;graph): G(graph)&#123; visited = new bool[G.V()]; ccount = 0 ; for(int i = 0 ; i &lt; G.V() ; i ++) visited[i] = false; for(int i = 0 ; i &lt; G.V() ; i ++)&#123; if( !visited[i] )&#123; //如果当前的节点没有被访问过的话，就进行深度优先遍历 dfs(i); //遍历之后，所有连通的节点都置为true ccount ++; //遍历之后，连通分量+1；然后接着找没有被遍历的节点作为起点进行深度遍历 &#125; &#125; &#125; ~Component()&#123; delete[] visited; &#125; int count()&#123; return ccount; &#125;&#125;;#endif // COMPONENT_H_INCLUDED 新建文件testG3.txt，例子中的图表达为:1234567897 80 10 20 50 63 43 54 54 6 在main函数中进行测试:123456789101112131415161718192021222324252627282930313233343536373839#include &lt;iostream&gt;#include &lt;stdlib.h&gt;#include &lt;time.h&gt;#include "SparseGraph.h"#include "DenseGraph.h"#include "ReadGraph.h"#include "Component.h"using namespace std;int main() &#123; string filename1 = "testG1.txt"; SparseGraph g1 = SparseGraph(13, false); ReadGraph&lt;SparseGraph&gt; readGraph1(g1, filename1); Component&lt;SparseGraph&gt; component1(g1); cout&lt;&lt;"TestG1.txt, Using Sparse Graph, Component Count: "&lt;&lt;component1.count()&lt;&lt;endl; DenseGraph g2 = DenseGraph(13, false); ReadGraph&lt;DenseGraph&gt; readGraph2(g2, filename1); Component&lt;DenseGraph&gt; component2(g2); cout&lt;&lt;"TestG1.txt, Using Dense Graph, Component Count: "&lt;&lt;component2.count()&lt;&lt;endl; cout&lt;&lt;endl; // TestG3.txt - g3 and g4 string filename2 = "testG2.txt"; SparseGraph g3 = SparseGraph(7, false); ReadGraph&lt;SparseGraph&gt; readGraph3(g3, filename2); Component&lt;SparseGraph&gt; component3(g3); cout&lt;&lt;"TestG3.txt, Using Sparse Graph, Component Count: "&lt;&lt;component3.count()&lt;&lt;endl; DenseGraph g4 = DenseGraph(7, false); ReadGraph&lt;DenseGraph&gt; readGraph4(g4, filename2); Component&lt;DenseGraph&gt; component4(g4); cout&lt;&lt;"TestG3.txt, Using Dense Graph, Component Count: "&lt;&lt;component4.count()&lt;&lt;endl; return 0;&#125; 相应的，如果能求出连通分量，也能求出两个结点之间是否是相连的。对Component.h做出修改:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061#ifndef COMPONENT_H_INCLUDED#define COMPONENT_H_INCLUDED#include &lt;iostream&gt;#include &lt;cassert&gt;using namespace std;template &lt;typename Graph&gt;class Component&#123;private: Graph &amp;G; bool *visited; //记录某个节点是否已经被访问过 int ccount; //连通分量个数 int *id; void dfs( int v )&#123; visited[v] = true; id[v] = ccount; //在进行dfs操作时，将id[v]的值设置为ccount的值，这样不同连通分量部分的顶点其值不同 typename Graph::adjIterator adj(G, v); //加typename指定adjInterator是一个类型而不是一个成员变量 for( int i = adj.begin() ; !adj.end() ; i = adj.next() ) &#123;//从adj的第一个开始，只要不到adj的结尾，就进行循环，每次i变为adj的下一个元素 if( !visited[i] ) //如果没有被访问过，使用递归方法对它继续进行深度优先遍历， dfs(i); &#125; &#125;public: Component(Graph &amp;graph): G(graph)&#123; visited = new bool[G.V()]; id = new int[G.V()]; ccount = 0 ; for(int i = 0 ; i &lt; G.V() ; i ++)&#123; visited[i] = false; id[i] = -1; &#125; for(int i = 0 ; i &lt; G.V() ; i ++)&#123; if( !visited[i] )&#123; //如果当前的节点没有被访问过的话，就进行深度优先遍历 dfs(i); //遍历之后，所有连通的节点都置为true ccount ++; //遍历之后，连通分量+1；然后接着找没有被遍历的节点作为起点进行深度遍历 &#125; &#125; &#125; ~Component()&#123; delete[] visited; delete[] id; &#125; int count()&#123; return ccount; &#125; bool isConnected( int v , int w)&#123; assert( v &gt;= 0 &amp;&amp; v &lt; n ); assert( w &gt;= 0 &amp;&amp; w &lt; n ); return id[v] == id[w]; &#125;&#125;;#endif // COMPONENT_H_INCLUDED 获得两点间的一条路径使用深度优先的方式，不一定能获得一条最短路径。在Path.h类中新增一个数组from，用来记录每访问一个节点时，该节点是从哪个节点过来的，根据from数组，就可以倒推出两点之间相应的路径。Path.h1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798#ifndef PATH_H_INCLUDED#define PATH_H_INCLUDED#include &lt;vector&gt;#include &lt;stack&gt;#include &lt;iostream&gt;#include &lt;cassert&gt;using namespace std;template &lt;typename Graph&gt;class Path&#123;private: Graph &amp;G; int s; bool* visited; int* from; // 记录路径, from[i]表示查找的路径上i的上一个节点 void dfs( int v )&#123; visited[v] = true; typename Graph::adjIterator adj(G, v); //加typename指定adjInterator是一个类型而不是一个成员变量 for( int i = adj.begin() ; !adj.end() ; i = adj.next() ) &#123;//从adj的第一个开始，只要不到adj的结尾，就进行循环，每次i变为adj的下一个元素 if( !visited[i] ) &#123; //如果没有被访问过，使用递归方法对它继续进行深度优先遍历，并将该节点放入from[]数组，说明是从v节点过来的 from[i] = v; dfs(i); &#125; &#125; &#125;public: // 构造函数, 寻路算法, 寻找图graph从s点到其他点的路径 Path(Graph &amp;graph, int s):G(graph)&#123; //算法初始化 assert( s &gt;= 0 &amp;&amp; s &lt; G.V() ); visited = new bool[G.V()]; from = new int[G.V()]; for( int i = 0 ; i &lt; G.V() ; i ++ )&#123; visited[i] = false; from[i] = -1; &#125; this-&gt;s = s; // 寻路算法 // 其实就是进行深度优先遍历的过程中，记录一下 dfs(s); &#125; ~Path()&#123; delete[] visited; delete[] from; &#125; //从原点s到w是否有路 bool hasPath(int w)&#123; assert( w &gt;= 0 &amp;&amp; w &lt; G.V() ); return visited[w]; &#125; //从原点到w的具体路径是什么样的 //因为是从w节点倒着走，所以先存放在stack中，然后在依次放在vector中 void path(int w, vector&lt;int&gt; &amp;vec)&#123; // 通过from数组逆向查找到从s到w的路径, 存放到栈中 stack&lt;int&gt; s; int p = w; while( p != -1 )&#123; s.push(p); p = from[p]; &#125; vec.clear(); while( !s.empty() )&#123; vec.push_back( s.top() ); s.pop(); &#125; &#125; //打印出从s到w的路径 void showPath(int w)&#123; vector&lt;int&gt; vec; path(w,vec); for( int i = 0 ; i &lt; vec.size() ; i ++)&#123; cout &lt;&lt; vec[i]; if( i == vec.size() - 1) cout &lt;&lt; endl; else cout &lt;&lt; " -&gt; "; &#125; &#125;&#125;;#endif // PATH_H_INCLUDED 在main.h中进行测试，打印从0到6的路径。 123456789101112131415161718192021222324#include &lt;iostream&gt;#include &lt;stdlib.h&gt;#include &lt;time.h&gt;#include "SparseGraph.h"#include "DenseGraph.h"#include "ReadGraph.h"#include "Path.h"using namespace std;int main() &#123; string filename = "testG3.txt"; SparseGraph g = SparseGraph(7, false); ReadGraph&lt;SparseGraph&gt; readGraph1(g, filename); g.show(); cout&lt;&lt;endl; Path&lt;SparseGraph&gt; dfs(g,0); cout&lt;&lt;"DFS : "; dfs.showPath(6); return 0;&#125; 深度优先遍历的复杂度稀疏图（邻接表）:O(V+E)稠密图（邻接矩阵）:O(V^2) 深度优先遍历对有向图依然有效 广度优先遍历广度优先遍历，使用队列进行辅助。还是使用最开始的图进行讲解。首先将节点0推入队列中，然后开始遍历过程；（和树的遍历一致，先将根节点推入）(此时队列为0)遍历时，每次遍历都将队列首的元素取出来作为遍历的对象，现在队列首是节点0，将其取出，遍历节点0；(此时队列为空,遍历过的节点有0)然后将所有与节点0相邻的节点，如果还没有加入队列中，就将其加入，所以将节点1，节点2，节点5，节点6加入到队列中，这样就完成了一次操作；(此时队列为1 2 5 6,遍历过的节点有0)接着，将队列首的节点1拿出来作为遍历的对象，节点1就被遍历了，而与节点1相邻的节点只有节点0,0已经遍历过；所以继续将节点2从队列首拿出来，作为遍历的对象，而节点2相邻的节点也只有节点0,不用管；(此时队列为5 6，遍历过的节点有0 1 2)接着将队列首的节点5拿出来，作为遍历的对象，对于节点5，它相邻的节点有节点0，节点3，节点4，节点0已经加入过队列并遍历完，所以将节点3，节点4加入到队列中；(此时队列为6 3 4，遍历过的节点有0 1 2 5)继续将队列首的节点6拿出来，作为遍历的对象，节点6的相邻节点有节点0和节点4，需要注意节点0已经被遍历过不需要管，节点4虽然没有被遍历过，但是它已经被加入到队列中（所以在代码实现时，注意需要对加入到队列中的元素进行标记，因为一旦被加入到队列，那么它一定会被遍历），所以这次遍历就算完成;(此时队列为3 4,遍历过的节点有0 1 2 5 6)接着将队列首的节点3拿出来进行遍历，节点3相邻的节点有节点4和节点5，节点5和节点4都已经被加入过队列，这次遍历结束；(此时队列为4，遍历过的节点有0 1 2 5 6 3)最后将队列首的节点4拿出来进行遍历，对于节点4相邻的节点3，节点5，节点6都已经加入过队列，所以这次遍历结束。至此，队列为空，广度优先遍历结束。 其实广度优先遍历，是以距起始节点的距离为顺序进行遍历的。首先0节点自己到自己的距离为0，然后与节点0所有相邻的节点1、2、5、6被推进队列，它们距节点0的距离都为1；接着将节点3,4推入到队列，它们距节点0的距离都为2。所以广度优先遍历有时候也成为层序优先遍历，以遍历的起点开始，一层层的往下推，先遍历的节点距离起始节点近（更严谨的说法是先遍历的节点到起始节点的距离是小于等于后遍历的节点的，比如先遍历节点1后遍历节点6，但是它们到节点0 的距离都是1）。利用这种方法，可以在程序中记录这个距离。在无权图中记录到起始节点的距离，不仅如此，如果像深度优先遍历，同时用from[]记录每一个节点是从哪一个节点过来，还可以将最短路径同时求出来。所以，广度优先遍历的一个应用就是求出无权图的最短路径(这里注意是无权图，但是不局限于有向图还是无向图)。 新建ShortestPath.h类:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108#ifndef SHORTESTPATH_H_INCLUDED#define SHORTESTPATH_H_INCLUDED#include &lt;vector&gt;#include &lt;queue&gt;#include &lt;iostream&gt;#include &lt;cassert&gt;using namespace std;template &lt;typename Graph&gt;class ShortestPath&#123;private: Graph &amp;G; int s; bool *visited; // 记录dfs的过程中节点是否被访问 int *from; // 记录路径, from[i]表示查找的路径上i的上一个节点 int *ord; //记录s到每一个节点的距离是多少，即记录路径中节点的次序。ord[i]表示i节点在路径中的次序。public: ShortestPath( Graph &amp;graph, int s):G(graph)&#123; //算法初始化 assert( s &gt;= 0 &amp;&amp; s &lt; graph.V() ); visited = new bool[graph.V()]; from = new int[graph.V()]; ord = new int[graph.V()]; for(int i = 0 ; i &lt; graph.V() ; i++)&#123; visited[i] = false; from[i] = -1; ord[i] = -1; &#125; this-&gt;s = s; // 无向图最短路径算法, 从s开始广度优先遍历整张图 queue&lt;int&gt; q; q.push(s); //首先向队列中推入原点s visited[s] = true; //标记s已经被访问 ord[s] = 0; //标记原点到自己的距离为0 while( !q.empty() )&#123; int v = q.front(); //将队列首的元素取出 q.pop(); //队列首的元素出队 typename Graph::adjIterator adj(G,v); //遍历与队列首元素v所有相邻的元素 for( int i = adj.begin() ; !adj.end() ; i = adj.next() )&#123; if( !visited[i] )&#123; //查看i元素是不是已经被访问过，即是否已经被加入过队列，如果没有的话 q.push(i); //把i加入到队列中 visited[i] = true; from[i] = v; //是从v节点走到i的 ord[i] = ord[v] + 1; //从过来的节点的距离+1 &#125; &#125; &#125; &#125; ~ShortestPath()&#123; delete[] visited; delete[] from; delete[] ord; &#125; bool hasPath(int w)&#123; assert( w &gt;= 0 &amp;&amp; w &lt; G.V()); return visited[w]; &#125; void path(int w ,vector&lt;int&gt; &amp;vec)&#123; assert( w &gt;= 0 &amp;&amp; w &lt; G.V()); stack&lt;int&gt; s; int p = w; while( p != -1 )&#123; s.push(p); p = from[p]; &#125; vec.clear(); while( !s.empty() )&#123; vec.push_back( s.top() ); s.pop(); &#125; &#125; void showPath(int w)&#123; assert( w &gt;= 0 &amp;&amp; w &lt; G.V() ); vector&lt;int&gt; vec; path(w,vec); for(int i = 0 ; i &lt; vec.size() ; i ++ )&#123; cout&lt;&lt;vec[i]; if( i == vec.size() - 1 ) cout &lt;&lt; endl; else cout &lt;&lt; " -&gt; "; &#125; &#125; int length(int w)&#123; assert( w &gt;= 0 &amp;&amp; w &lt; G.V() ); return ord[w]; &#125;&#125;;#endif // SHORTESTPATH_H_INCLUDED 使用main函数测试: 12345678910111213141516171819202122232425262728#include &lt;iostream&gt;#include &lt;stdlib.h&gt;#include &lt;time.h&gt;#include "SparseGraph.h"#include "DenseGraph.h"#include "ReadGraph.h"#include "Path.h"#include "ShortestPath.h"using namespace std;int main() &#123; string filename = "testG3.txt"; SparseGraph g = SparseGraph(7, false); ReadGraph&lt;SparseGraph&gt; readGraph1(g, filename); g.show(); cout&lt;&lt;endl; Path&lt;SparseGraph&gt; dfs(g,0); cout&lt;&lt;"DFS : "; dfs.showPath(6); ShortestPath&lt;SparseGraph&gt; bfs(g,0); cout&lt;&lt;"BFS : "; bfs.showPath(6); return 0;&#125; 使用深度优先遍历有可能会找到最短路径，这与图的创建、具体的存储顺序有关；但是使用广度优先遍历，一定能够找到两个连通点的最短路径。此外，在这个图中，如果求0到4的最短路径，有可能取出0-&gt;6-&gt;4或者0-&gt;5-&gt;4，这与在图中遍历的顺序是怎样的有关。 广度优先遍历的复杂度广度优先遍历的复杂度和深度优先遍历的复杂度是一样的: 稀疏图（邻接表）:O(V+E)稠密图（邻接矩阵）:O(V^2) java实现 其中几个主要的类实现参考图论基础 Path.java:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788import java.util.Vector;import java.util.Stack;public class Path &#123; private Graph G; // 图的引用 private int s; //起始点 private boolean[] visited; // 记录dfs的过程中节点是否被访问 private int[] from; // 记录路径，from[i]表示查找的路径上i的上一个节点 //图的深度优先遍历 private void dfs( int v )&#123; visited[v] = true; for( int i : G.adj(v) )&#123; // 遍历所有v的相邻点 if( !visited[i] )&#123; // 如果没有被遍历过 from[i] = v; //将v加入到路径数组 dfs(i); //递归进行遍历 &#125; &#125; &#125; // 构造函数, 寻路算法, 寻找图graph从s点到其他点的路径 public Path(Graph graph , int s)&#123; G = graph; if(!(s &gt;= 0 &amp;&amp; s &lt; G.V() ))&#123; throw new IllegalArgumentException("make sure that s &gt;= 0 and s &lt; G.V()"); &#125; visited = new boolean[G.V()]; from = new int[G.V()]; for(int i = 0 ; i &lt; G.V() ; i ++)&#123; visited[i] = false; from[i] = -1; &#125; this.s = s; //寻路算法 dfs(s); &#125; //查询从s点到w点是否有路径 boolean hasPath(int w)&#123; if(!(w &gt;= 0 &amp;&amp; w &lt; G.V() ))&#123; throw new IllegalArgumentException("make sure that w &gt;= 0 and w &lt; G.V()"); &#125; return visited[w]; &#125; //查询从s点到w点的路径,存放在vec中 Vector&lt;Integer&gt; path(int w)&#123; if(!(hasPath(w)))&#123; throw new IllegalArgumentException("no path between s and w"); &#125; Stack&lt;Integer&gt; s = new Stack&lt;&gt;(); //通过from数组逆向查找从s到w的路径，存放到栈中 int p = w; while (p != -1)&#123; s.push(p); p = from[p]; &#125; //从栈中取出元素，获得顺序的从s到w的路径 Vector&lt;Integer&gt; res = new Vector&lt;&gt;(); while( !s.empty() )&#123; res.add( s.pop() ); &#125; return res; &#125; //打印出从s点到w点的路径 void showPath(int w)&#123; if(!hasPath(w))&#123; throw new IllegalArgumentException("no path between s and w"); &#125; Vector&lt;Integer&gt; vec = path(w); for( int i = 0 ; i &lt; vec.size() ; i++)&#123; System.out.print(vec.elementAt(i)); if( i == vec.size() - 1 ) System.out.println(); else System.out.print(" -&gt; "); &#125; &#125;&#125; ShortestPath.java:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104import java.util.Vector;import java.util.Stack;import java.util.LinkedList;import java.util.Queue;public class ShortestPath &#123; private Graph G; //图的引用 private int s; //起始点 private boolean[] visited; // 记录dfs的过程中节点是否被访问 private int[] from; //记录路径，from[i]表示查找的路径上i的上一个节点 private int[] ord; //记录路径中节点的次序，ord[i]表示i节点在路径中的次序 //构造函数，寻路算法，寻找图graph从s点到其他点的路径 public ShortestPath(Graph graph, int s)&#123; G = graph; if(!(s &gt;= 0 &amp;&amp; s &lt; G.V() ))&#123; throw new IllegalArgumentException("make sure that s &gt;= 0 and s &lt; G.V()"); &#125; visited = new boolean[G.V()]; from = new int[G.V()]; ord = new int[G.V()]; for(int i = 0 ; i &lt; G.V() ; i ++)&#123; visited[i] = false; from[i] = -1; ord[i] = -1; &#125; this.s = s; //无向图最短路径算法，s开始广度优先遍历整张图 Queue&lt;Integer&gt; q = new LinkedList&lt;&gt;(); q.add(s); visited[s] = true; ord[s] = 0 ; while(!q.isEmpty())&#123; int v = q.remove(); for(int i : G.adj(v)) if(!visited[i])&#123; q.add(i); visited[i] = true; from[i] = v; ord[i] = ord[v] + 1; &#125; &#125; &#125; //查询从s点到w点是否有路径 public boolean hasPath(int w)&#123; if(!(w &gt;= 0 &amp;&amp; w &lt; G.V() ))&#123; throw new IllegalArgumentException("make sure that w &gt;= 0 and w &lt; G.V()"); &#125; return visited[w]; &#125; //查询从s点到w点的路径，存放在vec中 public Vector&lt;Integer&gt; path(int w)&#123; if(!(hasPath(w)))&#123; throw new IllegalArgumentException("no path between s and w"); &#125; Stack&lt;Integer&gt; s = new Stack&lt;&gt;(); //通过from数组逆向查找从s到w的路径,存放在栈中 int p = w; while(p != -1)&#123; s.push(p); p = from[p]; &#125; //从栈中一次取出元素,获得顺序的从s到w的路径 Vector&lt;Integer&gt; res = new Vector&lt;&gt;(); while (!s.empty())&#123; res.add(s.pop()); &#125; return res; &#125; //打印出从s到w的路径 public void showPath(int w)&#123; if(!(hasPath(w)))&#123; throw new IllegalArgumentException("no path between s and w"); &#125; Vector&lt;Integer&gt; vec = path(w); for(int i = 0 ; i &lt; vec.size() ; i ++)&#123; System.out.print(vec.elementAt(i)); if(i == vec.size() - 1) System.out.println(); else System.out.print(" -&gt; "); &#125; &#125; //查看从s点到w点的最短路径长度 //若从s到w不可达，则返回-1 public int length(int w)&#123; if(!(w &gt;= 0 &amp;&amp; w &lt; G.V() ))&#123; throw new IllegalArgumentException("make sure that w &gt;= 0 and w &lt; G.V()"); &#125; return ord[w]; &#125;&#125; testG3.txt:1234567897 80 10 20 50 63 43 54 54 6 Main.java:12345678910111213141516171819202122public class Main &#123; public static void main(String[] args)&#123; String filename = "testG3.txt"; SparseGraph g = new SparseGraph(7,false); ReadGraph readGraph = new ReadGraph(g,filename); g.show(); //比较使用深度优先遍历和广度优先遍历获得路径的不同 //广度优先遍历获得的是无权图的最短路径 Path dfs = new Path(g,0); System.out.print("DFS: "); dfs.showPath(6); ShortestPath bfs = new ShortestPath(g,0); System.out.print("BFS: "); bfs.showPath(6); System.out.println(); &#125;&#125; 实验结果:]]></content>
      <tags>
        <tag>数据结构</tag>
        <tag>图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图论基础]]></title>
    <url>%2F2019%2F03%2F29%2F%E5%9B%BE%E8%AE%BA%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[图主要是节点和边组成的模型。图可以用来表达真实世界中的很多关系，如交通运输、社交网络、互联网等。图分为有向图（Directed Graph）和无向图（Undirected Graph），有向图由于其不对称性，所有有时候会涉及很多比较难的算法。可以把无向图看成一种特殊的有向图。图也可以分为有权图（Weighted Graph）和无权图（Unweighted Graph），权是指节点与节点之间的边的数值。图不一定都要连接起来，比如一个模型中有三个没有连通起来的图。有的图中有可能会有自环边（self-loop）和平行边（parallel-edges），简单图（Simple Graph）就是指没有自环边和平行边的图，因为有了自环边和平行边，那么算法可能会更加复杂。 图的表示邻接矩阵一种方法是使用邻接矩阵（Adjacency Matrix）表示一张图。使用邻接矩阵表示这个无向图，a[i,j]代表的值为0或1。0代表不相连，1代表相连。这个矩阵是关于对角线对称的。也可以使用邻接矩阵表示有向图。例如，0-&gt;1存在有向边，那么a[0,1]=1;但是1-&gt;0不存在有向边，所以a[1,0]=0。 邻接表邻接表（Adjacency Lists）只表达和某个顶点相连接的顶点的信息。对于每一行来说都相当于一个链表，存放了对于该节点相连的所有节点。同样，邻接表也可以表达有向图。 由此可见，邻接表的占用空间要比邻接矩阵小。邻接表适合表示稀疏图（Sparse Graph）邻接矩阵适合表示稠密图（Dense Graph） 数据结构中对于稀疏图的定义为：有很少条边或弧（边的条数|E|远小于|V|²）的图称为稀疏图（sparse graph），反之边的条数|E|接近|V|²，称为稠密图（dense graph）。——百度百科稠密图的一个极端情况就是完全图，即所有的点之间都互相连接。这种情况使用邻接矩阵进行存储会更好。 代码实现C++DenseGraph.h12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#ifndef GRAPH_DENSEGRAPH_H#define GRAPH_DENSEGRAPH_H#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;cassert&gt;using namespace std;//稠密图 - 邻接矩阵class DenseGraph&#123;private: int n,m;//点数和边数 bool directed;//有向还是无向图 vector&lt;vector&lt;bool&gt; &gt; g; //true表示有这条边,false表示没有public: DenseGraph( int n , bool directed)&#123; this-&gt;n = n;//n个顶点 this-&gt;m = 0;//初始为0条边 this-&gt;directed = directed; for( int i = 0 ; i &lt; n ; i ++) g.push_back( vector&lt;bool&gt;( n, false )); &#125; ~DenseGraph() &#123; &#125; int V()&#123;return n;&#125;//返回顶点数 int E()&#123;return m;&#125;//返回边数 void addEdge( int v , int w)&#123; //在顶点v和w间建立一条边 //保证v和w不越界 assert( v &gt;= 0 &amp;&amp; v &lt; n ); assert( w &gt;= 0 &amp;&amp; w &lt; n ); //先判断v和w之间是否已经有边 if( hasEdge( v, w )) //这里避免了产生平行边的情况，这也是使用邻接矩阵的优点 return; g[v][w] = true; //如果是有向图，只需运行这一句话 if( !directed ) //如果是无向图 g[w][v] = true; m++; //维护边数 &#125; bool hasEdge( int v , int w )&#123; //判断两个顶点是否已经有边 //保证v和w不越界 assert( v &gt;= 0 &amp;&amp; v &lt; n ); assert( w &gt;= 0 &amp;&amp; w &lt; n ); return g[v][w]; &#125;&#125;;#endif // GRAPH_DENSEGRAPH_H SparseGraph.h12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061#ifndef GRAPH_SPARSEGRAPH_H#define GRAPH_SPARSEGRAPH_H#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;cassert&gt;using namespace std;//稀疏图 - 邻接表class SparseGraph&#123;private: int n,m; bool directed; vector&lt;vector&lt;int&gt; &gt; g; // 图的具体数据public: SparseGraph( int n , bool directed )&#123; this-&gt;n = n; this-&gt;m = 0; this-&gt;directed = directed; for( int i = 0 ; i &lt; n ; i ++)&#123; g.push_back(vector&lt;int&gt;()); //初始化时为空，因为初始化时没有顶点相连接，这里也可以用链表实现，使用链表在删除时效率高 &#125; &#125; ~SparseGraph()&#123;&#125; int V()&#123;return n;&#125; int E()&#123;return m;&#125; //在使用邻接表进行表达的时候，通常允许有平行边 //因为addEdge时，每进行一次添加，都要调用hasEdge进行判断，而hasEdge最差的时间复杂度为O(n) //所以addEdge也会退化为O(n)，一般在所有的边添加完成后再进行一次综合检查，去除平行边 void addEdge( int v , int w )&#123; //保证v和w不越界 assert( v &gt;= 0 &amp;&amp; v &lt; n ); assert( w &gt;= 0 &amp;&amp; w &lt; n ); g[v].push_back(w);//v和w相连 if( v != w &amp;&amp; !directed ) //如果v不是自环边且不是有向图 g[w].push_back(v); m ++; &#125; bool hasEdge( int v , int w )&#123; assert( v &gt;= 0 &amp;&amp; v &lt; n ); assert( w &gt;= 0 &amp;&amp; w &lt; n ); for( int i = 0 ; i &lt; g[v].size() ; i++ ) if( g[v][i] == w ) return true; return false; &#125;&#125;;#endif // GRAPH_SPARSEGRAPH_H 遍历临边——相邻点迭代器在图的操作中，有一个基础操作——遍历临边。下图是对0节点表达临边时邻接矩阵和邻接表的不同。这个图也反映了遍历0的临边的方法。 这个是一个很重要的操作，在后面的很多方法中都用到了它！！！！ 下面制作一个迭代器，用来访问一个顶点的所有临边。 SparseGraph.h123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#ifndef GRAPH_SPARSEGRAPH_H#define GRAPH_SPARSEGRAPH_H#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;cassert&gt;using namespace std;//稀疏图 - 邻接表class SparseGraph&#123; //...........省略代码 // 邻边迭代器, 传入一个图和一个顶点, // 迭代在这个图中和这个顶点向连的所有顶点class adjIterator &#123;private: SparseGraph &amp;G; // 图G的引用 int v; int index;public: // 构造函数 adjIterator(SparseGraph &amp;graph, int v ): G(graph)&#123; this-&gt;v = v; this-&gt;index = 0; &#125; ~adjIterator()&#123;&#125; // 返回图G中与顶点v相连接的第一个顶点 int begin()&#123; index = 0; if( G.g[v].size() ) return G.g[v][index];//即g[v][0] // 若没有顶点和v相连接, 则返回-1 return -1 ; &#125; // 返回图G中与顶点v相连接的下一个顶点 int next()&#123; index ++; if( index &lt; G.g[v].size() ) return G.v[v][index]; // 若没有顶点和v相连接, 则返回-1 return -1; &#125; bool end()&#123; //判断是否迭代完成 return index &gt;= G.g[v].size(); &#125;&#125;&#125;#endif // GRAPH_SPARSEGRAPH_H 在main中进行测试：123456789101112131415161718192021222324252627282930313233343536#include &lt;iostream&gt;#include &lt;stdlib.h&gt;#include &lt;time.h&gt;#include "SparseGraph.h"#include "DenseGraph.h"using namespace std;int main() &#123; int N = 20; int M = 100; srand( time(NULL) ); // Sparse Graph SparseGraph g1(N , false); for( int i = 0 ; i &lt; M ; i ++ )&#123; int a = rand()%N; int b = rand()%N; g1.addEdge( a , b ); &#125; // O(E) for( int v = 0 ; v &lt; N ; v ++ )&#123; cout&lt;&lt;v&lt;&lt;" : "; SparseGraph::adjIterator adj( g1 , v ); for( int w = adj.begin() ; !adj.end() ; w = adj.next() ) cout&lt;&lt;w&lt;&lt;" "; cout&lt;&lt;endl; &#125; cout&lt;&lt;endl; return 0;&#125; 可以看到稀疏图冲存在平行边，但是稠密图中没有平行边。稀疏图的遍历时间复杂度为O(E),稠密图的时间复杂度为O(V^2)。E为边数，V为顶点数。 图的算法框架创建文件testG1.txt，用文件来表示一个图。如下面的数据：第一行表示有13个节点和13条边，第二行0 5则表示节点0 和 5之间有一条边。123456789101112131413 130 54 30 19 126 45 40 211 129 100 67 89 115 3 同样创建第二个文件testG2.txt表示另一个图：1234567896 80 10 20 51 21 31 43 43 5 下面设计一个从文件中读取图的算法（无论是稀疏图还是稠密图都通用）,封装在ReadGraph.h类中。该类是一个模板类，指定一个图的引用和一个用来表示图的文件，将文件中存放的数据存到图中。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#ifndef READGRAPH_H_INCLUDED#define READGRAPH_H_INCLUDED#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;fstream&gt;#include &lt;sstream&gt;#include &lt;cassert&gt;using namespace std;template &lt;typename Graph&gt;class ReadGraph&#123;public: ReadGraph(Graph &amp;graph,const string &amp;filename)&#123; ifstream file(filename); string line; int V,E; assert( file.is_open() ); //首先读取顶点数和边数 assert( getline(file,line) ); stringstream ss(line); ss&gt;&gt;V&gt;&gt;E; assert( V == graph.V() ); //然后创建图 for( int i = 0 ; i &lt; E ; i++)&#123; assert( getline(file,line) ); stringstream ss(line); int a,b; ss&gt;&gt;a&gt;&gt;b; assert( a &gt;= 0 &amp;&amp; a &lt; V ); assert( b &gt;= 0 &amp;&amp; b &lt; V ); graph.addEdge( a , b ); &#125; &#125;&#125;;#endif // READGRAPH_H_INCLUDED 在SparseGraph.h和DenseGraph.h中添加一个方法show()用来打印图:SparseGraph.h123456789void show()&#123; for( int i = 0 ; i &lt; n ; i++)&#123; cout&lt;&lt;"vertex "&lt;&lt;i&lt;&lt;":\t"; for(int j = 0 ; j &lt; g[i].size() ; j++ )&#123; cout&lt;&lt;g[i][j]&lt;&lt;"\t"; &#125; cout&lt;&lt;endl; &#125;&#125; DenseGraph.h1234567void show()&#123; for(int i = 0 ; i &lt; n ; i++)&#123; for( int j = 0 ; j &lt; n ; j ++) cout&lt;&lt;g[i][j]&lt;&lt;"\t"; cout&lt;&lt;endl; &#125;&#125; 在main函数中进行测试:123456789101112131415161718192021222324252627282930313233343536373839404142#include &lt;iostream&gt;#include &lt;stdlib.h&gt;#include &lt;time.h&gt;#include "SparseGraph.h"#include "DenseGraph.h"#include "ReadGraph.h"using namespace std;int main() &#123; // 使用两种图的存储方式读取testG1.txt文件 string filename = "testG1.txt"; SparseGraph g1( 13 , false ); ReadGraph&lt;SparseGraph&gt; readGraph1( g1 , filename ); cout&lt;&lt;"test G1 in Sparse Graph:" &lt;&lt; endl; g1.show(); cout&lt;&lt;endl; DenseGraph g2( 13 , false ); ReadGraph&lt;DenseGraph&gt; readGraph2( g2 , filename ); cout&lt;&lt;"test G1 in Dense Graph:" &lt;&lt; endl; g2.show(); cout&lt;&lt;endl; // 使用两种图的存储方式读取testG2.txt文件 filename = "testG2.txt"; SparseGraph g3( 6 , false ); ReadGraph&lt;SparseGraph&gt; readGraph3( g3 , filename ); cout&lt;&lt;"test G2 in Sparse Graph:" &lt;&lt; endl; g3.show(); cout&lt;&lt;endl; DenseGraph g4( 6 , false ); ReadGraph&lt;DenseGraph&gt; readGraph4( g4 , filename ); cout&lt;&lt;"test G2 in Dense Graph:" &lt;&lt; endl; g4.show(); return 0;&#125; 可以看到使用邻接矩阵和邻接表创建的图的结果: java表达首先定义图的接口Graph.java12345678910//图的接口public interface Graph &#123; public int V(); //获取图的顶点数 public int E(); //获取图的边数 public void addEdge( int v ,int w); //在v和w两个顶点间添加一条边 boolean hasEdge( int v , int w);//查看v和w两个顶点间是否有边 void show();//打印图 public Iterable&lt;Integer&gt; adj(int v); //获取与v顶点连接的所有边&#125; 定义稠密图DenseGraph.java:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687import java.util.Vector;//稠密图 - 邻接矩阵public class DenseGraph implements Graph&#123; private int n; //节点数 private int m; //边数 private boolean directed; // 是否为有向图 private boolean[][] g; //图的具体数据，用二维数组表达 //构造函数 public DenseGraph( int n , boolean directed)&#123; if(n &lt; 0)//保证n要&gt;=0 throw new IllegalArgumentException("the value of n should be &gt;= 0."); this.n = n; this.m = 0 ;//初始化时没有任何边 this.directed = directed; //g初始化时为n*n的布尔型矩阵,每个g[i][j]均为false，因为开始时没有任何边 //false为boolean型变量的默认值 g = new boolean[n][n]; &#125; //返回节点个数 @Override public int V() &#123; return n; &#125; //返回边数 @Override public int E() &#123; return m; &#125; //向图中添加一条边 @Override public void addEdge(int v, int w) &#123; if(!(v &gt;= 0 &amp;&amp; v &lt; n)) return; if(!(w &gt;= 0 &amp;&amp; w &lt; n)) return; //如果v和w间已存在边，则直接退出 if(hasEdge( v , w )) return; g[v][w] = true;//v和w间建立边 if( !directed ) //如果不是有向图,则继续建立w到v的边 g[w][v] = true; m ++;//维护边数，边数加一 &#125; //判断图中是否有v到w的边 @Override public boolean hasEdge(int v, int w) &#123; if(!(v &gt;= 0 &amp;&amp; v &lt; n)) throw new IllegalArgumentException("the value of v is Illegal!you should type the value of v between 0 and n.(v &gt;=0 &amp;&amp; v &lt; n)."); if(!(w &gt;= 0 &amp;&amp; w &lt; n)) throw new IllegalArgumentException("the value of w is Illegal!you should type the value of w between 0 and n.(w &gt;=0 &amp;&amp; w &lt; n)."); return g[v][w]; &#125; //打印显示图的信息 @Override public void show() &#123; for(int i = 0 ; i &lt; n ; i ++)&#123; for( int j = 0 ; j &lt; n ; j ++) System.out.print(g[i][j]+"\t"); System.out.println(); &#125; &#125; //返回图中v顶点的所有邻边 //由于java使用引用机制，返回一个Vector不会带来额外开销 @Override public Iterable&lt;Integer&gt; adj(int v) &#123; if(!(v &gt;= 0 &amp;&amp; v &lt; n)) throw new IllegalArgumentException("the value of v is Illegal!you should type the value of v between 0 and n.(v &gt;=0 &amp;&amp; v &lt; n)."); Vector&lt;Integer&gt; adjV = new Vector&lt;Integer&gt;(); for(int i = 0 ; i &lt; n ; i++) if(g[v][i]) adjV.add(i); return adjV; &#125;&#125; 稀疏图SparseGraph.java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485import java.util.Vector;//稀疏图 - 邻接表public class SparseGraph implements Graph&#123; private int n; //节点数 private int m; //边数 private boolean directed; //是否为有向图 private Vector&lt;Integer&gt;[] g; //图的具体数据 //构造函数 public SparseGraph(int n , boolean directed)&#123; if(n &lt; 0) throw new IllegalArgumentException("the value of n should be &gt;= 0."); this.n = n; this.m = 0; this.directed = directed; //g初始化为n个空的vector,表示每一个g[i]都为空，即没有任何边 g = (Vector&lt;Integer&gt;[]) new Vector[n]; for(int i = 0 ; i &lt; n ; i ++) g[i] = new Vector&lt;Integer&gt;(); &#125; //返回节点个数 @Override public int V() &#123; return n; &#125; //返回边数 @Override public int E() &#123; return m; &#125; //向图中添加一条边 @Override public void addEdge(int v, int w) &#123; if(!(v &gt;= 0 &amp;&amp; v &lt; n)) throw new IllegalArgumentException("the value of v is Illegal!you should type the value of v between 0 and n.(v &gt;=0 &amp;&amp; v &lt; n)."); if(!(w &gt;= 0 &amp;&amp; w &lt; n)) throw new IllegalArgumentException("the value of w is Illegal!you should type the value of w between 0 and n.(w &gt;=0 &amp;&amp; w &lt; n)."); g[v].add(w); if(v != w &amp;&amp; !directed) //如果不是自环边，并且它是无向图，则创建w到v的边 g[w].add(v); m ++; &#125; //验证图中是否有v到w的边 @Override public boolean hasEdge(int v, int w) &#123; if(!(v &gt;= 0 &amp;&amp; v &lt; n)) throw new IllegalArgumentException("the value of v is Illegal!you should type the value of v between 0 and n.(v &gt;=0 &amp;&amp; v &lt; n)."); if(!(w &gt;= 0 &amp;&amp; w &lt; n)) throw new IllegalArgumentException("the value of w is Illegal!you should type the value of w between 0 and n.(w &gt;=0 &amp;&amp; w &lt; n)."); for(int i = 0 ; i &lt; g[v].size() ; i++) if( g[v].elementAt(i) == w) return true; return false; &#125; //显示图的信息 @Override public void show() &#123; for(int i = 0; i &lt; n ; i ++)&#123; System.out.printf("vertex %d :\t",i); for(int j = 0 ; j &lt; g[i].size() ; j++) System.out.print(g[i].elementAt(j) + "\t"); System.out.println(); &#125; &#125; // 返回图中一个顶点的所有邻边 // 由于java使用引用机制，返回一个Vector不会带来额外开销, @Override public Iterable&lt;Integer&gt; adj(int v) &#123; if(!(v &gt;= 0 &amp;&amp; v &lt; n)) throw new IllegalArgumentException("the value of v is Illegal!you should type the value of v between 0 and n.(v &gt;=0 &amp;&amp; v &lt; n)."); return g[v]; &#125;&#125; 读取图文件ReadGraph.java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import java.io.BufferedInputStream;import java.io.File;import java.io.FileInputStream;import java.io.IOException;import java.util.Scanner;import java.util.Locale;import java.util.InputMismatchException;import java.util.NoSuchElementException;public class ReadGraph &#123; private Scanner scanner; public ReadGraph(Graph graph, String filename)&#123; readFile(filename); try &#123; int V = scanner.nextInt(); if (V &lt; 0) throw new IllegalArgumentException("number of vertices in a Graph must be nonnegative"); assert V == graph.V(); int E = scanner.nextInt(); if (E &lt; 0) throw new IllegalArgumentException("number of edges in a Graph must be nonnegative"); for (int i = 0; i &lt; E; i++) &#123; int v = scanner.nextInt(); int w = scanner.nextInt(); assert v &gt;= 0 &amp;&amp; v &lt; V; assert w &gt;= 0 &amp;&amp; w &lt; V; graph.addEdge(v, w); &#125; &#125; catch (InputMismatchException e) &#123; String token = scanner.next(); throw new InputMismatchException("attempts to read an 'int' value from input stream, but the next token is \"" + token + "\""); &#125; catch (NoSuchElementException e) &#123; throw new NoSuchElementException("attemps to read an 'int' value from input stream, but there are no more tokens available"); &#125; &#125; private void readFile(String filename)&#123; assert filename != null; try &#123; File file = new File(filename); if (file.exists()) &#123; FileInputStream fis = new FileInputStream(file); scanner = new Scanner(new BufferedInputStream(fis), "UTF-8"); scanner.useLocale(Locale.ENGLISH); &#125; else throw new IllegalArgumentException(filename + " doesn't exist."); &#125; catch (IOException ioe) &#123; throw new IllegalArgumentException("Could not open " + filename, ioe); &#125; &#125;&#125; 书写main函数然后测试:123456789101112131415161718192021222324252627282930313233public class Main &#123; public static void main(String[] args)&#123; //使用两种方式读取testG1.txt文件 String filename = "testG1.txt"; SparseGraph g1 = new SparseGraph(13,false); ReadGraph readGraph1 = new ReadGraph(g1,filename); System.out.println("test G1 in Sparse Graph:"); g1.show(); System.out.println(); DenseGraph g2 = new DenseGraph(13,false); ReadGraph readGraph2 = new ReadGraph(g2,filename); System.out.println("test G1 in Dense Graph:"); g2.show(); System.out.println(); // 使用两种图的存储方式读取testG2.txt文件 filename = "testG2.txt"; SparseGraph g3 = new SparseGraph(6, false); ReadGraph readGraph3 = new ReadGraph(g3, filename); System.out.println("test G2 in Sparse Graph:"); g3.show(); System.out.println(); DenseGraph g4 = new DenseGraph(6, false); ReadGraph readGraph4 = new ReadGraph(g4, filename); System.out.println("test G2 in Dense Graph:"); g4.show(); &#125;&#125; 读取结果:]]></content>
      <tags>
        <tag>数据结构</tag>
        <tag>图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[哈希表]]></title>
    <url>%2F2019%2F03%2F14%2F%E5%93%88%E5%B8%8C%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[哈希表中，每一个元素都和一个索引对应。 比如在leetcode中，的387号问题; 相关解决方案:1234567891011121314class Solution &#123; public int firstUniqChar(String s) &#123; int[] freq = new int[26]; for(int i = 0 ; i &lt; s.length() ; i ++) freq[s.charAt(i) - 'a'] ++; for(int i = 0 ; i &lt; s.length() ; i ++) if(freq[s.charAt(i) - 'a'] == 1) return i; return -1; &#125;&#125; 其中的每一个字符对应一个索引。每一个类型转化为索引的函数称为哈希函数（上例中就是`ch - ‘a’）。然后在哈希表上进行相应操作。 在建立哈希表时很难保证每一个“键”通过哈希函数的转换对应不同的“索引”，这样会造成哈希冲突。所以关键在于如何解决哈希冲突。哈希表使用空间换时间，是时间和空间之间的平衡。 设计哈希函数哈希函数设计时，“键”通过哈希函数得到的“索引”分布越均匀越好。对于一些特殊的领域，有特殊的哈希函数设计方式。 哈希函数的设计遵循三个原则: 一致性：如果a==b,则hash(a) == hash(b) 高效性：计算高效简便 均匀性：哈希值均匀分布 基本类型 整型对于整型，小范围正整数可以直接使用 小范围的负整数进行偏移相应的能够转化为索引的函数称为哈希函数。 对于大整数，比如身份证号可以采用取模的方法（比如可以取模后四位、后6位）。但是后6位中，有两位是表示日期的，日期最多是1-31，那么就会产生分布不均匀的情况。所以要根据具体问题具体分析，很难有一个通用的解决方法。 在取模时，模一个素数是一个通常的解决方案。 浮点型 基本数据类型在计算机中都是32位或者64位的二进制表示，只不过计算机根据一定的法则给解析成了浮点数。在处理时，可以吧浮点数的32位或者64位空间转化为整型进行处理。 字符串 字符串也可以转为整型进行处理。 比如:166 = 1 10^2 + 6 10^1 + 6 10 ^0code = c 26^3 + o 26^2 + d 26^1 + e 26^0 (看成26进制的整型)code = c B^3 + o B^2 + d B^1 + e B^0 (看成B进制的整型)hash(code) = ( c B^3 + o B^2 + d B^1 + e B^0 ) % M (M为取模时选取的素数)也等价于 hash(code) = ( ( ( ( c B ) + o ) B + d ) B + e ) % M 为了避免整型的溢出，也可以先取模在继续进行下一步：hash(code) = ( ( ( ( c % M) B + o ) % M B + d ) % M * B + e ) % M 123int hash = 0;for(int i = 0 ; i &lt; s.length() ; i ++) hash = (hash * B + s.charAt(i)) % M 符合类型 优先考虑方法也是转化为整型处理。例如 Date: year,month,day hash(date) = (((date.year % M ) B + date.month) % M B) + date.day) % M java中的hashCodejava中给我们提供了很多现有的hashCode方法。Object对象有hashCode()方法，调用此方法可以返回这个对象的hash值。java中的hashCode()方法返回的hash值都是整型的，如果有需要，用户可以进一步处理。 1234567891011121314151617public class Main &#123; public static void main(String[] args)&#123; int a = 2; System.out.println(((Integer)a).hashCode()); int b = -2; System.out.println(((Integer)b).hashCode()); double c = 3.1415926; System.out.println(((Double)c).hashCode()); String d = "hashCode"; System.out.println(d.hashCode()); &#125;&#125; 12342-2219937201147696667 如果用户想对于自定义的类也能转换为相对应的哈希函数，则只需在类中覆盖来自于Object父类的hashCode方法。1234567891011121314151617181920212223public class Student &#123; int grade; int cls; String name; Student(int grade,int clas,String name)&#123; this.grade = grade; this.cls = cls; this.name = name; &#125; @Override public int hashCode()&#123; int B = 31;//将此类的看成有三个作用域的复合类型，直接把三部分看成三个数字，符合类型看成由这三个部分组成的B进制的数，进制数可以自己定义； int hash = 0; hash = hash * B + grade; hash = hash * B + cls; hash = hash * B + name.hashCode(); //如果出现整型溢出，计算机会做处理，这样不满足数学上的规范，不过对于计算机仍然是有效的 return hash; &#125;&#125; 123Student student = new Student(100,20,"homxu");System.out.println(student.hashCode()); 199557671 当一个类能够获得其hash值之后，便能够使用java提供的对于hash的操作。如java.util.HashSet和java.util.HashMap 12345HashSet&lt;Student&gt; set = new HashSet&lt;&gt;();set.add(student);HashMap&lt;Student,Integer&gt; scores = new HashMap&lt;&gt;();scores.put(student,100); 当然，如果一个类没有自定义其hashCode方法，那么会使用默认的方法，默认方法是将对象的地址映射成一个整型值。所以，当new出来两个数值相同的对象时，他们的hashCode值也是不同的。 在自定义hashCode方法的时候，如果传过去的值相等，那么可能会产生hash冲突，所以一般还要在重写一个Object的equals()方法。以上面的Student类为例。 12345678910111213@Overridepublic boolean equals(Object o)&#123; //注意这里的参数是object类型 if(this == o) return true; //如果是同一个引用，则直接返回true if(o == null) return false; if(getClass() != o.getClass()) return false; //如果类不同，返回false Student another = (Student)o; return this.grade == another.grade &amp;&amp; this.cls == another.cls &amp;&amp; this.name.equals(another.name);&#125; 这样，如果产生了hash冲突，即使是一样的hash值，也可以区分两个类对象的不同。 链地址法(Seperate Chaining)链地址法是哈希冲突的一种处理方法。由之前的内容可知道，在处理哈希值的存储的时候，其实就是将对应的整数取模后，这个值就对应它在数组中的索引。对于会产生负号的值，则进行取绝对值或者(hashCode(value) &amp; 0x7fffffff) % M(按位与进行取负号，效果和取绝对值一样)。 假设产生了哈希冲突，本来索引为1的位置已经存在了元素v1,后来新元素v2也和v1的索引一致，那么这时候扔把v2放在v1的位置，也就是索引处存放的是一个链表，每个索引位置存的都是一个链表，所以Seperate Chaining也由此得名。当然，每一个位置的本质其实是存放的一个查找表，查找表不一定非要用链表，也可以用平衡树结构来实现这个查找表，所以每个位置也可以是一个查找表的结构。在产生哈希冲突时，找到对应的索引位置，然后插入到所以位置对应的TreeMap就可以了。HashMap本质就是一个TreeMap数组。HashSet就是一个TreeSet数组。 在Java8之前，每一个索引位置对应的就是一个链表。Java8开始，当哈希冲突达到一定的程度时，每个位置从链表转为红黑树(即TreeMap，因为Java8的TreeMap底层实现就是红黑树)。 实现一个哈希表实现一个简单的哈希表，包括增删改查等 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import java.util.TreeMap;public class HashTable&lt;K,V&gt; &#123; private TreeMap&lt;K,V&gt;[] hashtable; private int M; private int size; public HashTable(int M)&#123; this.M = M; this.size = 0; hashtable = new TreeMap[M]; for(int i = 0 ; i &lt; M ; i ++) hashtable[i] = new TreeMap&lt;&gt;(); &#125; public HashTable()&#123; this(97); &#125; private int hash(K key)&#123; return (key.hashCode() &amp; 0x7fffffff) % M;//将key值转变为当前哈希表中的索引 &#125; public int getSize()&#123; return size; &#125; //哈希表的添加操作 public void add(K key,V value)&#123; TreeMap&lt;K,V&gt; map = hashtable[hash(key)]; //先用map暂存当前的hashtable索引对应的TreeMap if(map.containsKey(key))//如果对应的索引处包含了当前的元素 map.put(key,value); else&#123; map.put(key,value); size ++; &#125; &#125; //哈希表的删除操作 public V remove(K key)&#123; TreeMap&lt;K,V&gt; map = hashtable[hash(key)]; V ret = null; if(map.containsKey(key))&#123; ret = map.remove(key); size --; &#125; return ret; &#125; //哈希表的修改操作 public void set(K key,V value)&#123; TreeMap&lt;K,V&gt; map = hashtable[hash(key)]; if(!map.containsKey(key)) throw new IllegalArgumentException(key + "doesn't exist!"); map.put(key,value); &#125; //哈希表的查询操作 public boolean contains(K key)&#123; return hashtable[hash(key)].containsKey(key); &#125; public V get(K key)&#123; return hashtable[hash(key)].get(key); &#125;&#125; 当然，对于这个简单实现的哈希表，其M值是开始已经确定好的。选取合适的M值将对哈希表的性能有很大的影响。 时间复杂度和动态空间处理在前面的介绍中，假设哈希表有M个地址，如果向其中放入N个元素，那么每个地址平均有N/M个元素。如果每个地址是链表，那么时间复杂度为O(N/M)。如果每个地址是一个平衡树，那么时间复杂度为O(log(N/M))。 哈希表的目的是为了让时间复杂度为或者接近O(1).如果N趋于无穷大的话，那么对于一个静态的数组，它总会产生哈希冲突。所以一个解决办法就是使用动态存储，使其进行自适应。所以和静态数组一样，固定的地址空间是不合理的，所以需要resize操作。但是哈希表的动态操作又和静态数组不同，哈希表的每个索引位置存放的是一个链表或者一个树，如果是链表，那么它是不存在这个位置被“填满”的概念的。所以这里需要考虑的是，平均每个地址承载的元素多过一定的程度，即进行扩容，可以将容忍度定义为upperTol,当N/M &gt;= upperTol时，进行扩容。同理，平均每个地址承载的元素少过一定程度就进行缩容：N / M &lt; lowerTol. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081import java.util.TreeMap;public class HashTable&lt;K,V&gt; &#123; private static final int upperTol = 10; //定义upperTol private static final int lowerTol = 2; //定义lowerTol private static final int initCapacity = 7; //定义初始的大小 private TreeMap&lt;K,V&gt;[] hashtable; private int M; private int size; public HashTable(int M)&#123; this.M = M; this.size = 0; hashtable = new TreeMap[M]; for(int i = 0 ; i &lt; M ; i ++) hashtable[i] = new TreeMap&lt;&gt;(); &#125; public HashTable()&#123; this(initCapacity); &#125; private int hash(K key)&#123; return (key.hashCode() &amp; 0x7fffffff) % M;//将key值转变为当前哈希表中的索引 &#125; public int getSize()&#123; return size; &#125; //哈希表的添加操作 public void add(K key,V value)&#123; TreeMap&lt;K,V&gt; map = hashtable[hash(key)]; //先用map暂存当前的hashtable索引对应的TreeMap if(map.containsKey(key))//如果对应的索引处包含了当前的元素 map.put(key,value); else&#123; map.put(key,value); size ++; if(size &gt;= upperTol * M) //平均每个地址承载的元素多过一定的程度,即N / M &gt;= upperTol resize(2*M); &#125; &#125; //哈希表的删除操作 public V remove(K key)&#123; TreeMap&lt;K,V&gt; map = hashtable[hash(key)]; V ret = null; if(map.containsKey(key))&#123; ret = map.remove(key); size --; if(size &lt; lowerTol * M &amp;&amp; M / 2 &gt;= initCapacity) //平均每个地址承载的元素少过一定的程度,即N / M &lt;= lowerTol；并且缩容时的空间至少为initCapacity resize(M / 2); &#125; return ret; &#125; //哈希表的修改操作 ... //哈希表的查询操作 ... private void resize(int newM)&#123; TreeMap&lt;K,V&gt;[] newHashTable = new TreeMap[newM]; for(int i = 0 ;i &lt; newM ; i ++) newHashTable[i] = new TreeMap&lt;&gt;(); int oldM = M; //暂存一下之前的M,在循环时作为循环边界 this.M = newM; // 注意hash(key)是对之前的M进行取模，这里要让M变为newM，对现在的M进行取模 for(int i = 0 ; i &lt; oldM ; i++)&#123; TreeMap&lt;K,V&gt; map = hashtable[i]; for(K key : map.keySet()) newHashTable[hash(key)].put(key,map.get(key)); &#125; this.hashtable = newHashTable; &#125;&#125; 哈希表的复杂度分析动态数组的均摊复杂度分析，平均复杂度 O(1) 对于哈希表，元素数从N增加到upperTol * N ，地址空间翻倍。平均复杂度O(1).每个操作在O(lowerTol) ~ O(upperTol) → O(1)。缩容同理 在扩容时有一个问题，对于素数M，如果进行扩容，那么扩容后2*M不是素数。其中一个解决方案是，按照素数表中的数字进行扩容，而不是简单的二倍关系。 当然哈希表的均摊复杂度虽然为O(1)，但是它与平衡树结构相比丢失了数据的顺序性。 这样回顾集合和映射的底层实现，如果想实现有序集合和有序映射，那么底层可使用平衡树；如果实现无序集合和无序映射，则使用哈希表。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374import java.util.TreeMap;public class HashTable&lt;K,V&gt; &#123; private int[] capacity= &#123;53, 97, 193, 389, 769, 1543, 3079, 6151, 12289, 24593, 49157, 98317,196613, 393241, 786433, 1572869, 3145739, 6291469, 12582917, 25165843, 50331653, 100663319, 201326611, 402653189, 805306457, 1610612741&#125;; private static final int upperTol = 10; //定义upperTol private static final int lowerTol = 2; //定义lowerTol private int capacityIndex = 0; //定义capacity数组初始的索引 private TreeMap&lt;K,V&gt;[] hashtable; private int M; private int size; public HashTable()&#123; this.M = capacity[capacityIndex]; size = 0; hashtable = new TreeMap[M]; for(int i = 0 ; i &lt; M ; i ++) hashtable[i] = new TreeMap&lt;&gt;(); &#125; private int hash(K key)&#123; return (key.hashCode() &amp; 0x7fffffff) % M;//将key值转变为当前哈希表中的索引 &#125; public int getSize()&#123; return size; &#125; //哈希表的添加操作 public void add(K key,V value)&#123; TreeMap&lt;K,V&gt; map = hashtable[hash(key)]; //先用map暂存当前的hashtable索引对应的TreeMap if(map.containsKey(key))//如果对应的索引处包含了当前的元素 map.put(key,value); else&#123; map.put(key,value); size ++; if(size &gt;= upperTol * M &amp;&amp; capacityIndex + 1 &lt; capacity.length)&#123; //平均每个地址承载的元素多过一定的程度,即N / M &gt;= upperTol,并且判断边界不会越界 capacityIndex ++; resize(capacity[capacityIndex]); &#125; &#125; &#125; //哈希表的删除操作 public V remove(K key)&#123; TreeMap&lt;K,V&gt; map = hashtable[hash(key)]; V ret = null; if(map.containsKey(key))&#123; ret = map.remove(key); size --; if(size &lt; lowerTol * M &amp;&amp; capacityIndex - 1 &gt;= 0) &#123;//平均每个地址承载的元素少过一定的程度,即N / M &lt;= lowerTol；并且缩容时不会越界 capacityIndex --; resize(capacity[capacityIndex]); &#125; &#125; return ret; &#125; //哈希表的修改操作 //同原来的函数 //哈希表的查询操作 //同原来的函数 private void resize(int newM)&#123; //同原来的函数 &#125;&#125; 彩蛋上述的代码实现中，HashTable的K值是不要求可比较的(Comparable)，但是在使用TreeMap定义hashtable数组时，TreeMap的K是要求可比较的，所以这是一个小bug。 前面提到在Java8之前，每一个索引位置对应的就是一个链表。Java8开始，当哈希冲突达到一定的程度时，每个位置从链表转为红黑树(即TreeMap，因为Java8的TreeMap底层实现就是红黑树)，当然，这种情况要求HashTable的K值是可比较的，不然它的每个索引仍然对应一个链表。]]></content>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[红黑树]]></title>
    <url>%2F2019%2F03%2F07%2F%E7%BA%A2%E9%BB%91%E6%A0%91%2F</url>
    <content type="text"><![CDATA[红黑树的定义红黑树是一棵二分搜索树。 红黑树的五个性质： 1、每个结点或者是红的，或者是黑的。 2、根结点是黑的。 3、每一个叶结点（nil结点,即最后的空结点）是黑的。 4、如果一个结点是红的，则其孩子结点都是黑的。 5、对于任意结点，从该结点到其子孙叶结点（nil结点）的所有路径上包含相同数目的黑结点。 红黑树是保持“黑平衡”的二叉树。严格意义上并不是平衡二叉树。因为有红色结点，所以红黑树的最大高度为O(logn)。 与AVL树相比，如果添加和删除操作比较频繁，那么用红黑树将有很大的优势。但是如果使用的数据近乎是不会改变，只涉及到查询操作，那么AVL树比红黑树有优势（虽然它们的查询复杂度都是O(logn)）。 2-3树2-3树满足二分搜索树的基本性质。 结点可以存放一个元素或者两个元素。 在《算法4》中，2-3树的定义如下： 一棵2-3查找树或为一棵空树，或由以下结点组成：2-结点，含有一个建（与其对应的值）和两条链接，左链接指向的2-3树中的键都小于该结点，右链接指向的2-3树中的键都大于该结点。3-结点，含有两个键（及其对应的值）和三条链接，左链接指向的2-3树中的键都小于该结点，中链接指向的2-3树中的键都位于该结点的两个键之间，右链接指向的2-3树中的键都大于该结点。 2-3树是一棵绝对平衡的树。 一棵完美平衡的2-3查找树中的所有空链接到根结点的距离都应该是相同的。 2-3树的绝对平衡性插入新键 要在2-3树中插入一个新结点，我们可以和二叉查找树一样先进行一次未命中的查找，然后把新结点挂在树的底部。但这样的话树就无法保持完美平衡性。我们使用2-3树的蛀牙原因就在于它能够在插入后继续保持平衡。如果未命中的查找结束于一个2-结点，事情就好办了：我们只要把这个2-结点替换为一个3-结点，将要插入的键保存在其中即可。 在2-3树中，添加一个结点将永远不会添加到一个空的位置。如图，如果将37插入到以42为根的2-3树中，插入37时，因为42的左孩子为空，新结点将融合到之前添加过程中最后的一个叶子结点上，即42，所以产生了结点融合.42由2-结点变成了3-结点。而若如果继续添加一个12结点，那么由于12小于37，它要添加到3-树的左子树，又因为2-3树的性质，不能添加到空的位置，那么先将12与3-树进行融合，产生一个4-树，接着进行分裂操作。这时候一个4-结点就转变成了一个由3个2-结点组成的平衡的树，同时这棵树保持着绝对平衡。此时，如果继续添加一个18结点，那么18要添加到12的右子树，12的右子树为空，不能添加到空结点，而是与它找到的最后一个叶子结点，即12结点进行融合，从而成为一个3-结点。接着插入6结点，6从根结点出发，比37小，要插入到37的左子树上，而其左子树是12 18的3-结点，它比12要小，所以要插入到其左子树，但它的左子树是一棵空树，添加结点不能添加到一个空的位置，而是找到它最后添加位置的叶子结点，与叶子结点进行融合，此时的叶子结点是一个3-结点，暂时与其融合成为4-结点，然后进行分裂拆解，但是此时如果拆解，那么树将不是一个绝对平衡的二叉树，而是应该将此时拆解后的新的根结点12与其父结点37进行融合，37是一个2-结点，进行融合就很容易了，37融合成3-结点，进而12结点对应的左右孩子变为新的根结点的左孩子和中孩子。继续添加11结点，11应插入到根结点的总左子树，11比6大，本应插入到6的右子树，但是6的右子树为空，所以11和6融合。再插入5,5应插入到12 37的左子树6 11的左结点，其左结点为空，此时应与其融合，首先融合成4-结点，再分裂为3个2-结点，6接着应与父结点融合，形成一个4-结点，4-结点应该进行分裂，分裂成3个2-结点。至此所有结点都是2-结点，仍是平衡的。 其实总结来说添加元素，添加的结点不会添加到空的位置，它会添加到最后搜索到的叶子结点，与其融合。如果其本身是2-结点，那么融合为3-结点，如果本身是3-结点，先融合为4-结点后，在分裂为3个2-结点。 2-3树和红黑树的等价性在使用2-3树进行表达和实现时，有的结点包含2个子结点，有的结点包含3个子结点。使用红黑树，均使用含有两个子结点的结点进行表达。 以下是《算法4》中关于替换3-结点的描述： 红黑二叉查找树背后的基本思想是用标准的二叉查找树（完全由2-结点构成）和一些额外的信息（替换3-结点）来表示2-3树。我们将树中的链接分为两种类型：红连接将两个2-结点连接起来构成一个3-结点，黑链接则是2-3树中的普通链接。确切地说，我们将3-结点表示为由一条左斜的红色链接（两个2-结点其中之一是另一个的左子结点）相连的两个2-结点。这种表示法的一个优点是，我们无需修改就可以直接使用标准二叉查找树的get()方法。对于任意的2-3树，只要对结点进行转换，我们都可以立即派生出一棵对应的二叉查找树。我们将这种表示2-3树的二叉查找树称为红黑二叉树（以下简称为红黑树）。 红黑树的另一种定义是含有红黑链接并满足下列条件的二叉查找树：1.红链接均为左链接2.没有任何一个结点同时和两条红链接相连3.该树是完美黑色平衡的，即任意空链接到根结点的路径上的黑链接数量相同满足这样定义的红黑树和相应的2-3树是一一对应的 即红结点和它的父结点一起表示原来2-3树中的3-结点。所有的红色结点都是向左倾斜的。 在原来的2-3树中有3个3-结点，所以在红黑树中有3个红结点。 颜色表示直接上代码吧:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import java.util.ArrayList;public class RBTree&lt;K extends Comparable&lt;K&gt;,V&gt; &#123; private static final boolean RED = true; private static final boolean BLACK = false; private class Node&#123; public K key; public V value; public Node left,right; public boolean color;//由其父结点指向它的链接的颜色 public Node(K key,V value)&#123; this.key = key; this.value = value; left = null; right = null; color = RED; //初始化为红结点，相当于刚创建时就与其父结点进行融合，融合后的操作是后续的事。 &#125; &#125; private Node root; private int size; public RBTree() &#123; root = null; size = 0; &#125; // 返回以node为根节点的二分搜索树中，key所在的节点 private Node getNode(Node node, K key)&#123; if(node == null) return null; if(key.equals(node.key)) return node; else if(key.compareTo(node.key) &lt; 0) return getNode(node.left, key); else // if(key.compareTo(node.key) &gt; 0) return getNode(node.right, key); &#125; public boolean contains(K key)&#123;return getNode(root,key) != null;&#125; public V get(K key)&#123; Node node = getNode(root,key); return node == null ? null : node.value; &#125;&#125; 在红黑树中添加新元素在2-3树中添加一个新结点时，它都是或者添加进2-结点，行程一个3-结点；或者是添加进3-结点，暂时形成一个4-结点，然后在进行后续的操作。总之就是把要添加的元素融合到已有的结点中。所以在红黑树中假设永远添加红色结点的意图也就是，添加的结点和其他结点先融合。 在上面的代码中可以看到，我们将新添加的结点定义为红结点。所以在添加时有几种情况。 1.最初情况是刚开始的树为空，然后让红黑树的根为新添加的元素，然后让根结点的红色变为黑色。 2.要添加的结点小于根结点，即添加到根结点的左孩子位置，则直接添加过去就可以了。 3.要添加的结点大于根结点，则先添加到根结点的右孩子位置，然后进行左旋转。左旋转方法和AVL树的左旋转类似。在左旋转后，有可能会产生两个红色结点(x.color = node.color;node.color = RED;)，这时候不用担心，因为返回的根结点如果为红色，它将会继续和上一层进行相关转换操作。 4.向红黑树中的3-结点添加元素时：①：向3-结点的右孩子添加时，直接添加到右孩子，然后所有子结点变为黑结点，也就是相当于在2-3树中先将3-结点变为4-结点，然后再分离为3个2-结点，然后再和其上面的父结点进行融合，所以这里要将其根结点变为红色（这样才能保证在红黑树中这个结点和其父结点进行融合）②：向3-结点添加元素时，新元素小于根结点的左孩子（即新键小于原树中的两个键），需要继续添加到根结点的左孩子的左孩子（即连接到最左边的空链接），它应该是一个红色结点，这样就产生了两个连续的红链接。此时需要将上层的红链接右旋转（中值键作为根结点并和其他两个结点用红链接相连）③：如果新键的值介于原树中的两个树之间，这样又会产生两个连续的红链接，一条红色左链接一条红色右链接。这时候需要将下层的红链接左旋转得到第②种情况，然后在②情况的基础上继续操作。 维护红黑树的时机和AVL树一样，在基于二分搜索树的基础上，添加了新结点后回溯向上维护。 相关代码:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980//判断节点node的颜色private boolean isRed(Node node)&#123; if(node ==null) &#123; return BLACK; &#125; return node.color;&#125;//左旋转private Node leftRotate(Node node) &#123; Node x = node.right; //左旋转操作 node.right = x.left; x.left = node; x.color = node.color; node.color = RED; return x;&#125;//右旋转private Node rightRotate(Node node) &#123; Node x = node.left; //右旋转 node.left = x.right; x.right = node; x.color = node.color; node.color = RED; return x;&#125;//颜色翻转private void flipColors(Node node)&#123; node.color = RED; node.left.color = BLACK; node.right.color = BLACK;&#125;//向红黑树中添加新元素(key,value)public void add(K key,V value)&#123; root = add(root,key,value); root.color = BLACK;//保证最后的根节点为黑色&#125;//向以node为根的红黑树中插入元素(key,value),递归算法//返回插入新节点后红黑树的根private Node add(Node node,K key,V value)&#123; if(node == null)&#123; size ++; return new Node(key,value); //默认插入红色结点 &#125; if(key.compareTo(node.key) &lt; 0) node.left = add(node.left,key,value); else if(key.compareTo(node.key) &gt; 0) node.right = add(node.right,key,value); else node.value = value; //插入结点结束后，进行红黑树性质的维护 //如果右孩子是红色，左孩子不是红色，则进行左旋转 if(isRed(node.right) &amp;&amp; !isRed(node.left)) node = leftRotate(node); //如果左孩子是红色，左孩子的左孩子也是红色，则进行右旋转 if(isRed(node.left) &amp;&amp; isRed(node.left.left)) node = rightRotate(node); //如果node的左孩子和右孩子都是红结点，则进行颜色反转 if(isRed(node.left) &amp;&amp; isRed(node.right)) flipColors(node); return node;&#125; 与其他树的比较如果对于完全随机的数据，普通的二分搜索树BST就已经很好用了。缺点：极端情况下退化成链表（高度不平衡） 对于查询较多的情况，AVL树的性能很好。红黑树牺牲了平衡性（2logn的高度） 红黑树的统计性能更优（综合增删改查所有的操作） 红黑树中删除节点删除节点的操作较为复杂，以后有缘在整理 0.0]]></content>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AVL]]></title>
    <url>%2F2019%2F02%2F27%2FAVL%2F</url>
    <content type="text"><![CDATA[AVL的定义和平衡因子AVL树即平衡二叉树。AVL树的定义首先要求该树是二叉查找树（满足排序规则），并在此基础上增加了每个节点的平衡因子的定义，一个节点的平衡因子是该节点的左子树树高减去右子树树高的值。特点：它是一棵空树或它的左右两个子树的高度差的绝对值不超过1，并且左右两个子树都是一棵平衡二叉树。在AVL树中任何节点的两个子树的高度最大差别为一，所以它也被称为平衡二叉树 平衡因子BF：该节点的左子树的深度减去它的右子树深度。 可见图中的平衡因子有2,那么这棵树不是一个平衡二叉树。 计算节点的高度和平衡因子由上面的定义可以知道，如果想保持二分搜索树的平衡，那么要计算每个节点的高度值，以便计算平衡因子，根据平衡因子对二分搜索树进行一定操作，以便保持平衡。 在之前BST的基础上进行改写(BST见附录),实现新的AVL. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100import java.util.ArrayList;public class BST&lt;K extends Comparable&lt;K&gt;, V&gt; &#123; private class Node&#123; public K key; public V value; public Node left, right; public int heigth; //树的高度 public Node(K key, V value)&#123; this.key = key; this.value = value; left = null; right = null; heigth = 1; //初始化时为1 &#125; &#125; //判断该二叉树是否是一棵二分搜索树 //二分搜索树的一个性质之一：在进行中序遍历时，遍历的结果所有元素是按顺序排列的 public boolean isBST()&#123; ArrayList&lt;K&gt; keys = new ArrayList&lt;&gt;(); inOrder(root,keys); for(int i = 1 ; i &lt; keys.size() ; i ++)&#123; if(keys.get(i - 1).compareTo(keys.get(i)) &gt; 0) return false; &#125; return true; &#125; private void inOrder(Node node,ArrayList&lt;K&gt; keys)&#123; if(node == null) return; inOrder(node.left,keys); keys.add(node.key); inOrder(node.right,keys); &#125; //判断该二叉树是否是一棵平衡二叉树 public boolean isBalanced()&#123; return isBalanced(root); &#125; //判断以Node为根的二叉树是否是一颗平衡二叉树,递归算法 private boolean isBalanced(Node node)&#123; if(node == null) return true; int balanceFactor = getBalanceFactor(node); if(Math.abs(balanceFactor) &gt; 1 ) return false; return isBalanced(node.left) &amp;&amp; isBalanced(node.right); &#125; //获得节点node的高度 private int getHeight(Node node)&#123; if(node == null) return 0; return node.heigth; &#125; //获得节点的平衡因子 private int getBalanceFactor(Node node)&#123; if(node == null) return 0; return getHeight(node.left) - getHeight(node.right); &#125; // 向以node为根的二分搜索树中插入元素(key, value)，递归算法 // 返回插入新节点后二分搜索树的根 //在这个函数中要对height进行维护 private Node add(Node node, K key, V value)&#123; if(node == null)&#123; size ++; return new Node(key, value); &#125; if(key.compareTo(node.key) &lt; 0) node.left = add(node.left, key, value); else if(key.compareTo(node.key) &gt; 0) node.right = add(node.right, key, value); else // key.compareTo(node.key) == 0 node.value = value; // 更新height node.heigth = 1 + Math.max(getHeight(node.left),getHeight(node.right)); //计算平衡因子 int balanceFactor = getBalanceFactor(node); if(Math.abs(balanceFactor) &gt; 1) System.out.println("unbalanced :" + balanceFactor); return node; &#125;&#125; 以上代码对AVL树的基本操作进行了编写,其中省略了部分和BST相同的代码 旋转操作在进行了插入节点的操作后，才有可能破坏当前树的平衡性。这时候需要重新计算其父节点和祖先节点的平衡因子，并判断是否失去了平衡性。因为本身插入节点的add操作就是用递归实现的，所以在递归代码中进行改写。 右旋转如果平衡因子大于一并且是在树的左侧的左侧添加 如图所示，是两种最简单的向左添加破坏了树的平衡性的情况，以绿色表示的节点其整体向左倾斜.左子树的高度大于其右子树的高度并且其左孩子也是如此。 这时候的解决方法是进行右旋转。 在add函数中进行修改:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051//对节点y进行向右旋转操作，返回旋转后的新的根节点x// y x// / \ / \// x T4 向右旋转 (y) z y// / \ - - - - - - - -&gt; / \ / \// z T3 T1 T2 T3 T4// / \// T1 T2private Node rightRotate(Node y)&#123; Node x = y.left; Node T3 = x.right; //向右旋转 x.right = y; y.left = T3; //更新height y.heigth = Math.max(getHeight(y.left),getHeight(y.right)); x.heigth = Math.max(getHeight(x.left),getHeight(x.right)); return x;&#125;private Node add(Node node, K key, V value)&#123; if(node == null)&#123; size ++; return new Node(key, value); &#125; if(key.compareTo(node.key) &lt; 0) node.left = add(node.left, key, value); else if(key.compareTo(node.key) &gt; 0) node.right = add(node.right, key, value); else // key.compareTo(node.key) == 0 node.value = value; // 更新height node.heigth = 1 + Math.max(getHeight(node.left),getHeight(node.right)); //计算平衡因子 int balanceFactor = getBalanceFactor(node); // if(Math.abs(balanceFactor) &gt; 1) // System.out.println("unbalanced :" + balanceFactor); ///平衡维护 if(balanceFactor &gt; 1 &amp;&amp; getBalanceFactor(node.left) &gt;= 0) return rightRotate(node); return node;&#125; 左旋转同理，当插入的元素在不平衡的节点的右侧的右侧时，进行坐旋转操作以满足平衡性。即左旋转的情况是和右旋转完全对称的。 1234567891011121314151617181920212223242526272829303132333435// 对节点y进行向左旋转操作，返回旋转后新的根节点x// y x// / \ / \// T1 x 向左旋转 (y) y z// / \ - - - - - - - -&gt; / \ / \// T2 z T1 T2 T3 T4// / \// T3 T4private Node leftRotate(Node y)&#123; Node x = y.right; Node T2 = x.left; //向左旋转 x.left = y; y.right = T2; //更新height y.heigth = Math.max(getHeight(y.left),getHeight(y.right)) + 1; x.heigth = Math.max(getHeight(x.left),getHeight(x.right)) + 1; return x;&#125;private Node add(Node node, K key, V value)&#123; //...省略部分代码 //平衡维护 if(balanceFactor &gt; 1 &amp;&amp; getBalanceFactor(node.left) &gt;= 0) //左子树比右子树高 return rightRotate(node); if(balanceFactor &lt; -1 &amp;&amp; getBalanceFactor(node.right) &lt;= 0) //右子树比左子树高 return leftRotate(node); return node;&#125; LR和RLLR当插入的元素在不平衡节点的左侧的右侧(即LR)，这时候不能仅仅使用右旋转。 这时候需要先对x进行左旋转，转化为LL的情况 到了这种情况继续对y进行右旋转就可以了。继续完善平衡维护的代码: 12345678910111213141516171819//..省略代码//平衡维护// LLif(balanceFactor &gt; 1 &amp;&amp; getBalanceFactor(node.left) &gt;= 0) //左子树比右子树高 return rightRotate(node);// RRif(balanceFactor &lt; -1 &amp;&amp; getBalanceFactor(node.right) &lt;= 0) //右子树比左子树高 return leftRotate(node);// LRif(balanceFactor &gt; 1 &amp;&amp; getBalanceFactor(node.left) &lt; 0) &#123; node.left = leftRotate(node.left); //转换为LL return rightRotate(node);&#125;//..省略代码 RLRL是LR对称的一种情况，先将其转化为RR的情况，在进行左旋转。就不在赘述了。 12345// RLif(balanceFactor &lt; -1 &amp;&amp; getBalanceFactor(node.right) &gt; 0) &#123; node.right = rightRotate(node.right); //转化为RR return leftRotate(node);&#125; 删除元素删除元素也和添加元素类似，在删除元素后有可能会破坏原树的平衡性，所以在删除元素后要进行向上递归的操作，逐层判断，调整树的平衡。 在平衡二叉树的删除操作中，node.left或者node.right接到了递归操作的新的node节点后，当前的node节点的平衡性有可能已经遭到了破坏，所以不能直接返回当前的node，要先维护node的平衡，不能这么早的return node;,所以先用retNode暂存一下要返回的node.这样就有机会对retNode进行平衡维护，然后再返回。同理,key.compareTo(node.key) == 0时,待删除节点左子树或者右子树为空时返回的node也暂时赋值给retNode.同样待删除节点左右子树均不为空的情况也是用retNode暂存。 这样,retNode中存储的就是删除了节点后要返回的新的node.所以在得到了retNode后要对retNode进行判断，然后维护平衡。这时候的维护平衡操作其实和添加是一样的，先更新height，然后计算平衡因子，最后根据不同情况进行旋转。只是这时候处理的对象是retNode。 要注意在removeMin(Node node)函数操作时也有可能打破平衡，所以也要在这个函数中进行平衡维护。其实可以改写为successor.right = remove(node.right,successor.key);因为remove()函数添加了对平衡性的处理，所以这里的删除最小值操作这样改写是可以的。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091 // 从二分搜索树中删除键为key的节点public V remove(K key)&#123; Node node = getNode(root, key); if(node != null)&#123; root = remove(root, key); return node.value; &#125; return null;&#125;private Node remove(Node node, K key)&#123; if( node == null ) return null; Node retNode; if( key.compareTo(node.key) &lt; 0 )&#123; node.left = remove(node.left , key); retNode = node; &#125; else if(key.compareTo(node.key) &gt; 0 )&#123; node.right = remove(node.right, key); retNode = node; &#125; else&#123; // key.compareTo(node.key) == 0 // 待删除节点左子树为空的情况 if(node.left == null)&#123; Node rightNode = node.right; node.right = null; size --; retNode = rightNode; &#125; // 待删除节点右子树为空的情况 else if(node.right == null)&#123; Node leftNode = node.left; node.left = null; size --; retNode = leftNode; &#125; else &#123;// 待删除节点左右子树均不为空的情况 // 找到比待删除节点大的最小节点, 即待删除节点右子树的最小节点 // 用这个节点顶替待删除节点的位置 Node successor = minimum(node.right); successor.right = remove(node.right, successor.key); successor.left = node.left; node.left = node.right = null; retNode = successor; &#125; &#125; //判断为空的情况，以免访问height会出错 if(retNode == null) return null; //更新height retNode.heigth = 1 + Math.max(getHeight(retNode.left),getHeight(retNode.right)); //计算平衡因子 int balanceFactor = getBalanceFactor(retNode); //平衡维护 // LL if(balanceFactor &gt; 1 &amp;&amp; getBalanceFactor(retNode.left) &gt;= 0) //左子树比右子树高 return rightRotate(retNode); // RR if(balanceFactor &lt; -1 &amp;&amp; getBalanceFactor(retNode.right) &lt;= 0) //右子树比左子树高 return leftRotate(retNode); // LR if(balanceFactor &gt; 1 &amp;&amp; getBalanceFactor(retNode.left) &lt; 0) &#123; retNode.left = leftRotate(retNode.left); //转换为LL return rightRotate(retNode); &#125; // RL if(balanceFactor &lt; -1 &amp;&amp; getBalanceFactor(retNode.right) &gt; 0) &#123; retNode.right = rightRotate(retNode.right); //转化为RR return leftRotate(retNode); &#125; return retNode;&#125; 基于AVL树的set和mapMap123456789public interface Map&lt;K,V&gt; &#123; void add(K key,V value); V remove(K key); //删除key对应的键值对,并返回值 boolean contains(K key); //是否存已在key V get(K key); //获得key对应的value值 void set(K key,V newValue); //更新值 int getSize(); boolean isEmpty();&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142public class AVLMap&lt;K extends Comparable&lt;K&gt;,V&gt; implements Map&lt;K,V&gt; &#123; private AVLTree&lt;K,V&gt; avl; public AVLMap()&#123; avl = new AVLTree&lt;&gt;(); &#125; @Override public int getSize()&#123; return avl.getSize(); &#125; @Override public boolean isEmpty()&#123; return avl.isEmpty(); &#125; @Override public void add(K key,V value)&#123; avl.add(key,value); &#125; @Override public boolean contains(K key)&#123; return avl.contains(key); &#125; @Override public V get(K key)&#123; return avl.get(key); &#125; @Override public void set(K key,V newValue)&#123; avl.set(key,newValue); &#125; @Override public V remove(K key)&#123; return avl.remove(key); &#125;&#125; Set1234567public interface Set&lt;E&gt; &#123; void add(E e); void remove(E e); boolean isEmpty(); boolean containes(E e); int getSize();&#125; 12345678910111213141516171819202122232425262728293031323334public class AVLSet&lt;E extends Comparable&lt;E&gt;&gt; implements Set&lt;E&gt; &#123; private AVLTree&lt;E, Object&gt; avl; public AVLSet()&#123; avl = new AVLTree&lt;&gt;(); &#125; @Override public int getSize()&#123; return avl.getSize(); &#125; @Override public boolean isEmpty()&#123; return avl.isEmpty(); &#125; @Override public boolean containes(E e) &#123; return avl.contains(e); &#125; @Override public void add(E e)&#123; avl.add(e, null); &#125; @Override public void remove(E e)&#123; avl.remove(e); &#125;&#125; 附录BST123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191import java.util.ArrayList;public class BST&lt;K extends Comparable&lt;K&gt;, V&gt; &#123; private class Node&#123; public K key; public V value; public Node left, right; public Node(K key, V value)&#123; this.key = key; this.value = value; left = null; right = null; &#125; &#125; private Node root; private int size; public BST()&#123; root = null; size = 0; &#125; public int getSize()&#123; return size; &#125; public boolean isEmpty()&#123; return size == 0; &#125; // 向二分搜索树中添加新的元素(key, value) public void add(K key, V value)&#123; root = add(root, key, value); &#125; // 向以node为根的二分搜索树中插入元素(key, value)，递归算法 // 返回插入新节点后二分搜索树的根 private Node add(Node node, K key, V value)&#123; if(node == null)&#123; size ++; return new Node(key, value); &#125; if(key.compareTo(node.key) &lt; 0) node.left = add(node.left, key, value); else if(key.compareTo(node.key) &gt; 0) node.right = add(node.right, key, value); else // key.compareTo(node.key) == 0 node.value = value; return node; &#125; // 返回以node为根节点的二分搜索树中，key所在的节点 private Node getNode(Node node, K key)&#123; if(node == null) return null; if(key.equals(node.key)) return node; else if(key.compareTo(node.key) &lt; 0) return getNode(node.left, key); else // if(key.compareTo(node.key) &gt; 0) return getNode(node.right, key); &#125; public boolean contains(K key)&#123; return getNode(root, key) != null; &#125; public V get(K key)&#123; Node node = getNode(root, key); return node == null ? null : node.value; &#125; public void set(K key, V newValue)&#123; Node node = getNode(root, key); if(node == null) throw new IllegalArgumentException(key + " doesn't exist!"); node.value = newValue; &#125; // 返回以node为根的二分搜索树的最小值所在的节点 private Node minimum(Node node)&#123; if(node.left == null) return node; return minimum(node.left); &#125; // 删除掉以node为根的二分搜索树中的最小节点 // 返回删除节点后新的二分搜索树的根 private Node removeMin(Node node)&#123; if(node.left == null)&#123; Node rightNode = node.right; node.right = null; size --; return rightNode; &#125; node.left = removeMin(node.left); return node; &#125; // 从二分搜索树中删除键为key的节点 public V remove(K key)&#123; Node node = getNode(root, key); if(node != null)&#123; root = remove(root, key); return node.value; &#125; return null; &#125; private Node remove(Node node, K key)&#123; if( node == null ) return null; if( key.compareTo(node.key) &lt; 0 )&#123; node.left = remove(node.left , key); return node; &#125; else if(key.compareTo(node.key) &gt; 0 )&#123; node.right = remove(node.right, key); return node; &#125; else&#123; // key.compareTo(node.key) == 0 // 待删除节点左子树为空的情况 if(node.left == null)&#123; Node rightNode = node.right; node.right = null; size --; return rightNode; &#125; // 待删除节点右子树为空的情况 if(node.right == null)&#123; Node leftNode = node.left; node.left = null; size --; return leftNode; &#125; // 待删除节点左右子树均不为空的情况 // 找到比待删除节点大的最小节点, 即待删除节点右子树的最小节点 // 用这个节点顶替待删除节点的位置 Node successor = minimum(node.right); successor.right = removeMin(node.right); successor.left = node.left; node.left = node.right = null; return successor; &#125; &#125; public static void main(String[] args)&#123; System.out.println("sorcerers.stone.txt"); ArrayList&lt;String&gt; words = new ArrayList&lt;&gt;(); if(FileOperation.readFile("sorcerers.stone.txt", words)) &#123; System.out.println("Total words: " + words.size()); BST&lt;String, Integer&gt; map = new BST&lt;&gt;(); for (String word : words) &#123; if (map.contains(word)) map.set(word, map.get(word) + 1); else map.add(word, 1); &#125; System.out.println("Total different words: " + map.getSize()); System.out.println("Frequency of go: " + map.get("go")); System.out.println("Frequency of stone: " + map.get("stone")); &#125; System.out.println(); &#125;&#125;]]></content>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并查集]]></title>
    <url>%2F2019%2F01%2F22%2F%E5%B9%B6%E6%9F%A5%E9%9B%86%2F</url>
    <content type="text"><![CDATA[并查集(Union Find)的概念并查集是由孩子指向父亲的树结构。可以高效的回答连接问题，判断网络中节点间的连接问题。并查集对于一组数据主要支持两个动作: union(p,q) isConnected(p,q) 并查集的简单实现定义接口123456public interface UF &#123; int getSize(); //并查集一共有多少个元素 boolean isConnectted(int p,int q); //id为p何id为q的元素是否相连 void unionElements(int p,int q); //连接id为p和id为q的元素&#125; 在这里不考虑并查集的增删操作。 基本数据表示假设0~9是十个不同的数据，它可以代表实际生活中的任何物体，但是在这里只把它抽象为十个数据。对于每一个元素，并查集存储的是这个元素所属的集合的id.比如图中0~4属于集合0,5~9属于集合1. 将上面的数组称为id,通过这个数组,就能很轻松的回答连接问题，即只要对应的id值相同，那么他们就是一类，也即它们是连接的。那么回答isConnected(p,q) 就是求 find(p) == find(q) Quick Find基于上面的内容，可知find操作的时间复杂度是O(1)的。实现QuickFind的代码: 12345678910111213141516171819202122232425262728293031323334/*QuickFind方法unionElements(p,q)的时间复杂度为0(n)isConnectted(p,q)的时间复杂度为0(1) */public class UnionFind1 implements UF &#123; private int[] id; public UnionFind1(int size)&#123; id = new int[size]; //初始化时，每个元素对应的id都是不同的，将每个id的值设置为i对应的值 for(int i = 0 ; i &lt; size ; i++)&#123; id[i] = i; &#125; &#125; @Override public int getSize()&#123; return id.length; &#125; //查找元素p对应的集合编号 private int find(int p)&#123; if(p &lt; 0 || p &gt;= id.length) throw new IllegalArgumentException("p is outof bound"); return id[p]; &#125; //查看元素p和元素q是否属于一个集合 @Override public boolean isConnectted(int p, int q)&#123; return find(p) == find(q); &#125; 在QuickFind方法下，虽然查找的时间复杂度很低，但是union操作却很耗费时间。比如在上图中，4和5属于不同的集合，如果union(4,5),那么对应的0~4和5~9都应该属于一个集合，这时候他们的id都为0或1,所以要对整个数组进行遍历，将id值对比并且改变为0或1.所以QuickFind下的Union操作的时间复杂度为O(n). 实现Union操作的代码:1234567891011121314151617 //合并元素p和元素q所属的集合 @Override public void unionElements(int p ,int q)&#123; int pID = find(p); int qID = find(q); if(pID == qID) return; for(int i = 0 ; i &lt; id.length; i++)&#123; if(id[i] == pID)&#123; id[i] = qID; &#125; &#125; &#125;&#125; QuickUnionQuickUnion的思路是把每一个元素看成树中的一个节点，并查集中的树结构是孩子指向父亲的树结构。 如果有更复杂的情况，一棵树中的元素要和另一元素进行合并，就让一棵树的根节点指向另一颗树的根节点即可。如下图: 在QuickUnion下的数据表示: 在QuickUnion下，parent[i]表示这个节点指向的父节点是谁。初始化时，parent[i]都是i.在这种情况下，假设要Union4和3,就将parent[4] = 3即可. 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 3 5 6 7 8 9 如果继续union(3,8) 0 1 2 3 4 5 6 7 8 9 0 1 2 8 3 5 6 7 8 9 继续union(9,4),查询4的根节点:4-&gt;3-&gt;8-&gt;8,则4的根节点是8.则让9指向8.当然这里有可能使8指向9,如果数据过多，成为链表就体现不出树的优势了。 0 1 2 3 4 5 6 7 8 9 0 1 2 8 3 5 6 7 8 8 所以Union操作无非就是找到当前要合并的两个元素的根节点，然后让其中一个指向另一个即可。所以Union操作的时间复杂度是O(h),h为树的高度。QucikUnion下的find操作就要比QuickFind下的find操作慢一些，因为这牵扯到查找元素的根,消耗的时间是要比QuickFind多的,查询所需要的时间复杂度为O(h) 实现代码:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/*UnionFind2QuickUnion方法parent数组表示当前元素指向的父元素是谁 */public class UnionFind2 implements UF&#123; private int[] parent; public UnionFind2(int size)&#123; parent = new int[size]; for(int i = 0 ; i &lt; size ; i++)&#123; parent[i] = i ; &#125; &#125; @Override public int getSize()&#123; return parent.length; &#125; //查找过程,查找元素p所对应的元素编号,即父节点,直到最后找到根节点 //O(h)复杂度,h为树的高度 private int find(int p)&#123; if(p&lt; 0 || p &gt;= parent.length)&#123; throw new IllegalArgumentException("p is out of bound."); &#125; while (p != parent[p]) p = parent[p]; return p; &#125; //查看元素p和元素q是否属于一个集合 //O(h)复杂度,h为树的高度 @Override public boolean isConnectted(int p,int q)&#123; return find(p) == find(q); &#125; //合并元素p和元素q所属的集合 //O(h)复杂度,h为树的高度 @Override public void unionElements(int p,int q)&#123; int pRoot = find(p); int qRoot = find(q); if(pRoot == qRoot) return; parent[pRoot] = qRoot; &#125;&#125; 基于size的优化在进行测试时，当数据的size很大，进行的合并或者查找操作较小时，UnionFind2有很大的时间优势，而当size很大，进行合并或者查找的操作也很大时，UnionFind2的时间会增长。其中一个原因是，在使用UnionFind1进行合并操作时，其实就是对一片连续的空间进行循环操作，这种操作在java的jvm中有很好的优化，所以运行速度很快。而UnionFind2中的find操作，是一个不断索引的过程，不是顺次访问一片连续的空间，在不同的地址间进行跳转，所以会慢。另一个原因是，UnionFind2中的各个操作都是O(h)级别的，当合并或者查找操作数较多，在Union操作时，过多的元素被放在了一个集合中，所以得到的这棵树就很大，对应的它的深度就有可能很高，所以后续的isConnected的时候所消耗的时间也会很高。 所以可以在Union时，针对树的特点进行优化。 可以考虑这种情况，在不断执行union操作时，union(0,1),union(0,2)….union(0,9),在最坏的情况下，树的高度就是元素的个数，也就是树退化为了一个链表。这是因为合并的时候没有对树的高度进行判断，其中一个方法就是对当前的树考虑它有多少个子树。 在上图中，如果要进行union(4,9)操作，那么为了减小树的高度，应该让9-&gt;8,得到结果如下图: 即将元素个数少的树的根节点指向元素个数多的树的根节点，这样就是基于size的优化. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667/*UnionFind3对UnionFind2的union进行优化,基于size的优化QuickUnion方法parent数组表示当前元素指向的父元素是谁 */public class UnionFind3 implements UF&#123; private int[] parent; private int[] sz; //sz[i]表示以i为根的集合中元素的个数 public UnionFind3(int size)&#123; parent = new int[size]; sz = new int[size]; for(int i = 0 ; i &lt; size ; i++)&#123; parent[i] = i ; sz[i] = 1; &#125; &#125; @Override public int getSize()&#123; return parent.length; &#125; //查找过程,查找元素p所对应的元素编号,即父节点,直到最后找到根节点 //O(h)复杂度,h为树的高度 private int find(int p)&#123; if(p&lt; 0 || p &gt;= parent.length)&#123; throw new IllegalArgumentException("p is out of bound."); &#125; while (p != parent[p]) p = parent[p]; return p; &#125; //查看元素p和元素q是否属于一个集合 //O(h)复杂度,h为树的高度 @Override public boolean isConnectted(int p,int q)&#123; return find(p) == find(q); &#125; //合并元素p和元素q所属的集合 //O(h)复杂度,h为树的高度 @Override public void unionElements(int p,int q)&#123; int pRoot = find(p); int qRoot = find(q); if(pRoot == qRoot) return; //根据两个元素所在树的元素个数不同判断合并方向 //将元素个数少的集合合并到元素个数多的集合上 if(sz[pRoot] &lt; sz[qRoot])&#123; parent[pRoot] = qRoot; sz[qRoot] += sz[pRoot]; //qRoot为根的集合树变大了，元素数多了sz[pRoot]个 &#125; else&#123; parent[qRoot] = pRoot; sz[pRoot] += sz[qRoot]; &#125; &#125;&#125; 初始时,sz[i] = 1表示每个集合中的元素只有一个,unionElements方法中，在改变根节点的同时，维护sz的大小. 基于rank的优化 0 1 2 3 4 5 6 7 8 9 7 7 7 8 3 7 7 7 8 9 对于上面的并查集，进行union(4,2),如果是基于size的优化方法，则是节点8指向节点7，如下图: 但是树的深度增加了大于2的高度，树的高度高的节点指向了树的高度低的节点。 所以一个更好的优化union方法，应该是让深度低的树的根节点指向深度高的树的根节点。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869/*UnionFind4对UnionFind3的union进行优化,基于rank的优化QuickUnion方法parent数组表示当前元素指向的父元素是谁 */public class UnionFind4 implements UF&#123; private int[] parent; private int[] rank; //rank[i]表示以i为根的集合中树的高度 public UnionFind4(int size)&#123; parent = new int[size]; rank = new int[size]; for(int i = 0 ; i &lt; size ; i++)&#123; parent[i] = i ; rank[i] = 1; &#125; &#125; @Override public int getSize()&#123; return parent.length; &#125; //查找过程,查找元素p所对应的元素编号,即父节点,直到最后找到根节点 //O(h)复杂度,h为树的高度 private int find(int p)&#123; if(p&lt; 0 || p &gt;= parent.length)&#123; throw new IllegalArgumentException("p is out of bound."); &#125; while (p != parent[p]) p = parent[p]; return p; &#125; //查看元素p和元素q是否属于一个集合 //O(h)复杂度,h为树的高度 @Override public boolean isConnectted(int p,int q)&#123; return find(p) == find(q); &#125; //合并元素p和元素q所属的集合 //O(h)复杂度,h为树的高度 @Override public void unionElements(int p,int q)&#123; int pRoot = find(p); int qRoot = find(q); if(pRoot == qRoot) return; //根据两个元素所在树的rank不同判断合并方向 //将rank低的集合合并到rank高多的集合上 if(rank[pRoot] &lt; rank[qRoot])&#123; parent[pRoot] = qRoot; &#125; else if(rank[qRoot] &lt; rank[pRoot])&#123; parent[qRoot] = pRoot; &#125; else&#123; parent[qRoot] = pRoot; rank[pRoot] += 1; &#125; &#125;&#125; 修改的方法只有unionElements方法，合并过程是在rank的基础上进行合并。当rank[pRoot] &lt; rank[qRoot]时，即使parent[pRoot]指向了qRoot,那么本身qRoot的深度就要比pRoot的大，多了这个子树它的深度依然不会改变。同理rank[qRoot] &lt; rank[pRoot]时也一样。只有在两棵树的深度一样时，它们两个指向任意一个都可以，最后让树高度+1即可。 路径压缩 如上图，这三种树结构所表达的并查集的含义相同，但是它们的效率是不同的。在之前的union方法中，不断的让根节点指向另一个树的根节点，会导致让树的高度越来越高。路径压缩则可以让一棵比较高的树压缩为一棵比较矮的树。只要能让树的高度降低，那么对并查集的性能都会有所提升。 路径压缩发生在find操作中。 0 1 2 3 4 0 0 1 2 3 在查找4的根节点时，只关心4的根节点是多少，而不去考虑中间的父节点是谁，所以4在向上遍历的时候，同时执行parent[p] = parent[parent[p]] 从4向上遍历，执行一次操作后,将4的parent指向3的parent2: 0 1 2 3 4 0 0 1 2 2 接着向上遍历,将2的parent指向1的parent0: 0 1 2 3 4 0 0 0 2 2 这时，由深度为5降低为深度为3. 实现代码:1234567891011121314151617181920212223/*UnionFind5对UnionFind4的find进行优化,路径压缩QuickUnion方法parent数组表示当前元素指向的父元素是谁 */public class UnionFind5 implements UF&#123; //其他代码相同 private int find(int p)&#123; if(p&lt; 0 || p &gt;= parent.length)&#123; throw new IllegalArgumentException("p is out of bound."); &#125; while (p != parent[p])&#123; parent[p] = parent[parent[p]]; p = parent[p]; &#125; return p; &#125;//..其他代码相同 在这个过程中没有去管理rank的值，之所以取名为rank而不是h(树的高度)或者其他,是因为rank实际上表示的是一个排名或者序,在进行完路径压缩后,排名和序并没有变化，依然是rank值比较低的节点在下面，rank值比较高的值在上面，只是有可能出现同一层的元素它们的rank值不同，但这并没有什么影响。只是它不再反应节点所代表的高度值。不做rank的维护，既没有必要也会浪费性能。 压缩为高度为2的树最理想化的路径压缩是将每个集合压缩为高度为2的树. 0 1 2 3 4 0 0 0 0 0 实现这种压缩需要使用递归。 12345678910111213//其他部分代码同上private int find(int p)&#123; if(p&lt; 0 || p &gt;= parent.length)&#123; throw new IllegalArgumentException("p is out of bound."); &#125; //如果当前节点不是根节点，则find它的父亲节点所对应的根节点，其实也就是p节点的根节点，然后返回parent[p],也就是整棵树的根节点 //这是从宏观语义角度进行分析 if (p != parent[p]) parent[p] = find(parent[p]); return parent[p];&#125; 这种方式是在查询一个节点的根节点时，将这个节点和这个节点的父亲节点直接指向根节点。 并查集的时间复杂度非严格意义上:o(h) 严格意义上: o(log*n) (iterated logarithm) logn = 0 (n&lt;=1)logn = 1 + log*(logn) (n&gt;1) o(log*n)的时间复杂度比log(n)还要快，近乎于o(1)]]></content>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC学习笔记-四]]></title>
    <url>%2F2018%2F12%2F11%2FSpringMVC%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%9B%9B%2F</url>
    <content type="text"><![CDATA[json数据交互导入处理json的依赖包： @RequestBody作用：@RequestBody注解用于读取http请求的内容(字符串)，通过springmvc提供的HttpMessageConverter接口将读到的内容（json数据）转换为java对象并绑定到Controller方法的参数上。@RequestBody注解实现接收http请求的json数据，将json数据转换为java对象进行绑定 @ResponseBody作用：@ResponseBody注解用于将Controller的方法返回的对象，通过springmvc提供的HttpMessageConverter接口转换为指定格式的数据如：json,xml等，通过Response响应给客户端 本例子应用：@ResponseBody注解实现将Controller方法返回java对象转换为json响应给客户端。 编写程序前端代码:12345678910111213141516&lt;script src="$&#123;pageContext.request.contextPath &#125;/js/jquery-3.2.1.min.js"&gt;&lt;/script&gt;&lt;script &gt;var jsonString = '&#123;"id": 1,"name": "测试商品","price": 99.9,"detail": "测试商品描述","pic": "123456.jpg"&#125;';$(function()&#123; $.ajax(&#123; url : "$&#123;pageContext.request.contextPath &#125;/json.action", data : jsonString, contentType : 'application/json;charset=UTF-8', type : 'post', dataType : 'json', success : function(data)&#123; alert(data.name + " " + data.price); &#125; &#125;);&#125;)&lt;/script&gt; 后台Controller:12345//json @RequestMapping(value = "/json.action" ) public @ResponseBody Items jsonAction(@RequestBody Items items) &#123; return items; &#125; 配置json转换器如果不使用注解驱动&lt;mvc:annotation-driven /&gt;，就需要给处理器适配器配置json转换器，参考之前学习的自定义参数绑定。 在springmvc.xml配置文件中，给处理器适配器加入json转换器：12345678&lt;!--处理器适配器 --&gt; &lt;bean class="org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter"&gt; &lt;property name="messageConverters"&gt; &lt;list&gt; &lt;bean class="org.springframework.http.converter.json.MappingJacksonHttpMessageConverter"&gt;&lt;/bean&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt; 拦截器定义Spring Web MVC 的处理器拦截器类似于Servlet 开发中的过滤器Filter，用于对处理器进行预处理和后处理。实现HandlerInterceptor接口，如下：12345678910111213141516171819202122232425262728public class HandlerInterceptor1 implements HandlerInterceptor &#123; // controller执行后且视图返回后调用此方法 // 这里可得到执行controller时的异常信息 // 这里可记录操作日志 @Override public void afterCompletion(HttpServletRequest arg0, HttpServletResponse arg1, Object arg2, Exception arg3) throws Exception &#123; System.out.println("HandlerInterceptor1....afterCompletion"); &#125; // controller执行后但未返回视图前调用此方法 // 这里可在返回用户前对模型数据进行加工处理，比如这里加入公用信息以便页面显示 @Override public void postHandle(HttpServletRequest arg0, HttpServletResponse arg1, Object arg2, ModelAndView arg3) throws Exception &#123; System.out.println("HandlerInterceptor1....postHandle"); &#125; // Controller执行前调用此方法 // 返回true表示继续执行，返回false中止执行 // 这里可以加入登录校验、权限拦截等 @Override public boolean preHandle(HttpServletRequest arg0, HttpServletResponse arg1, Object arg2) throws Exception &#123; System.out.println("HandlerInterceptor1....preHandle"); // 设置为true，测试使用 return true; &#125;&#125; 上面定义的拦截器再复制一份HandlerInterceptor2，注意新的拦截器修改代码：System.out.println(“HandlerInterceptor2….preHandle”); 拦截器配置在springmvc.xml中配置拦截器；123456789101112131415&lt;!-- 配置拦截器 --&gt;&lt;mvc:interceptors&gt; &lt;mvc:interceptor&gt; &lt;!-- 所有的请求都进入拦截器 --&gt; &lt;mvc:mapping path="/**" /&gt; &lt;!-- 配置具体的拦截器 --&gt; &lt;bean class="my.study.springmvc.Interceptor.HandlerInterceptor1" /&gt; &lt;/mvc:interceptor&gt; &lt;mvc:interceptor&gt; &lt;!-- 所有的请求都进入拦截器 --&gt; &lt;mvc:mapping path="/**" /&gt; &lt;!-- 配置具体的拦截器 --&gt; &lt;bean class="my.study.springmvc.Interceptor.HandlerInterceptor2" /&gt; &lt;/mvc:interceptor&gt;&lt;/mvc:interceptors&gt; 测试正常运行流程控制台打印：HandlerInterceptor1….preHandleHandlerInterceptor2….preHandleHandlerInterceptor2….postCompletionHandlerInterceptor1….postCompletionHandlerInterceptor2….afterCompletionHandlerInterceptor1….afterCompletion 中断流程测试HandlerInterceptor1的preHandler方法返回false，HandlerInterceptor2返回true，运行流程如下： HandlerInterceptor1..preHandle.. 从日志看出第一个拦截器的preHandler方法返回false后第一个拦截器只执行了preHandler方法，其它两个方法没有执行，第二个拦截器的所有方法不执行，且Controller也不执行了。 HandlerInterceptor1的preHandler方法返回true，HandlerInterceptor2返回false，运行流程如下： HandlerInterceptor1..preHandle..HandlerInterceptor2..preHandle..HandlerInterceptor1..afterCompletion.. 从日志看出第二个拦截器的preHandler方法返回false后第一个拦截器的postHandler没有执行，第二个拦截器的postHandler和afterCompletion没有执行，且controller也不执行了。 总结：preHandle按拦截器定义顺序调用postHandler按拦截器定义逆序调用afterCompletion按拦截器定义逆序调用 postHandler在拦截器链内所有拦截器返成功调用afterCompletion只有preHandle返回true才调用 拦截器应用处理流程1、有一个登录页面，需要写一个Controller访问登录页面2、登录页面有一提交表单的动作。需要在Controller中处理。 a)判断用户名密码是否正确（在控制台打印） b)如果正确,向session中写入用户信息（写入用户名username） c)跳转到商品列表3、拦截器。 a)拦截用户请求，判断用户是否登录（登录请求不能拦截） b)如果用户已经登录。放行 c)如果用户未登录，跳转到登录页面。 编写jsp见附录 用户登陆Controller1234567891011121314151617181920212223242526272829303132333435363738394041package my.study.springmvc.controller;import javax.servlet.http.HttpSession;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;@Controller@RequestMapping(value = "/user")public class UserController &#123; /** * 跳转到登录页面 * * @return */ @RequestMapping("toLogin") public String toLogin() &#123; return "login"; &#125; /** * 用户登录 * * @param username * @param password * @param session * @return */ @RequestMapping("login") public String login(String username, String password, HttpSession session) &#123; // 校验用户登录 System.out.println(username); System.out.println(password); // 把用户名放到session中 session.setAttribute("username", username); return "redirect:/item/itemList.action"; &#125;&#125; 编写拦截器12345678910111213141516171819202122232425262728293031package my.study.springmvc.Interceptor;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import javax.servlet.http.HttpSession;import org.springframework.web.servlet.HandlerInterceptor;import org.springframework.web.servlet.ModelAndView;public class LoginHandlerInterceptor implements HandlerInterceptor &#123; //...省略部分代码 @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object arg2) throws Exception &#123; // 从request中获取session HttpSession session = request.getSession(); // 从session中获取username Object username = session.getAttribute("username"); // 判断username是否为null if (username != null) &#123; // 如果不为空则放行 return true; &#125; else &#123; // 如果为空则跳转到登录页面 response.sendRedirect(request.getContextPath() + "/user/toLogin.action"); &#125; return false; &#125;&#125; 配置拦截器123456&lt;mvc:interceptor&gt;&lt;!-- 用户登录配置 --&gt; &lt;mvc:mapping path="/item/**" /&gt; &lt;!-- 配置具体的拦截器 --&gt; &lt;bean class="my.study.springmvc.Interceptor.LoginHandlerInterceptor" /&gt;&lt;/mvc:interceptor&gt; 未登录状态下访问/item/**下的请求都会跳转到登录页面 附录longin.jsp12345678910111213141516171819202122232425&lt;%@ page language="java" contentType="text/html; charset=UTF-8" pageEncoding="UTF-8"%&gt;&lt;!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd"&gt;&lt;html&gt;&lt;head&gt;&lt;meta http-equiv="Content-Type" content="text/html; charset=UTF-8"&gt;&lt;title&gt;Insert title here&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;form action="$&#123;pageContext.request.contextPath &#125;/user/login.action"&gt;&lt;label&gt;用户名：&lt;/label&gt;&lt;br&gt;&lt;input type="text" name="username"&gt;&lt;br&gt;&lt;label&gt;密码：&lt;/label&gt;&lt;br&gt;&lt;input type="password" name="password"&gt;&lt;br&gt;&lt;input type="submit"&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt;]]></content>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC学习笔记-三]]></title>
    <url>%2F2018%2F12%2F05%2FSpringMVC%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%B8%89%2F</url>
    <content type="text"><![CDATA[在SpringMVC学习笔记-二的基础上继续进行开发 高级参数绑定绑定数组需求:在商品列表页面选中多个商品，然后删除。功能要求商品列表页面中的每个商品前有一个checkbok，选中多个商品后点击删除按钮把商品id传递给Controller，根据商品id删除商品信息。前端页面代码进行修改，修改后的页面见附录。Controller代码:12345678//删除多个商品 @RequestMapping(value = "/item/deleteItems.action" ) public ModelAndView deleteItems(Integer[] ids) &#123; ModelAndView mav = new ModelAndView(); mav.setViewName("success"); return mav; &#125; 调试查看接收到值： 如果将数组作为pojo的内部属性名，则直接在POJO中添加该成员变量，名字与前台的name继续保持一致即可.在QueryVo中添加私有成员变量: 修改Controller: 测试结果: 将表单的数据绑定到List需求：实现商品数据的批量修改： 在商品列表页面中可以对商品信息进行修改。 可以批量提交修改后的商品数据。 修改QueryVo.java,添加Items的List属性:12345...private Items items;private Integer[] ids;private List&lt;Items&gt; itemLists;//gets/sets... 在Controller中添加方法:12345678//修改多个商品 @RequestMapping(value = "/item/updatesItems.action" ) public ModelAndView updatesItems(QueryVo vo) &#123; ModelAndView mav = new ModelAndView(); mav.setViewName("success"); return mav; &#125; 修改jsp页面,name属性必须是list属性名+下标+元素属性。:${current} 当前这次迭代的（集合中的）项${status.first} 判断当前项是否为集合中的第一项，返回值为true或false${status.last} 判断当前项是否为集合中的最varStatus属性常用参数总结下：${status.index} 输出行号，从0开始。${status.count} 输出行号，从1开始。${status.后一项，返回值为true或falsebegin、end、step分别表示：起始序号，结束序号，跳跃步伐。 测试: 注意:接收List时，只能放在包装类的属性中，不能直接接收List RequestMapping通过@RequestMapping注解可以定义不同的处理器映射规则。 URL路径映射@RequestMapping(value=”item”)或@RequestMapping(“/item”）value的值是数组，可以将多个url映射到同一个方法如:1@RequestMapping(value = &#123; "itemList", "itemListAll" &#125;) 添加在类上面在class上添加@RequestMapping(url)指定通用请求前缀， 限制此类下的所有方法请求url必须以请求前缀开头 请求方法限定除了可以对url进行设置，还可以限定请求进来的方法 限定GET方法@RequestMapping(method = RequestMethod.GET) 如果通过POST访问则报错：HTTP Status 405 - Request method ‘POST’ not supported 例如：@RequestMapping(value = “itemList”,method = RequestMethod.POST) 限定POST方法@RequestMapping(method = RequestMethod.POST) 如果通过GET访问则报错：HTTP Status 405 - Request method ‘GET’ not supported GET和POST都可以@RequestMapping(method = {RequestMethod.GET,RequestMethod.POST}) Controller方法返回值返回ModelAndViewcontroller方法中定义ModelAndView对象并返回，对象中可添加model数据、指定view。参考前面的代码 返回void在Controller方法形参上可以定义request和response，使用request或response指定响应结果： 1、使用request转发页面，如下：request.getRequestDispatcher(“页面路径”).forward(request, response);request.getRequestDispatcher(“/WEB-INF/jsp/success.jsp”).forward(request, response); 2、可以通过response页面重定向：response.sendRedirect(“url”)response.sendRedirect(“/springmvc-web2/itemEdit.action”); 3、可以通过response指定响应结果，例如响应json数据如下：response.getWriter().print(“{\”abc\”:123}”); 异步请求使用void比较合适 12345678910111213141516171819/** * 返回void测试 * * @param request * @param response * @throws Exception */@RequestMapping("queryItem")public void queryItem(HttpServletRequest request, HttpServletResponse response) throws Exception &#123; // 1 使用request进行转发 // request.getRequestDispatcher("/WEB-INF/jsp/success.jsp").forward(request, // response); // 2 使用response进行重定向到编辑页面 // response.sendRedirect("/springmvc-web2/itemEdit.action"); // 3 使用response直接显示 response.getWriter().print("&#123;\"abc\":123&#125;");&#125; 返回字符串逻辑视图名controller方法返回字符串可以指定逻辑视图名，通过视图解析器解析为物理视图地址。12//指定逻辑视图名，经过视图解析器解析为jsp物理路径：/WEB-INF/jsp/itemList.jspreturn "itemList"; Redirect重定向Contrller方法返回字符串可以重定向到一个url地址如下商品修改提交后重定向到商品编辑页面。1234567891011121314151617/** * 更新商品 * * @param item * @return */@RequestMapping("updateItem")public String updateItemById(Item item) &#123; // 更新商品 this.itemService.updateItemById(item); // 修改商品成功后，重定向到商品编辑页面 // 重定向后浏览器地址栏变更为重定向的地址， // 重定向相当于执行了新的request和response，所以之前的请求参数都会丢失 // 如果要指定请求参数，需要在重定向的url后面添加 ?itemId=1 这样的请求参数 return "redirect:/itemEdit.action?itemId=" + item.getId();&#125; forward转发Controller方法执行后继续执行另一个Controller方法如下商品修改提交后转向到商品修改页面，修改商品的id参数可以带到商品修改方法中。12345678910111213141516171819202122232425/** * 更新商品 * * @param item * @return */@RequestMapping("updateItem")public String updateItemById(Item item) &#123; // 更新商品 this.itemService.updateItemById(item); // 修改商品成功后，重定向到商品编辑页面 // 重定向后浏览器地址栏变更为重定向的地址， // 重定向相当于执行了新的request和response，所以之前的请求参数都会丢失 // 如果要指定请求参数，需要在重定向的url后面添加 ?itemId=1 这样的请求参数 // return "redirect:/itemEdit.action?itemId=" + item.getId(); // 修改商品成功后，继续执行另一个方法 // 使用转发的方式实现。转发后浏览器地址栏还是原来的请求地址， // 转发并没有执行新的request和response，所以之前的请求参数都存在 return "forward:/itemEdit.action";&#125;//结果转发到editItem.action，request可以带过去return "forward: /itemEdit.action"; 异常处理器springmvc在处理请求过程中出现异常信息交由异常处理器进行处理，自定义异常处理器可以实现一个系统的异常处理逻辑。 异常处理思路系统中异常包括两类：预期异常和运行时异常RuntimeException，前者通过捕获异常从而获取异常信息，后者主要通过规范代码开发、测试通过手段减少运行时异常的发生。系统的dao、service、controller出现都通过throws Exception向上抛出，最后由springmvc前端控制器交由异常处理器进行异常处理，如下图: 自定义异常类为了区别不同的异常,通常根据异常类型进行区分，这里我们创建一个自定义系统异常。如果controller、service、dao抛出此类异常说明是系统预期处理的异常信息。1234567891011121314151617181920public class MyException extends Exception &#123; // 异常信息 private String message; public MyException() &#123; super(); &#125; public MyException(String message) &#123; super(); this.message = message; &#125; public String getMessage() &#123; return message; &#125; public void setMessage(String message) &#123; this.message = message; &#125; 自定义异常处理器123456789101112131415161718192021222324252627282930313233343536373839404142434445package my.study.springmvc.exception;import java.io.PrintWriter;import java.io.StringWriter;import java.io.Writer;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import org.springframework.web.servlet.HandlerExceptionResolver;import org.springframework.web.servlet.ModelAndView;public class CustomExceptionResolver implements HandlerExceptionResolver&#123; @Override public ModelAndView resolveException(HttpServletRequest request, HttpServletResponse response, Object obj, Exception e) &#123; // 定义异常信息 String msg; // 判断异常类型 if (e instanceof MyException) &#123; // 如果是自定义异常，读取异常信息 msg = e.getMessage(); &#125; else &#123; // 如果是运行时异常，则取错误堆栈，从堆栈中获取异常信息 Writer out = new StringWriter(); PrintWriter s = new PrintWriter(out); e.printStackTrace(s); msg = out.toString(); &#125; // 把错误信息发给相关人员,邮件,短信等方式 // TODO // 返回错误页面，给用户友好页面显示错误信息 ModelAndView modelAndView = new ModelAndView(); modelAndView.addObject("msg", msg); modelAndView.setViewName("error"); return modelAndView; &#125;&#125; 异常处理器配置在springmvc.xml中：12&lt;!-- SpringMVC异常处理器 --&gt; &lt;bean class="my.study.springmvc.exception.CustomExceptionResolver" /&gt; 错误页面error.jsp:123456789101112&lt;%@ page language="java" contentType="text/html; charset=ISO-8859-1" pageEncoding="ISO-8859-1"%&gt;&lt;!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd"&gt;&lt;html&gt;&lt;head&gt;&lt;meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"&gt;&lt;title&gt;Insert title here&lt;/title&gt;&lt;/head&gt;&lt;body&gt;$&#123;error &#125;&lt;/body&gt;&lt;/html&gt; 异常测试 上传图片配置虚拟目录 加入jar包实现图片上传需要加入的jar包，把两个jar包放到工程的lib文件夹中，如下图： jsp页面修改 配置上传解析器在springmvc.xml中配置文件上传解析器123456&lt;!-- 文件上传,id必须设置为multipartResolver --&gt;&lt;bean id="multipartResolver" class="org.springframework.web.multipart.commons.CommonsMultipartResolver"&gt; &lt;!-- 设置文件上传大小 --&gt; &lt;property name="maxUploadSize" value="5000000" /&gt;&lt;/bean&gt; 图片上传更改代码12345678910111213141516171819//使用包装的pojo @RequestMapping(value = "/updateitemQueryVo.action") public String itemEdit(QueryVo vo,MultipartFile pictureFile) throws Exception &#123; //图片上传 //设置图片名称，不能重复，可以使用uuid String picName = UUID.randomUUID().toString(); //获取文件名 String oriName = pictureFile.getOriginalFilename(); //获取图片后缀名 String extName = oriName.substring(oriName.lastIndexOf(".")); //开始上传 pictureFile.transferTo(new File("C:\\Users\\homxu\\Desktop\\测试\\"+picName+extName)); vo.getItems().setPic(picName+extName); //更新信息 itemservice.updateItemsByPojo(vo.getItems()); return "forward:/itemEdit.action?id="+vo.getItems().getId() ; &#125; 测试可能报错，是因为之前用的日期格式测试不对，在相应代码处改过来即可 附录itemList.jsp123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&lt;%@ page language="java" contentType="text/html; charset=UTF-8" pageEncoding="UTF-8"%&gt;&lt;%@ taglib uri="http://java.sun.com/jsp/jstl/core" prefix="c" %&gt;&lt;%@ taglib uri="http://java.sun.com/jsp/jstl/fmt" prefix="fmt"%&gt;&lt;!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd"&gt;&lt;html&gt;&lt;head&gt;&lt;meta http-equiv="Content-Type" content="text/html; charset=UTF-8"&gt;&lt;title&gt;查询商品列表&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;form action="$&#123;pageContext.request.contextPath &#125;/item/queryitem.action" method="post"&gt;查询条件：&lt;table width="100%" border=1&gt;&lt;tr&gt;&lt;td&gt;&lt;input type="submit" value="查询"/&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/form&gt;商品列表：&lt;form action="$&#123;pageContext.request.contextPath &#125;/item/deleteItems.action" method="post"&gt;&lt;table width="100%" border=1&gt;&lt;tr&gt; &lt;td&gt;选择&lt;/td&gt; &lt;td&gt;商品名称&lt;/td&gt; &lt;td&gt;商品价格&lt;/td&gt; &lt;td&gt;生产日期&lt;/td&gt; &lt;td&gt;商品描述&lt;/td&gt; &lt;td&gt;操作&lt;/td&gt;&lt;/tr&gt;&lt;c:forEach items="$&#123;itemList &#125;" var="item"&gt;&lt;tr&gt; &lt;td&gt;&lt;input type="checkbox" name="ids" value="$&#123;item.id&#125;"/&gt;&lt;/td&gt; &lt;td&gt;$&#123;item.name &#125;&lt;/td&gt; &lt;td&gt;$&#123;item.price &#125;&lt;/td&gt; &lt;td&gt;&lt;fmt:formatDate value="$&#123;item.createtime&#125;" pattern="yyyy-MM-dd HH:mm:ss"/&gt;&lt;/td&gt; &lt;td&gt;$&#123;item.detail &#125;&lt;/td&gt; &lt;td&gt;&lt;a href="$&#123;pageContext.request.contextPath &#125;/itemEdit.action?id=$&#123;item.id&#125;"&gt;修改&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/c:forEach&gt;&lt;/table&gt;&lt;input type="submit" value="删除"/&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt;]]></content>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC学习笔记-二]]></title>
    <url>%2F2018%2F12%2F02%2FSpringMVC%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[参数绑定默认支持的参数类型需求：打开商品编辑页面，展示商品信息 在SpringMVC+Mybatis整合的基础上，进行商品编辑的功能开发。 需求分析:编辑商品信息，首先要显示商品详情需要根据商品id查询商品信息，然后展示到页面。请求的url：/itemEdit.action参数：id（商品id）响应结果：商品编辑页面，展示商品详细信息。 在ItemService.java接口中添加方法:12//通过id查询某个商品public Items selectItemsById(Integer id); 在ItemServiceImpl.java类中实现方法:1234//查询某个商品public Items selectItemsById(Integer id) &#123; return itemsmapper.selectByPrimaryKey(id);&#125; 在Controller中书写方法:123456789101112131415//通过id查询某个商品@RequestMapping(value = "/itemEdit.action")public ModelAndView itemEdit(HttpServletRequest request,HttpServletResponse response ,HttpSession session,Model model) &#123; //获取id String id = request.getParameter("id"); //查询商品 Items item = itemservice.selectItemsById(Integer.parseInt(id)); ModelAndView mav = new ModelAndView(); mav.addObject("item", item); mav.setViewName("editItem"); return mav;&#125; 测试结果: 绑定简单类型当请求的参数名称和处理器形参名称一致时会将请求参数与形参进行绑定。这样，从Request取参数的方法就可以进一步简化。123456789101112//通过id查询某个商品 @RequestMapping(value = "/itemEdit.action") public ModelAndView itemEdit(Integer id,HttpServletRequest request,HttpServletResponse response ,HttpSession session,Model model) &#123; //查询商品 Items item = itemservice.selectItemsById(id); ModelAndView mav = new ModelAndView(); mav.addObject("item", item); mav.setViewName("editItem"); return mav; &#125; @RequestParam当请求的参数名称和处理器形参名称不一致时，使用@RequestParam常用于处理简单类型的绑定。 value：参数名字，即入参的请求参数名字，如value=“itemId”表示请求的参数区中的名字为itemId的参数的值将传入 required：是否必须，默认是true，表示请求中一定要有相应的参数，否则将报错TTP Status 400 - Required Integer parameter ‘XXXX’ is not present如果想设置为可以为空，则将requied设置为false defaultValue：默认值，表示如果请求中没有同名参数时的默认值 123456789101112//通过id查询某个商品 @RequestMapping(value = "/itemEdit.action") public ModelAndView itemEdit(@RequestParam(value="id",required=false,defaultValue="1") Integer value,HttpServletRequest request,HttpServletResponse response ,HttpSession session,Model model) &#123; //查询商品 Items item = itemservice.selectItemsById(value); ModelAndView mav = new ModelAndView(); mav.addObject("item", item); mav.setViewName("editItem"); return mav; &#125; 参数绑定之POJO如果提交的参数很多，或者提交的表单中的内容很多的时候,可以使用简单类型接受数据,也可以使用pojo接收数据。要求：pojo对象中的属性名和表单中input的name属性一致。 ItemService里编写接口方法12//通过pojo更新商品信息 public void updateItemsByPojo(Items item); ItemServiceImpl.java编写方法12345//通过pojo更新商品信息 public void updateItemsByPojo(Items items) &#123; items.setCreatetime(new Date()); itemsmapper.updateByPrimaryKeyWithBLOBs(items); &#125; 在Controller中书写代码:12345678910@RequestMapping(value = "/updateitem.action") public ModelAndView itemEdit(Items items) &#123; //更新信息 itemservice.updateItemsByPojo(items); ModelAndView mav = new ModelAndView(); mav.addObject("item", items); mav.setViewName("success"); return mav; &#125; 但是提交的内容会有乱码 解决提交内容乱码问题post在web.xml中加入过滤器，解决post提交乱码问题：1234567891011121314&lt;!-- 解决post乱码问题 --&gt; &lt;filter&gt; &lt;filter-name&gt;encoding&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;!-- 设置编码参是UTF8 --&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;encoding&lt;/filter-name&gt; &lt;url-pattern&gt;*.action&lt;/url-pattern&gt; &lt;/filter-mapping&gt; 再次修改测试： 参数绑定之包装的POJO使用包装的pojo接收商品信息的查询条件。创建包装对象:123456package my.study.springmvc.pojo;public class QueryVo &#123; private Items items; //get/set...&#125; Controller中的代码:12345678910//使用包装的pojo @RequestMapping(value = "/updateitemQueryVo.action") public ModelAndView itemEdit(QueryVo vo) &#123; //更新信息 itemservice.updateItemsByPojo(vo.getItems()); ModelAndView mav = new ModelAndView(); mav.setViewName("success"); return mav; &#125; 这时候前端页面要修改为其成员变量.属性名的格式:修改form的提交地址后测试，可以看到后台正确接收到值，并且修改成功 自定义参数绑定需求：在商品修改页面可以修改商品的生产日期，并且根据业务需求自定义日期格式。 由于日期数据有很多种格式，springmvc没办法把字符串转换成日期类型。所以需要自定义参数绑定。负责处理这部分的是springMVC的适配器在jsp页面添加显示和修改日期的部分:12345&lt;tr&gt; &lt;td&gt;商品生产日期&lt;/td&gt; &lt;td&gt;&lt;input type="text" name="items.createtime" value="&lt;fmt:formatDate value="$&#123;item.createtime&#125;" pattern="yyyy-MM-dd HH:mm:ss"/&gt;" /&gt;&lt;/td&gt;&lt;/tr&gt; 吧ItemServiceImpl.java中写入时间的代码删除:12345//通过pojo更新商品信息 public void updateItemsByPojo(Items items) &#123;// items.setCreatetime(new Date()); itemsmapper.updateByPrimaryKeyWithBLOBs(items); &#125; 自定义Converter,假如传入的日期为2018:12-04 16_:_34-26：123456789101112131415161718192021//Converter&lt;S, T&gt;//S:source,需要转换的源的类型//T:target,需要转换的目标类型public class DateConverter implements Converter&lt;String, Date&gt; &#123; @Override public Date convert(String source) &#123; try &#123; // 把字符串转换为日期类型 SimpleDateFormat simpleDateFormat = new SimpleDateFormat("yyyy:MM-dd HH_mm-ss"); Date date = simpleDateFormat.parse(source); return date; &#125; catch (ParseException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; // 如果转换异常则返回空 return null; &#125;&#125; 在springmvc.xml中配置Converter:12345678910&lt;mvc:annotation-driven conversion-service="ConversionServiceFactory" /&gt;&lt;!-- 转换器配置 --&gt;&lt;bean id="ConversionServiceFactory" class="org.springframework.format.support.FormattingConversionServiceFactoryBean"&gt; &lt;property name="converters"&gt; &lt;list&gt; &lt;bean class="my.study.springmvc.conversion.DateConverter"/&gt; &lt;/list&gt; &lt;/property&gt;&lt;/bean&gt; 在修改日期处填写日期2016:12-04 16_43-52进行测试: 附录editItem.jsp12345678910111213141516171819202122232425262728293031323334353637383940&lt;%@ page language="java" contentType="text/html; charset=UTF-8" pageEncoding="UTF-8"%&gt;&lt;%@ taglib uri="http://java.sun.com/jsp/jstl/core" prefix="c" %&gt;&lt;%@ taglib uri="http://java.sun.com/jsp/jstl/fmt" prefix="fmt"%&gt;&lt;!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd"&gt;&lt;html&gt;&lt;head&gt;&lt;meta http-equiv="Content-Type" content="text/html; charset=UTF-8"&gt;&lt;title&gt;修改商品信息&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;!-- 上传图片是需要指定属性 enctype="multipart/form-data" --&gt; &lt;!-- &lt;form id="itemForm" action="" method="post" enctype="multipart/form-data"&gt; --&gt; &lt;form id="itemForm" action="$&#123;pageContext.request.contextPath &#125;/updateitem.action" method="post"&gt; &lt;input type="hidden" name="id" value="$&#123;item.id &#125;" /&gt; 修改商品信息： &lt;table width="100%" border=1&gt; &lt;tr&gt; &lt;td&gt;商品名称&lt;/td&gt; &lt;td&gt;&lt;input type="text" name="name" value="$&#123;item.name &#125;" /&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;商品价格&lt;/td&gt; &lt;td&gt;&lt;input type="text" name="price" value="$&#123;item.price &#125;" /&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;商品简介&lt;/td&gt; &lt;td&gt;&lt;textarea rows="3" cols="30" name="detail"&gt;$&#123;item.detail &#125;&lt;/textarea&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td colspan="2" align="center"&gt;&lt;input type="submit" value="提交" /&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/form&gt;&lt;/body&gt;&lt;/html&gt;]]></content>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC+Mybatis整合]]></title>
    <url>%2F2018%2F12%2F01%2FSpringMVC-Mybatis%E6%95%B4%E5%90%88%2F</url>
    <content type="text"><![CDATA[控制层采用springmvc、持久层使用mybatis实现。首先参考Spring与Mybatis整合 所需依赖包 配置数据库和逆向工程数据库表:使用逆向工程生成pojo和dao:这里可能和上一篇文章中User类和Items类的代码不同，需要进行修改 配置文件applicationContext.xml:123456789101112131415161718192021222324252627282930313233343536&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:context="http://www.springframework.org/schema/context" xmlns:p="http://www.springframework.org/schema/p" xmlns:aop="http://www.springframework.org/schema/aop" xmlns:tx="http://www.springframework.org/schema/tx" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.0.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.0.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-4.0.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-4.0.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util-4.0.xsd"&gt; &lt;!-- 加载配置文件 --&gt; &lt;context:property-placeholder location="classpath:db.properties"/&gt; &lt;!-- 配置数据库连接池 --&gt; &lt;bean id="dataSource" class="org.apache.commons.dbcp.BasicDataSource" destroy-method="close"&gt; &lt;property name="driverClassName" value="$&#123;jdbc.driver&#125;" /&gt; &lt;property name="url" value="$&#123;jdbc.url&#125;" /&gt; &lt;property name="username" value="$&#123;jdbc.username&#125;" /&gt; &lt;property name="password" value="$&#123;jdbc.password&#125;" /&gt; &lt;property name="maxActive" value="10" /&gt; &lt;property name="maxIdle" value="5" /&gt; &lt;/bean&gt; &lt;!-- 配置SqlSessionFactory --&gt; &lt;bean id="sqlSessionFactoryBean" class="org.mybatis.spring.SqlSessionFactoryBean"&gt; &lt;!-- 配置数据源 --&gt; &lt;property name="dataSource" ref="dataSource" /&gt; &lt;!-- 配置mybatis核心配置文件 --&gt; &lt;property name="configLocation" value="classpath:sqlmapConfig.xml"/&gt; &lt;/bean&gt; &lt;!-- Mapper动态代理开发 扫描包 --&gt; &lt;bean class="org.mybatis.spring.mapper.MapperScannerConfigurer"&gt; &lt;!-- 配置基本包 --&gt; &lt;property name="basePackage" value="my.study.springmvc.mapper"/&gt; &lt;/bean&gt;&lt;/beans&gt; db.properties:1234jdbc.driver=com.mysql.jdbc.Driverjdbc.url=jdbc:mysql://localhost:3306/springmvcjdbc.username=rootjdbc.password=12345 log4j.properties:123456# Global logging configurationlog4j.rootLogger=DEBUG, stdout# Console output...log4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=%5p [%t] - %m%n sqlmapConfit.xml:1234567891011121314&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;!DOCTYPE configurationPUBLIC "-//mybatis.org//DTD Config 3.0//EN""http://mybatis.org/dtd/mybatis-3-config.dtd"&gt;&lt;configuration&gt; &lt;!-- 设置别名 --&gt; &lt;typeAliases&gt; &lt;!-- 指定扫描包，会把保内所有的类都设置别名， 别名的名就是类名，大小写不敏感 --&gt; &lt;package name="my.study.springmvc.mapper"/&gt; &lt;/typeAliases&gt;&lt;/configuration&gt; 在web.xml中配置监听器，去读取配置：123456789&lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:applicationContext.xml&lt;/param-value&gt; &lt;/context-param&gt; &lt;!-- Spring监听器 --&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt; 配置至此，如果开启tomcat,那么就可以连接数据库了 配置事务在applicationContext.xml中配置注解事务,并开启注解:123456&lt;!-- 配置事务 --&gt;&lt;bean class="org.springframework.jdbc.datasource.DataSourceTransactionManager"&gt; &lt;property name="dataSource" ref="dataSource"/&gt;&lt;/bean&gt;&lt;!-- 开启注解 --&gt; &lt;tx:annotation-driven transaction-manager="transactionManager"/&gt; 从mysql中读取信息并显示书写service层代码:ItemService.java:123456789package my.study.springmvc.service;import java.util.List;import my.study.springmvc.pojo.Items;public interface ItemService &#123; public List&lt;Items&gt; selectItemsList();&#125; ItemServiceImpl.java:1234567891011121314151617181920212223242526package my.study.springmvc.service;import java.util.List;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import my.study.springmvc.dao.ItemsMapper;import my.study.springmvc.pojo.Items;/** * 查询商品信息 * @author homxu * */@Servicepublic class ItemServiceImpl implements ItemService &#123; @Autowired private ItemsMapper itemsmapper; public List&lt;Items&gt; selectItemsList()&#123; return itemsmapper.selectByExample(null); &#125;&#125; 将service代码注入到controller： 12345678910111213141516171819202122232425262728package my.study.springmvc.controller;import java.util.List;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.servlet.ModelAndView;import my.study.springmvc.pojo.Items;import my.study.springmvc.service.ItemService;@Controllerpublic class ItemController &#123; @Autowired private ItemService itemservice; @RequestMapping(value = "/item/itemlist.action") public ModelAndView itemList()&#123; //从mysql中查询 List&lt;Items&gt; items = itemservice.selectItemsList(); ModelAndView mav = new ModelAndView(); mav.addObject("itemList",items); mav.setViewName("itemList"); return mav; &#125; &#125; 其中@Autowired 注释，它可以对类成员变量、方法及构造函数进行标注，完成自动装配的工作。 通过 @Autowired的使用来消除 set ，get方法。关于此注释：https://www.cnblogs.com/caoyc/p/5626365.html 附录sql12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061/*Navicat MySQL Data TransferSource Server : localhost_3306Source Server Version : 50611Source Host : localhost:3306Source Database : springmvcTarget Server Type : MYSQLTarget Server Version : 50611File Encoding : 65001Date: 2016-05-09 19:45:13*/SET FOREIGN_KEY_CHECKS=0;-- ------------------------------ Table structure for items-- ----------------------------DROP TABLE IF EXISTS `items`;CREATE TABLE `items` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(32) NOT NULL COMMENT '商品名称', `price` float(10,1) NOT NULL COMMENT '商品定价', `detail` text COMMENT '商品描述', `pic` varchar(64) DEFAULT NULL COMMENT '商品图片', `createtime` datetime NOT NULL COMMENT '生产日期', PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8;-- ------------------------------ Records of items-- ----------------------------INSERT INTO `items` VALUES ('1', '台式机', '13000.0', '该电脑质量非常好！！！！', null, '2016-02-03 13:22:53');INSERT INTO `items` VALUES ('2', '笔记本', '16000.0', '笔记本性能好，质量好！！！！！', null, '2015-02-09 13:22:57');INSERT INTO `items` VALUES ('3', '背包', '200.0', '名牌背包，容量大质量好！！！！', null, '2015-02-06 13:23:02');-- ------------------------------ Table structure for user-- ----------------------------DROP TABLE IF EXISTS `user`;CREATE TABLE `user` ( `id` int(11) NOT NULL AUTO_INCREMENT, `username` varchar(32) NOT NULL COMMENT '用户名称', `birthday` date DEFAULT NULL COMMENT '生日', `sex` char(1) DEFAULT NULL COMMENT '性别', `address` varchar(256) DEFAULT NULL COMMENT '地址', PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=27 DEFAULT CHARSET=utf8;-- ------------------------------ Records of user-- ----------------------------INSERT INTO `user` VALUES ('1', '王五', null, '2', null);INSERT INTO `user` VALUES ('10', '张三', '2014-07-10', '1', '北京市');INSERT INTO `user` VALUES ('16', '张小明', null, '1', '河南郑州');INSERT INTO `user` VALUES ('22', '陈小明', null, '1', '河南郑州');INSERT INTO `user` VALUES ('24', '张三丰', null, '1', '河南郑州');INSERT INTO `user` VALUES ('25', '陈小明', null, '1', '河南郑州');INSERT INTO `user` VALUES ('26', '王五', null, null, null); dao逆向工程生成的dao：ItemsMapper.java:12345678910111213141516171819202122232425262728293031323334353637package my.study.springmvc.dao;import my.study.springmvc.pojo.Items;import my.study.springmvc.pojo.ItemsExample;import java.util.List;import org.apache.ibatis.annotations.Param;public interface ItemsMapper &#123; int countByExample(ItemsExample example); int deleteByExample(ItemsExample example); int deleteByPrimaryKey(Integer id); int insert(Items record); int insertSelective(Items record); List&lt;Items&gt; selectByExampleWithBLOBs(ItemsExample example); List&lt;Items&gt; selectByExample(ItemsExample example); Items selectByPrimaryKey(Integer id); int updateByExampleSelective(@Param("record") Items record, @Param("example") ItemsExample example); int updateByExampleWithBLOBs(@Param("record") Items record, @Param("example") ItemsExample example); int updateByExample(@Param("record") Items record, @Param("example") ItemsExample example); int updateByPrimaryKeySelective(Items record); int updateByPrimaryKeyWithBLOBs(Items record); int updateByPrimaryKey(Items record);&#125; UserMapper.java:123456789101112131415161718192021222324252627282930package my.study.springmvc.dao;import my.study.springmvc.pojo.User;import my.study.springmvc.pojo.UserExample;import java.util.List;import org.apache.ibatis.annotations.Param;public interface UserMapper &#123; int countByExample(UserExample example); int deleteByExample(UserExample example); int deleteByPrimaryKey(Integer id); int insert(User record); int insertSelective(User record); List&lt;User&gt; selectByExample(UserExample example); User selectByPrimaryKey(Integer id); int updateByExampleSelective(@Param("record") User record, @Param("example") UserExample example); int updateByExample(@Param("record") User record, @Param("example") UserExample example); int updateByPrimaryKeySelective(User record); int updateByPrimaryKey(User record);&#125; ItemsMapper.xml:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;!DOCTYPE mapper PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN" "http://mybatis.org/dtd/mybatis-3-mapper.dtd" &gt;&lt;mapper namespace="my.study.springmvc.dao.ItemsMapper" &gt; &lt;resultMap id="BaseResultMap" type="my.study.springmvc.pojo.Items" &gt; &lt;id column="id" property="id" jdbcType="INTEGER" /&gt; &lt;result column="name" property="name" jdbcType="VARCHAR" /&gt; &lt;result column="price" property="price" jdbcType="REAL" /&gt; &lt;result column="pic" property="pic" jdbcType="VARCHAR" /&gt; &lt;result column="createtime" property="createtime" jdbcType="TIMESTAMP" /&gt; &lt;/resultMap&gt; &lt;resultMap id="ResultMapWithBLOBs" type="my.study.springmvc.pojo.Items" extends="BaseResultMap" &gt; &lt;result column="detail" property="detail" jdbcType="LONGVARCHAR" /&gt; &lt;/resultMap&gt; &lt;sql id="Example_Where_Clause" &gt; &lt;where &gt; &lt;foreach collection="oredCriteria" item="criteria" separator="or" &gt; &lt;if test="criteria.valid" &gt; &lt;trim prefix="(" suffix=")" prefixOverrides="and" &gt; &lt;foreach collection="criteria.criteria" item="criterion" &gt; &lt;choose &gt; &lt;when test="criterion.noValue" &gt; and $&#123;criterion.condition&#125; &lt;/when&gt; &lt;when test="criterion.singleValue" &gt; and $&#123;criterion.condition&#125; #&#123;criterion.value&#125; &lt;/when&gt; &lt;when test="criterion.betweenValue" &gt; and $&#123;criterion.condition&#125; #&#123;criterion.value&#125; and #&#123;criterion.secondValue&#125; &lt;/when&gt; &lt;when test="criterion.listValue" &gt; and $&#123;criterion.condition&#125; &lt;foreach collection="criterion.value" item="listItem" open="(" close=")" separator="," &gt; #&#123;listItem&#125; &lt;/foreach&gt; &lt;/when&gt; &lt;/choose&gt; &lt;/foreach&gt; &lt;/trim&gt; &lt;/if&gt; &lt;/foreach&gt; &lt;/where&gt; &lt;/sql&gt; &lt;sql id="Update_By_Example_Where_Clause" &gt; &lt;where &gt; &lt;foreach collection="example.oredCriteria" item="criteria" separator="or" &gt; &lt;if test="criteria.valid" &gt; &lt;trim prefix="(" suffix=")" prefixOverrides="and" &gt; &lt;foreach collection="criteria.criteria" item="criterion" &gt; &lt;choose &gt; &lt;when test="criterion.noValue" &gt; and $&#123;criterion.condition&#125; &lt;/when&gt; &lt;when test="criterion.singleValue" &gt; and $&#123;criterion.condition&#125; #&#123;criterion.value&#125; &lt;/when&gt; &lt;when test="criterion.betweenValue" &gt; and $&#123;criterion.condition&#125; #&#123;criterion.value&#125; and #&#123;criterion.secondValue&#125; &lt;/when&gt; &lt;when test="criterion.listValue" &gt; and $&#123;criterion.condition&#125; &lt;foreach collection="criterion.value" item="listItem" open="(" close=")" separator="," &gt; #&#123;listItem&#125; &lt;/foreach&gt; &lt;/when&gt; &lt;/choose&gt; &lt;/foreach&gt; &lt;/trim&gt; &lt;/if&gt; &lt;/foreach&gt; &lt;/where&gt; &lt;/sql&gt; &lt;sql id="Base_Column_List" &gt; id, name, price, pic, createtime &lt;/sql&gt; &lt;sql id="Blob_Column_List" &gt; detail &lt;/sql&gt; &lt;select id="selectByExampleWithBLOBs" resultMap="ResultMapWithBLOBs" parameterType="my.study.springmvc.pojo.ItemsExample" &gt; select &lt;if test="distinct" &gt; distinct &lt;/if&gt; &lt;include refid="Base_Column_List" /&gt; , &lt;include refid="Blob_Column_List" /&gt; from items &lt;if test="_parameter != null" &gt; &lt;include refid="Example_Where_Clause" /&gt; &lt;/if&gt; &lt;if test="orderByClause != null" &gt; order by $&#123;orderByClause&#125; &lt;/if&gt; &lt;/select&gt; &lt;select id="selectByExample" resultMap="BaseResultMap" parameterType="my.study.springmvc.pojo.ItemsExample" &gt; select &lt;if test="distinct" &gt; distinct &lt;/if&gt; &lt;include refid="Base_Column_List" /&gt; from items &lt;if test="_parameter != null" &gt; &lt;include refid="Example_Where_Clause" /&gt; &lt;/if&gt; &lt;if test="orderByClause != null" &gt; order by $&#123;orderByClause&#125; &lt;/if&gt; &lt;/select&gt; &lt;select id="selectByPrimaryKey" resultMap="ResultMapWithBLOBs" parameterType="java.lang.Integer" &gt; select &lt;include refid="Base_Column_List" /&gt; , &lt;include refid="Blob_Column_List" /&gt; from items where id = #&#123;id,jdbcType=INTEGER&#125; &lt;/select&gt; &lt;delete id="deleteByPrimaryKey" parameterType="java.lang.Integer" &gt; delete from items where id = #&#123;id,jdbcType=INTEGER&#125; &lt;/delete&gt; &lt;delete id="deleteByExample" parameterType="my.study.springmvc.pojo.ItemsExample" &gt; delete from items &lt;if test="_parameter != null" &gt; &lt;include refid="Example_Where_Clause" /&gt; &lt;/if&gt; &lt;/delete&gt; &lt;insert id="insert" parameterType="my.study.springmvc.pojo.Items" &gt; insert into items (id, name, price, pic, createtime, detail ) values (#&#123;id,jdbcType=INTEGER&#125;, #&#123;name,jdbcType=VARCHAR&#125;, #&#123;price,jdbcType=REAL&#125;, #&#123;pic,jdbcType=VARCHAR&#125;, #&#123;createtime,jdbcType=TIMESTAMP&#125;, #&#123;detail,jdbcType=LONGVARCHAR&#125; ) &lt;/insert&gt; &lt;insert id="insertSelective" parameterType="my.study.springmvc.pojo.Items" &gt; insert into items &lt;trim prefix="(" suffix=")" suffixOverrides="," &gt; &lt;if test="id != null" &gt; id, &lt;/if&gt; &lt;if test="name != null" &gt; name, &lt;/if&gt; &lt;if test="price != null" &gt; price, &lt;/if&gt; &lt;if test="pic != null" &gt; pic, &lt;/if&gt; &lt;if test="createtime != null" &gt; createtime, &lt;/if&gt; &lt;if test="detail != null" &gt; detail, &lt;/if&gt; &lt;/trim&gt; &lt;trim prefix="values (" suffix=")" suffixOverrides="," &gt; &lt;if test="id != null" &gt; #&#123;id,jdbcType=INTEGER&#125;, &lt;/if&gt; &lt;if test="name != null" &gt; #&#123;name,jdbcType=VARCHAR&#125;, &lt;/if&gt; &lt;if test="price != null" &gt; #&#123;price,jdbcType=REAL&#125;, &lt;/if&gt; &lt;if test="pic != null" &gt; #&#123;pic,jdbcType=VARCHAR&#125;, &lt;/if&gt; &lt;if test="createtime != null" &gt; #&#123;createtime,jdbcType=TIMESTAMP&#125;, &lt;/if&gt; &lt;if test="detail != null" &gt; #&#123;detail,jdbcType=LONGVARCHAR&#125;, &lt;/if&gt; &lt;/trim&gt; &lt;/insert&gt; &lt;select id="countByExample" parameterType="my.study.springmvc.pojo.ItemsExample" resultType="java.lang.Integer" &gt; select count(*) from items &lt;if test="_parameter != null" &gt; &lt;include refid="Example_Where_Clause" /&gt; &lt;/if&gt; &lt;/select&gt; &lt;update id="updateByExampleSelective" parameterType="map" &gt; update items &lt;set &gt; &lt;if test="record.id != null" &gt; id = #&#123;record.id,jdbcType=INTEGER&#125;, &lt;/if&gt; &lt;if test="record.name != null" &gt; name = #&#123;record.name,jdbcType=VARCHAR&#125;, &lt;/if&gt; &lt;if test="record.price != null" &gt; price = #&#123;record.price,jdbcType=REAL&#125;, &lt;/if&gt; &lt;if test="record.pic != null" &gt; pic = #&#123;record.pic,jdbcType=VARCHAR&#125;, &lt;/if&gt; &lt;if test="record.createtime != null" &gt; createtime = #&#123;record.createtime,jdbcType=TIMESTAMP&#125;, &lt;/if&gt; &lt;if test="record.detail != null" &gt; detail = #&#123;record.detail,jdbcType=LONGVARCHAR&#125;, &lt;/if&gt; &lt;/set&gt; &lt;if test="_parameter != null" &gt; &lt;include refid="Update_By_Example_Where_Clause" /&gt; &lt;/if&gt; &lt;/update&gt; &lt;update id="updateByExampleWithBLOBs" parameterType="map" &gt; update items set id = #&#123;record.id,jdbcType=INTEGER&#125;, name = #&#123;record.name,jdbcType=VARCHAR&#125;, price = #&#123;record.price,jdbcType=REAL&#125;, pic = #&#123;record.pic,jdbcType=VARCHAR&#125;, createtime = #&#123;record.createtime,jdbcType=TIMESTAMP&#125;, detail = #&#123;record.detail,jdbcType=LONGVARCHAR&#125; &lt;if test="_parameter != null" &gt; &lt;include refid="Update_By_Example_Where_Clause" /&gt; &lt;/if&gt; &lt;/update&gt; &lt;update id="updateByExample" parameterType="map" &gt; update items set id = #&#123;record.id,jdbcType=INTEGER&#125;, name = #&#123;record.name,jdbcType=VARCHAR&#125;, price = #&#123;record.price,jdbcType=REAL&#125;, pic = #&#123;record.pic,jdbcType=VARCHAR&#125;, createtime = #&#123;record.createtime,jdbcType=TIMESTAMP&#125; &lt;if test="_parameter != null" &gt; &lt;include refid="Update_By_Example_Where_Clause" /&gt; &lt;/if&gt; &lt;/update&gt; &lt;update id="updateByPrimaryKeySelective" parameterType="my.study.springmvc.pojo.Items" &gt; update items &lt;set &gt; &lt;if test="name != null" &gt; name = #&#123;name,jdbcType=VARCHAR&#125;, &lt;/if&gt; &lt;if test="price != null" &gt; price = #&#123;price,jdbcType=REAL&#125;, &lt;/if&gt; &lt;if test="pic != null" &gt; pic = #&#123;pic,jdbcType=VARCHAR&#125;, &lt;/if&gt; &lt;if test="createtime != null" &gt; createtime = #&#123;createtime,jdbcType=TIMESTAMP&#125;, &lt;/if&gt; &lt;if test="detail != null" &gt; detail = #&#123;detail,jdbcType=LONGVARCHAR&#125;, &lt;/if&gt; &lt;/set&gt; where id = #&#123;id,jdbcType=INTEGER&#125; &lt;/update&gt; &lt;update id="updateByPrimaryKeyWithBLOBs" parameterType="my.study.springmvc.pojo.Items" &gt; update items set name = #&#123;name,jdbcType=VARCHAR&#125;, price = #&#123;price,jdbcType=REAL&#125;, pic = #&#123;pic,jdbcType=VARCHAR&#125;, createtime = #&#123;createtime,jdbcType=TIMESTAMP&#125;, detail = #&#123;detail,jdbcType=LONGVARCHAR&#125; where id = #&#123;id,jdbcType=INTEGER&#125; &lt;/update&gt; &lt;update id="updateByPrimaryKey" parameterType="my.study.springmvc.pojo.Items" &gt; update items set name = #&#123;name,jdbcType=VARCHAR&#125;, price = #&#123;price,jdbcType=REAL&#125;, pic = #&#123;pic,jdbcType=VARCHAR&#125;, createtime = #&#123;createtime,jdbcType=TIMESTAMP&#125; where id = #&#123;id,jdbcType=INTEGER&#125; &lt;/update&gt;&lt;/mapper&gt; UserMapper.xml:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;!DOCTYPE mapper PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN" "http://mybatis.org/dtd/mybatis-3-mapper.dtd" &gt;&lt;mapper namespace="my.study.springmvc.dao.UserMapper" &gt; &lt;resultMap id="BaseResultMap" type="my.study.springmvc.pojo.User" &gt; &lt;id column="id" property="id" jdbcType="INTEGER" /&gt; &lt;result column="username" property="username" jdbcType="VARCHAR" /&gt; &lt;result column="birthday" property="birthday" jdbcType="DATE" /&gt; &lt;result column="sex" property="sex" jdbcType="CHAR" /&gt; &lt;result column="address" property="address" jdbcType="VARCHAR" /&gt; &lt;/resultMap&gt; &lt;sql id="Example_Where_Clause" &gt; &lt;where &gt; &lt;foreach collection="oredCriteria" item="criteria" separator="or" &gt; &lt;if test="criteria.valid" &gt; &lt;trim prefix="(" suffix=")" prefixOverrides="and" &gt; &lt;foreach collection="criteria.criteria" item="criterion" &gt; &lt;choose &gt; &lt;when test="criterion.noValue" &gt; and $&#123;criterion.condition&#125; &lt;/when&gt; &lt;when test="criterion.singleValue" &gt; and $&#123;criterion.condition&#125; #&#123;criterion.value&#125; &lt;/when&gt; &lt;when test="criterion.betweenValue" &gt; and $&#123;criterion.condition&#125; #&#123;criterion.value&#125; and #&#123;criterion.secondValue&#125; &lt;/when&gt; &lt;when test="criterion.listValue" &gt; and $&#123;criterion.condition&#125; &lt;foreach collection="criterion.value" item="listItem" open="(" close=")" separator="," &gt; #&#123;listItem&#125; &lt;/foreach&gt; &lt;/when&gt; &lt;/choose&gt; &lt;/foreach&gt; &lt;/trim&gt; &lt;/if&gt; &lt;/foreach&gt; &lt;/where&gt; &lt;/sql&gt; &lt;sql id="Update_By_Example_Where_Clause" &gt; &lt;where &gt; &lt;foreach collection="example.oredCriteria" item="criteria" separator="or" &gt; &lt;if test="criteria.valid" &gt; &lt;trim prefix="(" suffix=")" prefixOverrides="and" &gt; &lt;foreach collection="criteria.criteria" item="criterion" &gt; &lt;choose &gt; &lt;when test="criterion.noValue" &gt; and $&#123;criterion.condition&#125; &lt;/when&gt; &lt;when test="criterion.singleValue" &gt; and $&#123;criterion.condition&#125; #&#123;criterion.value&#125; &lt;/when&gt; &lt;when test="criterion.betweenValue" &gt; and $&#123;criterion.condition&#125; #&#123;criterion.value&#125; and #&#123;criterion.secondValue&#125; &lt;/when&gt; &lt;when test="criterion.listValue" &gt; and $&#123;criterion.condition&#125; &lt;foreach collection="criterion.value" item="listItem" open="(" close=")" separator="," &gt; #&#123;listItem&#125; &lt;/foreach&gt; &lt;/when&gt; &lt;/choose&gt; &lt;/foreach&gt; &lt;/trim&gt; &lt;/if&gt; &lt;/foreach&gt; &lt;/where&gt; &lt;/sql&gt; &lt;sql id="Base_Column_List" &gt; id, username, birthday, sex, address &lt;/sql&gt; &lt;select id="selectByExample" resultMap="BaseResultMap" parameterType="my.study.springmvc.pojo.UserExample" &gt; select &lt;if test="distinct" &gt; distinct &lt;/if&gt; &lt;include refid="Base_Column_List" /&gt; from user &lt;if test="_parameter != null" &gt; &lt;include refid="Example_Where_Clause" /&gt; &lt;/if&gt; &lt;if test="orderByClause != null" &gt; order by $&#123;orderByClause&#125; &lt;/if&gt; &lt;/select&gt; &lt;select id="selectByPrimaryKey" resultMap="BaseResultMap" parameterType="java.lang.Integer" &gt; select &lt;include refid="Base_Column_List" /&gt; from user where id = #&#123;id,jdbcType=INTEGER&#125; &lt;/select&gt; &lt;delete id="deleteByPrimaryKey" parameterType="java.lang.Integer" &gt; delete from user where id = #&#123;id,jdbcType=INTEGER&#125; &lt;/delete&gt; &lt;delete id="deleteByExample" parameterType="my.study.springmvc.pojo.UserExample" &gt; delete from user &lt;if test="_parameter != null" &gt; &lt;include refid="Example_Where_Clause" /&gt; &lt;/if&gt; &lt;/delete&gt; &lt;insert id="insert" parameterType="my.study.springmvc.pojo.User" &gt; insert into user (id, username, birthday, sex, address) values (#&#123;id,jdbcType=INTEGER&#125;, #&#123;username,jdbcType=VARCHAR&#125;, #&#123;birthday,jdbcType=DATE&#125;, #&#123;sex,jdbcType=CHAR&#125;, #&#123;address,jdbcType=VARCHAR&#125;) &lt;/insert&gt; &lt;insert id="insertSelective" parameterType="my.study.springmvc.pojo.User" &gt; insert into user &lt;trim prefix="(" suffix=")" suffixOverrides="," &gt; &lt;if test="id != null" &gt; id, &lt;/if&gt; &lt;if test="username != null" &gt; username, &lt;/if&gt; &lt;if test="birthday != null" &gt; birthday, &lt;/if&gt; &lt;if test="sex != null" &gt; sex, &lt;/if&gt; &lt;if test="address != null" &gt; address, &lt;/if&gt; &lt;/trim&gt; &lt;trim prefix="values (" suffix=")" suffixOverrides="," &gt; &lt;if test="id != null" &gt; #&#123;id,jdbcType=INTEGER&#125;, &lt;/if&gt; &lt;if test="username != null" &gt; #&#123;username,jdbcType=VARCHAR&#125;, &lt;/if&gt; &lt;if test="birthday != null" &gt; #&#123;birthday,jdbcType=DATE&#125;, &lt;/if&gt; &lt;if test="sex != null" &gt; #&#123;sex,jdbcType=CHAR&#125;, &lt;/if&gt; &lt;if test="address != null" &gt; #&#123;address,jdbcType=VARCHAR&#125;, &lt;/if&gt; &lt;/trim&gt; &lt;/insert&gt; &lt;select id="countByExample" parameterType="my.study.springmvc.pojo.UserExample" resultType="java.lang.Integer" &gt; select count(*) from user &lt;if test="_parameter != null" &gt; &lt;include refid="Example_Where_Clause" /&gt; &lt;/if&gt; &lt;/select&gt; &lt;update id="updateByExampleSelective" parameterType="map" &gt; update user &lt;set &gt; &lt;if test="record.id != null" &gt; id = #&#123;record.id,jdbcType=INTEGER&#125;, &lt;/if&gt; &lt;if test="record.username != null" &gt; username = #&#123;record.username,jdbcType=VARCHAR&#125;, &lt;/if&gt; &lt;if test="record.birthday != null" &gt; birthday = #&#123;record.birthday,jdbcType=DATE&#125;, &lt;/if&gt; &lt;if test="record.sex != null" &gt; sex = #&#123;record.sex,jdbcType=CHAR&#125;, &lt;/if&gt; &lt;if test="record.address != null" &gt; address = #&#123;record.address,jdbcType=VARCHAR&#125;, &lt;/if&gt; &lt;/set&gt; &lt;if test="_parameter != null" &gt; &lt;include refid="Update_By_Example_Where_Clause" /&gt; &lt;/if&gt; &lt;/update&gt; &lt;update id="updateByExample" parameterType="map" &gt; update user set id = #&#123;record.id,jdbcType=INTEGER&#125;, username = #&#123;record.username,jdbcType=VARCHAR&#125;, birthday = #&#123;record.birthday,jdbcType=DATE&#125;, sex = #&#123;record.sex,jdbcType=CHAR&#125;, address = #&#123;record.address,jdbcType=VARCHAR&#125; &lt;if test="_parameter != null" &gt; &lt;include refid="Update_By_Example_Where_Clause" /&gt; &lt;/if&gt; &lt;/update&gt; &lt;update id="updateByPrimaryKeySelective" parameterType="my.study.springmvc.pojo.User" &gt; update user &lt;set &gt; &lt;if test="username != null" &gt; username = #&#123;username,jdbcType=VARCHAR&#125;, &lt;/if&gt; &lt;if test="birthday != null" &gt; birthday = #&#123;birthday,jdbcType=DATE&#125;, &lt;/if&gt; &lt;if test="sex != null" &gt; sex = #&#123;sex,jdbcType=CHAR&#125;, &lt;/if&gt; &lt;if test="address != null" &gt; address = #&#123;address,jdbcType=VARCHAR&#125;, &lt;/if&gt; &lt;/set&gt; where id = #&#123;id,jdbcType=INTEGER&#125; &lt;/update&gt; &lt;update id="updateByPrimaryKey" parameterType="my.study.springmvc.pojo.User" &gt; update user set username = #&#123;username,jdbcType=VARCHAR&#125;, birthday = #&#123;birthday,jdbcType=DATE&#125;, sex = #&#123;sex,jdbcType=CHAR&#125;, address = #&#123;address,jdbcType=VARCHAR&#125; where id = #&#123;id,jdbcType=INTEGER&#125; &lt;/update&gt;&lt;/mapper&gt;]]></content>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC学习笔记-一]]></title>
    <url>%2F2018%2F11%2F30%2FSpringMVC%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%B8%80%2F</url>
    <content type="text"><![CDATA[Spring web mvc和Struts2都属于表现层的框架,它是Spring框架的一部分,我们可以从Spring的整体结构中看得出来,如下图： SpringMVC简单的处理流程图:前端控制器可以说是SpringMVC的心脏，是核心部分。 创建入门程序创建动态Web工程，导入相关jar包: 配置前端控制器在web.xml中配置前端控制器:123456789101112131415&lt;!-- 前端控制器 --&gt;&lt;servlet&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;!-- 默认找 /WEB-INF/[servlet的名称]-servlet.xml --&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:springmvc.xml&lt;/param-value&gt; &lt;/init-param&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;url-pattern&gt;*.action&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 注意，在&lt;url-pattern&gt;配置时：/*拦截所有，包括jsp和各种静态资源*.action或者*.do只拦截以action和do结尾的请求/拦截所有，但是不包括jsp 创建springmvc.xml配置文件12345678910111213&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:p="http://www.springframework.org/schema/p" xmlns:context="http://www.springframework.org/schema/context" xmlns:mvc="http://www.springframework.org/schema/mvc" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.0.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc-4.0.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.0.xsd"&gt; &lt;!-- 扫描包下有@Controller @Service 注解的代码 --&gt; &lt;context:component-scan base-package="my.study.springmvc.controller"&gt; &lt;/context:component-scan&gt;&lt;/beans&gt; 创建Controller，书写入门程序1234567891011121314151617181920212223242526272829package my.study.springmvc.controller;import java.util.ArrayList;import java.util.Date;import java.util.List;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.servlet.ModelAndView;import my.study.springmvc.pojo.Items;@Controllerpublic class ItemController &#123; @RequestMapping(value = "/item/itemlist.action") public ModelAndView itemList()&#123; List&lt;Items&gt; list = new ArrayList&lt;Items&gt;(); list.add(new my.study.springmvc.pojo.Items(1, "1", 2399f, new Date(), "a")); list.add(new Items(2, "28", 2399f, new Date(), "d")); list.add(new Items(3, "38", 2399f, new Date(), "s")); list.add(new Items(4, "48", 2399f, new Date(), "f")); list.add(new Items(5, "58", 2399f, new Date(), "x")); ModelAndView mav = new ModelAndView(); mav.addObject("itemList", list); mav.setViewName("/WEB-INF/jsp/itemList.jsp"); return mav; &#125; &#125; 在类上添加@Controller注解，把Controller交由Spring管理@RequestMapping(value = &quot;请求地址&quot;)该方法请求的地址,其中.action可以加也可以不加。ModelAndView从字面意思就可以理解，是数据和视图 在/WEB-INF/jsp/路径下创建jsp，用于显示:1234567891011121314151617181920212223242526272829303132333435363738394041&lt;%@ page language="java" contentType="text/html; charset=UTF-8" pageEncoding="UTF-8"%&gt;&lt;%@ taglib uri="http://java.sun.com/jsp/jstl/core" prefix="c" %&gt;&lt;%@ taglib uri="http://java.sun.com/jsp/jstl/fmt" prefix="fmt"%&gt;&lt;!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd"&gt;&lt;html&gt;&lt;head&gt;&lt;meta http-equiv="Content-Type" content="text/html; charset=UTF-8"&gt;&lt;title&gt;查询商品列表&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;form action="$&#123;pageContext.request.contextPath &#125;/item/queryitem.action" method="post"&gt;查询条件：&lt;table width="100%" border=1&gt;&lt;tr&gt;&lt;td&gt;&lt;input type="submit" value="查询"/&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;商品列表：&lt;table width="100%" border=1&gt;&lt;tr&gt; &lt;td&gt;商品名称&lt;/td&gt; &lt;td&gt;商品价格&lt;/td&gt; &lt;td&gt;生产日期&lt;/td&gt; &lt;td&gt;商品描述&lt;/td&gt; &lt;td&gt;操作&lt;/td&gt;&lt;/tr&gt;&lt;c:forEach items="$&#123;itemList &#125;" var="item"&gt;&lt;tr&gt; &lt;td&gt;$&#123;item.name &#125;&lt;/td&gt; &lt;td&gt;$&#123;item.price &#125;&lt;/td&gt; &lt;td&gt;&lt;fmt:formatDate value="$&#123;item.createtime&#125;" pattern="yyyy-MM-dd HH:mm:ss"/&gt;&lt;/td&gt; &lt;td&gt;$&#123;item.detail &#125;&lt;/td&gt; &lt;td&gt;&lt;a href="$&#123;pageContext.request.contextPath &#125;/itemEdit.action?id=$&#123;item.id&#125;"&gt;修改&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/c:forEach&gt;&lt;/table&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 创建pojo：12345678910111213141516171819202122232425262728package my.study.springmvc.pojo;import java.util.Date;public class Items &#123; private Integer id; private String name; private Float price; private String pic; private Date createtime; private String detail; public Items(Integer id, String name, Float price, Date createtime, String detail) &#123; super(); this.id = id; this.name = name; this.price = price; this.createtime = createtime; this.detail = detail; &#125; //get/set...&#125; 启动服务器访问: SpringMVC架构架构流程：1、 用户发送请求至前端控制器DispatcherServlet2、 DispatcherServlet收到请求调用HandlerMapping处理器映射器。3、 处理器映射器根据请求url找到具体的处理器，生成处理器对象及处理器拦截器(如果有则生成)一并返回给DispatcherServlet。返回的是包名+类名+方法名4、 DispatcherServlet通过HandlerAdapter处理器适配器调用处理器5、 执行处理器(Controller，也叫后端控制器)。6、 Controller执行完成返回ModelAndView7、 HandlerAdapter将controller执行结果ModelAndView返回给DispatcherServlet8、 DispatcherServlet将ModelAndView传给ViewReslover视图解析器9、 ViewReslover解析后返回具体View10、 DispatcherServlet对View进行渲染视图（即将模型数据填充至视图中）。11、 DispatcherServlet响应用户 组件说明:以下组件通常使用框架提供实现： DispatcherServlet：前端控制器用户请求到达前端控制器，它就相当于mvc模式中的c，dispatcherServlet是整个流程控制的中心，由它调用其它组件处理用户的请求，dispatcherServlet的存在降低了组件之间的耦合性。 HandlerMapping：处理器映射器HandlerMapping负责根据用户请求url找到Handler即处理器，springmvc提供了不同的映射器实现不同的映射方式，例如：配置文件方式，实现接口方式，注解方式等。 Handler：处理器Handler 是继DispatcherServlet前端控制器的后端控制器，在DispatcherServlet的控制下Handler对具体的用户请求进行处理。由于Handler涉及到具体的用户业务请求，所以一般情况需要程序员根据业务需求开发Handler。 HandlAdapter：处理器适配器通过HandlerAdapter对处理器进行执行，这是适配器模式的应用，通过扩展适配器可以对更多类型的处理器进行执行。 ViewResolver：视图解析器View Resolver负责将处理结果生成View视图，View Resolver首先根据逻辑视图名解析成物理视图名即具体的页面地址，再生成View视图对象，最后对View进行渲染将处理结果通过页面展示给用户。 View：视图springmvc框架提供了很多的View视图类型的支持，包括：jstlView、freemarkerView、pdfView等。我们最常用的视图就是jsp。一般情况下需要通过页面标签或页面模版技术将模型数据通过页面展示给用户，需要由程序员根据业务需求开发具体的页面。 在springmvc的各个组件中，处理器映射器、处理器适配器、视图解析器称为springmvc的三大组件。需要用户开发的组件有handler、view 默认加载的组件我们没有做任何配置，就可以使用这些组件因为框架已经默认加载这些组件了，配置文件位置如下图： 123456789101112131415161718192021222324# Default implementation classes for DispatcherServlet&apos;s strategy interfaces.# Used as fallback when no matching beans are found in the DispatcherServlet context.# Not meant to be customized by application developers.org.springframework.web.servlet.LocaleResolver=org.springframework.web.servlet.i18n.AcceptHeaderLocaleResolverorg.springframework.web.servlet.ThemeResolver=org.springframework.web.servlet.theme.FixedThemeResolverorg.springframework.web.servlet.HandlerMapping=org.springframework.web.servlet.handler.BeanNameUrlHandlerMapping,\ org.springframework.web.servlet.mvc.annotation.DefaultAnnotationHandlerMappingorg.springframework.web.servlet.HandlerAdapter=org.springframework.web.servlet.mvc.HttpRequestHandlerAdapter,\ org.springframework.web.servlet.mvc.SimpleControllerHandlerAdapter,\ org.springframework.web.servlet.mvc.annotation.AnnotationMethodHandlerAdapterorg.springframework.web.servlet.HandlerExceptionResolver=org.springframework.web.servlet.mvc.annotation.AnnotationMethodHandlerExceptionResolver,\ org.springframework.web.servlet.mvc.annotation.ResponseStatusExceptionResolver,\ org.springframework.web.servlet.mvc.support.DefaultHandlerExceptionResolverorg.springframework.web.servlet.RequestToViewNameTranslator=org.springframework.web.servlet.view.DefaultRequestToViewNameTranslatororg.springframework.web.servlet.ViewResolver=org.springframework.web.servlet.view.InternalResourceViewResolverorg.springframework.web.servlet.FlashMapManager=org.springframework.web.servlet.support.SessionFlashMapManager 组件扫描器使用组件扫描器省去在spring容器配置每个Controller类的繁琐。使用&lt;context:component-scan&gt;自动扫描标记@Controller的控制器类，在springmvc.xml配置文件中配置如下：12345&lt;!-- 配置controller扫描包，多个包之间用,分隔 扫描包下有@Controller @Service 注解的代码--&gt; &lt;context:component-scan base-package="my.study.springmvc"&gt; &lt;/context:component-scan&gt; 注解映射器和适配器配置处理器映射器注解式处理器映射器，对类中标记了@ResquestMapping的方法进行映射。根据@ResquestMapping定义的url匹配@ResquestMapping标记的方法，匹配成功返回HandlerMethod对象给前端控制器。HandlerMethod对象中封装url对应的方法Method。 从spring3.1版本开始，废除了DefaultAnnotationHandlerMapping的使用，推荐使用RequestMappingHandlerMapping完成注解式处理器映射。 在springmvc.xml配置文件中配置如下：12&lt;!-- 配置处理器映射器 --&gt;&lt;bean class="org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping" /&gt; 注解描述：@RequestMapping：定义请求url到处理器功能方法的映射 配置处理器适配器注解式处理器适配器，对标记@ResquestMapping的方法进行适配。 从spring3.1版本开始，废除了AnnotationMethodHandlerAdapter的使用，推荐使用RequestMappingHandlerAdapter完成注解式处理器适配。 在springmvc.xml配置文件中配置如下：123&lt;!-- 配置处理器适配器 --&gt;&lt;bean class="org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter" /&gt; 注解驱动直接配置处理器映射器和处理器适配器比较麻烦，可以使用注解驱动来加载。SpringMVC使用&lt;mvc:annotation-driven&gt;自动加载RequestMappingHandlerMapping和RequestMappingHandlerAdapter可以在springmvc.xml配置文件中使用&lt;mvc:annotation-driven&gt;替代注解处理器和适配器的配置。 12&lt;!-- 注解驱动 --&gt;&lt;mvc:annotation-driven /&gt; 视图解析器视图解析器使用SpringMVC框架默认的InternalResourceViewResolver，这个视图解析器支持JSP视图解析在springmvc.xml配置文件中配置如下：12345678910&lt;!-- Example: prefix="/WEB-INF/jsp/", suffix=".jsp", viewname="test" -&gt; "/WEB-INF/jsp/test.jsp" --&gt; &lt;!-- 配置视图解析器 --&gt; &lt;bean class="org.springframework.web.servlet.view.InternalResourceViewResolver"&gt; &lt;!-- 配置逻辑视图的前缀 --&gt; &lt;property name="prefix" value="/WEB-INF/jsp/" /&gt; &lt;!-- 配置逻辑视图的后缀 --&gt; &lt;property name="suffix" value=".jsp" /&gt; &lt;/bean&gt; 逻辑视图名需要在controller中返回ModelAndView指定，比如逻辑视图名为itemList，则最终返回的jsp视图地址:“WEB-INF/jsp/itemList.jsp” 最终jsp物理地址：前缀+逻辑视图名+后缀]]></content>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring与Mybatis整合]]></title>
    <url>%2F2018%2F11%2F29%2FSpring%E4%B8%8EMybatis%E6%95%B4%E5%90%88%2F</url>
    <content type="text"><![CDATA[在不使用Spring的情况下，创建sqlSessionFactory需要两步:12345//1.加载核心配置文件String resource = "sqlMapConfig.xml";InputStream in = Resources.getResourceAsStream(resource);//2.创建SqlSessionFactorySqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(in); 原始Dao开发时，每一个实现类都要注入一个工厂，然后工厂调用openSession()方法创建SqlSession对象，然后sqlSession再执行sql语句，这一系列存在大量重复代码。 整合思路1、SqlSessionFactory对象应该放到spring容器中作为单例存在。2、传统dao的开发方式中，应该从spring容器中获得sqlsession对象。3、Mapper代理形式中，应该从spring容器中直接获得mapper的代理对象。4、数据库的连接以及数据库连接池事务管理都交给spring容器来完成。 依赖包 创建配置文件applicationContext.xml:1234567891011121314151617181920212223242526272829&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:context="http://www.springframework.org/schema/context" xmlns:p="http://www.springframework.org/schema/p" xmlns:aop="http://www.springframework.org/schema/aop" xmlns:tx="http://www.springframework.org/schema/tx" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.0.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.0.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-4.0.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-4.0.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util-4.0.xsd"&gt; &lt;!-- 加载配置文件 --&gt; &lt;context:property-placeholder location="classpath:db.properties"/&gt; &lt;!-- dbcp数据源(配置数据库连接池) --&gt; &lt;bean id="dataSource" class="org.apache.commons.dbcp.BasicDataSource" destroy-method="close"&gt; &lt;property name="driverClassName" value="$&#123;jdbc.driver&#125;" /&gt; &lt;property name="url" value="$&#123;jdbc.url&#125;" /&gt; &lt;property name="username" value="$&#123;jdbc.username&#125;" /&gt; &lt;property name="password" value="$&#123;jdbc.password&#125;" /&gt; &lt;property name="maxActive" value="10" /&gt; &lt;property name="maxIdle" value="5" /&gt; &lt;/bean&gt; &lt;!-- 配置SqlSessionFactory --&gt; &lt;bean id="sqlSessionFactory" class="org.mybatis.spring.SqlSessionFactoryBean"&gt; &lt;!-- 配置mybatis核心配置文件 --&gt; &lt;property name="configLocation" value="classpath:sqlmapConfig.xml"/&gt; &lt;!-- 配置数据源 --&gt; &lt;property name="dataSource" ref="dataSource" /&gt; &lt;/bean&gt;&lt;/beans&gt; maxActive表示连接池最大连接数，maxIdle表示最大空闲 db.properties:1234jdbc.driver=com.mysql.jdbc.Driverjdbc.url=jdbc:mysql://localhost:3306/mybatisjdbc.username=rootjdbc.password=12345 log4j.properties:123456# Global logging configurationlog4j.rootLogger=DEBUG, stdout# Console output...log4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=%5p [%t] - %m%n sqlmapConfig.xml Mapper代理形式开发dao配置mapper代理程序结构: 1234567package my.study.mybatis.mapper;import my.study.mybatis.pojo.User;public interface UserMapper &#123; public User selectUserById(Integer id);&#125; applicationContext.xml:12345&lt;!-- Mpper动态代理开发 --&gt;&lt;bean id="userMapper" class="org.mybatis.spring.mapper.MapperFactoryBean"&gt; &lt;property name="sqlSessionFactory" ref="sqlSessionFactoryBean"/&gt; &lt;property name="mapperInterface" value="my.study.mybatis.mapper.UserMapper"/&gt;&lt;/bean&gt; sqlmapConfig.xml:12345678910&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE mapperPUBLIC "-//mybatis.org//DTD Mapper 3.0//EN""http://mybatis.org/dtd/mybatis-3-mapper.dtd"&gt;&lt;mapper namespace="my.study.mybatis.mapper.UserMapper"&gt; &lt;!-- 根据用户id查询 --&gt; &lt;select id="selectUserById" parameterType="Integer" resultType="my.study.mybatis.pojo.User"&gt; select * from user where id = #&#123;id&#125; &lt;/select&gt;&lt;/mapper&gt; 123456789101112131415package my.study.mybatis.pojo;import java.io.Serializable;import java.util.Date;public class User implements Serializable &#123; private static final long serialVersionUID = 1L; private Integer id; private String username; private String sex; private Date birthday; private String address; //get/set...&#125; 测试代码：12345678910111213141516171819202122232425package my.study.mybatis.test;import org.junit.Before;import org.junit.Test;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;import my.study.mybatis.mapper.UserMapper;import my.study.mybatis.pojo.User;public class UserDaoTest &#123; private ApplicationContext context; @Before public void setUp() throws Exception &#123; this.context = new ClassPathXmlApplicationContext("classpath:applicationContext.xml"); &#125; @Test public void testQueryUserById() &#123; UserMapper mapper = (UserMapper) this.context.getBean("userMapper"); User user = mapper.selectUserById(1); System.out.println(user); &#125;&#125; 扫描包形式配置mapper使用扫描方式不需要手动再注入工厂，它会扫描spring中配置的工厂,只需制定基本包就可以12345&lt;!-- Mapper动态代理开发 扫描包 --&gt;&lt;bean class="org.mybatis.spring.mapper.MapperScannerConfigurer"&gt; &lt;!-- 配置基本包 --&gt; &lt;property name="basePackage" value="my.study.mybatis.mapper"/&gt;&lt;/bean&gt; 使用这种方式，每个mapper代理对象的id就是类名，首字母小写 注解掉之前配置的Mpper动态代理开发: 使用扫描方式进行注入，不需要指定id,没有id的情况下，无法指定id，则需要指定实现类:然后进行测试: 传统dao的开发方式 UserDao接口:1234567package my.study.mybatis.dao;import my.study.mybatis.pojo.User;public interface UserDao &#123; User selectUserById(int id);&#125; UserDaoImpl类:123456789101112131415161718192021package my.study.mybatis.dao;import org.apache.ibatis.session.SqlSession;import org.mybatis.spring.support.SqlSessionDaoSupport;import my.study.mybatis.pojo.User;//继承SqlSessionDaoSupport可以不用再在此类声明工厂，而是交给父类public class UserDaoImpl extends SqlSessionDaoSupport implements UserDao&#123; @Override public User selectUserById(int id) &#123; // 获取SqlSession SqlSession sqlSession = super.getSqlSession(); // 使用SqlSession执行操作 User user = sqlSession.selectOne("selectUserById", id); // 不要关闭sqlSession return user; &#125;&#125; UserDaoImpl继承SqlSessionDaoSupport可以使Spring在父类创建工厂 sqlmapConfig.xml配置:12345678910&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;!DOCTYPE configurationPUBLIC "-//mybatis.org//DTD Config 3.0//EN""http://mybatis.org/dtd/mybatis-3-config.dtd"&gt;&lt;configuration&gt; &lt;mappers&gt; &lt;package name="my.study.mybatis.mapper"/&gt; &lt;/mappers&gt;&lt;/configuration&gt; applicationContext.xml:12345&lt;!-- dao --&gt; &lt;bean id="userDao" class="my.study.mybatis.dao.UserDaoImpl"&gt; &lt;!-- 注入工厂 --&gt; &lt;property name="sqlSessionFactory" ref="sqlSessionFactoryBean"/&gt; &lt;/bean&gt; 书写测试类UserDaoTest.java:123456789101112131415161718192021222324252627package my.study.mybatis.test;import org.junit.Before;import org.junit.Test;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;import my.study.mybatis.dao.UserDao;import my.study.mybatis.pojo.User;public class UserDaoTest &#123; private ApplicationContext context; @Before public void setUp() throws Exception &#123; this.context = new ClassPathXmlApplicationContext("classpath:applicationContext.xml"); &#125; @Test public void testQueryUserById() &#123; // 获取userDao UserDao userDao = this.context.getBean(UserDao.class); User user = userDao.selectUserById(1); System.out.println(user); &#125;&#125; 逆向工程使用官方网站的Mapper自动生成工具mybatis-generator-core-1.3.2来生成po类和Mapper映射文件 运行时，主程序会读取配置文件，进行生成 配置文件generatorConfig.xml:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE generatorConfiguration PUBLIC "-//mybatis.org//DTD MyBatis Generator Configuration 1.0//EN" "http://mybatis.org/dtd/mybatis-generator-config_1_0.dtd"&gt;&lt;generatorConfiguration&gt; &lt;!-- id随便取 targetRuntime是MyBatis3版本--&gt; &lt;context id="testTables" targetRuntime="MyBatis3"&gt; &lt;commentGenerator&gt; &lt;!-- 是否去除自动生成的注释 true：是 ： false:否 --&gt; &lt;property name="suppressAllComments" value="true" /&gt; &lt;/commentGenerator&gt; &lt;!--数据库连接的信息：驱动类、连接地址、用户名、密码 --&gt; &lt;jdbcConnection driverClass="com.mysql.jdbc.Driver" connectionURL="jdbc:mysql://localhost:3306/mybatis" userId="root" password="12345"&gt; &lt;/jdbcConnection&gt; &lt;!-- &lt;jdbcConnection driverClass="oracle.jdbc.OracleDriver" connectionURL="jdbc:oracle:thin:@127.0.0.1:1521:yycg" userId="yycg" password="yycg"&gt; &lt;/jdbcConnection&gt; --&gt; &lt;!-- 默认false，把JDBC DECIMAL 和 NUMERIC 类型解析为 Integer，为 true时把JDBC DECIMAL 和 NUMERIC 类型解析为java.math.BigDecimal --&gt; &lt;javaTypeResolver&gt; &lt;property name="forceBigDecimals" value="false" /&gt; &lt;/javaTypeResolver&gt; &lt;!-- targetProject:生成PO类的位置 --&gt; &lt;javaModelGenerator targetPackage="my.study.mybatis.pojo" targetProject=".\src"&gt; &lt;!-- enableSubPackages:是否让schema作为包的后缀 --&gt; &lt;property name="enableSubPackages" value="false" /&gt; &lt;!-- 从数据库返回的值被清理前后的空格 --&gt; &lt;property name="trimStrings" value="true" /&gt; &lt;/javaModelGenerator&gt; &lt;!-- targetProject:mapper映射文件生成的位置 --&gt; &lt;sqlMapGenerator targetPackage="my.study.mybatis.mapper" targetProject=".\src"&gt; &lt;!-- enableSubPackages:是否让schema作为包的后缀 --&gt; &lt;property name="enableSubPackages" value="false" /&gt; &lt;/sqlMapGenerator&gt; &lt;!-- targetPackage：mapper接口生成的位置 --&gt; &lt;javaClientGenerator type="XMLMAPPER" targetPackage="my.study.mybatis.mapper" targetProject=".\src"&gt; &lt;!-- enableSubPackages:是否让schema作为包的后缀 --&gt; &lt;property name="enableSubPackages" value="false" /&gt; &lt;/javaClientGenerator&gt; &lt;!-- 指定数据库表 --&gt; &lt;table schema="" tableName="user"&gt;&lt;/table&gt; &lt;table schema="" tableName="orders"&gt;&lt;/table&gt; &lt;!-- 有些表的字段需要指定java类型 &lt;table schema="" tableName="user"&gt; &lt;columnOverride column="id" javaType="Long" /&gt; &lt;/table&gt; --&gt; &lt;/context&gt;&lt;/generatorConfiguration&gt; 主程序:12345678910111213141516171819202122232425262728293031323334353637383940import java.io.File;import java.io.IOException;import java.util.ArrayList;import java.util.List;import org.mybatis.generator.api.MyBatisGenerator;import org.mybatis.generator.config.Configuration;import org.mybatis.generator.config.xml.ConfigurationParser;import org.mybatis.generator.exception.XMLParserException;import org.mybatis.generator.internal.DefaultShellCallback;public class GeneratorSqlmap &#123; public void generator() throws Exception&#123; List&lt;String&gt; warnings = new ArrayList&lt;String&gt;(); boolean overwrite = true; //指定逆向工程配置文件 File configFile = new File("generatorConfig.xml"); ConfigurationParser cp = new ConfigurationParser(warnings); Configuration config = cp.parseConfiguration(configFile); DefaultShellCallback callback = new DefaultShellCallback(overwrite); MyBatisGenerator myBatisGenerator = new MyBatisGenerator(config, callback, warnings); myBatisGenerator.generate(null); &#125; public static void main(String[] args) throws Exception &#123; try &#123; GeneratorSqlmap generatorSqlmap = new GeneratorSqlmap(); generatorSqlmap.generator(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 在主程序运行,查看日志:刷新后，可以看到已经生成了对应的文件: 1.逆向工程生成的代码只能做单表查询2.不能在生成的代码上进行扩展，因为如果数据库变更，需要重新使用逆向工程生成代码，原来编写的代码就被覆盖了。3.一张表会生成4个文件4.*Example.java文件中封装了很多方便的方法可以用来调用]]></content>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mybatis学习笔记-二]]></title>
    <url>%2F2018%2F11%2F21%2FMybatis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[parameterType(输入类型)传递简单类型和传递pojo对象简单类型：使用#{}占位符，或者${}进行sql拼接。 opjo对象：Mybatis使用ognl表达式解析对象字段的值，#{}或者${}括号中的值为pojo属性名称。 传递pojo包装对象开发中通过可以使用pojo传递查询条件。查询条件可能是综合的查询条件，不仅包括用户查询条件还包括其它的查询条件（比如查询用户信息的时候，将用户购买商品信息也作为查询条件），这时可以使用包装对象传递输入参数。 包装对象：Pojo类中的一个属性是另外一个pojo。 需求：根据用户名模糊查询用户信息，查询条件放到QueryVo的user属性中。 在Mybatis学习笔记-一的基础上，继续编写QueryVo类进行开发: 1234567891011121314package my.study.pojo;public class QueryVo &#123; //包含其他的pojo private User user; public User getUser() &#123; return user; &#125; public void setUser(User user) &#123; this.user = user; &#125;&#125; 在my.study.mapper包下的User.xml中书写sql语句:1234&lt;!-- 使用包装类型查询用户 --&gt; &lt;select id="findUserByQueryVo" parameterType="my.study.pojo.QueryVo" resultType="my.study.pojo.User"&gt; select * from user where username like "%"#&#123;user.username&#125;"%" &lt;/select&gt; 在UserMapper.java接口中添加方法:123456789101112package my.study.mapper;import java.util.List;import my.study.pojo.QueryVo;import my.study.pojo.User;public interface UserMapper &#123; public User findUserById(Integer id); //添加方法 public List&lt;User&gt; findUserByQueryVo(QueryVo vo);&#125; 书写测试代码：123456789101112131415161718192021@Testpublic void testfindUserByQueryVo() throws Exception&#123; //1.加载核心配置文件 String resource = "sqlMapConfig.xml"; InputStream in = Resources.getResourceAsStream(resource); //2.创建SqlSessionFactory SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(in); //3.创建SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); //4.SqlSession根据接口生成一个实现类 UserMapper mapper = sqlSession.getMapper(UserMapper.class); QueryVo vo =new QueryVo(); User u = new User(); u.setUsername("五"); vo.setUser(u); List&lt;User&gt; us = mapper.findUserByQueryVo(vo); for(User user:us) &#123; System.out.println(user); &#125; &#125; resultType(输出类型)输出简单类型需求：查询user的数量 在UserMapper.java接口中添加方法:1234... //查询数据条数 public Integer findUserCount();... Mapper.xml中添加查询语句:1234&lt;!-- 查询用户条数 --&gt; &lt;select id="findUserCount" resultType="Integer"&gt; select count(*) from user &lt;/select&gt; 书写测试类：12345678910111213141516//查询用户条数@Testpublic void testfindUserCount() throws Exception&#123; //1.加载核心配置文件 String resource = "sqlMapConfig.xml"; InputStream in = Resources.getResourceAsStream(resource); //2.创建SqlSessionFactory SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(in); //3.创建SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); //4.SqlSession根据接口生成一个实现类 UserMapper mapper = sqlSession.getMapper(UserMapper.class); Integer count = mapper.findUserCount(); System.out.println(count);&#125; 输出pojo对象和对象列表见 https://homxuwang.github.io/2018/11/19/Mybatis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%B8%80/ resultMapresultType可以指定将查询结果映射为pojo，但需要pojo的属性名和sql查询的列名一致方可映射成功。 如果sql查询字段名和pojo的属性名不一致，可以通过resultMap将字段名和属性名作一个对应关系 ，resultMap实质上还需要将查询结果映射到pojo对象中。 resultMap可以实现将查询结果映射为复杂类型的pojo，比如在查询结果映射对象中包括pojo和list实现一对一查询和一对多查询。 需求：查询订单表order的所有数据,订单表创建见:Mybatis学习笔记-一sql：SELECT id, user_id, number, createtime, note FROM order 创建POJO类:1234567891011121314151617181920212223242526272829package my.study.pojo;import java.io.Serializable;import java.util.Date;public class Orders implements Serializable&#123; private static final long serialVersionUID = 1L; private Integer id; private Integer userId; private String number; private Date createtime; private String note; private User user; 添加 get/set 方法 @Override public String toString() &#123; return "Orders [id=" + id + ", userId=" + userId + ", number=" + number + ", createtime=" + createtime + ", note=" + note + "]"; &#125;&#125; 可以看到类中userId字段和数据库表中的user_id字段名不一致 在my.study.mapper包中添加OrderMapper接口:1234567891011package my.study.mapper;import java.util.List;import my.study.pojo.Orders;import my.study.pojo.User;public interface OrderMapper &#123; // 查询订单表order的所有数据 public List&lt;Orders&gt; selectOrdersList(); &#125; 定义OrderMapper.xml:123456789101112&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE mapperPUBLIC "-//mybatis.org//DTD Mapper 3.0//EN""http://mybatis.org/dtd/mybatis-3-mapper.dtd"&gt;&lt;mapper namespace="my.study.mapper.OrderMapper"&gt; &lt;!-- 查询所有的订单数据 --&gt; &lt;select id="selectOrdersList" resultType="my.study.pojo.Orders"&gt; SELECT id, user_id, number, createtime, note FROM orders &lt;/select&gt;&lt;/mapper&gt; 记得在核心配置文件中引入OrderMapper.xml：1234&lt;mappers&gt; &lt;mapper resource="my/study/mapper/User.xml"/&gt; &lt;mapper resource="my/study/mapper/OrderMapper.xml"/&gt;&lt;/mappers&gt; 书写测试类:123456789101112131415161718//查询订单表的所有数据 @Test public void testfindUserCount() throws Exception&#123; //1.加载核心配置文件 String resource = "sqlMapConfig.xml"; InputStream in = Resources.getResourceAsStream(resource); //2.创建SqlSessionFactory SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(in); //3.创建SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); //4.SqlSession根据接口生成一个实现类 OrderMapper mapper = sqlSession.getMapper(OrderMapper.class); List&lt;Orders&gt; orderlist = mapper.selectOrdersList(); for(Orders order:orderlist) &#123; System.out.println(order); &#125; &#125; 可以看到，对于POJO类中和数据库表中字段不一致的，将会查询出null.除了修改为一样的字段值外，可以使用resultMap:其实，与数据库字段不一样的只有user_id和userId，没有必要都写出来，所以可以简写为:123&lt;resultMap type="my.study.pojo.Orders" id="orders"&gt; &lt;result column="user_id" property="userId"/&gt;&lt;/resultMap&gt; column对应数据库表字段;property对应POJO字段;javaType对应java类型(可以省略);jdbcType对应数据库中的类型(可以省略) 再次执行测试代码: 动态sql通过mybatis提供的各种标签方法实现动态拼接sql if标签需求：根据性别和名字查询用户查询sql：SELECT id, username, birthday, sex, address FROM user WHERE sex = 1 AND username LIKE ‘%张%’ 在UserMapper.java接口中添加方法:1234...//根据性别和名称查询用户 public List&lt;User&gt; selectUserBySexAndUsername(User user);... 在User.xml中书写sql语句:1234567891011&lt;!-- 使用if和where标签 --&gt; &lt;select id="selectUserBySexAndUsername" parameterType="my.study.pojo.User" resultType="my.study.pojo.User"&gt; select * from user where &lt;if test="sex != null and sex != ''"&gt; sex = #&#123;sex&#125; &lt;/if&gt; &lt;if test="username != null and username != ''"&gt; and username = #&#123;username&#125; &lt;/if&gt; &lt;/select&gt; 书写测试类:1234567891011121314151617181920//查询用户条数 @Test public void testselectUserBySexAndUsername() throws Exception&#123; //1.加载核心配置文件 String resource = "sqlMapConfig.xml"; InputStream in = Resources.getResourceAsStream(resource); //2.创建SqlSessionFactory SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(in); //3.创建SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); //4.SqlSession根据接口生成一个实现类 UserMapper mapper = sqlSession.getMapper(UserMapper.class); User u = new User(); u.setSex("1"); u.setUsername("张小明"); List&lt;User&gt; users = mapper.selectUserBySexAndUsername(u); for(User user:users) &#123; System.out.println(user); &#125; &#125; 查询结果： 因为通过user实例查询的sex和username字段都不为空，所以可以看到sql语句是正常的. 如果注释掉u.setUsername(&quot;张小明&quot;);再进行测试 :可以看到查询出的是sex为1的所有数据，因为username=null，所以在if标签中没有添加后面的and username = #{username} 但是如果注释掉u.setSex(&quot;1&quot;);，将u.setUsername(&quot;张小明&quot;);还原，再次进行测试：可以看到，查询错误，sql语句是拼接错误的.这时候就需要用到where标签 where标签where标签可以自动添加where，同时处理sql语句中第一个前and关键字对上面的查询语句做处理:1234567891011&lt;select id="selectUserBySexAndUsername" parameterType="my.study.pojo.User" resultType="my.study.pojo.User"&gt; select * from user &lt;where&gt; &lt;if test="sex != null and sex != ''"&gt; sex = #&#123;sex&#125; &lt;/if&gt; &lt;if test="username != null and username != ''"&gt; and username = #&#123;username&#125; &lt;/if&gt; &lt;/where&gt; &lt;/select&gt; 再次进行测试：上面提到where标签可以处理sql语句中第一个前and关键字 SQL片段Sql中可将重复的sql提取出来，使用时用include引用即可，最终达到sql重用的目的。先定义一个用于共用的sql片段,然后使用include进行引用:12345678910111213141516&lt;sql id="selectFromUser"&gt; select * from user&lt;/sql&gt;&lt;!--使用sql片段--&gt;&lt;select id="selectUserBySexAndUsername" parameterType="my.study.pojo.User" resultType="my.study.pojo.User"&gt; &lt;include refid="selectFromUser"&gt;&lt;/include&gt; &lt;where&gt; &lt;if test="sex != null and sex != ''"&gt; sex = #&#123;sex&#125; &lt;/if&gt; &lt;if test="username != null and username != ''"&gt; and username = #&#123;username&#125; &lt;/if&gt; &lt;/where&gt; &lt;/select&gt; foreach标签需求：根据多个id查询用户信息在UserMapper接口中添加方法:12//根据多个id查询用户信息public List&lt;User&gt; selectUserByIds(QueryVo vo); 在QueryVo中改造：12private List&lt;Integer&gt; idsList;//添加get/set方法 书写sql：123456789&lt;select id="selectUserByIds" parameterType="my.study.pojo.QueryVo" resultType="my.study.pojo.User"&gt; &lt;include refid="selectFromUser"/&gt; &lt;where&gt; id in &lt;foreach collection="idsList" item="id" separator="," open="(" close=")"&gt; #&#123;id&#125; &lt;/foreach&gt; &lt;/where&gt;&lt;/select&gt; collection：遍历的集合，这里是QueryVo的idsList属性item：遍历的项目，可以随便写，但是和后面的#{}里面要一致open：在前面添加的sql片段close：在结尾处添加的sql片段separator：指定遍历的元素之间使用的分隔符注意:如果接口的方法参数处传递的是数组类型的参数，则需要将collection处值设置为array;同理，如果接口的方法参数处传递的是list类型的参数，则需要将collection处值设置为list测试:1234567891011121314151617181920212223//多个id @Test public void testselectUserByidsList() throws Exception&#123; //1.加载核心配置文件 String resource = "sqlMapConfig.xml"; InputStream in = Resources.getResourceAsStream(resource); //2.创建SqlSessionFactory SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(in); //3.创建SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); //4.SqlSession根据接口生成一个实现类 UserMapper mapper = sqlSession.getMapper(UserMapper.class); List&lt;Integer&gt; ids = new ArrayList&lt;&gt;(); ids.add(16); ids.add(22); ids.add(24); QueryVo vo = new QueryVo(); vo.setIdsList(ids); List&lt;User&gt; users = mapper.selectUserByIds(vo); for(User user:users) &#123; System.out.println(user); &#125; &#125; 测试结果: 关联查询 一对一关联查询改造UserMapper,在接口中添加方法12//一对一关联 查询 以订单为中心 关联用户public List&lt;Orders&gt; selectOrders(); 在User.xml中书写sql(需要用到嵌套映射):12345678910111213141516171819202122232425262728&lt;resultMap type="my.study.pojo.Orders" id="orderUserResultMap"&gt; &lt;id column="id" property="id"/&gt; &lt;result column="user_id" property="userId"/&gt; &lt;result column="number" property="number"/&gt; &lt;result column="createtime" property="createtime"/&gt; &lt;result column="note" property="note"/&gt; &lt;!-- association ：配置一对一属性 --&gt; &lt;!-- property:order里面的User属性名 --&gt; &lt;!-- javaType:属性类型 --&gt; &lt;association property="user" javaType="my.study.pojo.User"&gt; &lt;!-- id:声明主键，表示user_id是关联查询对象的唯一标识--&gt; &lt;id column="user_id" property="id"/&gt; &lt;result column="username" property="username"/&gt; &lt;/association&gt; &lt;/resultMap&gt; &lt;select id="selectOrders" resultMap="orderUserResultMap"&gt; select o.id, o.user_id, o.number, o.createtime, o.note, u.username from orders o LEFT JOIN user u on o.user_id = u.id &lt;/select&gt; 测试:12345678910111213141516171819//一对一@Testpublic void testselectOrders() throws Exception&#123; //1.加载核心配置文件 String resource = "sqlMapConfig.xml"; InputStream in = Resources.getResourceAsStream(resource); //2.创建SqlSessionFactory SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(in); //3.创建SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); //4.SqlSession根据接口生成一个实现类 UserMapper mapper = sqlSession.getMapper(UserMapper.class); List&lt;Orders&gt; orderlist = mapper.selectOrders(); for(Orders order:orderlist) &#123; System.out.println(order); System.out.println(order.getUser()); &#125; &#125; 查询结果: 一对多关联查询一个用户有可能对应多个订单，所以以user表作为左表进行查询，是一对多的关系。 在User类中添加订单列表属性字段,并添加get和set方法:1234...private List&lt;Orders&gt; ordersList;//get/set... OrderMapper接口中添加方法:12//一对多关联public List&lt;User&gt; selectUserList(); 在OrderMapper.xml中书写sql代码:123456789101112131415161718192021222324252627&lt;resultMap type="my.study.pojo.User" id="userResultMap"&gt; &lt;id column="user_id" property="id"/&gt; &lt;result column="username" property="username"/&gt; &lt;!-- 配置一对多属性 ofType是指List中每个元素的类型 --&gt; &lt;collection property="ordersList" ofType="my.study.pojo.Orders"&gt; &lt;id column="id" property="id"/&gt; &lt;result column="user_id" property="userId"/&gt; &lt;result column="number" property="number"/&gt; &lt;result column="createtime" property="createtime"/&gt; &lt;result column="note" property="note"/&gt; &lt;/collection&gt; &lt;/resultMap&gt; &lt;select id="selectUserList" resultMap="userResultMap"&gt; select o.id, o.user_id, o.number, o.createtime, o.note, u.username from user u LEFT JOIN orders o on o.user_id = u.id &lt;/select&gt; 测试代码：12345678910111213141516171819//一对多@Testpublic void testselectUserList() throws Exception&#123; //1.加载核心配置文件 String resource = "sqlMapConfig.xml"; InputStream in = Resources.getResourceAsStream(resource); //2.创建SqlSessionFactory SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(in); //3.创建SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); //4.SqlSession根据接口生成一个实现类 OrderMapper mapper = sqlSession.getMapper(OrderMapper.class); List&lt;User&gt; userlist = mapper.selectUserList(); for(User user:userlist) &#123; System.out.println(user); System.out.println(user.getOrdersList()); &#125; &#125;]]></content>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mybatis学习笔记-一]]></title>
    <url>%2F2018%2F11%2F19%2FMybatis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%B8%80%2F</url>
    <content type="text"><![CDATA[Mybatis架构图 准备测试数据库1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/*Navicat MySQL Data TransferSource Server : localhost_3306Source Server Version : 50521Source Host : localhost:3306Source Database : mybatisTarget Server Type : MYSQLTarget Server Version : 50521File Encoding : 65001Date: 2015-04-09 16:03:53*/SET FOREIGN_KEY_CHECKS=0;-- ------------------------------ Table structure for `orders`-- ----------------------------DROP TABLE IF EXISTS `orders`;CREATE TABLE `orders` ( `id` int(11) NOT NULL AUTO_INCREMENT, `user_id` int(11) NOT NULL COMMENT '下单用户id', `number` varchar(32) NOT NULL COMMENT '订单号', `createtime` datetime NOT NULL COMMENT '创建订单时间', `note` varchar(100) DEFAULT NULL COMMENT '备注', PRIMARY KEY (`id`), KEY `FK_orders_1` (`user_id`), CONSTRAINT `FK_orders_id` FOREIGN KEY (`user_id`) REFERENCES `user` (`id`) ON DELETE NO ACTION ON UPDATE NO ACTION) ENGINE=InnoDB AUTO_INCREMENT=6 DEFAULT CHARSET=utf8;-- ------------------------------ Records of orders-- ----------------------------INSERT INTO `orders` VALUES ('3', '1', '1000010', '2015-02-04 13:22:35', null);INSERT INTO `orders` VALUES ('4', '1', '1000011', '2015-02-03 13:22:41', null);INSERT INTO `orders` VALUES ('5', '10', '1000012', '2015-02-12 16:13:23', null);-- ------------------------------ Table structure for `user`-- ----------------------------DROP TABLE IF EXISTS `user`;CREATE TABLE `user` ( `id` int(11) NOT NULL AUTO_INCREMENT, `username` varchar(32) NOT NULL COMMENT '用户名称', `birthday` date DEFAULT NULL COMMENT '生日', `sex` char(1) DEFAULT NULL COMMENT '性别', `address` varchar(256) DEFAULT NULL COMMENT '地址', PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=27 DEFAULT CHARSET=utf8;-- ------------------------------ Records of user-- ----------------------------INSERT INTO `user` VALUES ('1', '王五', null, '2', null);INSERT INTO `user` VALUES ('10', '张三', '2014-07-10', '1', '北京市');INSERT INTO `user` VALUES ('16', '张小明', null, '1', '河南郑州');INSERT INTO `user` VALUES ('22', '陈小明', null, '1', '河南郑州');INSERT INTO `user` VALUES ('24', '张三丰', null, '1', '河南郑州');INSERT INTO `user` VALUES ('25', '陈小明', null, '1', '河南郑州');INSERT INTO `user` VALUES ('26', '王五', null, null, null); 环境搭建导入所需依赖包： 根据1中的架构图，首先准备配置文件：SqlMapConfig.xml:123456789101112131415161718192021&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE configurationPUBLIC "-//mybatis.org//DTD Config 3.0//EN""http://mybatis.org/dtd/mybatis-3-config.dtd"&gt;&lt;configuration&gt; &lt;!-- 用于配置数据库连接环境 和spring整合后 environments配置将废除 --&gt; &lt;environments default="development"&gt; &lt;environment id="development"&gt; &lt;!-- 使用jdbc事务管理 --&gt; &lt;transactionManager type="JDBC" /&gt; &lt;!-- 数据库连接池 --&gt; &lt;dataSource type="POOLED"&gt; &lt;property name="driver" value="com.mysql.jdbc.Driver" /&gt; &lt;property name="url" value="jdbc:mysql://localhost:3306/mybatis" /&gt; &lt;property name="username" value="root" /&gt; &lt;property name="password" value="12345" /&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt;&lt;/configuration&gt; 其中environments标签是用来配置数据库连接环境的，因为这里是单独是mybatis工程，所以要进行配置。后期在整合Spring工程时，数据库的配置由Spring来负责，可以将environments标签除掉。 在src包下创建log4j.properties.log4j.properties:123456# Global logging configurationlog4j.rootLogger=DEBUG, stdout# Console output...log4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=%5p [%t] - %m%n 创建user表中对应的pojo类.pojo类作为mybatis进行sql映射使用，po类通常与数据库表对应.123456789101112131415161718192021222324package my.study.pojo;import java.io.Serializable;import java.util.Date;public class User implements Serializable &#123; private static final long serialVersionUID = 1L; private Integer id; private String username; private String sex; private Date birthday; private String address; //省略get和set方法 //... @Override public String toString() &#123; return "User [id=" + id + ", username=" + username + ", sex=" + sex + ", birthday=" + birthday + ", address=" + address + "]"; &#125;&#125; 在my.study.mapper包下创建user.xml:12345678&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE mapperPUBLIC "-//mybatis.org//DTD Mapper 3.0//EN""http://mybatis.org/dtd/mybatis-3-mapper.dtd"&gt;&lt;!-- 书写sql语句 --&gt;&lt;mapper&gt;&lt;/mapper&gt; 在xml中引入约束后，其根标签就是&lt;mapper&gt;&lt;/mapper&gt;,用来在其中书写sql语句。 在这里定义了user.xml文件后，根据架构图，需要和核心配置文件结合，所以要在核心配置文件中使用&lt;mappers&gt;&lt;/mappers&gt;标签：1234&lt;!-- Mapper的位置 Mapper.xml 写Sql语句的文件的位置 --&gt; &lt;mappers&gt; &lt;mapper resource="my/study/mapper/User.xml"/&gt; &lt;/mappers&gt; 这样准备工作就基本完成了，接下来就要进行架构中的第二部，创建工厂和session并进行测试。 操作数据库在配置文件中书写sql在User.xml配置文件中书写查询sql: 其中&lt;&gt;cache和&lt;&gt;cache-ref表示缓存,它们和&lt;&gt;parameterMap都很少用到.&lt;&gt;sql表示sql片段,&lt;&gt;result表示手动映射。剩下的增删改查和更新。 书写查询sql：1234&lt;!-- 查询用户 --&gt; &lt;select id="findUserById" parameterType="Integer" resultType="my.study.pojo.User"&gt; select * from user where id = #&#123;v&#125; &lt;/select&gt; 标签中的id表示这条sql语句在配置文件中的唯一标识;parameterType表示传参的类型;resultType表示自动映射查询到的结果，填写pojo的全路径.自动映射要求所创建的类要和数据库表中的各个字段和类型相对应,如果不对应则需要手动映射.#{}表示占位符，类似于jdbc中的?,大括号中的值可以随便写. 如果有另一个配置文件中的查询语句的id和已存在的重复，这时候可以在&lt;mapper&gt;添加namespace=&quot;&quot;,namespace表示命名空间，用于隔离sql,避免发生这种情况.比如修改为:123&lt;mapper namespace="user"&gt;...&lt;/mapper&gt; 这时候可以使用user.findUserById访问这个sql. 书写测试类12345678910111213141516171819202122232425262728package my.study.test;import java.io.InputStream;import org.apache.ibatis.io.Resources;import org.apache.ibatis.session.SqlSession;import org.apache.ibatis.session.SqlSessionFactory;import org.apache.ibatis.session.SqlSessionFactoryBuilder;import org.junit.Test;import my.study.pojo.User;public class MybatisTest &#123; @Test public void test1() throws Exception &#123; //1.加载核心配置文件 String resource = "sqlMapConfig.xml"; InputStream in = Resources.getResourceAsStream(resource); //2.创建SqlSessionFactory SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(in); //3.创建SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); //4.执行Sql语句 User user = sqlSession.selectOne("user.findUserById", 10); System.out.println(user); &#125;&#125; 这里sqlSession.selectOne()方法的第一个参数就是传入&lt;mapper&gt;的namespace+sql语句的id;第二个就是传入要进行查询的参数(也就是占位符位置的值); 测试结果:因为加入了log日志，这里会打印出各个信息 实现模糊查询书写sql语句:1234&lt;!-- 模糊查询 --&gt; &lt;select id="findUserByUsername" parameterType="String" resultType="my.study.pojo.User"&gt; select * from user where username like '%$&#123;value&#125;%' &lt;/select&gt; 注意，这里的模糊查询中不是用占位符#{}，而是使用${}。类比jdbc中的占位符，#{}相当于?而在需要使用字符串拼接功能时，使用${}.并且其中要填写value,而#{}则没有这种限制。占位符可以防止sql注入，而字符串拼接不能防止sql注入，#{}和${}也是一个道理。在测试类中添加测试方法：12345678910111213141516//实现模糊查询 @Test public void test2() throws Exception &#123; //1.加载核心配置文件 String resource = "sqlMapConfig.xml"; InputStream in = Resources.getResourceAsStream(resource); //2.创建SqlSessionFactory SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(in); //3.创建SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); //4.执行Sql语句 List&lt;User&gt; users = sqlSession.selectList("user.findUserByUsername", "五"); for(User user2:users) &#123; System.out.println(user2); &#125; &#125; 因为模糊查询查出来的可能有很多结果，所以用List&lt;&gt;.测试结果: 上面提了这样不能防止sql注入，所以可以这样写select标签中的sql语句1select * from user where username like "%"#&#123;v&#125;"%" 可以看到打印结果中成功出现了占位符，其实最终的sql语句相当于1select * from user where username like "%"'五'"%" 这种写法是正确的，只是平时很少这么用 新增数据书写sql语句123456&lt;!-- 新增用户 --&gt; &lt;insert id="insertUser" parameterType="my.study.pojo.User"&gt; insert into user (username,birthday,address,sex) values (#&#123;username&#125;,#&#123;birthday&#125;,#&#123;address&#125;,#&#123;sex&#125;) &lt;/insert&gt; #{}中的参数要和User.java类中的字段对应，这样才能知道传参是和哪个字段对应 测试类:12345678910111213141516171819//新增用户 @Test public void testInsert() throws Exception &#123; //1.加载核心配置文件 String resource = "sqlMapConfig.xml"; InputStream in = Resources.getResourceAsStream(resource); //2.创建SqlSessionFactory SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(in); //3.创建SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); //4.执行Sql语句 User u = new User(); u.setUsername("李雷"); u.setBirthday(new Date()); u.setAddress("火星"); u.setSex("男"); int i =sqlSession.insert("user.insertUser", u); sqlSession.commit(); &#125; 其中insert的返回值为int型，表示影响的行数.注意最后要提交事务才能成功插入. 新增数据后返回主键ID如果想实现在新增完用户后立刻返回这条新增数据的主键，则需要在&lt;insert&gt;语句中再嵌套一层&lt;selectKey&gt;用来查询主键.123&lt;selectKey keyProperty="id" resultType="Integer" order="AFTER"&gt; select LAST_INSERT_ID()&lt;/selectKey&gt; keyProperty表示返回值为User的id属性，resultType表示类型是Integer型;注意：在sql中，如果主键是自增的int型，那么主键是在数据插入完成后再生成主键;如果不是自增类型，比如如果是varchar类型，则会先插入主键，再插入数据。order表示执行顺序,所以这里要用AFTER其中1select LAST_INSERT_ID() 是由sql提供的查询语句，它跟在insert语句后面的话，会返回刚插入的数据的主键,比如:12insert into ....select LAST_INSERT_ID() 在上面的测试类中添加1System.out.println(u.getId()); 即可查看到已经获取到最新插入的主键ID 修改/更新数据123456&lt;!-- 更新 --&gt; &lt;update id="updateUserById" parameterType="my.study.pojo.User"&gt; update user set username = #&#123;username&#125;,sex =#&#123;sex&#125;,address = #&#123;address&#125;,birthday = #&#123;birthday&#125; where id =#&#123;id&#125; &lt;/update&gt; 1234567891011121314151617181920//更新用户 @Test public void testUpdate() throws Exception &#123; //1.加载核心配置文件 String resource = "sqlMapConfig.xml"; InputStream in = Resources.getResourceAsStream(resource); //2.创建SqlSessionFactory SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(in); //3.创建SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); //4.执行Sql语句 User u = new User(); u.setId(29); u.setUsername("李雷update"); u.setBirthday(new Date()); u.setAddress("火星update"); u.setSex("2"); int i =sqlSession.update("user.updateUserById", u); sqlSession.commit(); &#125; 删除12345&lt;!-- 删除 --&gt; &lt;delete id="deleteUserById" parameterType="Integer"&gt; delete from user where id = #&#123;id&#125; &lt;/delete&gt; 1234567891011121314//删除用户 @Test public void testdelete() throws Exception &#123; //1.加载核心配置文件 String resource = "sqlMapConfig.xml"; InputStream in = Resources.getResourceAsStream(resource); //2.创建SqlSessionFactory SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(in); //3.创建SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); //4.执行Sql语句 int d = sqlSession.delete("user.deleteUserById", 29); sqlSession.commit(); &#125; Mapper动态代理开发使用动态代理方式，动态生成代码，避免书写重复代码.原始Dao开发中存在以下问题：Dao方法体存在重复代码：通过SqlSessionFactory创建SqlSession，调用SqlSession的数据库操作方法调用sqlSession的数据库操作方法需要指定statement的id，这里存在硬编码，不便于开发维护。 Mapper接口开发方法只需要程序员编写Mapper接口（相当于Dao接口），由Mybatis框架根据接口定义创建接口的动态代理对象。 Mapper接口开发需要遵循以下规范：1、Mapper.xml文件中的namespace与mapper接口的类路径相同。2、Mapper接口方法名和Mapper.xml中定义的每个statement的id相同3、Mapper接口方法的输入参数类型和mapper.xml中定义的每个sql 的parameterType的类型相同4、Mapper接口方法的输出参数类型和mapper.xml中定义的每个sql的resultType的类型相同 这里定义一个UserMapper接口:123456789101112package my.study.mapper;import my.study.pojo.User;public interface UserMapper &#123; //遵循四个原则 //接口方法名 == User.xml 中的 id名 //返回值类型 与 User.xml文件中返回值的类型一致 //方法的入参类型与User.xml中入参的类型一致 //mapper标签的命名空间绑定此接口 public User findUserById(Integer id);&#125; 命名空间与接口绑定： 书写测试类：12345678910111213141516171819202122232425262728293031package my.study.test;import java.io.InputStream;import org.apache.ibatis.io.Resources;import org.apache.ibatis.session.SqlSession;import org.apache.ibatis.session.SqlSessionFactory;import org.apache.ibatis.session.SqlSessionFactoryBuilder;import org.junit.Test;import my.study.mapper.UserMapper;import my.study.pojo.User;public class MybatisMapperTest &#123; @Test public void test1() throws Exception&#123; //1.加载核心配置文件 String resource = "sqlMapConfig.xml"; InputStream in = Resources.getResourceAsStream(resource); //2.创建SqlSessionFactory SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(in); //3.创建SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); //4.SqlSession根据接口生成一个实现类 UserMapper mapper = sqlSession.getMapper(UserMapper.class); User user = mapper.findUserById(10); System.out.println(user); &#125;&#125; 这样使用动态开发，只需要书写一个接口和一个mapper.xml配置文件就可以了. Mybatis相比于JDBC的优点1、数据库连接创建、释放频繁造成系统资源浪费从而影响系统性能，如果使用数据库连接池可解决此问题。解决：在SqlMapConfig.xml中配置数据连接池，使用连接池管理数据库链接。2、Sql语句写在代码中造成代码不易维护，实际应用sql变化的可能较大，sql变动需要改变java代码。解决：将Sql语句配置在XXXXmapper.xml文件中与java代码分离。3、向sql语句传参数麻烦，因为sql语句的where条件不一定，可能多也可能少，占位符需要和参数一一对应。解决：Mybatis自动将java对象映射至sql语句，通过statement中的parameterType定义输入参数的类型。4、对结果集解析麻烦，sql变化导致解析代码变化，且解析前需要遍历，如果能将数据库记录封装成pojo对象解析比较方便。解决：Mybatis自动将sql执行结果映射至java对象，通过statement中的resultType定义输出结果的类型。 Mybatis和Hibernate区别Mybatis和hibernate不同，它不完全是一个ORM框架，因为MyBatis需要程序员自己编写Sql语句。mybatis可以通过XML或注解方式灵活配置要运行的sql语句，并将java对象和sql语句映射生成最终执行的sql，最后将sql执行的结果再映射生成java对象。 Mybatis学习门槛低，简单易学，程序员直接编写原生态sql，可严格控制sql执行性能，灵活度高，非常适合对关系数据模型要求不高的软件开发，例如互联网软件、企业运营类软件等，因为这类软件需求变化频繁，一但需求变化要求成果输出迅速。但是灵活的前提是mybatis无法做到数据库无关性，如果需要实现支持多种数据库的软件则需要自定义多套sql映射文件，工作量大。 Hibernate对象/关系映射能力强，数据库无关性好，对于关系模型要求高的软件（例如需求固定的定制化软件）如果用hibernate开发可以节省很多代码，提高效率。但是Hibernate的学习门槛高，要精通门槛更高，而且怎么设计O/R映射，在性能和对象模型之间如何权衡，以及怎样用好Hibernate需要具有很强的经验和能力才行。]]></content>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[整合Struts2+Spring+Hibernate]]></title>
    <url>%2F2018%2F11%2F16%2F%E6%95%B4%E5%90%88Struts2-Spring-Hibernate%2F</url>
    <content type="text"><![CDATA[项目所需依赖包： 单独配置Spring创建配置文件 导入xml约束约束包括beans|context|aop|tx 创建UserAction,并在Spring配置文件中配置.1234567package my.study.web.action;import com.opensymphony.xwork2.ActionSupport;public class UserAction extends ActionSupport &#123;&#125; 1&lt;bean name="userAction" class="my.study.web.action.UserAction"&gt;&lt;/bean&gt; 配置tomcat的web.xml在web.xml中配置让spring随web启动而创建的监听器,并配置spring配置文件配置参数:123456789&lt;!-- 让spring随web启动而创建的监听器 --&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;!-- 配置spring配置文件参数 --&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:applicationContext.xml&lt;/param-value&gt; &lt;/context-param&gt; 此时可以启动服务看看有没有报错，如果没有报错，这一步就配置成功了。 单独配置Struts2配置struts2主配置文件首先创建struts.xml，添加约束,并配置UserAction(约束可以在struts2-core-2.3.35.jar包下的struts-2.3.dtd中查找到)：123456789101112&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE struts PUBLIC "-//Apache Software Foundation//DTD Struts Configuration 2.3//EN" "http://struts.apache.org/dtds/struts-2.3.dtd"&gt; &lt;struts&gt; &lt;package name="ssh" namespace="/" extends="struts-default"&gt; &lt;action name="UserAction_*" class="my.study.web.action.UserAction" method="&#123;1&#125;"&gt; &lt;result name="success"&gt;/success.jsp&lt;/result&gt; &lt;/action&gt; &lt;/package&gt;&lt;/struts&gt; jsp文件自己定义一个放在WebContent目录下即可. 配置struts2核心过滤器到web.xml在web.xml中配置struts2核心过滤器123456789101112...&lt;!-- struts2核心过滤器 --&gt; &lt;filter&gt; &lt;filter-name&gt;struts2&lt;/filter-name&gt; &lt;filter-class&gt;org.apache.struts2.dispatcher.ng.filter.StrutsPrepareAndExecuteFilter&lt;/filter-class&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;struts2&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt;... url-pattern为/*表示所有路径都要经过核心过滤器如果启动服务器没有错误就说明配置成功 Spring和Struts2整合整合的目的是让struts的Action不再自己创建，而是由spring创建(依赖包是struts2-spring-plugin-2.3.35.jar)。 配置常量在struts2-core-2.3.35.jar-&gt;org.apache.struts2-&gt;default.properties中查找常量。 struts.objectFactory = spring是将action的创建交给spring容器struts.objectFactory.spring.autoWire = namespring负责装配Action一般只需配置struts.objectFactory第二个自动开启 在&lt;struts&gt;&lt;/struts&gt;标签中配置&lt;constant name=&quot;struts.objectFactory&quot; value=&quot;spring&quot;&gt; 两种方案创建各个类进行测试：12345678910111213141516package my.study.web.action;import com.opensymphony.xwork2.ActionSupport;import my.study.web.service.UserService;public class UserAction extends ActionSupport &#123; private UserService userService; public void setUserService(UserService userService) &#123; this.userService = userService; &#125; public void login() throws Exception&#123; System.out.println(userService); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344package my.study.web.domain;public class User &#123; private Long user_id; private String user_code; private String user_name; private String user_password; private Character user_state; public Long getUser_id() &#123; return user_id; &#125; public void setUser_id(Long user_id) &#123; this.user_id = user_id; &#125; public String getUser_code() &#123; return user_code; &#125; public void setUser_code(String user_code) &#123; this.user_code = user_code; &#125; public String getUser_name() &#123; return user_name; &#125; public void setUser_name(String user_name) &#123; this.user_name = user_name; &#125; public String getUser_password() &#123; return user_password; &#125; public void setUser_password(String user_password) &#123; this.user_password = user_password; &#125; public Character getUser_state() &#123; return user_state; &#125; public void setUser_state(Character user_state) &#123; this.user_state = user_state; &#125; @Override public String toString() &#123; return "User [user_id=" + user_id + ", user_code=" + user_code + ", user_name=" + user_name + ", user_password=" + user_password + "]"; &#125; &#125; 12345678910111213package my.study.web.impl;import my.study.web.domain.User;import my.study.web.service.UserService;public class UserServiceImpl implements UserService &#123; @Override public User getUserByCodePassword(User u) &#123; System.out.println("getUserByCodePassword"); return null; &#125;&#125; 1234567package my.study.web.service;import my.study.web.domain.User;public interface UserService &#123; User getUserByCodePassword(User u);&#125; 在spring配置文件中配置service:1234 &lt;!-- action配置 --&gt;&lt;bean name="userAction" class="my.study.web.action.UserAction"&gt;&lt;/bean&gt;&lt;!-- servie配置 --&gt;&lt;bean name="userService" class="my.study.web.impl.UserServiceImpl"&gt;&lt;/bean&gt; 整合方案1:class属性上仍然配置action的完整类名，这样struts2仍然创建action,由spring负责组装Action中的依赖属性UserAction中的login()方法就是为了检测是否是由spring装配属性。访问http://localhost:8080/SSH/UserAction_login看到打印结果:当然这种方法并不推荐，最好由spring完整管理action的生命周期。spring中功能才应用到Action上. 整合方案2:spring负责创建action以及组装.首先配置applicationContext.xml中的action，然后配置struts.xml这种方式下在class属性处填写spring中action对象的BeanName.完全由spring管理action生命周期，这样spring需要手动组装依赖属性。即需要在applicationContext.xml中action的&lt;bean&gt;&lt;/bean&gt;中使用&lt;property&gt;&lt;/property&gt;进行手动注入：同时，要注意，Action对象作用范围一定是多例的，这样才符合Struts2的架构。所以将scope属性配置为prototype 访问http://localhost:8080/SSH/UserAction_login依然可以打印出my.study.web.impl.UserServiceImpl至此，struts与spring就整合结束了。 单独配置Hibernate导入实体类和orm元数据User.hbm.xml:12345678910111213141516&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE hibernate-mapping PUBLIC "-//Hibernate/Hibernate Mapping DTD 3.0//EN" "http://www.hibernate.org/dtd/hibernate-mapping-3.0.dtd"&gt;&lt;hibernate-mapping package="my.study.web.domain" &gt; &lt;class name="User" table="sys_user" &gt; &lt;id name="user_id" &gt; &lt;generator class="native"&gt;&lt;/generator&gt; &lt;/id&gt; &lt;property name="user_code" &gt;&lt;/property&gt; &lt;property name="user_name" &gt;&lt;/property&gt; &lt;property name="user_password" &gt;&lt;/property&gt; &lt;property name="user_state" &gt;&lt;/property&gt; &lt;/class&gt;&lt;/hibernate-mapping&gt; Customer.hbm.xml:123456789101112131415161718192021222324252627282930313233343536&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE hibernate-mapping PUBLIC "-//Hibernate/Hibernate Mapping DTD 3.0//EN" "http://www.hibernate.org/dtd/hibernate-mapping-3.0.dtd"&gt;&lt;hibernate-mapping package="my.study.web.domain" &gt; &lt;class name="LinkMan" table="cst_linkman" &gt; &lt;id name="lkm_id" &gt; &lt;generator class="native"&gt;&lt;/generator&gt; &lt;/id&gt; &lt;property name="lkm_gender" &gt;&lt;/property&gt; &lt;property name="lkm_name" &gt;&lt;/property&gt; &lt;property name="lkm_phone" &gt;&lt;/property&gt; &lt;property name="lkm_email" &gt;&lt;/property&gt; &lt;property name="lkm_qq" &gt;&lt;/property&gt; &lt;property name="lkm_mobile" &gt;&lt;/property&gt; &lt;property name="lkm_memo" &gt;&lt;/property&gt; &lt;property name="lkm_position" &gt;&lt;/property&gt; &lt;!-- 多对一 --&gt; &lt;!-- name属性:引用属性名 column属性: 外键列名 class属性: 与我关联的对象完整类名 --&gt; &lt;!-- 级联操作: cascade save-update: 级联保存更新 delete:级联删除 all:save-update+delete 级联操作: 简化操作.目的就是为了少些两行代码. --&gt; &lt;!-- 多的一方: 不能放弃维护关系的.外键字段就在多的一方. --&gt; &lt;many-to-one name="customer" column="lkm_cust_id" class="Customer" &gt; &lt;/many-to-one&gt; &lt;/class&gt;&lt;/hibernate-mapping&gt; Customer.hbm.xml:1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE hibernate-mapping PUBLIC "-//Hibernate/Hibernate Mapping DTD 3.0//EN" "http://www.hibernate.org/dtd/hibernate-mapping-3.0.dtd"&gt; &lt;!-- 配置表与实体对象的关系 --&gt; &lt;!-- package属性:填写一个包名.在元素内部凡是需要书写完整类名的属性,可以直接写简答类名了. --&gt;&lt;hibernate-mapping package="my.study.web.domain" &gt; &lt;!-- class元素: 配置实体与表的对应关系的 name: 完整类名 table:数据库表名 --&gt; &lt;class name="Customer" table="cst_customer" &gt; &lt;!-- id元素:配置主键映射的属性 name: 填写主键对应属性名 column(可选): 填写表中的主键列名.默认值:列名会默认使用属性名 type(可选):填写列(属性)的类型.hibernate会自动检测实体的属性类型. 每个类型有三种填法: java类型|hibernate类型|数据库类型 not-null(可选):配置该属性(列)是否不能为空. 默认值:false length(可选):配置数据库中列的长度. 默认值:使用数据库类型的最大长度 --&gt; &lt;id name="cust_id" &gt; &lt;!-- generator:主键生成策略(明天讲) --&gt; &lt;generator class="native"&gt;&lt;/generator&gt; &lt;/id&gt; &lt;!-- property元素:除id之外的普通属性映射 name: 填写属性名 column(可选): 填写列名 type(可选):填写列(属性)的类型.hibernate会自动检测实体的属性类型. 每个类型有三种填法: java类型|hibernate类型|数据库类型 not-null(可选):配置该属性(列)是否不能为空. 默认值:false length(可选):配置数据库中列的长度. 默认值:使用数据库类型的最大长度 --&gt; &lt;property name="cust_name" column="cust_name" &gt; &lt;!-- &lt;column name="cust_name" sql-type="varchar" &gt;&lt;/column&gt; --&gt; &lt;/property&gt; &lt;property name="cust_source" column="cust_source" &gt;&lt;/property&gt; &lt;property name="cust_industry" column="cust_industry" &gt;&lt;/property&gt; &lt;property name="cust_level" column="cust_level" &gt;&lt;/property&gt; &lt;property name="cust_linkman" column="cust_linkman" &gt;&lt;/property&gt; &lt;property name="cust_phone" column="cust_phone" &gt;&lt;/property&gt; &lt;property name="cust_mobile" column="cust_mobile" &gt;&lt;/property&gt; &lt;/class&gt;&lt;/hibernate-mapping&gt; LinkMan.java:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485package my.study.web.domain;public class LinkMan &#123; private Long lkm_id; private Character lkm_gender; private String lkm_name; private String lkm_phone; private String lkm_email; private String lkm_qq; private String lkm_mobile; private String lkm_memo; private String lkm_position; private Customer customer ; private Long cust_id; public Long getCust_id() &#123; return cust_id; &#125; public void setCust_id(Long cust_id) &#123; this.cust_id = cust_id; &#125; public Customer getCustomer() &#123; return customer; &#125; public void setCustomer(Customer customer) &#123; this.customer = customer; &#125; public Long getLkm_id() &#123; return lkm_id; &#125; public void setLkm_id(Long lkm_id) &#123; this.lkm_id = lkm_id; &#125; public Character getLkm_gender() &#123; return lkm_gender; &#125; public void setLkm_gender(Character lkm_gender) &#123; this.lkm_gender = lkm_gender; &#125; public String getLkm_name() &#123; return lkm_name; &#125; public void setLkm_name(String lkm_name) &#123; this.lkm_name = lkm_name; &#125; public String getLkm_phone() &#123; return lkm_phone; &#125; public void setLkm_phone(String lkm_phone) &#123; this.lkm_phone = lkm_phone; &#125; public String getLkm_email() &#123; return lkm_email; &#125; public void setLkm_email(String lkm_email) &#123; this.lkm_email = lkm_email; &#125; public String getLkm_qq() &#123; return lkm_qq; &#125; public void setLkm_qq(String lkm_qq) &#123; this.lkm_qq = lkm_qq; &#125; public String getLkm_mobile() &#123; return lkm_mobile; &#125; public void setLkm_mobile(String lkm_mobile) &#123; this.lkm_mobile = lkm_mobile; &#125; public String getLkm_memo() &#123; return lkm_memo; &#125; public void setLkm_memo(String lkm_memo) &#123; this.lkm_memo = lkm_memo; &#125; public String getLkm_position() &#123; return lkm_position; &#125; public void setLkm_position(String lkm_position) &#123; this.lkm_position = lkm_position; &#125; &#125; Customer.java:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package my.study.web.domain;public class Customer &#123; private Long cust_id; private String cust_name; private String cust_source; private String cust_industry; private String cust_level; private String cust_linkman; private String cust_phone; private String cust_mobile; public Long getCust_id() &#123; return cust_id; &#125; public void setCust_id(Long cust_id) &#123; this.cust_id = cust_id; &#125; public String getCust_name() &#123; return cust_name; &#125; public void setCust_name(String cust_name) &#123; this.cust_name = cust_name; &#125; public String getCust_source() &#123; return cust_source; &#125; public void setCust_source(String cust_source) &#123; this.cust_source = cust_source; &#125; public String getCust_industry() &#123; return cust_industry; &#125; public void setCust_industry(String cust_industry) &#123; this.cust_industry = cust_industry; &#125; public String getCust_level() &#123; return cust_level; &#125; public void setCust_level(String cust_level) &#123; this.cust_level = cust_level; &#125; public String getCust_linkman() &#123; return cust_linkman; &#125; public void setCust_linkman(String cust_linkman) &#123; this.cust_linkman = cust_linkman; &#125; public String getCust_phone() &#123; return cust_phone; &#125; public void setCust_phone(String cust_phone) &#123; this.cust_phone = cust_phone; &#125; public String getCust_mobile() &#123; return cust_mobile; &#125; public void setCust_mobile(String cust_mobile) &#123; this.cust_mobile = cust_mobile; &#125; @Override public String toString() &#123; return "Customer [cust_id=" + cust_id + ", cust_name=" + cust_name + "]"; &#125;&#125; 在hibernate的配置文件中，不用配置隔离级别，在spring中进行配置。也不需配置session与当前线程绑定的配置，因为spring有管理session的机制。hibernate.cfg.xml:1234567891011121314151617181920212223242526272829303132333435363738&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE hibernate-configuration PUBLIC "-//Hibernate/Hibernate Configuration DTD 3.0//EN" "http://www.hibernate.org/dtd/hibernate-configuration-3.0.dtd"&gt;&lt;hibernate-configuration&gt; &lt;session-factory&gt; &lt;property name="hibernate.connection.driver_class"&gt;com.mysql.jdbc.Driver&lt;/property&gt; &lt;property name="hibernate.connection.url"&gt;jdbc:mysql:///customer1&lt;/property&gt; &lt;property name="hibernate.connection.username"&gt;root&lt;/property&gt; &lt;property name="hibernate.connection.password"&gt;12345&lt;/property&gt; &lt;!-- 数据库方言 注意: MYSQL在选择方言时,请选择最短的方言. --&gt; &lt;property name="hibernate.dialect"&gt;org.hibernate.dialect.MySQLDialect&lt;/property&gt; &lt;!-- #hibernate.show_sql true #hibernate.format_sql true --&gt; &lt;property name="hibernate.show_sql"&gt;true&lt;/property&gt; &lt;property name="hibernate.format_sql"&gt;true&lt;/property&gt; &lt;!-- ## auto schema export 自动导出表结构. 自动建表 #hibernate.hbm2ddl.auto create 自动建表.每次框架运行都会创建新的表.以前表将会被覆盖,表数据会丢失.(开发环境中测试使用) #hibernate.hbm2ddl.auto create-drop 自动建表.每次框架运行结束都会将所有表删除.(开发环境中测试使用) #hibernate.hbm2ddl.auto update(推荐使用) 自动生成表.如果已经存在不会再生成.如果表有变动.自动更新表(不会删除任何数据). #hibernate.hbm2ddl.auto validate 校验.不自动生成表.每次启动会校验数据库中表是否正确.校验失败. --&gt; &lt;property name="hibernate.hbm2ddl.auto"&gt;update&lt;/property&gt; &lt;!-- 引入orm元数据 路径书写: 填写src下的路径 --&gt; &lt;mapping resource="my/study/web/domain/Customer.hbm.xml"/&gt; &lt;mapping resource="my/study/web/domain/LinkMan.hbm.xml"/&gt; &lt;mapping resource="my/study/web/domain/User.hbm.xml"/&gt; &lt;/session-factory&gt;&lt;/hibernate-configuration&gt; 创建类进行测试：1234567891011121314151617181920212223242526272829303132333435package my.study.web.test;import org.hibernate.Session;import org.hibernate.SessionFactory;import org.hibernate.Transaction;import org.hibernate.cfg.Configuration;import org.junit.Test;import my.study.web.domain.User;public class HibernateTest &#123; @Test public void test() &#123; Configuration conf = new Configuration().configure(); SessionFactory sf = conf.buildSessionFactory(); Session session = sf.openSession(); Transaction tx = session.beginTransaction(); User u = new User(); u.setUser_code("hey"); u.setUser_name("哈喽"); u.setUser_password("12345"); session.save(u); tx.commit(); session.close(); sf.close(); &#125;&#125; 单独配置Hibernate就成功了 hibernate和spring整合整合原理：将SessionFactory对象交给spring容器管理 在spring中配置sessionFactory也就是要在配置文件中配置sessionFactory,然后sessionFactory加载配置有两种方案: 1.仍然使用外部的hibernate.cfg.xml配置信息 1234 &lt;!-- 将sessionFactory配置到spring容器中 --&gt;&lt;bean name="sessionFactory" class="org.springframework.orm.hibernate5.LocalSessionFactoryBean"&gt; &lt;property name="configLocation" value="classpath:hibernate.cfg.xml"&gt;&lt;/property&gt;&lt;/bean&gt; 注意选择和自己的hibernate版本一样的(我的是hibernate5) 书写代码进行测试，直接使用spring注入的sessionFactory: 1234567891011121314151617181920212223242526272829303132333435363738394041424344package my.study.web.test;import javax.annotation.Resource;import org.hibernate.Session;import org.hibernate.SessionFactory;import org.hibernate.Transaction;import org.hibernate.cfg.Configuration;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.test.context.ContextConfiguration;import org.springframework.test.context.junit4.SpringJUnit4ClassRunner;import my.study.web.domain.User;@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration("classpath:applicationContext.xml")public class HibernateTest &#123; @Resource(name="sessionFactory") private SessionFactory sf; @Test public void test1() &#123; .... &#125; @Test public void test2() &#123; Session session = sf.openSession(); Transaction tx = session.beginTransaction(); User u = new User(); u.setUser_code("hello"); u.setUser_name("你好"); u.setUser_password("8765"); session.save(u); tx.commit(); session.close(); &#125;&#125; 其中 sessionFactory的关闭可以交给spring来管理,所以close()方法可以不用写 2.在spring配置中放置hibernate配置信息第二种配置方法就是在spring中配置hibernate配置信息，包括必选配置，可选配置和引入orm元数据。12345678910111213141516171819&lt;bean name="sessionFactory" class="org.springframework.orm.hibernate5.LocalSessionFactoryBean"&gt; &lt;!-- 配置hibernate基本信息 --&gt; &lt;property name="hibernateProperties"&gt; &lt;props&gt; &lt;!-- 必选配置 --&gt; &lt;prop key="hibernate.connection.driver_class"&gt;com.mysql.jdbc.Driver&lt;/prop&gt; &lt;prop key="hibernate.connection.url"&gt;jdbc:mysql:///customer1&lt;/prop&gt; &lt;prop key="hibernate.connection.username"&gt;root&lt;/prop&gt; &lt;prop key="hibernate.connection.password"&gt;12345&lt;/prop&gt; &lt;prop key="hibernate.dialect"&gt;org.hibernate.dialect.MySQLDialect&lt;/prop&gt; &lt;!-- 可选配置 --&gt; &lt;prop key="hibernate.show_sql"&gt;true&lt;/prop&gt; &lt;prop key="hibernate.format_sql"&gt;true&lt;/prop&gt; &lt;prop key="hibernate.hbm2ddl.auto"&gt;update&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;!-- 引入orm元数据，指定orm元数据所在的包路径，spring会读取包中的所有配置 --&gt; &lt;property name="mappingDirectoryLocations" value="classpath:my/study/web/domain"&gt;&lt;/property&gt;&lt;/bean&gt; 将方法1中的配置换成上面的配置，进行测试:这样就完成了spring和hibernate的整合 重点总结，整合原理就是将SessionFactory对象交给spring容器管理 spring整合c3p0连接池c3p0的配置如下：123456789&lt;!-- 读取db.properties --&gt;&lt;context:property-placeholder location="classpath:db.properties"/&gt;&lt;!-- 配置c3p0连接池 --&gt;&lt;bean name="dataSource" class="com.mchange.v2.c3p0.ComboPooledDataSource"&gt; &lt;property name="JdbcUrl" value="$&#123;jdbc.JdbcUrl&#125;"&gt;&lt;/property&gt; &lt;property name="DriverClass" value="$&#123;jdbc.DriverClass&#125;"&gt;&lt;/property&gt; &lt;property name="User" value="$&#123;jdbc.User&#125;"&gt;&lt;/property&gt; &lt;property name="Password" value="$&#123;jdbc.Password&#125;"&gt;&lt;/property&gt;&lt;/bean&gt; 将c3p0注入到sessionFactory中,hibernate会通过连接池获得连接:12345678910111213141516&lt;bean name="sessionFactory" class="org.springframework.orm.hibernate5.LocalSessionFactoryBean"&gt; &lt;!-- 配置hibernate基本信息 --&gt; &lt;!-- 将连接池注入到sessionFactory中 --&gt; &lt;property name="dataSource" ref="dataSource"&gt;&lt;/property&gt; &lt;property name="hibernateProperties"&gt; &lt;props&gt; &lt;prop key="hibernate.dialect"&gt;org.hibernate.dialect.MySQLDialect&lt;/prop&gt; &lt;!-- 可选配置 --&gt; &lt;prop key="hibernate.show_sql"&gt;true&lt;/prop&gt; &lt;prop key="hibernate.format_sql"&gt;true&lt;/prop&gt; &lt;prop key="hibernate.hbm2ddl.auto"&gt;update&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;!-- 引入orm元数据，指定orm元数据所在的包路径，spring会读取包中的所有配置 --&gt; &lt;property name="mappingDirectoryLocations" value="classpath:my/study/web/domain"&gt;&lt;/property&gt; &lt;/bean&gt; spring整合hibernate环境操作数据库先创建测试类。创建UserDao接口：12345678910package my.study.web.dao;import my.study.web.domain.User;public interface UserDao &#123; //根据登陆名称查询user对象 User getByUserCode(String usercode); //保存用户 void save(User u);&#125; 然后创建UserDaoImpl实现类，在整合环境中操作数据库，spring提供了一个hibernate模板对象HibernateTemplate.可以让实现类继承HibernateDaoSupport,从而简少Dao的依赖关系,不用再去配置HibernateTemplate.这样在使用模板时，直接使用getHibernateTemplate()方法就可以了.要明确其中的依赖关系：HibernateTemplate通过HibernateDaoSupport创建出来，需要依赖SessionFactory,这个模板中封装的操作都是session中的操作.所以要先为dao注入SessionFactory.1234567891011121314151617181920212223242526272829303132333435363738394041package my.study.web.impl;import org.hibernate.HibernateException;import org.hibernate.Query;import org.hibernate.Session;import org.springframework.orm.hibernate5.HibernateCallback;import org.springframework.orm.hibernate5.support.HibernateDaoSupport;import my.study.web.dao.UserDao;import my.study.web.domain.User;//HibernateDaoSupport 要为dao注入sessionFactorypublic class UserDaoImpl extends HibernateDaoSupport implements UserDao &#123; @Override public User getByUserCode(final String usercode) &#123; //HQL return getHibernateTemplate().execute(new HibernateCallback&lt;User&gt;() &#123; @Override public User doInHibernate(Session session) throws HibernateException &#123; String hql = "from User where user_code = ? "; Query query = session.createQuery(hql); query.setParameter(0, usercode); User user = (User) query.uniqueResult(); return user; &#125; &#125;); //Criteria /*DetachedCriteria dc = DetachedCriteria.forClass(User.class); dc.add(Restrictions.eq("user_code", usercode)); List&lt;User&gt; list = (List&lt;User&gt;) getHibernateTemplate().findByCriteria(dc); if(list != null &amp;&amp; list.size()&gt;0)&#123; return list.get(0); &#125;else&#123; return null; &#125;*/ &#125;&#125; 在spring中配置UserDao,并注入sessionFactory：12345&lt;!-- 配置UserDao --&gt;&lt;bean name="userDao" class="my.study.web.impl.UserDaoImpl"&gt; &lt;!-- 注入sessionFactory --&gt; &lt;property name="sessionFacroty" ref="sessionFactory"&gt;&lt;/property&gt;&lt;/bean&gt; 在HibernateTest.java中添加测试代码：12345678910...@Resource(name="userDao") private UserDao ud; @Test //测试Dao Hibernate模板 public void test3() &#123; User user = ud.getByUserCode("hello"); System.out.println(user); &#125; Spring中的aop事务准备配置事务，最核心的是要配置核心事务管理器.其中包含了所有事务的打开提交关闭操作.所以要先在spring配置核心事务管理器.ctrl+shift+h,搜索一下TransactionManager,可以看到HibernateTransactionManager就是需要的: 将其配置到spring配置中,并且，使用它管理事务，要通过sessionFactory进行管理，所以要配置注入sessionFactory1234&lt;!-- 配置核心事务管理器 --&gt; &lt;bean name="transactionManager" class="org.springframework.orm.hibernate5.HibernateTransactionManager"&gt; &lt;property name="sessionFactory" ref="sessionFactory"&gt;&lt;/property&gt; &lt;/bean&gt; xml配置aop事务准备目标对象-&gt;配置通知-&gt;将配置织入目标对象(配置切点和切面)。在前面已经有了目标对象，所以这里只需要配置通知和织入目标对象.具体可见博客上一篇文章123456789101112131415161718192021&lt;!-- 配置通知 --&gt;&lt;tx:advice id="txAdvice" transaction-manager="transactionManager"&gt; &lt;tx:attributes&gt; &lt;tx:method name="save*" isolation="REPEATABLE_READ" propagation="REQUIRED" read-only="false" /&gt; &lt;tx:method name="persist*" isolation="REPEATABLE_READ" propagation="REQUIRED" read-only="false" /&gt; &lt;tx:method name="update*" isolation="REPEATABLE_READ" propagation="REQUIRED" read-only="false" /&gt; &lt;tx:method name="modify*" isolation="REPEATABLE_READ" propagation="REQUIRED" read-only="false" /&gt; &lt;tx:method name="delete*" isolation="REPEATABLE_READ" propagation="REQUIRED" read-only="false" /&gt; &lt;tx:method name="remove*" isolation="REPEATABLE_READ" propagation="REQUIRED" read-only="false" /&gt; &lt;tx:method name="get*" isolation="REPEATABLE_READ" propagation="REQUIRED" read-only="true" /&gt; &lt;tx:method name="find*" isolation="REPEATABLE_READ" propagation="REQUIRED" read-only="true" /&gt; &lt;/tx:attributes&gt;&lt;/tx:advice&gt;&lt;!-- 配置将通知织入目标对象 --&gt;&lt;aop:config&gt; &lt;!-- 配置切点 --&gt; &lt;aop:pointcut expression="execution(* my.study.web.impl.*ServiceImpl.*(..))" id="transactionPoincut"/&gt; &lt;!-- 配置切面 --&gt; &lt;aop:advisor advice-ref="txAdvice" pointcut-ref="transactionPoincut"/&gt;&lt;/aop:config&gt; 测试：在UserService接口中添加方法:1void saveUser(User u); 在UserDao接口中添加方法:12//保存用户 void save(User u); 在UserDaoImpl中添加方法:1234@Overridepublic void save(User u) &#123; getHibernateTemplate().save(u);&#125; 在UserServiceImpl中添加方法:123456789private UserDao ud;@Overridepublic void saveUser(User u) &#123; ud.save(u); &#125;//为ud添加setter方法public void setUd(UserDao ud) &#123; this.ud = ud;&#125; 在此之前，要在spring中的userService下将userDao注入进去:1234&lt;!-- servie配置 --&gt; &lt;bean name="userService" class="my.study.web.impl.UserServiceImpl"&gt; &lt;property name="ud" ref="userDao"&gt;&lt;/property&gt; &lt;/bean&gt; 测试代码：123456789101112@Resource(name="userService")private UserService us;@Test//测试aop事务public void test4() &#123; User u = new User(); u.setUser_code("aoptest"); u.setUser_name("aop测试"); u.setUser_password("aop123"); us.saveUser(u);&#125; 注解配置aop事务在spring配置文件中，首先开启注解事务:12&lt;!-- 开启注解事务 --&gt;&lt;tx:annotation-driven transaction-manager="transactionManager"/&gt; 改写UserServiceImpl:12345678910111213141516171819202122232425262728293031package my.study.web.impl;import org.springframework.transaction.annotation.Isolation;import org.springframework.transaction.annotation.Propagation;import org.springframework.transaction.annotation.Transactional;import my.study.web.dao.UserDao;import my.study.web.domain.User;import my.study.web.service.UserService;@Transactional(isolation=Isolation.REPEATABLE_READ,propagation=Propagation.REQUIRED,readOnly=true)public class UserServiceImpl implements UserService &#123; private UserDao ud; @Override public User getUserByCodePassword(User u) &#123; System.out.println("getUserByCodePassword"); return null; &#125; @Override @Transactional(isolation=Isolation.REPEATABLE_READ,propagation=Propagation.REQUIRED,readOnly=false) public void saveUser(User u) &#123; ud.save(u); &#125; public void setUd(UserDao ud) &#123; this.ud = ud; &#125; &#125; 测试:123456789101112@Resource(name="userService") private UserService us; @Test //测试aop事务 public void test4() &#123; User u = new User(); u.setUser_code("aop注解test"); u.setUser_name("aop测试注解"); u.setUser_password("aop123"); us.saveUser(u); &#125; 扩大session作用范围为了避免使用懒加载时出现no-session问题.需要扩大session的作用范围 在web.xml配置filter:123456789101112 &lt;!-- 扩大session作用域范围 注意:任何filter一定要在struts的filter之前调用 --&gt;&lt;filter&gt; &lt;filter-name&gt;openSessionInView&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.orm.hibernate5.support.OpenSessionInViewFilter&lt;/filter-class&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;openSessionInView&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 注意:任何filter一定要在struts的filter之前调用,所以要放在struts配置前面 配置完毕 实现用户登陆功能Web层改写UserAction方法:12345678910111213141516171819202122232425262728293031package my.study.web.action;import com.opensymphony.xwork2.ActionContext;import com.opensymphony.xwork2.ActionSupport;import com.opensymphony.xwork2.ModelDriven;import my.study.web.domain.User;import my.study.web.service.UserService;public class UserAction extends ActionSupport implements ModelDriven&lt;User&gt; &#123; private User user = new User(); private UserService userService; public void setUserService(UserService userService) &#123; this.userService = userService; &#125; public String login() throws Exception&#123; //1. 调用service执行登陆逻辑 User u = userService.getUserByCodePassword(user); //2. 将返回的user对象放到session域中 ActionContext.getContext().getSession().put("user", u); //3. 重定向到项目首页 return "toHome"; &#125; @Override public User getModel() &#123; return user; &#125;&#125; 改写struts.xml配置，改变跳转地址:1234567891011121314&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE struts PUBLIC "-//Apache Software Foundation//DTD Struts Configuration 2.3//EN" "http://struts.apache.org/dtds/struts-2.3.dtd"&gt; &lt;struts&gt; &lt;constant name="struts.objectFactory" value="spring"&gt; &lt;/constant&gt; &lt;package name="ssh" namespace="/" extends="struts-default"&gt; &lt;action name="UserAction_*" class="my.study.web.action.UserAction" method="&#123;1&#125;"&gt; &lt;result name="toHome"&gt;/index.html&lt;/result&gt; &lt;/action&gt; &lt;/package&gt;&lt;/struts&gt; Service层改写UserServiceImpl中的方法:12345678910111213141516171819... @Override public User getUserByCodePassword(User u) &#123; //1. 根据登录名称查询登陆用户 User exitU = ud.getByUserCode(u.getUser_code()); //2. 判断用户是否存在-&gt;不存在，抛出异常，提示用户名不存在 // -&gt;存在 // -&gt;密码错误，抛出异常，提示密码错误 if(exitU==null) &#123; throw new RuntimeException("用户不存在！"); &#125; if(!exitU.getUser_password().equals(u.getUser_password())) &#123; throw new RuntimeException("密码错误！"); &#125; //3.返回查询到的用户对象 return exitU; &#125;... 注意如果需要抛出异常，需要在struts.xml中配置:12345678910111213&lt;struts&gt; &lt;constant name="struts.objectFactory" value="spring"&gt; &lt;/constant&gt; &lt;package name="ssh" namespace="/" extends="struts-default"&gt; &lt;global-exception-mappings &gt; &lt;exception-mapping result="error" exception="java.lang.RuntimeException"&gt;&lt;/exception-mapping&gt; &lt;/global-exception-mappings&gt; &lt;action name="UserAction_*" class="my.study.web.action.UserAction" method="&#123;1&#125;"&gt; &lt;result name="toHome"&gt;/index.html&lt;/result&gt; &lt;result name="error"&gt;/login.jsp&lt;/result&gt; &lt;/action&gt; &lt;/package&gt;&lt;/struts&gt; dao层dao层在上面已经实现好了 测试用的页面login.jsp:12345678910111213141516171819&lt;%@ page language="java" contentType="text/html; charset=UTF-8" pageEncoding="ISO-8859-1"%&gt;&lt;%@ taglib prefix="s" uri="/struts-tags" %&gt;&lt;!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd"&gt;&lt;html&gt;&lt;head&gt;&lt;meta http-equiv="Content-Type" content="text/html; charset=UTF-8"&gt;&lt;title&gt;Insert title here&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;form action="$&#123;pageContext.request.contextPath&#125;/UserAction_login" method=post&gt; &lt;label&gt;username:&lt;/label&gt;&lt;input type="text" name="user_code"/&gt; &lt;br&gt; &lt;label&gt;password:&lt;/label&gt;&lt;input type=password name="user_password"/&gt; &lt;input type="submit" value="submit"/&gt; &lt;/form&gt; &lt;font color="red"&gt;&lt;s:property value="exception.message"&gt;&lt;/s:property&gt;&lt;/font&gt;&lt;/body&gt;&lt;/html&gt; 12345678910&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset="UTF-8"&gt;&lt;title&gt;Insert title here&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;首页&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt; 项目总览]]></content>
      <tags>
        <tag>学习笔记</tag>
        <tag>Spring</tag>
        <tag>SSH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring学习笔记-三]]></title>
    <url>%2F2018%2F11%2F13%2FSpring%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%B8%89%2F</url>
    <content type="text"><![CDATA[Spring结合JDBC其实，Spring中提供了一个可以操作数据库的对象，这个对象封装了各种JDBC技术，可以用它来操作数据库.——JDBCtemplate需要的依赖包： 不使用Spring操作数据库123456789101112131415161718192021222324252627package my.study.g_jdbcTemplate;import java.beans.PropertyVetoException;import org.junit.Test;import org.springframework.jdbc.core.JdbcTemplate;import com.mchange.v2.c3p0.ComboPooledDataSource;public class Demo &#123; @Test public void test1() throws Exception &#123; //1. 准备连接池 ComboPooledDataSource dataSource = new ComboPooledDataSource(); dataSource.setDriverClass("com.mysql.jdbc.Driver"); dataSource.setJdbcUrl("jdbc:mysql:///你的数据库名"); dataSource.setUser("root"); dataSource.setPassword("12345"); //2. 创建JDBC模板对象 JdbcTemplate jt = new JdbcTemplate(); jt.setDataSource(dataSource); //3. 书写SQL语句 String sql = "insert into t_user values(null,'hey')"; jt.update(sql); &#125;&#125; 可以去数据库中自己查看是否添加成功 使用Spring操作数据库1.使用JDBC模板完成增删改查定义UserDao接口123456789101112package my.study.g_jdbcTemplate;import java.util.List;public interface UserDao &#123; void save(User u); void delete(Integer id); void update(User u); User getById(Integer id); int getTotalCount(); List&lt;User&gt; getAll();&#125; 定义User类123456789101112131415161718package my.study.g_jdbcTemplate;public class User &#123; private Integer id; private String name; public Integer getId() &#123; return id; &#125; public void setId(Integer id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125;&#125; 定义UserDaoImpl类，书写方法:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475package my.study.g_jdbcTemplate;import java.sql.ResultSet;import java.sql.SQLException;import java.util.List;import org.springframework.jdbc.core.JdbcTemplate;import org.springframework.jdbc.core.RowMapper;public class UserDaoImpl implements UserDao &#123; //定义一个JdbcTemplate用来对SQL语句进行操作 private JdbcTemplate jt; @Override public void save(User u) &#123; String sql = "insert into t_user values(null,?)"; jt.update(sql, u.getName()); &#125; @Override public void delete(Integer id) &#123; String sql = "delete from t_user where id = ?"; jt.update(sql, id); &#125; @Override public void update(User u) &#123; String sql = "update t_user set name = ? where id = ?"; jt.update(sql, u.getName(),u.getId()); &#125; @Override public User getById(Integer id) &#123; String sql = "select * from t_user where id = ?"; User u = jt.queryForObject(sql, new RowMapper&lt;User&gt;() &#123; @Override public User mapRow(ResultSet rs, int arg1) throws SQLException &#123; User u = new User(); u.setId(rs.getInt("id")); u.setName(rs.getString("name")); return u; &#125; &#125;, id); return u; &#125; @Override public int getTotalCount() &#123; String sql = "select count(*) from t_user"; Integer count = jt.queryForObject(sql,Integer.class); return count; &#125; @Override public List&lt;User&gt; getAll() &#123; String sql = "select * from t_user"; List&lt;User&gt; list = jt.query(sql, new RowMapper&lt;User&gt;() &#123; @Override public User mapRow(ResultSet rs, int arg1) throws SQLException &#123; User u = new User(); u.setId(rs.getInt("id")); u.setName(rs.getString("name")); return u; &#125; &#125;); return list; &#125; public void setJt(JdbcTemplate jt) &#123; this.jt = jt; &#125;&#125; 然后需要配置到Spring中，让Spring管理：在配置文件中配置：123456789101112131415161718&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.2.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.2.xsd &quot;&gt; &lt;!-- 1.将DataSource连接池放入Spring容器 --&gt; &lt;bean name=&quot;dataSource&quot; class=&quot;com.mchange.v2.c3p0.ComboPooledDataSource&quot;&gt; &lt;property name=&quot;JdbcUrl&quot; value=&quot;jdbc:mysql:///hibernate_32&quot;&gt;&lt;/property&gt; &lt;property name=&quot;DriverClass&quot; value=&quot;com.mysql.jdbc.Driver&quot;&gt;&lt;/property&gt; &lt;property name=&quot;User&quot; value=&quot;root&quot;&gt;&lt;/property&gt; &lt;property name=&quot;Password&quot; value=&quot;12345&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 2.将JDBCTemplate放入Spring容器 --&gt; &lt;bean name=&quot;JdbcTemplate&quot; class=&quot;org.springframework.jdbc.core.JdbcTemplate&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 3.将UserDao放入Spring容器 --&gt; &lt;bean name=&quot;userDao&quot; class=&quot;my.study.g_jdbcTemplate.UserDaoImpl&quot;&gt; &lt;property name=&quot;jt&quot; ref=&quot;JdbcTemplate&quot;&gt;&lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; 测试代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172package my.study.g_jdbcTemplate;import java.beans.PropertyVetoException;import javax.annotation.Resource;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.jdbc.core.JdbcTemplate;import org.springframework.test.context.ContextConfiguration;import org.springframework.test.context.junit4.SpringJUnit4ClassRunner;import com.mchange.v2.c3p0.ComboPooledDataSource;@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration("classpath:my/study/g_jdbcTemplate/applicationContext.xml")public class Demo &#123; @Resource(name="userDao") private UserDao ud; @Test public void test1() throws Exception &#123; //1. 准备连接池 ComboPooledDataSource dataSource = new ComboPooledDataSource(); dataSource.setDriverClass("com.mysql.jdbc.Driver"); dataSource.setJdbcUrl("jdbc:mysql:///hibernate_32"); dataSource.setUser("root"); dataSource.setPassword("12345"); //2. 创建JDBC模板对象 JdbcTemplate jt = new JdbcTemplate(); jt.setDataSource(dataSource); //3. 书写SQL语句 String sql = "insert into t_user values(null,'hey')"; jt.update(sql); &#125; @Test public void test2() throws Exception &#123; User u = new User(); u.setName("hello"); ud.save(u); &#125; @Test public void test3() throws Exception &#123; User u = new User(); u.setId(2); u.setName("nihao"); ud.update(u); &#125; @Test public void test4() throws Exception &#123; ud.delete(2); &#125; @Test public void test5() throws Exception &#123; int totalCount = ud.getTotalCount(); System.out.println(totalCount); &#125; @Test public void test6() throws Exception &#123; System.out.println(ud.getById(2)); &#125; @Test public void test7() throws Exception &#123; System.out.println(ud.getAll()); &#125;&#125; 另外，可以让UserDaoImpl继承JDBCDaoSupport，父类可以根据连接池自己创建JDBC模板.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package my.study.g_jdbcTemplate;import java.sql.ResultSet;import java.sql.SQLException;import java.util.List;import org.springframework.jdbc.core.JdbcTemplate;import org.springframework.jdbc.core.RowMapper;import org.springframework.jdbc.core.support.JdbcDaoSupport;public class UserDaoImpl extends JdbcDaoSupport implements UserDao &#123; @Override public void save(User u) &#123; String sql = "insert into t_user values(null,?) "; super.getJdbcTemplate().update(sql, u.getName()); &#125; @Override public void delete(Integer id) &#123; String sql = "delete from t_user where id = ? "; super.getJdbcTemplate().update(sql,id); &#125; @Override public void update(User u) &#123; String sql = "update t_user set name = ? where id=? "; super.getJdbcTemplate().update(sql, u.getName(),u.getId()); &#125; @Override public User getById(Integer id) &#123; String sql = "select * from t_user where id = ? "; return super.getJdbcTemplate().queryForObject(sql,new RowMapper&lt;User&gt;()&#123; @Override public User mapRow(ResultSet rs, int arg1) throws SQLException &#123; User u = new User(); u.setId(rs.getInt("id")); u.setName(rs.getString("name")); return u; &#125;&#125;, id); &#125; @Override public int getTotalCount() &#123; String sql = "select count(*) from t_user "; Integer count = super.getJdbcTemplate().queryForObject(sql, Integer.class); return count; &#125; @Override public List&lt;User&gt; getAll() &#123; String sql = "select * from t_user "; List&lt;User&gt; list = super.getJdbcTemplate().query(sql, new RowMapper&lt;User&gt;()&#123; @Override public User mapRow(ResultSet rs, int arg1) throws SQLException &#123; User u = new User(); u.setId(rs.getInt("id")); u.setName(rs.getString("name")); return u; &#125;&#125;); return list; &#125;&#125; 那么在配置文件中可以简化配置，不需要配置JDBCTemplate。1234567891011121314&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.2.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.2.xsd &quot;&gt; &lt;bean name=&quot;dataSource&quot; class=&quot;com.mchange.v2.c3p0.ComboPooledDataSource&quot;&gt; &lt;property name=&quot;JdbcUrl&quot; value=&quot;jdbc:mysql:///hibernate_32&quot;&gt;&lt;/property&gt; &lt;property name=&quot;DriverClass&quot; value=&quot;com.mysql.jdbc.Driver&quot;&gt;&lt;/property&gt; &lt;property name=&quot;User&quot; value=&quot;root&quot;&gt;&lt;/property&gt; &lt;property name=&quot;Password&quot; value=&quot;12345&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;bean name=&quot;userDao&quot; class=&quot;my.study.g_jdbcTemplate.UserDaoImpl&quot; &gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot; &gt;&lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; Spring中整合properties配置:新建db.properties：1234JdbcUrl=jdbc:mysql:///hibernate_32DriverClass=com.mysql.jdbc.DriverUser=rootPassword=12345 更改配置文件中的配置:1234567891011... &lt;!-- 指定Properties的位置 --&gt; &lt;context:property-placeholder location=&quot;classpath:db.properties&quot;/&gt; &lt;!-- 1.将DataSource连接池放入Spring容器 --&gt; &lt;bean name=&quot;dataSource&quot; class=&quot;com.mchange.v2.c3p0.ComboPooledDataSource&quot;&gt; &lt;property name=&quot;JdbcUrl&quot; value=&quot;$&#123;jdbc.JdbcUrl&#125;&quot;&gt;&lt;/property&gt; &lt;property name=&quot;DriverClass&quot; value=&quot;$&#123;jdbc.DriverClass&#125;&quot;&gt;&lt;/property&gt; &lt;property name=&quot;User&quot; value=&quot;$&#123;jdbc.User&#125;&quot;&gt;&lt;/property&gt; &lt;property name=&quot;Password&quot; value=&quot;$&#123;jdbc.Password&#125;&quot;&gt;&lt;/property&gt; &lt;/bean&gt;... Spring中的aop事务事务相关概念事务的特性：ACID ⑴ 原子性（Atomicity） 原子性是指事务包含的所有操作要么全部成功，要么全部失败回滚，因此事务的操作如果成功就必须要完全应用到数据库，如果操作失败则不能对数据库有任何影响。 ⑵ 一致性（Consistency） 一致性是指事务必须使数据库从一个一致性状态变换到另一个一致性状态，也就是说一个事务执行之前和执行之后都必须处于一致性状态。 拿转账来说，假设用户A和用户B两者的钱加起来一共是5000，那么不管A和B之间如何转账，转几次账，事务结束后两个用户的钱相加起来应该还得是5000，这就是事务的一致性。 ⑶ 隔离性（Isolation） 隔离性是当多个用户并发访问数据库时，比如操作同一张表时，数据库为每一个用户开启的事务，不能被其他事务的操作所干扰，多个并发事务之间要相互隔离。 即要达到这么一种效果：对于任意两个并发的事务T1和T2，在事务T1看来，T2要么在T1开始之前就已经结束，要么在T1结束之后才开始，这样每个事务都感觉不到有其他事务在并发地执行。 关于事务的隔离性数据库提供了多种隔离级别。 ⑷ 持久性（Durability） 持久性是指一个事务一旦被提交了，那么对数据库中的数据的改变就是永久性的，即便是在数据库系统遇到故障的情况下也不会丢失提交事务的操作。 例如我们在使用JDBC操作数据库时，在提交事务方法后，提示用户事务操作完成，当我们程序执行完成直到看到提示后，就可以认定事务以及正确提交，即使这时候数据库出现了问题，也必须要将我们的事务完全执行完成，否则就会造成我们看到提示事务处理完毕，但是数据库因为故障而没有执行事务的重大错误。 事务并发问题： 1.脏读 脏读是指在一个事务处理过程里读取了另一个未提交的事务中的数据。2.不可重复读 不可重复读是指在对于数据库中的某个数据，一个事务范围内多次查询却返回了不同的数据值，这是由于在查询间隔，被另一个事务修改并提交了。 例如事务T1在读取某一数据，而事务T2立马修改了这个数据并且提交事务给数据库，事务T1再次读取该数据就得到了不同的结果，发送了不可重复读。 不可重复读和脏读的区别是，脏读是某一事务读取了另一个事务未提交的脏数据，而不可重复读则是读取了前一事务提交的数据。3.虚读(幻读) 幻读是事务非独立执行时发生的一种现象。例如事务T1对一个表中所有的行的某个数据项做了从“1”修改为“2”的操作，这时事务T2又对这个表中插入了一行数据项，而这个数据项的数值还是为“1”并且提交给数据库。而操作事务T1的用户如果再查看刚刚修改的数据，会发现还有一行没有修改，其实这行是从事务T2中添加的，就好像产生幻觉一样，这就是发生了幻读。 幻读和不可重复读都是读取了另一条已经提交的事务（这点就脏读不同），所不同的是不可重复读查询的都是同一个数据项，而幻读针对的是一批数据整体（比如数据的个数）。 现在来看看MySQL数据库为我们提供的四种隔离级别： ① Serializable (串行化)：可避免脏读、不可重复读、幻读的发生。 ② Repeatable read (可重复读)：可避免脏读、不可重复读的发生。 ③ Read committed (读已提交)：可避免脏读的发生。 ④ Read uncommitted (读未提交)：最低级别，任何情况都无法保证。 以上四种隔离级别最高的是Serializable级别，最低的是Read uncommitted级别，当然级别越高，执行效率就越低。像Serializable这样的级别，就是以锁表的方式(类似于Java多线程中的锁)使得其他的线程只能在锁外等待，所以平时选用何种隔离级别应该根据实际情况。在MySQL数据库中默认的隔离级别为Repeatable read (可重复读)。 Spring提供了PlatformTransactionManager接口，封装了事务操作对象的方法.对不同的平台，有不同的实现类.如:JDBC平台–DataSourceTransactionManager;Hibernate平台—TransactionManager..等.在Spring中使用事务管理，最核心的就是使用PlatformTransactionManager对象。 Spring事务管理可以管理事务的隔离级别、是否只读、事务的传播行为(比如在一个service方法中调用另一个service方法)、 事务的传播行为包括： 保证同一个事务中 PROPAGATION_REQUIRED 支持当前事务，如果不存在 就新建一个(默认) PROPAGATION_SUPPORTS 支持当前事务，如果不存在，就不使用事务 PROPAGATION_MANDATORY 支持当前事务，如果不存在，抛出异常 保证没有在同一个事务中 PROPAGATION_REQUIRES_NEW 如果有事务存在，挂起当前事务，创建一个新的事务 PROPAGATION_NOT_SUPPORTED 以非事务方式运行，如果有事务存在，挂起当前事务 PROPAGATION_NEVER 以非事务方式运行，如果有事务存在，抛出异常 PROPAGATION_NESTED 如果当前事务存在，则嵌套事务执行 定义场景并书写代码不添加事务部分定义一个转钱的场景，自行建一个表，包含id,姓名和钱数。定义接口,包含加钱和减钱两个方法：1234567package my.study.dao;public interface AccountDao &#123; public void increaseMoney(Integer id,Double money); public void decreaseMoney(Integer id,Double money);&#125; 定义实现类：12345678910111213141516package my.study.dao;import org.springframework.jdbc.core.support.JdbcDaoSupport;public class AccountDaoImpl extends JdbcDaoSupport implements AccountDao &#123; @Override public void increaseMoney(Integer id, Double money) &#123; getJdbcTemplate().update("update t_account set money = money + ? where id = ?", money,id); &#125; @Override public void decreaseMoney(Integer id, Double money) &#123; getJdbcTemplate().update("update t_account set money = money-? where id = ? ", money,id); &#125;&#125; 定义service方法,包含转账方法：12345package my.study.service;public interface AccountService &#123; void transfer(Integer from,Integer to,Double money);&#125; 定义service方法实现类：123456789101112131415161718package my.study.service;import my.study.dao.AccountDao;public class AccountServiceImpl implements AccountService &#123; private AccountDao ad ; @Override public void transfer(final Integer from,final Integer to,final Double money) &#123; //减钱 ad.decreaseMoney(from, money); //加钱 ad.increaseMoney(to, money); &#125; public void setAd(AccountDao ad) &#123; this.ad = ad; &#125;&#125; 书写配置文件:1234567891011121314151617181920&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.2.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.2.xsd &quot;&gt; &lt;!-- 指定Properties的位置 --&gt; &lt;context:property-placeholder location=&quot;classpath:db.properties&quot;/&gt; &lt;!-- 1.将DataSource连接池放入Spring容器 --&gt; &lt;bean name=&quot;dataSource&quot; class=&quot;com.mchange.v2.c3p0.ComboPooledDataSource&quot;&gt; &lt;property name=&quot;JdbcUrl&quot; value=&quot;$&#123;jdbc.JdbcUrl&#125;&quot;&gt;&lt;/property&gt; &lt;property name=&quot;DriverClass&quot; value=&quot;$&#123;jdbc.DriverClass&#125;&quot;&gt;&lt;/property&gt; &lt;property name=&quot;User&quot; value=&quot;$&#123;jdbc.User&#125;&quot;&gt;&lt;/property&gt; &lt;property name=&quot;Password&quot; value=&quot;$&#123;jdbc.Password&#125;&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 2.AccountDao --&gt; &lt;bean name=&quot;accountDao&quot; class=&quot;my.study.dao.AccountDaoImpl&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 3.将AccountService放入Spring容器 --&gt; &lt;bean name=&quot;accountService&quot; class=&quot;my.study.service.AccountServiceImpl&quot;&gt; &lt;property name=&quot;ad&quot; ref=&quot;accountDao&quot;&gt;&lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; 书写代码测试:1234567891011121314151617181920package my.study.dao;import javax.annotation.Resource;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.test.context.ContextConfiguration;import org.springframework.test.context.junit4.SpringJUnit4ClassRunner;import my.study.service.AccountService;@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration("classpath:my/study/dao/applicationContext.xml")public class Demo &#123; @Resource(name="accountService") private AccountService as; @Test public void test1()&#123; as.transfer(1, 2, 100d); &#125;&#125; 刷新查看数据库结果: 这是没有添加事务的程序.没有添加事务，就容易发生各种问题(前面提到的事务的问题)。 添加事务首先需要配置核心事务管理器,它依赖于连接池：123456... &lt;!-- 事务核心管理器,封装了所有事务操作. 依赖于连接池 --&gt; &lt;bean name=&quot;transactionManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot; &gt;&lt;/property&gt; &lt;/bean&gt;... 编码式需要在代码中进行事务管理，需要书写重复代码。并不推荐.编码式需要事务模板对象，在xml中配置,要配置tt属性:12345678910...&lt;!-- 事务模板对象 --&gt; &lt;bean name=&quot;transactionTemplate&quot; class=&quot;org.springframework.transaction.support.TransactionTemplate&quot; &gt; &lt;property name=&quot;transactionManager&quot; ref=&quot;transactionManager&quot; &gt;&lt;/property&gt; &lt;/bean&gt;... &lt;bean name=&quot;accountService&quot; class=&quot;my.study.service.AccountServiceImpl&quot; &gt; &lt;property name=&quot;ad&quot; ref=&quot;accountDao&quot; &gt;&lt;/property&gt; &lt;property name=&quot;tt&quot; ref=&quot;transactionTemplate&quot; &gt;&lt;/property&gt;&lt;/bean&gt; 使用编码式，需要在service中声明一个TransactionTemplate,并生成set方法,然后定义execute()方法，实现接口，在doInTransactionWithoutResult方法中书写事务操作的代码:123456789101112131415161718192021222324252627282930313233343536package my.study.service;import org.springframework.transaction.TransactionStatus;import org.springframework.transaction.support.TransactionCallbackWithoutResult;import org.springframework.transaction.support.TransactionTemplate;import my.study.dao.AccountDao;public class AccountServiceImpl implements AccountService &#123; private AccountDao ad ; private TransactionTemplate tt; @Override public void transfer(final Integer from,final Integer to,final Double money) &#123; tt.execute(new TransactionCallbackWithoutResult() &#123; @Override protected void doInTransactionWithoutResult(TransactionStatus arg0) &#123; //减钱 ad.decreaseMoney(from, money); //加钱 ad.increaseMoney(to, money); &#125; &#125;); &#125; public void setAd(AccountDao ad) &#123; this.ad = ad; &#125; public void setTt(TransactionTemplate tt) &#123; this.tt = tt; &#125;&#125; execute方法的执行步骤：打开事务-&gt;执行doInTransactionWithoutResult方法-&gt;提交事务.在doInTransactionWithoutResult方法中提供了try catch语句，如果发生错误则进行回滚操作。 xml配置配置AOP事务，AOP事务的配置和效果如图：进行xml配置之前，还需要导入新的命名空间约束，tx约束,并在XMLEditor下添加这个约束和aop约束:导入方法见Spring学习笔记-一和Spring学习笔记-二中相关部分。最终需要的配置如下： xml中的各个配置：beans:最基本的根元素context:读取properties配置文件aop:配置AOP，将通知织入目标对象tx:配置事务通知 配置事务通知接下来配置事务通知，是以方法为单位进行配置的，在配置文件中新增配置事务的部分：123456&lt;!-- 配置事务通知 --&gt; &lt;tx:advice id=&quot;txAdvice&quot; transaction-manager=&quot;transactionManager&quot;&gt; &lt;tx:attributes&gt; &lt;tx:method name=&quot;transfer&quot; isolation=&quot;REPEATABLE_READ&quot; propagation=&quot;REQUIRED&quot; read-only=&quot;false&quot;&gt;&lt;/tx:method&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; name为事务的方法名isolation:隔离级别: - DEFAULT - READ_UNCOMMITTED - READ_COMMITTED - REPEATABLE_READ - SERIALIZABLE propagation:传播行为： - REQUIRED - SUPPORTS - MANDATORY - REQUIRES_NEW - NOT_SUPPORTED - NEVER - NESTED read-only:”false”,这个就不多解释了吧 当然，配置的name方法名可以用*通配符来进行匹配相似的方法名，可以使用的例子如下：123456789101112&lt;tx:advice id=&quot;txAdvice&quot; transaction-manager=&quot;transactionManager&quot;&gt; &lt;tx:attributes&gt; &lt;tx:method name=&quot;save*&quot; isolation=&quot;REPEATABLE_READ&quot; propagation=&quot;REQUIRED&quot; read-only=&quot;false&quot; /&gt; &lt;tx:method name=&quot;persist*&quot; isolation=&quot;REPEATABLE_READ&quot; propagation=&quot;REQUIRED&quot; read-only=&quot;false&quot; /&gt; &lt;tx:method name=&quot;update*&quot; isolation=&quot;REPEATABLE_READ&quot; propagation=&quot;REQUIRED&quot; read-only=&quot;false&quot; /&gt; &lt;tx:method name=&quot;modify*&quot; isolation=&quot;REPEATABLE_READ&quot; propagation=&quot;REQUIRED&quot; read-only=&quot;false&quot; /&gt; &lt;tx:method name=&quot;delete*&quot; isolation=&quot;REPEATABLE_READ&quot; propagation=&quot;REQUIRED&quot; read-only=&quot;false&quot; /&gt; &lt;tx:method name=&quot;remove*&quot; isolation=&quot;REPEATABLE_READ&quot; propagation=&quot;REQUIRED&quot; read-only=&quot;false&quot; /&gt; &lt;tx:method name=&quot;get*&quot; isolation=&quot;REPEATABLE_READ&quot; propagation=&quot;REQUIRED&quot; read-only=&quot;true&quot; /&gt; &lt;tx:method name=&quot;find*&quot; isolation=&quot;REPEATABLE_READ&quot; propagation=&quot;REQUIRED&quot; read-only=&quot;true&quot; /&gt; &lt;tx:method name=&quot;transfer&quot; isolation=&quot;REPEATABLE_READ&quot; propagation=&quot;REQUIRED&quot; read-only=&quot;false&quot;&gt;&lt;/tx:method&gt; &lt;/tx:advice&gt; 注意其中get和find的只读为true 配置将事务织入目标对象配置完事务通知要进行配置织入:1234567&lt;!-- 配置织入 --&gt; &lt;aop:config &gt; &lt;!-- 配置切点表达式 --&gt; &lt;aop:pointcut expression=&quot;execution(* my.study.service.*ServiceImpl.*(..))&quot; id=&quot;txPc&quot;/&gt; &lt;!-- 配置切面 --&gt; &lt;aop:advisor advice-ref=&quot;txAdvice&quot; pointcut-ref=&quot;txPc&quot;/&gt; &lt;/aop:config&gt; 其中切面的构成是通知加切点,advice-ref：通知的名称;pointcut-ref：切点的名称.完整的xml配置:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.2.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.2.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-4.2.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-4.2.xsd &quot;&gt; &lt;!-- 指定Properties的位置 --&gt; &lt;context:property-placeholder location=&quot;classpath:db.properties&quot;/&gt; &lt;!-- 事务核心管理器,封装了所有事务操作. 依赖于连接池 --&gt; &lt;bean name=&quot;transactionManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot; &gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 事务模板对象 --&gt; &lt;bean name=&quot;transactionTemplate&quot; class=&quot;org.springframework.transaction.support.TransactionTemplate&quot; &gt; &lt;property name=&quot;transactionManager&quot; ref=&quot;transactionManager&quot; &gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 配置事务通知 --&gt; &lt;tx:advice id=&quot;txAdvice&quot; transaction-manager=&quot;transactionManager&quot;&gt; &lt;tx:attributes&gt; &lt;!-- name为方法名 isolation:隔离级别: - DEFAULT - READ_UNCOMMITTED - READ_COMMITTED - REPEATABLE_READ - SERIALIZABLE propagation:传播行为： - REQUIRED - SUPPORTS - MANDATORY - REQUIRES_NEW - NOT_SUPPORTED - NEVER - NESTED --&gt; &lt;tx:method name=&quot;transfer&quot; isolation=&quot;REPEATABLE_READ&quot; propagation=&quot;REQUIRED&quot; read-only=&quot;false&quot; &gt;&lt;/tx:method&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; &lt;!-- 配置织入 --&gt; &lt;aop:config &gt; &lt;!-- 配置切点表达式 --&gt; &lt;aop:pointcut expression=&quot;execution(* my.study.service.*ServiceImpl.*(..))&quot; id=&quot;txPc&quot;/&gt; &lt;!-- 配置切面 advice-ref：通知的名称 pointcut-ref：切点的名称 --&gt; &lt;aop:advisor advice-ref=&quot;txAdvice&quot; pointcut-ref=&quot;txPc&quot;/&gt; &lt;/aop:config&gt; &lt;!-- 1.将DataSource连接池放入Spring容器 --&gt; &lt;bean name=&quot;dataSource&quot; class=&quot;com.mchange.v2.c3p0.ComboPooledDataSource&quot;&gt; &lt;property name=&quot;JdbcUrl&quot; value=&quot;$&#123;jdbc.JdbcUrl&#125;&quot;&gt;&lt;/property&gt; &lt;property name=&quot;DriverClass&quot; value=&quot;$&#123;jdbc.DriverClass&#125;&quot;&gt;&lt;/property&gt; &lt;property name=&quot;User&quot; value=&quot;$&#123;jdbc.User&#125;&quot;&gt;&lt;/property&gt; &lt;property name=&quot;Password&quot; value=&quot;$&#123;jdbc.Password&#125;&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 2.AccountDao --&gt; &lt;bean name=&quot;accountDao&quot; class=&quot;my.study.dao.AccountDaoImpl&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 3.将AccountService放入Spring容器 --&gt; &lt;bean name=&quot;accountService&quot; class=&quot;my.study.service.AccountServiceImpl&quot;&gt; &lt;property name=&quot;ad&quot; ref=&quot;accountDao&quot;&gt;&lt;/property&gt; &lt;property name=&quot;tt&quot; ref=&quot;transactionTemplate&quot;&gt;&lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; 注解配置注解配置只需将上面的编制织入和配置事务通知替换为:12&lt;!-- 开启注解管理AOP事务 --&gt; &lt;tx:annotation-driven/&gt; 配置完成后可以使用注解配置事务：1234567891011121314151617181920212223242526272829303132package my.study.service;import org.springframework.transaction.TransactionStatus;import org.springframework.transaction.annotation.Isolation;import org.springframework.transaction.annotation.Propagation;import org.springframework.transaction.annotation.Transactional;import org.springframework.transaction.support.TransactionCallbackWithoutResult;import org.springframework.transaction.support.TransactionTemplate;import my.study.dao.AccountDao;public class AccountServiceImpl implements AccountService &#123; private AccountDao ad ; private TransactionTemplate tt; @Override @Transactional(isolation=Isolation.REPEATABLE_READ,propagation=Propagation.REQUIRED,readOnly=false) public void transfer(final Integer from,final Integer to,final Double money) &#123; //减钱 ad.decreaseMoney(from, money); // int i = 1/0; //加钱 ad.increaseMoney(to, money); &#125; public void setAd(AccountDao ad) &#123; this.ad = ad; &#125; public void setTt(TransactionTemplate tt) &#123; this.tt = tt; &#125;&#125; 可以在类之前加上注解配置，那么这个类的所有方法都遵循这个配置。如果有某个方法和类声明的注解配置不一样，则再这个方法前面声明一下注解配置即可。 可以再书写一个测试类进行测试，注意要读取正确的配置文件（即开启了注解配置的文件）]]></content>
      <tags>
        <tag>学习笔记</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring学习笔记-二]]></title>
    <url>%2F2018%2F11%2F10%2FSpring%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[使用注解配置Spring导入新的命名空间和依赖包导入方法参见Spring学习笔记-一，与导入beans命名空间类似。先导入spring-context-4.2.xsd，然后进行配置,还要在xml的Dsign视图下进行配置，在Prefix中填写context.同时需要导入依赖包spring-aop-4.2.4.RELEASE.jar. 使用注解代理配置文件12345&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://www.springframework.org/schema/beans" xmlns:context="http://www.springframework.org/schema/context" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.2.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.2.xsd "&gt; &lt;!-- 扫描指定包名及其子包下的所有注解 --&gt; &lt;context:component-scan base-package="my.study.bean2"&gt;&lt;/context:component-scan&gt;&lt;/beans&gt; 在类中使用注解进行配置1234567891011121314151617181920212223242526272829303132333435package my.study.bean2;import org.springframework.stereotype.Component;@Component("user")public class User &#123; private String name; private Integer age; private Car car; public User()&#123; System.out.println("User 的空参构造方法被调用"); &#125; public Car getCar() &#123; return car; &#125; public void setCar(Car car) &#123; this.car = car; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public Integer getAge() &#123; return age; &#125; public void setAge(Integer age) &#123; this.age = age; &#125; @Override public String toString() &#123; return "User [name=" + name + ", age=" + age + ", car=" + car + "]"; &#125; &#125; 注解@Component(&quot;user&quot;)相当于在配置文件中添加了&lt;bean name=&quot;user&quot; class=&quot;my.study.bean2.User&quot;&gt;&lt;/bean&gt;书写测试类，可以打印出：1User 的空参构造方法被调用 除了@Component注解，还有@Serviece,@Controller,@Repository注解，它们和@Component没有什么区别，只是为了区分注解的对象是在哪一层，便于解读。@Serviece—Service层@Controller—Web层@Repository—DAO层 其他注解关键字@Scope(scopeName=&quot;singleton|prototype&quot;)—与&lt;bean&gt;中的scope关键字一样，默认为singleton. @Value()—值类型注入 1.—放在属性前面，里面填写要注入的值，表示要注入的属性的值例如:12345678910@Component("user")@Scope(scopeName="prototype")public class User &#123; @Value("test1") private String name; @Value("11") private Integer age; private Car car; .....&#125; 就等于1234&lt;bean name=&quot;User&quot; class=&quot;my.study.bean2.User&quot; scope=&quot;prototype&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;test1&quot;&gt;&lt;/property&gt; &lt;property name=&quot;age&quot; value=&quot;11&quot;&gt;&lt;/property&gt;&lt;/bean&gt; 可以看打印结果：12User 的空参构造方法被调用User [name=test1, age=11, car=null] 这种方式是通过反射的Field对属性赋值,破坏了封装性 2.—放在set方法前面，里面填写要注入的值，表示要注入的属性的值如：123456789101112131415@Component("user")@Scope(scopeName="prototype")public class User &#123; ... @Value("test1") public void setName(String name) &#123; this.name = name; &#125; ... @Value(value="11") public void setAge(Integer age) &#123; this.age = age; &#125; ...&#125; 这种方式通过set方法赋值,没有破坏封装性. @Autowired—自动装配引用类型注入 先将要注入的对象注册到容器中，可以在这个对象中先注入各种类型，然后使用@Autowired注入到当前类。缺点：如果是匹配到多个类型一致的对象，无法选择注入哪一个 @Qualifier()—指定要注入的对象的名称(即name=&quot;&quot;) @Resource(name=&quot;&quot;)—手动注入，指定注入哪个名称的对象 @PostContruct—在初始化方法前添加此注解,对象被创建后调用,类似于init-method @PreDestory—在销毁前调用,相当于destory-method Spring中的AOP手动实现AOP简要概括AOP思想就是面向切面编程,横向重复,纵向抽取. 首先Spring能够为容器中管理的对象生成动态代理对象。以前要使用Proxy.newProxyInstance(xx,xx,xx)生成代理对象.而现在Spring只需通过配置就可以生成代理对象。通过这个功能，就可以使用AOP思想进行开发。 Srping实现AOP的原理：1.动态代理—但是，被代理对象必须要实现接口，才能产生代理对象.没有接口不能实现动态代理.如果有接口，优先使用动态代理 2.cglib代理—cglib代理属于第三方代理技术.可以对任何类生成代理.代理的原理是对目标对象进行继承代理. 如果目标对象被final修饰.那么该类无法被cglib代理.如果没有接口，实现此代理 手动实现AOP:1.创建一个UserService接口，定义UserServiceImpl类要实现这个接口的方法.创建UserService代理工厂UserServiceProxyFactory,提供一个工厂方法，返回UserService对象.在方法中生成代理并返回UserService对象. 现在需要在调用UserServiceImpl的每一个方法(save,delete,update,find)之前都进行一个操作(比如打开事务),之后都进行另一个操作(比如提交事务).12345678package my.study.service;public interface UserService &#123; void save(); void delete(); void update(); void find();&#125; 1234567891011121314151617181920package my.study.service;public class UserServiceImpl implements UserService &#123; @Override public void save() &#123; System.out.println("保存用户!"); &#125; @Override public void delete() &#123; System.out.println("删除用户!"); &#125; @Override public void update() &#123; System.out.println("更新用户!"); &#125; @Override public void find() &#123; System.out.println("查找用户!"); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940package my.study.proxy;import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;import cn.itcast.service.UserService;import cn.itcast.service.UserServiceImpl;public class UserServiceProxyFactory implements InvocationHandler&#123; //定义构造方法，必须传入当前代理对象的实例 public UserServiceProxyFactory(UserService us) &#123; super(); this.us = us; &#125; //定义传入当前代理对象的实例 private UserService us; //返回代理对象 // 编写工具方法：生成动态代理 public UserService getUserServiceProxy()&#123; //第一个参数传入类加载器,第二个参数传入被代理对象所实现的接口,第三个参数要说明这次代理要怎么增强，即增强的内容，要传入一个InvocationHandler UserService usProxy = (UserService) Proxy.newProxyInstance(UserServiceProxyFactory.class.getClassLoader(), UserServiceImpl.class.getInterfaces(), this); //返回 return usProxy; &#125; //实现接口 @Override //第一个参数是当前代理对象的实例,第二个参数是当前调用的方法,第三个参数是当前方法执行时的参数 public Object invoke(Object arg0, Method method, Object[] arg2) throws Throwable &#123; //业务方法执行前需要进行的操作，这里是打开事务 System.out.println("打开事务!"); //传入当前代理对象的实例us作为运行所在类 //业务方法的执行 Object invoke = method.invoke(us, arg2); //业务方法执行后要执行的操作，这里是提交事务 System.out.println("提交事务!"); return invoke; &#125;&#125; 书写测试代码：1234567891011121314151617...@Test //动态代理 public void test1()&#123; //创建被代理对象 UserService us = new UserServiceImpl(); //创建Factory,传入被代理对象 UserServiceProxyFactory factory = new UserServiceProxyFactory(us); //调用getUserServiceProxy()方法，返回代理对象 UserService usProxy = factory.getUserServiceProxy(); //调用代理对象的增加方法 usProxy.save(); //代理对象与被代理对象实现了相同的接口 //代理对象 与 被代理对象没有继承关系 System.out.println(usProxy instanceof UserServiceImpl );//false &#125; 打印结果:1234打开事务！保存用户！提交事务！false 2.使用cglib方式实现AOP 123456789101112131415161718192021222324252627282930313233343536373839404142package my.study.proxy;import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;import org.springframework.cglib.proxy.Callback;import org.springframework.cglib.proxy.Enhancer;import org.springframework.cglib.proxy.MethodInterceptor;import org.springframework.cglib.proxy.MethodProxy;import cn.itcast.service.UserService;import cn.itcast.service.UserServiceImpl;//cglib代理public class UserServiceProxyFactory2 implements MethodInterceptor &#123; public UserService getUserServiceProxy()&#123; // 创建 Cglib 的核心类: Enhancer en = new Enhancer();//帮我们生成代理对象 // 设置父类: en.setSuperclass(UserServiceImpl.class);//设置对谁进行代理 // 设置父类: en.setCallback(this);//代理要做什么 // 生成代理： UserService us = (UserService) en.create();//创建代理对象 return us; &#125; @Override public Object intercept(Object prxoyobj, Method method, Object[] arg, MethodProxy methodProxy) throws Throwable &#123; //打开事务 System.out.println("打开事务!"); //调用原有方法 Object returnValue = methodProxy.invokeSuper(prxoyobj, arg); //提交事务 System.out.println("提交事务!"); return returnValue; &#125;&#125; 书写测试代码：123456789101112131415...@Testpublic void test2()&#123; UserServiceProxyFactory2 factory = new UserServiceProxyFactory2(); UserService usProxy = factory.getUserServiceProxy(); usProxy.save(); //判断代理对象是否属于被代理对象类型 //代理对象继承了被代理对象=&gt;true System.out.println(usProxy instanceof UserServiceImpl );//true&#125;... 打印结果:1234打开事务！保存用户！提交事务！true AOP相关术语Joinpoint(连接点):所谓连接点是指那些被拦截到的点。在 spring 中,这些点指的是方法,因为 spring 只支持方法类型的连接点.即目标对象中，所有可以增强的方法。 Pointcut(切入点):所谓切入点是指我们要对哪些 Joinpoint 进行拦截的定义.即目标对象，已经增强的方法(已经增强了的连接点)。 Advice(通知/增强):所谓通知是指拦截到 Joinpoint 之后所要做的事情就是通知.通知分为前置通知,后置通知,异常通知,最终通知,环绕通知(切面要完成的功能)。即增强的代码,也就是上面的例子中的System.out.println(&quot;打开事务!&quot;);和System.out.println(&quot;提交事务!&quot;); Introduction(引介):引介是一种特殊的通知在不修改类代码的前提下, Introduction 可以在运行期为类动态地添加一些方法或 Field. Target(目标对象):代理的目标对象.即被代理对象 Weaving(织入):是指把增强应用到目标对象来创建新的代理对象的过程.spring 采用动态代理织入，而 AspectJ 采用编译期织入和类装在期织入.指的是将通知应用到连接点，形成切入点的过程(成形代理过程). Proxy（代理）:一个类被 AOP 织入增强后，就产生一个结果代理类.(将通知织入到目标对象后，形成代理对象) Aspect(切面): 是切入点和通知（引介）的结合,即切入点+通知 Spring中AOP的使用(xml配置)Spring中封装了动态代理的代码，所以不需要自己手写动态代理代码来实现AOP了。 需要第三方依赖包com.springsource.org.aopalliance-1.0.0.jar、com.springsource.org.aspectj.weaver-1.6.8.RELEASE.jar(具体版本可自己选择其他的) 准备目标对象使用上一节中的UserServiceImpl类. 准备通知通知类1234567891011121314151617181920212223242526272829package my.study.e_springaop;import org.aspectj.lang.ProceedingJoinPoint;public class MyAdvice &#123; //前置通知 目标方法运行之前调用 public void before() &#123; System.out.println("前置通知"); &#125; //后置通知(如果出现异常不调用) 目标方法运行之后调用 public void afterNoExc() &#123; System.out.println("后置通知(如果出现异常不调用)"); &#125; //环绕通知 目标方法之前和之后都调用 public Object around(ProceedingJoinPoint pjp) throws Throwable &#123; System.out.println("环绕通知,前置部分"); Object proceed = pjp.proceed(); System.out.println("环绕通知,后置部分"); return proceed; &#125; //异常拦截通知 如果出现异常则调用 public void exc() &#123; System.out.println("异常拦截通知"); &#125; //后置通知(无论是否出现异常都会调用) public void afterWithExc() &#123; System.out.println("后置通知(无论是否出现异常都会调用)"); &#125;&#125; 配置进行织入,将通知织入目标对象中首先导入aop的xml约束（命名空间），方法和之前的类似。 在my.study.e_springaop包内书写配置文件:12345678910111213141516171819202122232425262728293031323334353637&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:p="http://www.springframework.org/schema/p" xmlns="http://www.springframework.org/schema/beans" xmlns:aop="http://www.springframework.org/schema/aop" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.2.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-4.2.xsd "&gt;&lt;!-- 配置目标对象 --&gt; &lt;bean name="userServiceTarget" class="my.study.service.UserServiceImpl"&gt;&lt;/bean&gt;&lt;!-- 配置通知对象 --&gt; &lt;bean name="myAdvice" class="my.study.e_springaop.MyAdvice"&gt;&lt;/bean&gt;&lt;!-- 配置将通知织入目标对象 --&gt; &lt;aop:config&gt; &lt;!-- 配置切入点 --&gt; &lt;!-- id可以自定义 expression="execute()"为固定格式，传参为： public void my.study.service.UserServiceImpl.save() 默认为public: void my.study.service.UserServiceImpl.save() 返回值为任意的函数: * my.study.service.UserServiceImpl.save() UserServiceImpl类下的所有方法: * my.study.service.UserServiceImpl.*() 参数为任意: * my.study.service.*ServiceImpl.*(..) my.study.service包下及其子包下的所有以ServiceImpl为结尾的类的所有方法: * my.study.service..*ServiceImpl.*() --&gt; &lt;aop:pointcut expression="execution(* my.study.service.*ServiceImpl.*(..))" id="demo"/&gt; &lt;aop:aspect ref="myAdvice"&gt; &lt;!-- 指定myAdvice中的before方法切入到demo这个切入点中 --&gt; &lt;aop:before method="before" pointcut-ref="demo"/&gt; &lt;!-- 其他几个类似 --&gt; &lt;aop:after-returning method="afterNoexc" pointcut-ref="demo"/&gt; &lt;aop:around method="around" pointcut-ref="demo"/&gt; &lt;aop:after-throwing method="exc" pointcut-ref="demo"/&gt; &lt;aop:after method="afterWithExc" pointcut-ref="demo" /&gt; &lt;/aop:aspect&gt; &lt;/aop:config&gt;&lt;/beans&gt; 书写测试类:使用了spring与junit整合测试:12345678910111213141516171819202122package my.study.e_springaop;import javax.annotation.Resource;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.test.context.ContextConfiguration;import org.springframework.test.context.junit4.SpringJUnit4ClassRunner;import my.study.service.UserService;//创建容器@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration("classpath:my/study/e_springaop/applicationContext.xml")public class Demo &#123; @Resource(name="userServiceTarget") private UserService us; @Test public void test1()&#123; us.save(); &#125;&#125; 打印结果:123456前置通知环绕通知,前置部分保存用户!后置通知(无论是否出现异常都会调用)环绕通知,后置部分后置通知(如果出现异常不调用) Spring中AOP的使用(注解配置)注解配置的步骤和使用xml配置的步骤类似，只是在配置文件中不需要使用&lt;aop:config /&gt;标签进行配置，而是在配置文件中开启注解，继续接下来的配置。 12345678910&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:p="http://www.springframework.org/schema/p" xmlns="http://www.springframework.org/schema/beans" xmlns:aop="http://www.springframework.org/schema/aop" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.2.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-4.2.xsd "&gt;&lt;!-- 配置目标对象 --&gt; &lt;bean name="userServiceTarget" class="my.study.service.UserServiceImpl"&gt;&lt;/bean&gt;&lt;!-- 配置通知对象 --&gt; &lt;bean name="myAdvice" class="my.study.f_springannotionaop.MyAdvice"&gt;&lt;/bean&gt;&lt;!-- 开启使用注解完成织入 --&gt; &lt;aop:aspectj-autoproxy&gt;&lt;/aop:aspectj-autoproxy&gt;&lt;/beans&gt; 在my.study.f_springannotionaop包下的MyAdvice类中，添加注解：12345678910111213141516171819202122232425262728293031323334353637383940414243444546package my.study.f_springannotionaop;import org.aspectj.lang.ProceedingJoinPoint;import org.aspectj.lang.annotation.After;import org.aspectj.lang.annotation.AfterReturning;import org.aspectj.lang.annotation.AfterThrowing;import org.aspectj.lang.annotation.Around;import org.aspectj.lang.annotation.Aspect;import org.aspectj.lang.annotation.Before;import org.aspectj.lang.annotation.Pointcut;@Aspectpublic class MyAdvice &#123; @Pointcut("execution(* my.study.service.*ServiceImpl.*(..))") public void cut() &#123;&#125; //前置通知 目标方法运行之前调用 @Before("MyAdvice.cut()") public void before() &#123; System.out.println("前置通知"); &#125; //后置通知(如果出现异常不调用) 目标方法运行之后调用 @AfterReturning("execution(* my.study.service.*ServiceImpl.*(..))") public void afterNoExc() &#123; System.out.println("后置通知(如果出现异常不调用)"); &#125; //环绕通知 目标方法之前和之后都调用 @Around("execution(* my.study.service.*ServiceImpl.*(..))") public Object around(ProceedingJoinPoint pjp) throws Throwable &#123; System.out.println("环绕通知,前置部分"); Object proceed = pjp.proceed(); System.out.println("环绕通知,后置部分"); return proceed; &#125; //异常拦截通知 如果出现异常则调用 @AfterThrowing("execution(* my.study.service.*ServiceImpl.*(..))") public void exc() &#123; System.out.println("异常拦截通知"); &#125; //后置通知(无论是否出现异常都会调用) @After("execution(* my.study.service.*ServiceImpl.*(..))") public void afterWithExc() &#123; System.out.println("后置通知(无论是否出现异常都会调用)"); &#125;&#125; @Aspect表示该类是一个通知类@Before(&quot;execution(* my.study.service.*ServiceImpl.*(..))&quot;)表示该通知是一个前置通知，参数为execution(* my.study.service.*ServiceImpl.*(..))&quot;)进行指定切入点.@AfterReturning(&quot;execution(* my.study.service.*ServiceImpl.*(..))&quot;)表示该通知是一个后置通知.@Around(&quot;execution(* my.study.service.*ServiceImpl.*(..))&quot;)表示该通知是一个环绕通知@AfterThrowing(&quot;execution(* my.study.service.*ServiceImpl.*(..))&quot;)表示该通知是一个异常拦截通知@After(&quot;execution(* my.study.service.*ServiceImpl.*(..))&quot;)表示该通知是一个后置通知1234567@Pointcut("execution(* my.study.service.*ServiceImpl.*(..))") public void cut() &#123;&#125; @Before("MyAdvice.cut()") public void before() &#123; System.out.println("前置通知"); &#125; 其中 @Before(&quot;MyAdvice.cut()&quot;)就等于@Before(&quot;execution(* my.study.service.*ServiceImpl.*(..))&quot;)，这样方便管理切入点 书写测试类，并打印结果:123456环绕通知,前置部分前置通知保存用户!环绕通知,后置部分后置通知(无论是否出现异常都会调用)后置通知(如果出现异常不调用)]]></content>
      <tags>
        <tag>学习笔记</tag>
        <tag>Spring</tag>
        <tag>JavaWeb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring学习笔记-一]]></title>
    <url>%2F2018%2F11%2F09%2FSpring%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%B8%80%2F</url>
    <content type="text"><![CDATA[Spring是一个一站式框架。Spring可以看做是一个容器，所有对象都可以在其中进行管理 Spring 搭建基本环境首先导入Spring所需依赖包： 创建一个对象用于测试： 123456789101112131415161718192021package my.study.bean;public class User &#123; private String name; private Integer age; public User()&#123; System.out.println("User 的空参构造方法被调用"); &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public Integer getAge() &#123; return age; &#125; public void setAge(Integer age) &#123; this.age = age; &#125; &#125; 然后需要书写配置文件，并将创建好的对象注册到Spring容器中。建议将配置文件放到源码文件夹根目录(/src)下，文件名可以任意取。 导入配置文件的约束：Preference中搜索XML Catlog，打开后点击Add-&gt;FileSystem...选择Spring解压目录下的schema-&gt;beans-&gt;spring-beans-4.2.xsd(选择最新版本(我的是4.2)即可) 继续选择Kye Type-&gt;Scheme location：把Location中spring-beans-4.2.xsd文件名复制，追加到Key中(注意补一个/). 然后在配置文件中，输入&lt;beans&gt;&lt;/beans&gt;后，在Eclipse下切换到设计视图，选中beans-&gt;右键选中Edit Namespaces-&gt;点击Add-&gt;勾选xsi-&gt;点击ok-&gt;继续点击Add-&gt;选择Specify New Namespace-&gt;选择Browse-&gt;选择Select XML Catalog entry-&gt;找到刚才上图的Key-&gt;Namespace Name填写Key中http://www.spri.....beans的内容，Prefix不用写 导入成功 最后将对象注册到容器进行测试： ApplicationContext&amp;BeanFactory 简单介绍：其中BeanFactory作为最顶层的接口，功能较为单一。BeanFactory接口实现类的容器，特点是每次获得对象时才会创建对象。 ApplicationContext在每次容器启动时，就会创建容器中的所有对象 Spring的配置bean 元素以前面配置User对象为例。 bean标签：使用该元素描述需要spring容器管理的对象class属性:被管理对象的完整类名.name属性:给被管理的对象起个名字.获得对象时根据该名称获得对象. 特点:可以重复.可以使用特殊字符.id属性: 与name属性一样.特点:名称不可重复.不能使用特殊字符. Scope属性singleton(默认值)单例对象.被标识为单例的对象在spring容器中只会存在一个实例，即默认为：1&lt;bean name=&quot;User&quot; class=&quot;my.study.bean.User&quot; scope=&quot;singleton&quot;&gt;&lt;/bean&gt; 测试：12345678910@Testpublic void test1() &#123; //1.创建容器对象 ApplicationContext ac = new ClassPathXmlApplicationContext("my/study/b_create/applicationContext.xml"); //2.在容器中寻找"User"对象 User u = (User) ac.getBean("User"); User u1 = (User) ac.getBean("User"); //3打印user对象 System.out.println(u == u1);&#125; 结果1true prototype多例原型.被标识为多例的对象,每次再获得才会创建.每次创建都是新的对象.注意，当整合struts2时,ActionBean必须配置为多例的。因为Action对象是由Spring容器管理的，struts2每次请求都会创建一个Action将配置文件改为:1&lt;bean name=&quot;User&quot; class=&quot;my.study.bean.User&quot; scope=&quot;prototype&quot;&gt;&lt;/bean&gt; 继续用test1()测试，打印结果为false request在web环境下.对象与request生命周期一致.即每当请求完成，对象就会在Spring中移除 session在web环境下,对象与session生命周期一致.即在一次会话中，会话完成后,对象就会在Spring中被移除 生命周期属性如果希望Bean在创建时有初始化方法，可以指定Bean中的一个方法为其初始化方法，Spring会在对象创建完成后调用。即init-method同样可以配置一个销毁方法，Spring容器在关闭前调用此方法。destory-method注意scope=&quot;prototype&quot;不能和destory-method标签一起使用 Spring的分模块配置在主配置文件中引入其他配置文件，1&lt;import resource=&quot;&quot;/&gt; 目录写在src目录下的全包名和配置文件名 Spring创建对象的方式创建新的配置文件和类进行测试 UserFactory:1234567891011121314package my.study.bean;public class UserFactory &#123; public static User createUser() &#123; System.out.println("静态工厂方式创建User"); return new User(); &#125; public User createUser2() &#123; System.out.println("实例工厂方式创建User"); return new User(); &#125;&#125; applicationContext.xml:123456789101112&lt;!-- 方式1：空参构造创建 --&gt; &lt;bean name=&quot;User&quot; class=&quot;my.study.bean.User&quot;&gt;&lt;/bean&gt; &lt;!-- 方式2：静态工厂创建 调用UserFactory的createUser方法创建名为user2的对象，放入容器 --&gt; &lt;bean name=&quot;User2&quot; class=&quot;my.study.bean.UserFactory&quot; factory-method=&quot;createUser&quot;&gt;&lt;/bean&gt; &lt;!-- 方式3：实例工厂创建 调用UserFactory的createUser方法创建名为user3的对象，放入容器 --&gt; &lt;bean name=&quot;User3&quot; factory-bean=&quot;userFactory&quot; factory-method=&quot;createUser2&quot;&gt;&lt;/bean&gt; &lt;bean name=&quot;userFactory&quot; class=&quot;my.study.bean.UserFactory&quot;&gt;&lt;/bean&gt; 空参构造方式空参构造方式就是最开始配置bean时的构造方式，前面已经有结果。当然这里要注意，创建容器对象时，传参时要传入的配置文件需要加上包名1ApplicationContext ac = new ClassPathXmlApplicationContext(&quot;my/study/b_create/applicationContext.xml&quot;); 静态工厂方式12345678910@Test//静态工厂方式public void test2() &#123; //1.创建容器对象 ApplicationContext ac = new ClassPathXmlApplicationContext("my/study/b_create/applicationContext.xml"); //2.向容器请求"user"对象 User u = (User) ac.getBean("User2"); //3打印user对象 System.out.println(u);&#125; 实例工厂方式12345678910@Test//动态工厂方式public void test3() &#123; //1.创建容器对象 ApplicationContext ac = new ClassPathXmlApplicationContext("my/study/b_create/applicationContext.xml"); //2.向容器请求"user"对象 User u = (User) ac.getBean("User3"); //3打印user对象 System.out.println(u);&#125; Spring属性注入注入方式set方法注入新建一个包，在配置文件&lt;bean&gt;&lt;/bean&gt;中添加property标签，其中name表示要注入的属性名,value表示要注入的值 12345&lt;!-- set方式注入： --&gt;&lt;bean name=&quot;User&quot; class=&quot;my.study.bean.User&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;jay&quot;&gt;&lt;/property&gt; &lt;property name=&quot;age&quot; value=&quot;17&quot;&gt;&lt;/property&gt;&lt;/bean&gt; 书写测试类：1234567891011121314151617package my.study.c_injection;import org.junit.Test;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;import my.study.bean.User;public class Demo &#123; @Test public void test() &#123; //1.创建容器对象 ApplicationContext ac = new ClassPathXmlApplicationContext("my/study/c_injection/applicationContext.xml"); User u = (User) ac.getBean("User"); System.out.println(u.getName() + " , " +u.getAge()); &#125;&#125; 打印出1User [name=jay, age=17] 这是值类型注入。 下面是引用类型注入。然后新建一个Car类，12345678910111213141516171819202122package my.study.bean;public class Car &#123; private String name; private String color; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getColor() &#123; return color; &#125; public void setColor(String color) &#123; this.color = color; &#125; @Override public String toString() &#123; return "Car [name=" + name + ", color=" + color + "]"; &#125;&#125; 在User中添加car属性，并添加get和set方法如果要注入Car类型，需要先把Car配置到容器中。在配置文件中添加：1234&lt;bean name=&quot;Car&quot; class=&quot;my.study.bean.Car&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;Q5&quot;&gt;&lt;/property&gt; &lt;property name=&quot;color&quot; value=&quot;black&quot;&gt;&lt;/property&gt;&lt;/bean&gt; 然后为User的car属性注入刚才配置的Car对象：12345&lt;bean name=&quot;User&quot; class=&quot;my.study.bean.User&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;jay&quot;&gt;&lt;/property&gt; &lt;property name=&quot;age&quot; value=&quot;17&quot;&gt;&lt;/property&gt; &lt;property name=&quot;car&quot; ref=&quot;Car&quot;&gt;&lt;/property&gt;&lt;/bean&gt; 重新书写User的toString()方法，重新运行test()打印结果：1User [name=jay, age=17, car=Car [name=Q5, color=black]] 构造函数注入在User类中添加有参构造函数1234public User(String name, Car car) &#123; this.name = name; this.car = car;&#125; 然后是bean的配置，添加如下配置:1234&lt;bean name=&quot;User2&quot; class=&quot;my.study.bean.User&quot;&gt; &lt;constructor-arg name=&quot;name&quot; value=&quot;chou&quot;&gt;&lt;/constructor-arg&gt; &lt;constructor-arg name=&quot;car&quot; ref=&quot;Car&quot;&gt;&lt;/constructor-arg&gt;&lt;/bean&gt; ref依然传入上面定义的Car.然后书写代码测试：12345public void test() &#123; ApplicationContext ac = new ClassPathXmlApplicationContext("my/study/c_injection/applicationContext.xml"); User u = (User) ac.getBean("User2"); System.out.println(u);&#125; 打印结果为:1User [name=chou, age=null, car=Car [name=Q5, color=black]] 如果定义一个重载构造函数：1234public User(Car car,String name) &#123; this.name = name; this.car = car;&#125; 想让Spring在使用构造函数注入时，使用User(String name, Car car)来创建对象，可以在&lt;contructor-arg&gt;&lt;/contructor-arg&gt;中使用index.使用index来表示构造函数的参数索引,index越小表示越靠前的位置.1234&lt;bean name=&quot;User2&quot; class=&quot;my.study.bean.User&quot;&gt; &lt;constructor-arg name=&quot;name&quot; value=&quot;chou&quot; index=&quot;0&quot;&gt;&lt;/constructor-arg&gt; &lt;constructor-arg name=&quot;car&quot; ref=&quot;Car&quot; index=&quot;1&quot;&gt;&lt;/constructor-arg&gt;&lt;/bean&gt; 如果是构造函数是同样的注入顺序，但是是不同的类型，在index后面继续追加一个type，表示构造函数的参数类型：1234public User(Integer name, Car car) &#123; this.name = name+""; this.car = car;&#125; 要注意value中的传参要和type对应1234&lt;bean name=&quot;User2&quot; class=&quot;my.study.bean.User&quot;&gt; &lt;constructor-arg name=&quot;name&quot; value=&quot;123&quot; index=&quot;0&quot; type=&quot;java.lang.Integer&quot;&gt;&lt;/constructor-arg&gt; &lt;constructor-arg name=&quot;car&quot; ref=&quot;Car&quot; index=&quot;1&quot;&gt;&lt;/constructor-arg&gt;&lt;/bean&gt; 打印结果：1User [name=123, age=null, car=Car [name=Q5, color=black]] p名称空间注入首先在&lt;beans&gt;&lt;/beans&gt;中导入名称空间,添加xmlns:p=&quot;http://www.springframework.org/schema/p&quot;即添加后的beans名称空间为：12345&lt;beans xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:p=&quot;http://www.springframework.org/schema/p&quot; xmlns=&quot;http://www.springframework.org/schema/beans&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.2.xsd &quot;&gt;&lt;/beans&gt; 注入方法，使用p:属性完成注入，如果是值类型，则用p:属性名=&quot;值&quot;，如果是引用类型，则用p:属性名-ref=&quot;bean名称&quot;：12&lt;bean name=&quot;User3&quot; class=&quot;my.study.bean.User&quot; p:name=&quot;hey&quot; p:age=&quot;22&quot; p:car-ref=&quot;Car&quot;&gt;&lt;/bean&gt; 书写代码测试：12345public void test() &#123; ApplicationContext ac = new ClassPathXmlApplicationContext("my/study/c_injection/applicationContext.xml"); User u = (User) ac.getBean("User3"); System.out.println(u);&#125; 打印结果：1User [name=hey, age=22, car=Car [name=Q5, color=black]] spel注入即Spring Expression Language.有点类似于el和ognl的写法. 例如:12345678&lt;bean name=&quot;User4&quot; class=&quot;my.study.bean.User&quot;&gt; &lt;!-- 使用User的name属性值作为value的值 --&gt; &lt;property name=&quot;name&quot; value=&quot;#&#123;User.name&#125;&quot;&gt;&lt;/property&gt; &lt;!-- 使用User3的value属性值作为value的值 --&gt; &lt;property name=&quot;age&quot; value=&quot;#&#123;User3.age&#125;&quot;&gt;&lt;/property&gt; &lt;!-- 引用类型不支持spel表达式 --&gt; &lt;property name=&quot;car&quot; ref=&quot;Car&quot;&gt;&lt;/property&gt;&lt;/bean&gt; 打印结果为：1User [name=jay, age=22, car=Car [name=Q5, color=black]] 复杂类型注入新建CollectionBean类进行测试：1234567891011121314151617181920212223242526272829303132333435363738394041package my.study.c_injection;import java.util.List;import java.util.Map;import java.util.Properties;public class CollectionBean &#123; private Object[] arr; private List list; private Map map; private Properties prop; public Object[] getArr() &#123; return arr; &#125; public void setArr(Object[] arr) &#123; this.arr = arr; &#125; public List getList() &#123; return list; &#125; public void setList(List list) &#123; this.list = list; &#125; public Map getMap() &#123; return map; &#125; public void setMap(Map map) &#123; this.map = map; &#125; public Properties getProp() &#123; return prop; &#125; public void setProp(Properties prop) &#123; this.prop = prop; &#125; @Override public String toString() &#123; return "CollectionBean [arr=" + Arrays.toString(arr) + ", list=" + list + ", map=" + map + ", prop=" + prop + "]"; &#125;&#125; 数组在配置文件中书写配置：1234567891011121314&lt;bean name="CollectionBean_arr" class="my.study.c_injection.CollectionBean"&gt; &lt;!-- 如果数组中只准备注入一个元素，直接使用value|ref --&gt; &lt;!-- &lt;property name="arr" value="11"&gt;&lt;/property&gt; --&gt; &lt;!-- 多元素注入 --&gt; &lt;property name="arr"&gt; &lt;array&gt; &lt;value&gt;1&lt;/value&gt; &lt;value&gt;2&lt;/value&gt; &lt;ref bean="User4"&gt;&lt;/ref&gt; &lt;/array&gt; &lt;/property&gt;&lt;/bean&gt; 测试并打印结果：1CollectionBean [arr=[1, 2, User [name=jay, age=22, car=Car [name=Q5, color=black]]], list=null, map=null, prop=null] List在配置文件中添加配置：12345678910111213141516171819202122232425&lt;bean name=&quot;CollectionBean_arr&quot; class=&quot;my.study.c_injection.CollectionBean&quot;&gt;&lt;!-- arr注入 --&gt; &lt;!-- 如果数组中只准备注入一个元素，直接使用value|ref --&gt; &lt;!-- &lt;property name=&quot;arr&quot; value=&quot;11&quot;&gt;&lt;/property&gt; --&gt; &lt;!-- 多元素注入 --&gt; &lt;property name=&quot;arr&quot;&gt; &lt;array&gt; &lt;value&gt;1&lt;/value&gt; &lt;value&gt;2&lt;/value&gt; &lt;ref bean=&quot;User4&quot;&gt;&lt;/ref&gt; &lt;/array&gt; &lt;/property&gt;&lt;!-- list注入 --&gt; &lt;!-- 如果List中只准备注入一个元素，直接使用value|ref --&gt; &lt;!-- 多元素注入 --&gt; &lt;property name=&quot;list&quot;&gt; &lt;list&gt; &lt;value&gt;list1&lt;/value&gt; &lt;value&gt;list2&lt;/value&gt; &lt;ref bean=&quot;User3&quot;&gt;&lt;/ref&gt; &lt;/list&gt; &lt;/property&gt;&lt;/bean&gt; 打印结果：1CollectionBean [arr=[1, 2, User [name=jay, age=22, car=Car [name=Q5, color=black]]], list=[list1, list2, User [name=hey, age=22, car=Car [name=Q5, color=black]]], map=null, prop=null] Map在配置文件中添加配置：123456789101112131415161718192021222324252627282930313233&lt;bean name=&quot;CollectionBean_arr&quot; class=&quot;my.study.c_injection.CollectionBean&quot;&gt;&lt;!-- arr注入 --&gt; &lt;!-- 如果数组中只准备注入一个元素，直接使用value|ref --&gt; &lt;!-- &lt;property name=&quot;arr&quot; value=&quot;11&quot;&gt;&lt;/property&gt; --&gt; &lt;!-- 多元素注入 --&gt; &lt;property name=&quot;arr&quot;&gt; &lt;array&gt; &lt;value&gt;1&lt;/value&gt; &lt;value&gt;2&lt;/value&gt; &lt;ref bean=&quot;User4&quot;&gt;&lt;/ref&gt; &lt;/array&gt; &lt;/property&gt;&lt;!-- list注入 --&gt; &lt;!-- 如果List中只准备注入一个元素，直接使用value|ref --&gt; &lt;!-- 多元素注入 --&gt; &lt;property name=&quot;list&quot;&gt; &lt;list&gt; &lt;value&gt;list1&lt;/value&gt; &lt;value&gt;list2&lt;/value&gt; &lt;ref bean=&quot;User3&quot;&gt;&lt;/ref&gt; &lt;/list&gt; &lt;/property&gt;&lt;!-- map注入 --&gt; &lt;property name=&quot;map&quot;&gt; &lt;map&gt; &lt;entry key=&quot;key1&quot; value=&quot;value1&quot;&gt;&lt;/entry&gt; &lt;entry key=&quot;user&quot; value-ref=&quot;User4&quot;&gt;&lt;/entry&gt; &lt;entry key-ref=&quot;User3&quot; value-ref=&quot;User2&quot;&gt;&lt;/entry&gt; &lt;/map&gt; &lt;/property&gt;&lt;/bean&gt; 测试并打印结果：1CollectionBean [arr=[1, 2, User [name=jay, age=22, car=Car [name=Q5, color=black]]], list=[list1, list2, User [name=hey, age=22, car=Car [name=Q5, color=black]]], map=&#123;key1=value1, user=User [name=jay, age=22, car=Car [name=Q5, color=black]], User [name=hey, age=22, car=Car [name=Q5, color=black]]=User [name=123, age=null, car=Car [name=Q5, color=black]]&#125;, prop=null] Properties配置文件中的配置：12345678910111213141516171819202122232425262728293031323334353637383940&lt;bean name=&quot;CollectionBean_arr&quot; class=&quot;my.study.c_injection.CollectionBean&quot;&gt;&lt;!-- arr注入 --&gt; &lt;!-- 如果数组中只准备注入一个元素，直接使用value|ref --&gt; &lt;!-- &lt;property name=&quot;arr&quot; value=&quot;11&quot;&gt;&lt;/property&gt; --&gt; &lt;!-- 多元素注入 --&gt; &lt;property name=&quot;arr&quot;&gt; &lt;array&gt; &lt;value&gt;1&lt;/value&gt; &lt;value&gt;2&lt;/value&gt; &lt;ref bean=&quot;User4&quot;&gt;&lt;/ref&gt; &lt;/array&gt; &lt;/property&gt;&lt;!-- list注入 --&gt; &lt;!-- 如果List中只准备注入一个元素，直接使用value|ref --&gt; &lt;!-- 多元素注入 --&gt; &lt;property name=&quot;list&quot;&gt; &lt;list&gt; &lt;value&gt;list1&lt;/value&gt; &lt;value&gt;list2&lt;/value&gt; &lt;ref bean=&quot;User3&quot;&gt;&lt;/ref&gt; &lt;/list&gt; &lt;/property&gt;&lt;!-- map注入 --&gt; &lt;property name=&quot;map&quot;&gt; &lt;map&gt; &lt;entry key=&quot;key1&quot; value=&quot;value1&quot;&gt;&lt;/entry&gt; &lt;entry key=&quot;user&quot; value-ref=&quot;User4&quot;&gt;&lt;/entry&gt; &lt;entry key-ref=&quot;User3&quot; value-ref=&quot;User2&quot;&gt;&lt;/entry&gt; &lt;/map&gt; &lt;/property&gt;&lt;!-- Properties注入 --&gt; &lt;property name=&quot;prop&quot;&gt; &lt;props&gt; &lt;prop key=&quot;propkey1&quot;&gt; propvalue1&lt;/prop&gt; &lt;prop key=&quot;propkey2&quot;&gt; propvalue2&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt;&lt;/bean&gt; 打印结果：1CollectionBean [arr=[1, 2, User [name=jay, age=22, car=Car [name=Q5, color=black]]], list=[list1, list2, User [name=hey, age=22, car=Car [name=Q5, color=black]]], map=&#123;key1=value1, user=User [name=jay, age=22, car=Car [name=Q5, color=black]], User [name=hey, age=22, car=Car [name=Q5, color=black]]=User [name=123, age=null, car=Car [name=Q5, color=black]]&#125;, prop=&#123;propkey2=propvalue2, propkey1=propvalue1&#125;]]]></content>
      <tags>
        <tag>学习笔记</tag>
        <tag>Spring</tag>
        <tag>JavaWeb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实现线段树]]></title>
    <url>%2F2018%2F08%2F13%2F%E5%AE%9E%E7%8E%B0%E7%BA%BF%E6%AE%B5%E6%A0%91%2F</url>
    <content type="text"><![CDATA[定义当更关心某个区间上的问题时，使用线段树（区间树）会更方便。 线段树是一种二叉搜索树 线段树每个节点存放一个区间内相应的信息 一般用静态数组表示 线段树不一定是一棵完全二叉树 线段树是平衡二叉树（最大深度和最小深度的差最大为1） 例如，如果线段树想表示区间的和，那么每个节点存放的不是对应的数组，而是这个区间的和。 线段树依然可以使用数组表示。那么对于一个区间有n个元素，数组的大小该如何确定？对于一个满二叉树，如果有h层（从0层到h-1层），那么h层就有2^h-1个节点，差不多是2^h，最后一层(h-1)层，有2^(h-1)个节点，最后一层的节点的数目大约是前面的几点数目之和。 所以，如果用数组开辟空间，那么如果n=2^k(即恰好为2的整数次幂),需要2n的空间（这是满二叉树的情况），但是如果n=2^k+1（即n&gt;2^k,也就是最坏的情况），则需要4n的空间。 结论：因为线段树不考虑添加元素，也就是区间的大小是固定的，所以使用4n的静态空间就可以满足所有情况。（这里有空间浪费） 创建线段树线段树的根节点的信息，是两个孩子节点的信息的综合。比如求和，根节点的值就是左右孩子节点的值之和，依次类推，那么可以采用递归的方法进行求值。 另外，在进行线段树的创建时，因为不知道要采取什么样的方法去创建（比如求和，求积，求最大值，求最小值等），所以可以定义一个Merger接口，要求在创建线段树时，其构造函数不但要传入一个初始的数组，也要传入一个merger，即相应的要采取的操作。 Merger: 1234//融合器，即线段树进行什么操作(求和或者求乘积等操作)public interface Merger&lt;E&gt; &#123; E merge(E a,E b);&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980public class SegmentTree&lt;E&gt; &#123; private E[] data;//数组arr的副本 private E[] tree; // 将数据以树的形式表示出来,看成一个满二叉树 private Merger&lt;E&gt; merger; //传入一个merger，定义用户要进行的操作 public SegmentTree(E[] arr,Merger&lt;E&gt; merger)&#123; this.merger = merger; data = (E[]) new Object[arr.length]; for(int i = 0 ; i &lt; arr.length ; i++)&#123; data[i] = arr[i]; &#125; tree = (E[]) new Object[4 * arr.length]; buildSegmentTree(0,0,data.length - 1 );//创建SegmentTree &#125; public int getSize()&#123; return data.length; &#125; public E get(int index)&#123; if(index &lt; 0 || index &gt;= data.length) throw new IllegalArgumentException("Index is illegal."); return data[index]; &#125; //返回以完全二叉树的数组表示中，一个索引表示的元素的左孩子所在节点的索引 private int leftChild(int index)&#123; return index * 2 + 1; &#125; //返回以完全二叉树的数组表示中，一个索引表示的元素的右孩子所在节点的索引 private int rightChild(int index)&#123; return index * 2 + 2; &#125; /** *在treeIndex的位置创建表示区间[l,r]的线段树 * @param treeIndex 要创建的线段树根节点对应的索引 * @param l 对于此节点对应的左端点 * @param r 对于此节点对应的右端点 */ private void buildSegmentTree(int treeIndex,int l , int r)&#123; if(l == r)&#123; tree[treeIndex] = data[l]; return; &#125; //l &lt; r //左右子树的index，即在数组中的索引 int leftTreeIndex = leftChild(treeIndex); int rightTreeIndex = rightChild(treeIndex); //左右子树相应的区间的中间位置 int mid = l + (r - l) / 2; //为了防止(l + r) / 2 溢出 buildSegmentTree(leftTreeIndex,l,mid); buildSegmentTree(rightTreeIndex,mid+1,r); //调用merger接口类对象，进行相应的操作 tree[treeIndex] = merger.merge(tree[leftTreeIndex],tree[rightTreeIndex]); &#125; @Override public String toString()&#123; StringBuilder res = new StringBuilder(); res.append('['); for(int i = 0 ;i &lt; tree.length ; i++)&#123; if(tree[i] != null) res.append(tree[i]); else res.append("null"); if(i != tree.length - 1) res.append(","); &#125; res.append(']'); return res.toString(); &#125;&#125; 在Main函数中进行测试，并打印结果：12345678910111213public class Main &#123; public static void main(String[] args)&#123; Integer[] nums = &#123;-2,0,3,-5,2,-1&#125;; SegmentTree&lt;Integer&gt; segTree = new SegmentTree&lt;&gt;(nums, new Merger&lt;Integer&gt;() &#123; @Override public Integer merge(Integer a, Integer b) &#123; return a + b; &#125; &#125;); System.out.println(segTree); &#125;&#125; 生成如下的线段树： 线段树的查询操作用户可以输入要查询的某个区间，返回这个区间内的对应的值。 相应的方法：1234567891011121314151617181920212223242526272829303132333435/* 返回要查询的区间[queryStart,queryEnd]的值 */ public E query(int queryStart,int queryEnd)&#123; if(queryStart &lt; 0 || queryStart &gt;= data.length || queryEnd &lt; 0 || queryEnd &gt;= data.length || queryStart &gt; queryEnd) throw new IllegalArgumentException("Illegal Index"); return query(0,0,data.length-1,queryStart,queryEnd); &#125; /** * 定义私有函数，在以treeIndex为根的线段树中[l,r]的范围里，搜索区间[queryStart,queryEnd]的值 * @param treeIndex 要查询的树的根节点 * @param l 树对应的数组的左范围 * @param r 树对应的数组的右范围 * @param queryStart 要查询的区间的左端 * @param queryEnd 要查询的区间的右端 */ private E query(int treeIndex,int l,int r,int queryStart,int queryEnd)&#123; if(l == queryStart &amp;&amp; r == queryEnd) return tree[treeIndex]; int mid = l + (r - l) / 2; int leftIndex = leftChild(treeIndex); int rightIndex = rightChild(treeIndex); if(queryStart &gt;= mid+1) return query(rightIndex,mid+1,r,queryStart,queryEnd); else if(queryEnd &lt;= mid) return query(leftIndex,l,mid,queryStart,queryEnd); //否则，跨左右区间，分别求左右区间的值，然后merg，返回 E leftResult = query(leftIndex,l,mid,queryStart,mid); E rightResult = query(rightIndex,mid+1,r,mid+1,queryEnd); return merger.merge(leftResult,rightResult); &#125; 线段树的更新操作线段树的更新，是针对某个index位置的数据进行更新，使用线段树进行更新操作，其时间有优势，时间复杂度为O(logn) 更新操作的代码如下：1234567891011121314151617181920212223242526272829/** * 将index位置的值更新为e * @param index 待更新的位置索引 * @param e 更新后的值 */public void set(int index,E e)&#123; if(index &lt; 0 || index &gt;= data.length) throw new IllegalArgumentException("Illegal index."); set(0,0,data.length - 1,index,e);&#125;//在以treeIndex为根的线段树中，更新index的值为eprivate void set(int treeIndex,int l , int r,int index,E e)&#123; if(l == r)&#123; tree[treeIndex] = e; return; &#125; int mid = l +(r - l) / 2; int leftTreeIndex = leftChild(treeIndex); int rightTreeIndex = rightChild(treeIndex); if(index &gt;= mid+1) set(rightTreeIndex,mid+1,r,index,e); else //index &lt;= mid set(leftTreeIndex,l,mid,index,e); tree[treeIndex] = merger.merge(tree[leftTreeIndex],tree[rightTreeIndex]);&#125;]]></content>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实现优先队列]]></title>
    <url>%2F2018%2F08%2F12%2F%E5%AE%9E%E7%8E%B0%E4%BC%98%E5%85%88%E9%98%9F%E5%88%97%2F</url>
    <content type="text"><![CDATA[优先队列的一个最普遍的应用就是计算机在进行任务调度时，动态选择优先级最高的任务执行。即完成了一个操作后，后面的操作并不一定按照之前的优先级顺序执行，有可能会有其他优先级更高的任务插入，或者出现更加复杂的情况。在其他领域，只要涉及动态选择优先级顺序的情况，都需要用到优先队列。 当然优先队列不只有利于解决动态问题，解决一些静态问题也有优势。比如取出N各元素中的前M个元素。如果使用排序，其时间复杂度为O(NlogN),但是如果使用优先队列的话，其时间复杂度为O(NlogM). 优先队列有两个主要操作:入队和出队(取出优先级最高的元素). 数据结构 插入元素(入队) 删除最大元素(出队) 普通数组 O(1) O(n) 顺序数组 O(n) O(1) 堆 O(logn) O(logn) 使用堆实现优先队列时，对于总共N个请求：使用普通数组或者顺序数组，最差的情况:O(n^2)，使用堆:O(nlogn) 优先队列一般都是基于堆的，所以先写堆的实现。 堆详情可以看 堆和堆排序 中堆的算法部分. 堆中某个节点的值总是不大于或不小于其父节点的值(所以分为最大堆和最小堆) 堆总是一棵完全二叉树 另外要注意：节点值的大小和节点所处的层次是没有关系的 本文以最大堆为例，进行书写。 堆的表示方法——动态数组堆的一种底层表示方法是使用动态数组实现，关于动态数组的实现见：https://homxuwang.github.io/2018/07/17/%E6%95%B0%E7%BB%84/ 堆的实现代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899public class MaxHeap&lt;E extends Comparable&lt;E&gt;&gt; &#123; private Array&lt;E&gt; data; public MaxHeap(int capacity)&#123; data = new Array&lt;&gt;(capacity); &#125; public MaxHeap()&#123; data = new Array&lt;&gt;(); &#125; //返回堆中的元素个数 public int size() &#123; return data.getSize(); &#125; //判断堆是否为空 public boolean isEmpty() &#123; return data.isEmpty(); &#125; //返回一个完全二叉树的数组表示中，一个索引所表示的元素的父亲节点的索引 private int parent(int index)&#123; if(index == 0) throw new IllegalArgumentException("index 0 doesn't have parent;"); return (index-1) / 2; &#125; //返回一个完全二叉树的数组表示中，一个索引所表示的元素的左孩子的索引 private int leftChild(int index)&#123; return index * 2 + 1; &#125; //返回一个完全二叉树的数组表示中，一个索引所表示的元素的右孩子的索引 private int rightChild(int index)&#123; return index * 2 + 2; &#125; //向堆中添加一个元素 public void add(E e)&#123; data.addLast(e); //先向末尾添加元素 //调用siftUp函数，进行上浮操作,以维护堆的性质 siftUp(data.getSize() - 1); &#125; //需要上浮的元素的索引 private void siftUp(int k)&#123; //如果k满足索引大于零,且其父亲节点的值小于它的值，则进行上浮操作 while (k &gt; 0 &amp;&amp; data.get(parent(k)).compareTo(data.get(k)) &lt; 0)&#123; swap(k,parent(k)); k = parent(k); &#125; &#125; //定义一个交换的函数 private void swap(int i , int j)&#123; if(i &lt; 0 || i &gt;= data.getSize() || j &lt; 0 || j&gt;= data.getSize())&#123; throw new IllegalArgumentException("Index is illegal."); &#125; E temp = data.get(i); data.set(i,data.get(j)); data.set(j,temp); &#125; //查看堆中的最大元素 public E findMax()&#123; if(data.getSize() == 0) throw new IllegalArgumentException("Can not findMac in a empty heap"); return data.get(0); &#125; //取出堆中的最大元素 public E extractMax()&#123; E ret = findMax(); swap(0,data.getSize() - 1); data.removeLast(); siftDown(0); return ret; &#125; //下沉操作 private void siftDown(int k)&#123; while (leftChild(k) &lt; data.getSize())&#123;//如果k没有左右孩子，则循环终止 int j = leftChild(k);//j为k的左孩子的索引 if(j + 1 &lt; data.getSize() &amp;&amp; data.get( j + 1 ).compareTo(data.get(j)) &gt; 0)&#123;//如果k有右孩子,并且右孩子的节点的值大于左孩子的节点的值 j = rightChild(k); //此时，data[j]是leftChild和rightChild中的最大值 &#125; if(data.get(k).compareTo(data.get(j)) &gt;= 0) // 如果k的值大于其左右孩子的值，则满足了最大堆的另一个性质，可以退出循环体 break; //否则交换k和j位置,并且k赋值为j，继续进行下一个循环 swap(k,j); k = j; &#125; &#125;&#125; 在数组中，一个父亲节点其所有节点的索引(假设索引以0开始)为： 12Index_leftChild = index_Father * 2 + 1;Index_rightChild = index_Father * 2 + 2; 如果知道一个孩子节点的索引(假设索引以0开始)求其父亲节点的索引: 1index_Father = ( index_Child - 1 ) / 2; 上面两个求法可以用数学归纳法进行证明。 Sift Up操作很简单，即新添加的元素先放到数组的末尾位置，这时候满足了完全二叉树的性质。但是它不一定满足总是不大于或者不小于其父亲节点的值。所以这时候的操作是，这个值要与它的父亲节点、爷爷节点……对比，直到放在合适的位置。因为方法中总结了找到一个节点的父亲节点的方法:faterIndex = (index-1) / 2。以一个最大堆为例，让新加入的节点和父亲节点对比，如果它大于其父亲节点，则交换，继续对比其父亲节点，直到它小于等于其父亲节点为止。 Sift Down操作，即取出最大堆的堆顶的元素（取出操作只能取出这个最大的元素，而不能取出别的元素）。因为最大的元素取出后，其左右的树结构就是两个单独的子树，那么要给这两个子树找一个新的父节点，操作如下： 将堆中的最后一个元素放在堆顶 删除掉最后一个元素（这时候满足完全二叉树的性质） 将堆顶元素与左右孩子中大于它且较大的数进行交换 交换后的新位置继续与孩子节点中大于它且较大的数进行交换 继续操作直到它大于其左后孩子或者它没有左右孩子 优点：add操作和extractMax操作的时间复杂度都是O(logn) 其中swap交换函数我写在了堆这个类中，可以在Array类中定义交换函数，在堆类中直接调用即可。 Heapify 和 replace操作replacereplace：取出最大元素后放入新元素 step1: extractMax -&gt; step2: add (2O(logn)) setp1: 替换堆顶元素 -&gt; step2: Sift Down (O(logn))1234567//取出堆中的最大元素，并替换成epublic E replace(E e)&#123; E ret = findMax(); data.set(0,e); siftDown(0); return ret;&#125; Heapify将任意数组整理成堆的形状 扫描数组，放如堆的新的对象中再返回 (O(nlogn)) 可以先把数组看成一棵完全二叉树,从最后一个非叶子节点开始进行Sift Down操作。(找到最后一个非叶子节点的方法：拿到最后一个节点的索引，然后计算他的父亲节点的索引即可) step1: 找到最后一个非叶子节点，进行Sift Down操作 step2: 倒数第二个非叶子节点进行Sift Down操作 … 以此类推 直到索引为0的非叶子节点完成Sift Down操作 O(n) Heapify一般可以在构造函数中进行(用一个数组初始化堆)),所以Array类也要支持一个用数组初始化动态数组的构造函数 Array类的构造函数： 1234567public Array(E[] arr) &#123; data = (E[])new Object[arr.length]; for(int i = 0 ; i &lt; arr.length ; i ++)&#123; data[i] = arr[i]; &#125; size = arr.length;&#125; MaxHeap的构造函数：12345public MaxHeap(E[] arr)&#123; data = new Array&lt;&gt;(arr); for(int i = parent(arr.length - 1) ; i &gt;= 0 ; i--) siftDown(i);&#125; 优先队列优先队列就是一个队列，要满足队列的所有属性方法，所以要实现队列接口，关于队列的实现见：https://homxuwang.github.io/2018/07/17/%E5%AE%9E%E7%8E%B0%E6%A0%88%E5%92%8C%E9%98%9F%E5%88%97/ 优先队列的底层是使用堆来实现，同样还是以最大堆为例。 123456789101112131415161718192021222324252627282930public class PriorityQueue&lt;E extends Comparable&lt;E&gt;&gt; implements Queue&lt;E&gt; &#123; private MaxHeap&lt;E&gt; maxHeap; public PriorityQueue()&#123; maxHeap = new MaxHeap&lt;&gt;(); &#125; @Override public int getSize() &#123; return maxHeap.size(); &#125; @Override public boolean isEmpty() &#123; return maxHeap.isEmpty(); &#125; @Override public void enqueue(E e) &#123; maxHeap.add(e); &#125; @Override public E dequeue() &#123; return maxHeap.extractMax(); &#125; @Override public E getFront() &#123; return maxHeap.findMax(); &#125;&#125; java.util中的PriorityQueue默认是用的最小堆，具体方法名也有一些区别]]></content>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用户日志统计分析]]></title>
    <url>%2F2018%2F08%2F04%2F%E7%94%A8%E6%88%B7%E6%97%A5%E5%BF%97%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[离线数据处理架构 用户通过APP或者PC访问服务器 这期间的操作日志信息用nginx记录下来(如记录在/var/log/access.log)，记录在nginx服务器 借助Flume框架，将日志信息抽取到HDFS中（命名规则根据自己的需要进行修改） 离线数据处理的第一步就是清理脏数据，清理完脏数据后继续写入HDFS 继续进行Hive（这里的Hive是一个统称，如Spark Sql） 处理完的结果写入RDBMS中或者NO SQL中 进行相应的数据输出(如表格，图形化输出) 离线处理的作业过程可以使用任务调度框架（Oozie或Azkaban）进行任务调度，安排每天进行作业的时间。可以将4.5.步骤串联起来 Kafka可以将日志进行实时处理，数据来了后进行立即处理（这是实时流处理） 本篇是写离线处理的一些代码分析和实现。 首先看数据(数据来自粉丝日志的教程，地址：http://blog.fens.me/hadoop-mapreduce-log-kpi/)： 摘取某一行的内容如下：1222.68.172.190 - - [18/Sep/2013:06:49:57 +0000] &quot;GET /images/my.jpg HTTP/1.1&quot; 200 19939 &quot;http://www.angularjs.cn/A00n&quot; &quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/29.0.1547.66 Safari/537.36&quot; 12345678remote_addr: 记录客户端的ip地址, 222.68.172.190remote_user: 记录客户端用户名称, –time_local: 记录访问时间与时区, [18/Sep/2013:06:49:57 +0000]request: 记录请求的url与http协议, “GET /images/my.jpg HTTP/1.1”status: 记录请求状态,成功是200, 200body_bytes_sent: 记录发送给客户端文件主体内容大小, 19939http_referer: 用来记录从那个页面链接访问过来的, “http://www.angularjs.cn/A00n”http_user_agent: 记录客户浏览器的相关信息, “Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/29.0.1547.66 Safari/537.36” 目的： 根据日志信息抽取出浏览器的信息 针对不同的浏览器进行统计 单机版代码分析和实现新建一个类，进行单机的测试下载配置userAgentParser工具类在github上下载工具类：https://github.com/LeeKemp/UserAgentParser 然后在本地用maven进行编译，编译的时候我出现了一些问题，见文章最后的踩坑记。 新建一个单元测试方法：输入一行数据进行测试 进行日志读取分析，统计访问网站的浏览器的数据相应的引入的文件：12345678910111213141516171819202122232425262728293031323334353637383940 public void readFileTest() throws IOException&#123; //文件路径 String path = "/home/hadoop/study/data/access.log.10"; BufferedReader reader = new BufferedReader( new InputStreamReader(new FileInputStream(new File(path))) ); String line = ""; UserAgentParser userAgentParser = new UserAgentParser(); UserAgent agent ; Map&lt;String,Integer&gt; broswerMap = new HashMap&lt;String,Integer&gt;(); int i = 0 ; while(line != null)&#123; line = reader.readLine(); //一次读一行数据 i ++; if(StringUtils.isNotBlank(line))&#123;//判断是否为空 String source = line.substring(getCharacterPosition(line,"\"",5) + 1); agent = userAgentParser.parse(source); String browser = agent.getBrowser(); String engine = agent.getEngine(); String engineVersion = agent.getEngineVersion(); String os = agent.getOs(); String platform = agent.getPlatform(); boolean isMobile = agent.isMobile(); Integer broswerValue = broswerMap.get(browser); if(broswerMap.get(browser) != null)&#123; broswerMap.put(browser,broswerValue + 1); &#125;else&#123; broswerMap.put(browser,1); &#125;// System.out.println(browser + "," + engine + " " + engineVersion + " " + os + " " + platform); &#125; &#125; //一共有多少条数据 System.out.println(i-1); System.out.println("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"); for(Map.Entry&lt;String,Integer&gt; entry : broswerMap.entrySet())&#123; System.out.println(entry.getKey() + " : " + entry.getValue()); &#125; &#125; 定义私有方法获取指定字符串中指定标识的字符串出现的索引，分析某一行的用户日志，可以使用&quot;进行分割，获取访问的客户端等信息在第5个&quot;，所以operator = &quot;,index = 5123456789101112131415/** * 获取指定字符串中指定标识的字符串出现的索引 * @return */private int getCharacterPosition(String value,String operator,int index)&#123; Matcher slashMatcher = Pattern.compile(operator).matcher(value); int mIndex = 0; while(slashMatcher.find())&#123; mIndex ++; if(mIndex == index)&#123; break; &#125; &#125; return slashMatcher.start();&#125; 可以查看打印结果： MapReduce实现需求统计新建类 代码编写可以参考 MapReduce的补充和WordCount简单实战(二):https://homxuwang.github.io/2018/07/29/MapReduce%E7%9A%84%E8%A1%A5%E5%85%85%E5%92%8CWordCount%E7%AE%80%E5%8D%95%E5%AE%9E%E6%88%982/123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115package com.hadoop.project;import com.kumkee.userAgent.UserAgent;import com.kumkee.userAgent.UserAgentParser;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;import java.util.regex.Matcher;import java.util.regex.Pattern;/** * 使用MapReduce 统计浏览器的访问次数 */public class LogApp &#123; public static class MyMapper extends Mapper&lt;LongWritable,Text,Text,LongWritable&gt; &#123; LongWritable plusone = new LongWritable(1); private UserAgentParser userAgentParser ; @Override protected void setup(Context context) throws IOException, InterruptedException &#123; userAgentParser = new UserAgentParser(); &#125; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; UserAgent agent ; //先读入没一行数据 String line = value.toString(); String source = line.substring(getCharacterPosition(line,"\"",5) + 1); agent = userAgentParser.parse(source); String browser = agent.getBrowser(); context.write(new Text(browser),plusone); &#125; @Override protected void cleanup(Context context) throws IOException, InterruptedException &#123; userAgentParser = null; &#125; &#125; public static class MyReducer extends Reducer&lt;Text,LongWritable,Text,LongWritable&gt;&#123; @Override protected void reduce(Text key, Iterable&lt;LongWritable&gt; values, Context context) throws IOException, InterruptedException &#123; long sum = 0 ; for(LongWritable value : values)&#123; sum += value.get();//通过get()转化为java中的类型 &#125; //最终统计结果输出 context.write(key,new LongWritable(sum)); &#125; &#125; /** * 获取指定字符串中指定标识的字符串出现的索引 * @return */ private static int getCharacterPosition(String value,String operator,int index)&#123; Matcher slashMatcher = Pattern.compile(operator).matcher(value); int mIndex = 0; while(slashMatcher.find())&#123; mIndex ++; if(mIndex == index)&#123; break; &#125; &#125; return slashMatcher.start(); &#125; public static void main(String[] args) throws IOException,ClassNotFoundException,InterruptedException &#123; Configuration configuration = new Configuration(); Path outputPath = new Path(args[1]); FileSystem fileSystem = FileSystem.get(configuration); if(fileSystem.exists(outputPath))&#123; fileSystem.delete(outputPath,true); System.out.println("filePath exists,but it has deleted"); &#125; Job job = Job.getInstance(configuration,"LogBrowser"); job.setJarByClass(LogApp.class); FileInputFormat.setInputPaths(job,new Path(args[0])); job.setMapperClass(LogApp.MyMapper.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(LongWritable.class); job.setCombinerClass(LogApp.MyReducer.class); job.setReducerClass(LogApp.MyReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(LongWritable.class); FileOutputFormat.setOutputPath(job,new Path(args[1])); System.out.println(job.waitForCompletion(true)? 0 : 1); &#125;&#125; 配置pom.xml文件在生产环境中hadoop的包是不需要的，所以加一个&lt;scope&gt;provided&lt;/scope&gt; 而因为UserAgentParser是第三方的包，在mvn打包的时候需要进行配置才能够添加进去，使用maven-assembly-plugin12345678910111213&lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;/configuration&gt;&lt;/plugin&gt; 打包命令使用1mvn assembly:assembly 可以看到打包成功的信息hdfs-1.0-SNAPSHOT-jar-with-dependencies.jar就是打包成功的文件 然后传到指定位置（或者服务器），我这里单机的伪分布式框架，所以传到本地一个目录就可以：1scp hdfs-1.0-SNAPSHOT-jar-with-dependencies.jar ~/lib 然后将日志数据上传到hdfs的根目录: 执行命令1hadoop jar /home/hadoop/lib/hdfs-1.0-SNAPSHOT-jar-with-dependencies.jar com.hadoop.project.LogApp hdfs://localhost:9000/access.log.10 hdfs://localhost:9000/Log/logBrowser 可以看到成功信息在目录中查看：查看结果：这与在单元测试中看到的信息是一样的： 下一步可以写到RDBMS中或者NOSQL数据库中，进一步进行可视化展示这只是单纯的统计了浏览器的记录，在实际应用中，应该根据日志中的时间记录统计每一天或者没一星期的时间周期中的访问记录。 踩到的坑mvn打包时的错误在github仓库下载了UserAgentParser工具类后，需要在自己电脑上打包（mvn clean package -DskipTests）。打包过程中报错：这里报错为(请使用 -source 5 或更高版本以启用泛型)和-source1.3 中不支持泛型.后来发现原来是下载的项目年代过久（已经7年了）。我本地的环境是maven 3.0.5和java 1.7.所以在下载的项目的pom.xml中添加一段代码，用来指定打包的项目所使用的java版本：123456789101112&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.7&lt;/source&gt; &lt;target&gt;1.7&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 这样打包就可以了 maven引入刚才打包的文件出现的问题在打包完成后,还需要将打包后的文件放入maven的本地仓库以便使用(mvn clean install -DskipTest).打包完成后的信息： 可以看到打包完成后的目录在/home/hadoop/.m2/respository/com/kumkee/下，这个是maven的本地默认仓库目录。而我在项目中使用的目录是后来指定的目录，所以在引用时一直不行，后来终于找到了这个问题 可以看到本地的仓库指定在/home/hadoop/文档/maven_repos，所以在打包的时候可以指定打包到本地的maven目录，即mvn clean install -DskipTest -Dmaven.repo.local=/home/hadoop/文档/maven_repos 这样就可以在项目中引入了 MapReduce作业时报错错误信息： 1TaskAttemptListenerImpl: Task: attempt_1533460421274_0001_m_000000_0 - exited : java.io.IOException: Type mismatch in key from map: expected org.apache.hadoop.io.LongWritable, received org.apache.hadoop.io.Text 原来是写成了两个job.setMapOutpuKeyClass()方法，改正后重新编译jar包并运行即可]]></content>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实现映射]]></title>
    <url>%2F2018%2F08%2F01%2F%E5%AE%9E%E7%8E%B0%E6%98%A0%E5%B0%84%2F</url>
    <content type="text"><![CDATA[映射也就是Map,这里是指一对一的映射。也成为字典。 映射迎来存储键值数据对(Key,Value) 根据Key寻找对应的Value 用链表或者二分搜索树实现比较简单 BST的结构:123456class Node &#123; K key; V value; Node left; Node right;&#125; 链表的结构:12345class Node &#123; K key; V value; Node next;&#125; 即本来存储一个数据的节点现在存储两个(Key,Value). 定义Map接口:123456789public interface Map&lt;K,V&gt; &#123; void add(K key,V value); V remove(K key); //删除key对应的键值对,并返回值 boolean contains(K key); //是否存已在key V get(K key); //获得key对应的value值 void set(K key,V newValue); //更新值 int getSize(); boolean isEmpty();&#125; 基于链表实现Map123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100public class LinkedListMap&lt;K,V&gt; implements Map&lt;K,V&gt; &#123; //定义节点 private class Node &#123; public K key; public V value; public Node next; public Node(K key,V value,Node node)&#123; this.key = key; this.value = value; this.next = null; &#125; public Node(K key)&#123; this(key,null,null); &#125; public Node()&#123; this(null,null,null); &#125; public String toString()&#123; return key.toString() + ":" + value.toString(); &#125; &#125; private Node dummyHead; private int size; public LinkedListMap()&#123; dummyHead = new Node(); size = 0; &#125; @Override public void add(K key, V value) &#123; Node node = getNode(key); if(node == null)&#123; dummyHead.next = new Node(key,value,dummyHead.next); size ++; &#125; else //如果已经存在,则进行值的覆盖 node.value = value; &#125; @Override public V remove(K key) &#123; Node prev = dummyHead; while(prev.next != null)&#123; if(prev.next.key.equals(key)) break; prev = prev.next; &#125; Node delNode; if(prev.next != null)&#123; delNode = prev.next; prev.next = delNode.next; delNode.next = null; return delNode.value; &#125; return null; &#125; @Override public boolean contains(K key) &#123; return getNode(key) != null; &#125; @Override public V get(K key) &#123; Node result = getNode(key); return result == null ? null : result.value; &#125; @Override public void set(K key, V newValue) &#123; Node result = getNode(key); if(result != null) result.value = newValue; else throw new IllegalArgumentException(key + "doesn't exists"); &#125; @Override public int getSize() &#123; return size; &#125; @Override public boolean isEmpty() &#123; return size == 0; &#125; private Node getNode(K k)&#123; Node cur = dummyHead.next; while(cur != null)&#123; if(cur.key.equals(k)) return cur; cur = cur.next; &#125; return null; &#125; &#125; 基于二分搜索树实现Map123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158public class BSTMap&lt;K extends Comparable&lt;K&gt;,V&gt; implements Map&lt;K,V&gt; &#123; private class Node &#123; public K key; public V value; public Node left,right; public Node(K key,V value)&#123; this.key = key; this.value = value; this.left = null; this.right = null; &#125; &#125; private Node root; private int size; @Override //向二分搜索树中添加新元素(key,value) public void add(K key, V value) &#123; root = add(root,key,value); &#125; @Override //借助辅助函数remove(Node node,K key) public V remove(K key) &#123; Node node = getNode(root,key); if(node != null)&#123; root = remove(root,key); return node.value; &#125; return null; &#125; @Override public boolean contains(K key) &#123; return getNode(root,key) != null ? true : false; &#125; @Override public V get(K key) &#123; Node node = getNode(root,key); return node == null ? null : node.value; &#125; @Override public void set(K key, V newValue) &#123; Node node = getNode(root,key); if(node == null) throw new IllegalArgumentException(key + "doesn't exist!"); node.value = newValue; &#125; @Override public int getSize() &#123; return size; &#125; @Override public boolean isEmpty() &#123; return size == 0; &#125; //向以node为根的二分搜索树中插入元素,采用递归算法 //返回插入新节点后二分搜索树的根 private Node add(Node node,K key,V value)&#123; if(node == null)&#123; size ++; return new Node(key,value); &#125; if(key.compareTo(node.key) &lt; 0)&#123; node.left = add(node.left,key,value); &#125; if(key.compareTo(node.key) &gt; 0)&#123; node.right = add(node.right,key,value); &#125; else //key.compareTo(node.key)==0 node.value = value; return node; &#125; //返回以node为根节点的二分搜索树中,key所在的节点 private Node getNode(Node node,K key)&#123; if(node == null)&#123; return null; &#125; if(key.compareTo(node.key) == 0)&#123; return node; &#125; if(key.compareTo(node.key) &lt; 0)&#123; return getNode(node.left,key); &#125; else&#123; //key.compareTo(node.key) &gt; 0) return getNode(node.right,key); &#125; &#125; //删除掉以node为根的二分搜索树中键为key的节点 //返回删除节点后新的二分搜索树的根 private Node remove(Node node,K key)&#123; if(node == null) return null; if(key.compareTo(node.key) &lt; 0)&#123; node.left = remove(node.left,key); return node; &#125; else if(key.compareTo(node.key) &gt; 0 )&#123; node.right = remove(node.right,key); return node; &#125; else&#123;//key.compareTo(node.key) == 0 //待删除左子树为空 if(node.left == null)&#123; Node rightNode = node.right; node.right = null; size --; return rightNode; &#125; //待删除右子树为空 if(node.right == null)&#123; Node leftNode = node.left; node.left = null; size --; return leftNode; &#125; //待删除左右子树均不为空 //先找到比待删除节点大的最小节点,即待删除节点右子树的最小节点 //用这个节点顶替待删除节点的位置 Node successor = minmum(node.right); successor.right = removeMin(node.right); successor.left = node.left; node.left = node.right = null; return successor; &#125; &#125; //返回以node为根的二分搜索树的最小值所在的节点 private Node minmum(Node node)&#123; if(node.left == null)&#123; return node; &#125; return minmum(node.left); &#125; //删除掉以node为根的二分搜索树中的最小节点 //返回删除节点后的最新的二分搜索树的根 private Node removeMin(Node node)&#123; if(node.left == null)&#123; Node rightNode = node.right; node.right = null; size --; return rightNode; &#125; node.left = removeMin(node.left); return node; &#125;&#125; 其实明显可以感受到BSTMap的存储速度要快于LinkedListMap 基于链表的Map和基于二分搜索树的Map的时间复杂度对比 操作 LinkedListMap BSTMap(平均) BSTMap最差) 增add O(n) O(logn) O(n) 删remove O(n) O(logn) O(n) 改set O(n) O(logn) O(n) 查get O(n) O(logn) O(n) 查contains O(n) O(logn) O(n)]]></content>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实现集合]]></title>
    <url>%2F2018%2F08%2F01%2F%E5%AE%9E%E7%8E%B0%E9%9B%86%E5%90%88%2F</url>
    <content type="text"><![CDATA[因为集合是不存放重复元素的，所以使用二分搜索树来实现集合很合适。集合的几个简单方法: void add(E) //添加元素，但不能添加重复元素 void remove(E) //删除元素E boolean contains(E) //是否包含元素E int getSize() //获得集合的size boolean isEmpty() //判断集合是否为空 基于二分搜索树的实现首先使用二分搜索树做底层实现集合类。二分搜索树的实现代码：https://homxuwang.github.io/2018/07/18/%E4%BA%8C%E5%88%86%E6%90%9C%E7%B4%A2%E6%A0%91/ 创建集合接口类Set：1234567public interface Set&lt;E&gt; &#123; void add(E e); void remove(E e); boolean isEmpty(); boolean containes(E e); int getSize();&#125; 然后创建实现集合的类BSTSet:123456789101112131415161718192021222324252627282930public class BSTSet&lt;E extends Comparable&lt;E&gt;&gt; implements Set&lt;E&gt; &#123; private BST&lt;E&gt; bst; public BSTSet() &#123; bst = new BST&lt;&gt;(); &#125; @Override public void add(E e) &#123; bst.add(e); &#125; @Override public void remove(E e) &#123; bst.remove(e); &#125; @Override public boolean isEmpty() &#123; return bst.isEmpty(); &#125; @Override public boolean containes(E e) &#123; return bst.contains(e); &#125; @Override public int getSize() &#123; return bst.size(); &#125;&#125; 其实BSTSet集合类只是简单的调用了其基础类BST类的方法。就可以实现集合类的各个功能。 基于链表的实现链表结构的实现首先链表的实现代码:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180public class LinkedList&lt;E&gt; &#123; private class Node&#123; public E e; public Node next; /** * 构造函数 * @param e * @param next */ public Node(E e,Node next)&#123; this.e = e; this.next = next; &#125; public Node(E e)&#123; this(e,null); &#125; public Node()&#123; this(null,null); &#125; @Override public String toString()&#123; return e.toString(); &#125; &#125; private Node dummyHead; private int size; public LinkedList()&#123; dummyHead = new Node(); size = 0; &#125; //返回链表中的元素个数 public int getSize()&#123; return size; &#125; //返回链表是否为空 public boolean isEmpty()&#123; return size == 0; &#125; //在链表头添加新的元素e public void addFirst(E e)&#123;// Node node = new Node(e);// node.next = head;// head = node; //和上面三行相等 add(0,e); &#125; //在链表末尾添加新的元素e public void addLast(E e)&#123; add(size,e); &#125; //在中间位置插入新的元素e public void add(int index,E e)&#123; if(index &lt; 0 || index &gt; size )&#123; throw new IllegalArgumentException("illegal index"); &#125; Node prev = dummyHead; for(int i = 0 ; i &lt; index ; i++)&#123; prev = prev.next; &#125; Node node = new Node(e); node.next = prev.next; prev.next = node; size ++; &#125; //获得链表中index位置的元素 public E get(int index)&#123; if(index &lt; 0 || index &gt;= size )&#123; throw new IllegalArgumentException("Get failed.Illegal index."); &#125; Node cur = dummyHead.next; for(int i = 0 ; i &lt; index ; i++)&#123; cur = cur.next; &#125; return cur.e; &#125; //获得链表的第一个元素 public E getFirst()&#123; return get(0); &#125; //获得链表的最后一个元素 public E getLast()&#123; return get(size - 1); &#125; //修改链表index位置的元素为e public void set(int index,E e)&#123; if(index &lt; 0 || index &gt;= size )&#123; throw new IllegalArgumentException("Set failed.Illegal index."); &#125; Node cur = dummyHead.next; for(int i = 0 ; i &lt; index ; i++ )&#123; cur = cur.next; &#125; cur.e = e; &#125; //检查链表中是否有元素e public boolean contains(E e)&#123; Node cur = dummyHead.next; while(cur != null)&#123; if(cur.e.equals(e)) return true; cur = cur.next; &#125; return false; &#125; // 从链表中删除index(0-based)位置的元素, 返回删除的元素 // 在链表中不是一个常用的操作，练习用：） public E remove(int index)&#123; if(index &lt; 0 || index &gt;= size) throw new IllegalArgumentException("Remove failed. Index is illegal."); Node prev = dummyHead; for(int i = 0 ; i &lt; index ; i ++) prev = prev.next; Node retNode = prev.next; prev.next = retNode.next; retNode.next = null; size --; return retNode.e; &#125; // 从链表中删除第一个元素, 返回删除的元素 public E removeFirst()&#123; return remove(0); &#125; // 从链表中删除最后一个元素, 返回删除的元素 public E removeLast()&#123; return remove(size - 1); &#125; // 从链表中删除元素e public void removeElement(E e)&#123; Node prev = dummyHead; while(prev.next != null)&#123; if(prev.next.e.equals(e)) break; prev = prev.next; &#125; if(prev.next != null)&#123; Node delNode = prev.next; prev.next = delNode.next; delNode.next = null; &#125; &#125; @Override public String toString()&#123; StringBuilder res = new StringBuilder(); Node cur = dummyHead.next; while (cur != null)&#123; res.append(cur.e + "-&gt;"); cur = cur.next; &#125; res.append("NULL"); return res.toString(); &#125;&#125; 基于链表的集合的实现12345678910111213141516171819202122232425262728293031public class LinkedListSet&lt;E&gt; implements Set&lt;E&gt; &#123; private LinkedList&lt;E&gt; list; public LinkedListSet() &#123; list = new LinkedList&lt;&gt;(); &#125; @Override public void add(E e) &#123; if(!list.contains(e)) //判断是否已存在元素e list.addFirst(e); //在链表头添加元素比较省时间 &#125; @Override public void remove(E e) &#123; list.removeElement(e); &#125; @Override public boolean isEmpty() &#123; return list.isEmpty(); &#125; @Override public boolean containes(E e) &#123; return list.contains(e); &#125; @Override public int getSize() &#123; return list.getSize(); &#125;&#125; 时间复杂度对比 操作 LinkedListSet BSTSet 增add O(n) O(logn) 删remove O(n) O(logn) 查contains O(n) O(logn) BSTSet的时间复杂度O(logn)的情况是指平均情况，在极端情况下,有可能就是一个链表，这时候的时间复杂度就是O(n). 测试用例用代码比较两种数据结构的时间,测试类FileOperation,找一个英文电子书进行统计分析,统计其词汇总数和词汇量.1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import java.io.BufferedInputStream;import java.io.File;import java.io.FileInputStream;import java.io.IOException;import java.util.ArrayList;import java.util.Locale;import java.util.Scanner;public class FileOperation &#123; //读取文件名称为filename中的内容，并且将其中包含的所有词语放进words中 public static boolean readFiles(String filename,ArrayList&lt;String&gt; words)&#123; if(filename == null || words == null)&#123; System.out.println("filename is null or words is null"); return false; &#125; //文件读取 Scanner scanner; try&#123; File file = new File(filename); if(file.exists())&#123; FileInputStream fis = new FileInputStream(file); scanner = new Scanner(new BufferedInputStream(fis),"UTF-8"); scanner.useLocale(Locale.ENGLISH); &#125;else return false; &#125;catch (IOException e)&#123; System.out.println("cannot open "+ filename); return false; &#125; ///简单分词 if(scanner.hasNextLine())&#123; String contents = scanner.useDelimiter("\\A").next(); int start = firstCharacterIndex(contents,0); for(int i = start + 1; i &lt; contents.length();)&#123; if(i == contents.length() || !Character.isLetter(contents.charAt(i)))&#123; String word = contents.substring(start,i).toLowerCase(); words.add(word); start = firstCharacterIndex(contents,i); i = start +1; &#125;else i++; &#125; &#125; return true; &#125; private static int firstCharacterIndex(String s,int start)&#123; for(int i = start;i&lt;s.length();i++)&#123; if(Character.isLetter(s.charAt(i))) return i; &#125; return s.length(); &#125;&#125; Main函数：1234567891011121314151617181920212223242526import java.util.ArrayList;public class Main &#123; public static void main(String[] srgs)&#123; System.out.println("Harry"); ArrayList&lt;String&gt; words1 = new ArrayList&lt;&gt;(); FileOperation.readFiles("C:\\Users\\homxu\\Desktop\\Harry Potter and the Sorcerer'_one .mobi",words1); System.out.println("Total words:"+words1.size()); BSTSet&lt;String&gt; set1 = new BSTSet&lt;&gt;(); for(String word:words1) set1.add(word); System.out.println("Dif Words:" + set1.getSize()); ArrayList&lt;String&gt; words2 = new ArrayList&lt;&gt;(); FileOperation.readFiles("C:\\Users\\homxu\\Desktop\\Harry Potter and the Sorcerer'_one .mobi",words2); System.out.println("Total words:"+words2.size()); LinkedListSet&lt;String&gt; set2 = new LinkedListSet&lt;&gt;(); for(String word:words2)&#123; set2.add(word); &#125; System.out.println("Dif Words:" + set2.getSize()); &#125;&#125; 我的电脑上的运行结果，使用两种底层的数据结构实现的集合类结果是一样的。但是运行过程中明显感觉到BSTSet的速度要比LinkedListSet的速度快]]></content>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce的补充和WordCount简单实战(二)]]></title>
    <url>%2F2018%2F07%2F29%2FMapReduce%E7%9A%84%E8%A1%A5%E5%85%85%E5%92%8CWordCount%E7%AE%80%E5%8D%95%E5%AE%9E%E6%88%982%2F</url>
    <content type="text"><![CDATA[MapReduce编程之WordCount基于JAVA开发 Text参数继承了BinaryComparable并实现了WritableComparable接口，可以把它理解为JAVA里面的字符串。 首先看一些基本的知识： Mapper类 Context代表的上下文 setup代表任务开始的时候执行的操作，且只执行一次。可以在这里进行数据库链接等操作 map方法，对于输入的input在每个键值对出发的时候就调用 cleanup方法是表示在任务结束的时候执行一次 run方法不需要手动调用 关键点是重写map方法 Reducer类 setup和cleanup方法同mapper方法类似 reduce方法，每个键值对都会被调用一次 run是写好的模板模式 关键点是重写reduce方法 先写好基本框架 然后复写map和reduce方法 完整代码：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697package hadoop.hdfs.mapreduce;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;/** * 使用MapReduce开发WordCount应用程序 */public class WordCountApp &#123; /** * Map:读取输入的文件 */ public static class MyMapper extends Mapper&lt;LongWritable,Text,Text,LongWritable&gt;&#123; //定义一个count LongWritable plusone = new LongWritable(1); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; //先读入每一行数据 String line = value.toString(); //然后按照指定分隔符分割 String[] words = line.split(" "); for(String word:words)&#123; //使用上下文context进行输出 context.write(new Text(word),plusone); &#125; &#125; &#125; /** * Reduce:归并操作 */ public static class MyReducer extends Reducer&lt;Text,LongWritable,Text,LongWritable&gt;&#123; @Override //Iterable&lt;LongWritable&gt; values是一个集合，因为一个单词交到一个reduce去处理，所以会出现多次 protected void reduce(Text key, Iterable&lt;LongWritable&gt; values, Context context) throws IOException, InterruptedException &#123; long sum = 0; for(LongWritable value:values)&#123; sum += value.get();//通过get()转化为java中的类型 &#125; //最终统计结果的输出 写入上下文 context.write(key,new LongWritable(sum)); &#125; &#125; /** * 定义Driver方法，封装MapReduce作业的所有信息 * @param args */ public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; //step1 创建configuration()(org.apache.hadoop.conf.Configuration) Configuration configuration = new Configuration(); //step2 Job.getInstance()拿到一个实例 (org.apache.hadoop.mapreduce.Job;) Job job = Job.getInstance(configuration,"wordcount"); //step3 设置job的处理类(即创建的这个类) job.setJarByClass(WordCountApp.class); //step4 设置作业处理的输入路径 FileInputFormat.setInputPaths(job,new Path(args[0])); //step5 设置Map相关参数 //step5.1 设置map处理类 job.setMapperClass(MyMapper.class); //step5.2 设置map的key输出的类型和map的value输出的类型那个 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(LongWritable.class); //step6 设置reduce相关参数 //step6.1 设置reduce处理类 job.setReducerClass(MyReducer.class); //step6.2 设置reduce的key输出类型和value输出类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(LongWritable.class); //step7 设置作业处理的输出路径 FileOutputFormat.setOutputPath(job,new Path(args[1])); //step8 提交作业并输出结果 System.exit(job.waitForCompletion(true)?0:1); &#125;&#125; 然后进行打包编译切换到项目目录运行mvn clean package -DskipTests 可以看到构建成功 编译成功后在target目录下 如果mvn命令不能使用需要安装相应环境 这里由于网络原因我下了好久 orz 将jar包拷贝到指定目录scp hdfs-1.0-SNAPSHOT.jar ~/lib 查看hdfs上的文件,并查看全路径 运行 jar包hadoop jar jar包目录 组类 (在idea中右键选中然后 CopyReference) 要传入的文件路径 要输出的文件路径 我的机器上的指令：hadoop jar /home/hadoop/lib/hdfs-1.0-SNAPSHOT.jar hadoop.hdfs.mapreduce.WordCountApp hdfs://localhost:9000/hello.txt hdfs://localhost:9000/hdfsapi/wordcountresult 输入文件的内容： 可以在浏览器 http://localhost:8088/cluster 查看yarn的作业 查看结果，操作成功 相同的代码和脚本再次运行会报错，因为在MR中，输出文件是不能存在的。所以要在每次运行后换新的路径或者删除旧的文件可以手工建一个shell脚本或者在JAVA代码中进行操作shell 脚本12hdfs dfs rm -r 文件路径hadoop jar jar包目录 组类 (在idea中右键选中然后 CopyReference) 要传入的文件路径 要输出的文件路径 要给脚本加上相应权限chmod u+x 脚本名称 在JAVA中完成自动删除功能在//step1 和 //step2 中间清理已经存在的目录1234567//step1.5 准备删除已经存在的文件目录 Path outputPath = new Path(args[1]); FileSystem fileSystem = FileSystem.get(configuration); if(fileSystem.exists(outputPath))&#123; fileSystem.delete(outputPath,true); System.out.println("filePath exists,but it has deleted"); &#125; 重新编译代码执行，就可以了 可以看到又一行输出ilePath exists,but it has deleted,同时执行成功 MapReduce编程之Combiner 本地的reducer 减少MapTask输出的数据量及网络传输量 在Map过程中先对key相同的值进行一个归并，然后在传输到reduce上，这样就减少了传输的数据数量。可以看成是在map端的一个小的reduce操作。 在上面代码的基础上，在step5的map操作和step6的reduce操作中间价一个combiner操作。12//step5.5 设置Combiner处理类，其实在逻辑上和reduce是一样的job.setCombinerClass(MyReducer.class); 重新编译，拷贝到hdfs目录下，执行 可以看到执行过程中有Combine信息，Combin input records=6和Combin output records=5,说明是生效了的(可以对比之前的操作，combine操作records=0) 虽然它减少了一些数据量，但是它是有适应场景的。在求和、求次数的时候适用，但是在求平均数等操作的时候，结果就会有问题，这里要注意 MapReduce编程之Partitioner Partitioner决定MapTask输出的数据交由哪个ReduceTask处理 默认实现：分发的key的hash值对ReduceTask个数取模 新建一个用来测试的文档：animal.txt 上传到hdfs中： 在ieda中，拷贝一份WordCountApp的代码，命名为PartitionerApp.按照空格拆分其实就是动物名字和动物数量，所以在Mapper类的map方法中修改代码：12345678910111213141516/** * Map:读取输入的文件 */ public static class MyMapper extends Mapper&lt;LongWritable,Text,Text,LongWritable&gt;&#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; //先读入每一行数据 String line = value.toString(); //然后按照指定分隔符分割 String[] words = line.split(" "); context.write(new Text(words[0]),new LongWritable(Long.parseLong(words[1]))); &#125; &#125; 还要新建一个Partition类123456789101112131415public static class MyPartitioner extends Partitioner&lt;Text,LongWritable&gt;&#123; @Override public int getPartition(Text key, LongWritable value, int numPartitions) &#123; if(key.equals("cat"))&#123; return 0; &#125; if (key.equals("dog"))&#123; return 1; &#125; if(key.equals("bird"))&#123; return 2; &#125; return 3; &#125; &#125; 在step6 和 step7之间添加代码：1234//step6.5 设置job的Partition job.setPartitionerClass(MyPartitioner.class); //step6.5.1 设置reduce的数量，不然不生效,这里设置4个，因为MyPartitioner类中有4种情况 job.setNumReduceTasks(4); 然后编译，将执行的类名和输入的数据改一下hadoop jar /home/hadoop/lib/hdfs-1.0-SNAPSHOT.jar hadoop.hdfs.mapreduce.PartitionerApphadoop.hdfs.mapreduce.PartitionerApp hdfs://localhost:9000//hdfsapi/animal.txt hdfs://localhost:9000/hdfsapi/Partitionresult 运行后查看结果 Partition会把符合规则的key送到指定的reduce处理，分别生成相应的结果。 配置jobHistory默认情况下这个功能是不开启的 找到mapreduce的配置mapred-site.xml 在&lt;configuration&gt;&lt;/configuration&gt;中间增加12345678910111213141516171819202122&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;localhost:10020&lt;/value&gt; &lt;description&gt;MR JobHistory Server管理的日志的存放位置&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;localhost:19888&lt;/value&gt; &lt;description&gt;查看历史服务器已经运行完的Mapreduce作业记录的web地址，需要启动该服务才行&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.done-dir&lt;/name&gt; &lt;value&gt;mr-history/done&lt;/value&gt; &lt;description&gt;MR JobHistory Server管理的日志的存放位置,默认:/mr-history/done&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.intermediate-done-dir&lt;/name&gt; &lt;value&gt;mr-history/mapred/tmp&lt;/value&gt; &lt;description&gt;MapReduce作业产生的日志存放位置，默认值:/mr-history/tmp&lt;/description&gt; 保存后先停掉yarn再重启，启动之后还要再启动mr-jobhistory-daemon.sh使用mr-jobhistory-daemon.sh start historyserver这样就启动成功了可以看到这时候多了一个进程JobHistoryServer 还要配置yarn-site.xml.在&lt;configuration&gt;&lt;/configuration&gt;中添加1234&lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; 再次重新启动yarn 使用mapreduce下的例子进行测试1/usr/local/hadoop/share/hadoop/mapreduce$ hadoop jar hadoop-mapreduce-examples-2.9.0.jar pi 2 3 我在启动后仍然出现问题，重启了hdfs和yarn都没用，不过后来尝试关闭jobhistory1./sbin/mr-jobhistory-daemon.sh stop historyserver 再重启1./sbin/mr-jobhistory-daemon.sh start historyserver 竟然可以了]]></content>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce的补充和WordCount简单实战(一)]]></title>
    <url>%2F2018%2F07%2F28%2FMapReduce%E7%9A%84%E8%A1%A5%E5%85%85%E5%92%8CWordCount%E7%AE%80%E5%8D%95%E5%AE%9E%E6%88%981%2F</url>
    <content type="text"><![CDATA[官网介绍：http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html 首先回顾一些MapReduce的基础知识：https://homxuwang.github.io/2018/04/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AD%EF%BC%89%E2%80%94%E2%80%94MapReduce/ Hadoop MapReduce是Google MapReduce的实现 MapReduce的优点： 海量数据离线处理 易开发(JAVA API) 易运行 MapReduce的缺点： 实时流式计算（MR是根据请求服务的方式进行计算；多个应用程序存在依赖关系，MR的作业，数据需要落地到HDFS或者磁盘。所以不能实现实时流式计算） MapReduce的执行过程 参考：https://www.cnblogs.com/ahu-lichang/p/6645074.html 官网的介绍： A MapReduce job usually splits the input data-set into independent chunks which are processed by the map tasks in a completely parallel manner. The framework sorts the outputs of the maps, which are then input to the reduce tasks. Typically both the input and the output of the job are stored in a file-system. The framework takes care of scheduling tasks, monitoring them and re-executes the failed tasks. Input –&gt; Spliting 一个文件被分成很多个块（默认情况下一个split对应HDFS中的一个block，用户可以进行修改） Spliting –&gt; Mapping 一个块交由一个Map任务处理，处理完的结果写到本地 Mapping –&gt; Shuffling –&gt; Reducing写到本地的文件通过Shuffle后进行传输，把相同的key写到一个Reduce中,在Reduce中进行统计 Reducing统计的结果最终写到文件系统上 看看官网的解释： The MapReduce framework operates exclusively on pairs, that is, the framework views the input to the job as a set of pairs and produces a set of pairs as the output of the job, conceivably of different types. The key and value classes have to be serializable by the framework and hence need to implement the Writable interface. Additionally, the key classes have to implement the WritableComparable interface to facilitate sorting by the framework. Input and Output types of a MapReduce job: (input) -&gt; map -&gt; -&gt; combine -&gt; -&gt; reduce -&gt; (output) 关于Writable接口在上面的介绍中看到，key和value需要实现Writable接口，并且key还需要实现WritableComparable接口 这个接口需要反复阅读 关于Writable接口的介绍：http://hadoop.apache.org/docs/current/api/org/apache/hadoop/io/Writable.html 关于WritableComparable接口的介绍：http://hadoop.apache.org/docs/current/api/org/apache/hadoop/io/WritableComparable.html 在Writable接口中主要实现write和readFields方法。 再看上文面的wordcount的图和(input) &lt;k1, v1&gt; -&gt; map -&gt; &lt;k2, v2&gt; -&gt; combine -&gt; &lt;k2, v2&gt; -&gt; reduce -&gt; &lt;k3, v3&gt; (output)这个过程： 其中k1就是偏移量。第一行的第一个字符从0开始，v1就是这一行的数据Deer Bear River。那么第二行的偏移量就是第一行的字符的长度相加，值就是Car Car River。以此类推 经过一层转换k2就是上面每一行的单词,每个单词相当于是从v1中拆分出来(Split(“ “))，是一个Text类型，每个单词就是一个1。v2就是一个IntWritable或LongWritable类型。 reduce输出的就是每个单词输出的总和。k3就是每个单词，v3就是单词出现的总和。 JAVA API的简单介绍 看上图，首先读取文件使用InputFormat类，它是一个接口，在源码中描述为 InputFormat describes the input-specification for a Map-Reduce job. InputFormat的实现类中，用的比较多的是FileInputFormat类.这是一个读取文件系统的基本的类.但是FileInputFormat类仍然是个抽象类。那么继续找它的子类可以看到TextInputFormat类.这时候它就是一个实现的类了 官方文档的介绍： An {@link InputFormat} for plain text files. Files are broken into lines. Either linefeed or carriage-return are used to signal end of line. Keys are the position in the file, and values are the line of text.. 其中InputFormat中有几个关键的方法： 1) InputSplit[] getSplits(JobConf job, int numSplits) throws IOException;即将一个输入文件分成很多Split，每一个Split交给一个MapTask处理的方法。它的返回值是一个数组，可见一个输入文件可能会的到好几个InputSplit 2) RecordReader&lt;K, V&gt; getRecordReader(InputSplit split,JobConf job,Reporter reporter) throws IOException;它是一个记录读取的方法，从参数可以看到，它从InputSplit[]数组中读进数据，可以知道每一行的数据是什么。 在InputFormat读进数据后(对于文本就是使用TextInputFormat），从图中可以看出，被拆分成好多个Split。拿到Split后，使用RR(RecordReader)把每个Split中的数据读取出来,一行一行的读，每读一行，交由一个map处理.Partitioner将相同的key交到同一个Reduce上，从图中可以看出，key可能会被发送到node1或者node2.中间有一个shuffle的过程，结果交由reduce处理。处理完的结果交给OutputFormat。 OutputFormat OutputFormat describes the output-specification for a Map-Reduce job. 其中getRecordWriter方法就对应InputFormat的getRecordReader方法 继续寻找它的实现类-&gt;FileOutputFormat-&gt;TextOutputFormat An {@link OutputFormat} that writes plain text files.将数据以文本的方式写出去 几个核心概念 Split Split是交由MapReduce作业来处理的数据块，是MapReduce中最小的计算单元 HDFS：blocksize是HDFS中最小的存储单元 128M（或者自己设定） 默认情况下：它们两个一一对应（也可以手动设置） InputFormatInputFormat将输入数据进行分片(split)：InputSplit[] getSplits(JobConf job, int numSplits) throws IOException默认实用比较多的是TextInputFormat,处理文本格式的数据 OutputFormat 和InputFormat对应 Combiner Partitioner 在通过一张图对以上内容进行梳理： MapReduce架构MapReduce1.x架构 1) JobTracker:JT 作业的管理者 将作业分解成一堆任务：Task（包括MapTask和ReduceTask) 将任务分派给TaskTracker运行 作业的监控、容错处理（task作业挂了，重启task） 在一定时间间隔内，JT没有收到TT的心跳信息,则将任务分配到其他TT上执行2) TaskTracker：TT 任务的执行者 在TT上执行Task（MapTask和ReduceTask） 会与JT进行交互：执行/启动/停止作业，发送心跳信息给JT3) MapTask 自己开发的map任务交由该Task来处理 解析每条记录的数据，交给自己的map方法处理 将map的输出结果写到本地磁盘（有些作业只有Map，则写到HDFS）4） ReduceTadk 将MapTask输出的数据进行读取 按照数据进行分组传给自己编写的reduce方法处理 处理结果输出 MapReduce2.x架构 和yarn中的流程类似,MapReduce可以在YARN上跑。 下一篇实战。]]></content>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IDEA+JAVA编写HDFS代码]]></title>
    <url>%2F2018%2F07%2F25%2FIDEA%2BJAVA%E7%BC%96%E5%86%99HDFS%E4%BB%A3%E7%A0%81%2F</url>
    <content type="text"><![CDATA[使用JAVA API操作HDFS文件 IDEA+Maven创建Java工程 添加HDFS相关依赖 开发JAVA API操作HDFS文件 IDEA环境配置首先打开IDEA,选择Maven项目，选择quickstart接着下一步 输入一些基本信息 这里IDEA有自己集成的Maven版本,在User settings file可以使用系统自带的xml文件，也可以选择自己下载的Maven。可以在网上下载一个Maven压缩包，解压到指定的文件夹，在使用的时候选择这个文件夹下的settings.xml就行了。 同样的Local repository也可以自己选择一个本地目录。 下一步填一些基本信息 然后等待Maven初始化完成，可以看到下面console窗口的SUCCESS信息。初始的项目集成了单元测试junit包。 接着配置开发hadoop需要的依赖。如图，添加12345&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt;&lt;/dependency&gt; 这里我配置的hadoop依赖版本是2.9.0,根据自己的实际版本进行修改另外，这里的${hadoop.version}在上面的&lt;properties&gt;&lt;/properties&gt;标签中进行了配置，其实和上面的代码的效果是一样的。 然后Maven会自己下载相应的依赖，可以看到在右侧已经下载成功所需的依赖。然后在右侧栏的hadoop.hdfs中新加一个包，然后新建一个类进行测试 以下是单元测试代码，其中的路径参数是我自己系统上的路径，在开发时自行替换为自己的开发测试路径。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148package hadoop.hdfs.hdfs;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.*;import org.apache.hadoop.io.IOUtils;import org.apache.hadoop.util.Progressable;import org.junit.After;import org.junit.Before;import org.junit.Test;import java.io.BufferedInputStream;import java.io.File;import java.io.FileInputStream;import java.io.InputStream;import java.net.URI;/** * hadoop HDFS java API 操作 */public class HDFSApp &#123; public static final String HDFS_PATH = "hdfs://localhost:9000"; FileSystem fileSystem = null; Configuration configuration = null; /** *创建hdfs目录 */ @Test public void mkdir() throws Exception &#123; fileSystem.mkdirs(new Path("/hdfsapi/test")); &#125; @Test public void createFile() throws Exception &#123; FSDataOutputStream out = fileSystem.create(new Path("/hdfsapi/test/a.txt")); out.write("hello hdfs".getBytes()); out.flush(); out.close(); &#125; /** * 查看hdfs文件的内容 */ @Test public void catFile() throws Exception &#123; FSDataInputStream in = fileSystem.open(new Path("/hdfsapi/test/a.txt")); IOUtils.copyBytes(in,System.out,1024); System.out.println(); in.close(); &#125; /** *重命名文件名 */ @Test public void renameFile() throws Exception &#123; Path oldPath = new Path("/hdfsapi/test/a.txt"); Path newPath = new Path("/hdfsapi/test/b.txt"); if(fileSystem.rename(oldPath,newPath))&#123; System.out.println("rename success!"); &#125; &#125; /** * 从本地拷贝一个文件 */ @Test public void copyFromLocalFile() throws Exception &#123; Path localPath = new Path("/home/hadoop/study/data/hello.txt"); Path hdfsPath = new Path("/hdfsapi/test"); fileSystem.copyFromLocalFile(localPath,hdfsPath); &#125; /** * 从本地拷贝一个大文件，并且带进度条 */ @Test public void copyFromLocalFileWithProgress() throws Exception &#123; InputStream in = new BufferedInputStream( new FileInputStream( new File("/home/hadoop/下载/hadoop-2.9.0.tar.gz"))); FSDataOutputStream out = fileSystem.create( new Path("/hdfsapi/test/hadoop-2.9.0.tar.gz"), new Progressable() &#123; @Override public void progress() &#123; System.out.print("."); &#125; &#125;); IOUtils.copyBytes(in,out,4096); &#125; /** * 下载HDFS文件到本地 */ @Test public void copyToLocalFile() throws Exception &#123; Path localpath = new Path("/home/hadoop/study/data/b.txt"); Path hdfspath = new Path("/hdfsapi/test/b.txt"); fileSystem.copyToLocalFile(hdfspath,localpath); &#125; /** * 获取目录下的所有文件和文件夹 */ @Test public void listFiles() throws Exception &#123; FileStatus[] fileStatus = fileSystem.listStatus(new Path("/")); for(FileStatus filestatus : fileStatus)&#123; String Status = filestatus.isDirectory() ? "文件夹" : "文件"; short replication = filestatus.getReplication(); //副本数 long len = filestatus.getLen(); //文件大小 String path = filestatus.getPath().toString(); System.out.println(Status + " " + replication+ " " +len+ " " +path); &#125; &#125; /** * 删除文件 */ @Test public void deleteFile() throws Exception &#123; fileSystem.delete(new Path(""),true); &#125; /** * 在所有的单元测试之前执行的 */ @Before public void setUp() throws Exception &#123; System.out.println("HDFS setUp"); configuration = new Configuration(); fileSystem = FileSystem.newInstance(new URI(HDFS_PATH),configuration); &#125; /** * 在所有的单元测试执行完之后执行的,释放资源 */ @After public void tearDown() throws Exception &#123; configuration = null; fileSystem = null; System.out.println("HDFS tearDown"); &#125;&#125; HDFS工作流程下面是在网上看到的漫画版的解释HDFS的工作流程，值得反复阅读，这里贴上来感谢作者！]]></content>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop遇到的一些坑]]></title>
    <url>%2F2018%2F07%2F24%2Fhadoop%E9%81%87%E5%88%B0%E7%9A%84%E4%B8%80%E4%BA%9B%E5%9D%91%2F</url>
    <content type="text"><![CDATA[最近开始了大数据的基础学习，刚开始就踩了一些坑。在这里做一下简要记录。 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform…由于好久没有打开ubuntu系统使用hadoop，不知道是什么原因，今天运行hadoop的一些命令的时候有警告： WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable百思不得其解，在开始安装好hadoop的时候是没有任何问题的，之后也没有动过。搜索了一圈问题后，都说是电脑上的本地库和hadoop的需求不一致，但是我改了一圈也还是没有解决问题。后在这里找到方法并且尝试后解决了。https://blog.csdn.net/znb769525443/article/details/51507283 这里根据上面博客的内容简要摘录： 增加调试信息，执行命令12export HADOOP_ROOT_LOGGER=DEBUG,consolehadoop fs -text est/data/origz/access.log.gz &gt; 这样就能够看到报错的信息解决方法：修改/HADOOP_HOME/etc/hadoop/中的hadoop_env.sh在头部添加12export HADOOP_COMMON_LIB_NATIVE_DIR=&quot;/usr/local/hadoop/lib/native/&quot;export HADOOP_OPTS=&quot;$HADOOP_OPTS -Djava.library.path=/usr/local/hadoop/lib/native/&quot; 再次执行./start-dfs.sh 问题解决。 当然每个人的错误虽然相似，但是错误原因可能不同，这里学会了一招看报错的log信息hadoop开启关闭调试信息： 开启：export HADOOP_ROOT_LOGGER=DEBUG,console 关闭：export HADOOP_ROOT_LOGGER=INFO,console hadoop集群启动之后dataNode节点没有启动今天遇到的另一个问题是，在hadoop操作的时候，向某个文件夹复制文件的时候报错：ARN hdfs.DataStreamer: DataStreamer Exception org.apache.hadoop.ipc.RemoteException然后锁定问题是dataNode节点没有启动，使用了命令stop-all.sh start-all.sh重启后仍然没有用。 使用了https://blog.csdn.net/qq_20124743/article/details/78668130 的方法得到了解决。 启动Hadoop集群之后slave机器的dataNode节点没有启动 master机器的nameNode节点启动了 1、在集群/usr/local/src/hadoop/bin目录下./stop-all.sh暂停所有服务 2、将/usr/local/src/hadoop/目录下的 logs、tmp文件夹删除(DataNode存放数据块的位置) 然后重新建立tmp logs文件夹 3、重新格式化: （同样是在bin目录下）./hadoop namenode -format 4、重新启动集群：./start-all.sh 5、通过jps查看进程 就好了]]></content>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二分搜索树]]></title>
    <url>%2F2018%2F07%2F18%2F%E4%BA%8C%E5%88%86%E6%90%9C%E7%B4%A2%E6%A0%91%2F</url>
    <content type="text"><![CDATA[二分搜索树（或者说二叉搜索树）是一种树形结构。 二分搜索树是二叉树 二分搜索树的每个节点的值：大于其左子树的所有节点的值小于其右子树的所有节点的值 每一棵子数也是二分搜索树 存储的元素必须有可比性(如果是想存储一个自定义的类型，那么要定义好这个数据类型的两个数据之间时如何比较的) 新建BST类，因为其节点值之间需要进行比较，所以继承Comparable类 基本代码12345678910111213141516171819202122232425262728public class BST&lt;E extends Comparable&lt;E&gt;&gt;&#123; //声明节点类 private class Node&#123; public E e; public Node left,right; public Node(E e)&#123; this.e = e; left = null; right = null; &#125; &#125; private Node root; //根节点 private int size; //记录存储的元素个数 //默认构造函数 public BST()&#123; root = null; size = 0; &#125; //获取节点数量 public int size()&#123; return size; &#125; //判断是否为空 public boolean isEmpty()&#123; return size == 0; &#125;&#125; 向二分搜索树添加元素采用递归方式向二分搜索树添加新元素，以下代码放入BST类中。根据二分搜索树的性质，根节点和每个子树的节点的左子树都比当前节点的值小，其右子树比当前节点的值都大。本文中的二分搜索树不包括重复元素。如果要插入的值等于当前根节点的值，则跳过。123456789101112131415161718192021222324252627/** * 向二分搜索树添加新的元素 */public void add(E e)&#123; root = add(root,e);&#125;/** * 返回插入新节点后二分搜索树的根 * @param node 要比较的节点 * @param e 要比较的值 * @return 新节点的根 */private Node add(Node node,E e)&#123; if(node == null)&#123; size ++; return new Node(e); &#125; if(e.compareTo(node.e) &lt; 0 )&#123; node.left = add(node.left,e); &#125; else if(e.compareTo(node.e) &gt; 0)&#123; node.right = add(node.right,e); &#125; return node;&#125; 二分搜索树的包含方法contains12345678910111213141516171819202122232425262728/** * 看二分搜索树种是否包含元素e,从根节点开始比较 * @param e 要比较的值 * @return true or false */public boolean contains(E e)&#123; return contains(root,e);&#125;/** * 看以node为根的二分搜索树中是否包含元素e,递归算法 * @param node 节点 * @param e 元素e * @return true or false */private boolean contains(Node node,E e)&#123; if(node==null)&#123; return false; &#125; if(e.compareTo(node.e) == 0)&#123; return true; &#125; else if(e.compareTo(node.e) &lt; 0)&#123; return contains(node.left,e); &#125; else //e.compareTo(node.e) &gt; 0 return contains(node.right,e);&#125; 二分搜索树的前序遍历从根节点访问左右子树，自上而下，所以称为前序遍历。 前序遍历的递归实现1234567891011121314151617181920/** * 二分搜索树的前序遍历 */public void preOrder()&#123; preOrder(root);&#125;/** * 前序遍历以node为根的二分搜索树，递归算法 * @param node 以node为根 */private void preOrder(Node node)&#123; //终止条件 if(node == null) return; System.out.println(node.e); preOrder(node.left); preOrder(node.right);&#125; 前序遍历的非递归实现12345678910111213141516/** * 前序遍历的非递归实现 */public void preOrderNR()&#123; Stack&lt;Node&gt; stack = new Stack&lt;&gt;(); stack.push(root); while(!stack.isEmpty())&#123; Node cur = stack.pop(); System.out.println(cur.e); if(cur.right != null) stack.push(cur.right); if(cur.left != null) stack.push(cur.left); &#125;&#125; 二分搜索树的中序遍历和前序遍历不同的是，中序遍历先访问左子树，再访问根节点，最后访问右子树。因为访问根节点在中间顺序，所以称为中序遍历。 1234567891011121314/** * 二分搜索树的中序遍历 */public void inOrder()&#123; inOrder(root);&#125;private void inOrder(Node node)&#123; if(node == null) return; inOrder(node.left); System.out.println(node.e); inOrder(node.right);&#125; 二分搜索树的中序遍历打印出的即是二分搜索树排序后的结果。 二分搜索树的后序遍历后序遍历的一个应用:为二分搜索树释放内存时，需要先从叶子节点开始释放，然后释放其父亲节点（JAVA有内存回收机制，不需考虑，但是c c++ 等语言需要考虑）。123456789101112131415/** * 二分搜索树的后序遍历 */public void postOrder()&#123; postOrder(root);&#125;private void postOrder(Node node)&#123; if(node == null) return; postOrder(node.left); postOrder(node.right); System.out.println(node.e);&#125; 二分搜索树的层序遍历二分搜索树的层序遍历采用队列的思想，首先把根节点入队，然后循环判断队列是否为空，如果不为空，则将队首元素出队，接着将这个元素的左右子树入队，如果为空则不进行入队。循环结束后也就实现了层序遍历。12345678910111213141516171819import java.util.Queue;import java.util.LinkedList; /** * 二分搜索树的层序遍历,使用队列 */ public void levelOrder()&#123; Queue&lt;Node&gt; q = new LinkedList&lt;&gt;(); //这里定义Queue底层采用链表进行实现 q.add(root); while(!q.isEmpty())&#123; Node cur = q.remove(); //队首元素出队 System.out.println(cur.e); if(cur.left != null) //队首元素的左子树 q.add(cur.left); if(cur.right != null) //队首元素的右子树 q.add(cur.right); &#125; &#125; 广度优先遍历的意义：可以更快找到要查询的元素。 寻找二分搜索树的最大最小值其实，寻找二分搜索树的最大最小值可以看成寻找一个链表的最后一个不为空的节点。这里给出递归写法。123456789101112131415161718192021222324252627282930313233343536373839404142/** * 寻找二分搜索树的最小元素 * @return */public E minimum()&#123; if(size == 0) throw new IllegalArgumentException("BST is empty"); return minimum(root).e;&#125;/** * 返回以node为根的二分搜索树的最小值所在的节点 * @param node * @return */private Node minimum(Node node)&#123; if(node.left == null) return node; return minimum(node.left);&#125;/** * 寻找二分搜索树的最大元素 * @return */public E maximum()&#123; if(size == 0) throw new IllegalArgumentException("BST is empty"); return maximum(root).e;&#125;/** * 返回以node为根的二分搜索树的最大值所在的节点 * @param node * @return */private Node maximum(Node node)&#123; if(node.right == null) return node; return maximum(node.right);&#125; 删除二分搜索树的最大最小节点12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * 从二分搜索树种删除最小值所在的节点，并返回最小值 * @return */public E removeMin()&#123; E min = minimum(); //先找到最小值是多少 root = removeMin(root); //进行删除操作 return min; //返回最小值&#125;/** * 删掉以node为根的二分搜索树中的最小节点 * @param node 以node为根 * @return 返回删除节点后新的二分搜索树的根 */private Node removeMin(Node node)&#123; if(node.left == null)&#123; Node rightNode = node.right; // 保存一下节点的右子树，即使为空也不违反逻辑 node.right = null; size --; //维护size return rightNode; //返回删除节点后新的二分搜索树的根,也即刚才保存的右子树 &#125; node.left = removeMin(node.left); return node;&#125;/** * 从二分搜索树种删除最大值所在的节点，并返回最大值 * @return */public E removeMax()&#123; E max = maximum(); root = removeMax(root); return max;&#125;/** * 删掉以node为根的二分搜索树中的最大节点 * @param node 以node为根 * @return 返回删除节点后新的二分搜索树的根 */private Node removeMax(Node node)&#123; if(node.right == null)&#123; Node leftNode = node.left; node.left = null; size --; return leftNode; &#125; node.right = removeMin(node.right); return node;&#125; 删除二分搜索树的任意节点 删除只有左子树或只有右子树的节点，即将其左子树或其右子树代替当前的节点位置即可 删除叶子节点可以将其看做只有左子树或只有右子树的节点，只是其左子树和右子树为空 删除一个既有左子树又有右子树的节点，是删除任意节点的难点。使用Hibbard Deletion方法：1) 找到d待删除节点d的右子树中最小的节点s = min(d-&gt;right),这个节点就是与d节点值相差最小的节点 2) 找到这个节点后，在d的右子树中删除这个节点,并让这个节点的右子树指向原来d节点的右子树，s-&gt;right = removeMin(d-&gt;right) 3) 让s节点的左子树等于d节点原来的左子树s-&gt;left = d-&gt;left,这时候s节点的左右子树就是d节点删除s节点后的左右子树了 4) 删除d,s称为新的子树的根，让其上一级的根指向此节点即可 或者与上述方法相同，只是在找替代节点时找d节点的左子树中最大值的节点作为s,其也符合逻辑 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** * 删除二分搜索树的任意元素 * @param e 要删除的元素的值 */public void remove(E e)&#123; root = remove(root,e);&#125;/** * 删除函数的递归调用 * @param node 当前传入的根节点 * @param e 要删除的元素的值 * @return 删除后的节点 */private Node remove(Node node,E e)&#123; if(node == null) return null; if(e.compareTo(node.e) &lt; 0)&#123; node.left = remove(node.left,e); return node; &#125; else if(e.compareTo(node.e) &gt; 0)&#123; node.right = remove(node.right,e); return node; &#125; else&#123; //e.compareTo(node.e) == 0 //待删除节点左子树为空的情况 if(node.left == null)&#123; Node rightNode = node.right; node.right = null; size --; return rightNode; &#125; //待删除节点右子树为空的情况 if(node.right == null)&#123; Node leftNode = node.left; node.left = null; size --; return leftNode; &#125; //待删除节点的左右子树均不为空的情况 //首先找到比待删除节点大的最小节点，即待删除节点右子树的最小节点 //用这个节点代替删除节点的位置 Node cur = minimum(node.right); cur.right = removeMin(node.right); cur.left = node.left; node.left = node.right = null; return cur; &#125;]]></content>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git笔记]]></title>
    <url>%2F2018%2F07%2F18%2Fgit%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[因为hexo博客之前出过一些问题，导致之前做的git笔记被覆盖掉丢失了。所以这次再一次整理一下git的一些常用操作并记录下来。 创建仓库进入到要创建仓库的文件夹,输入命令 git init这时候会有提示 然后目录下会多一个.git的隐藏文件夹。 另外一种方式 git init demo 初始化到一个叫demo的自定义文件夹 另外可以从远程仓库初始化 git clone https://github.com/xxxxxxxxx.git 克隆项目 git clone https://github.com/xxxxxxxxx.git demo 克隆项目到一个叫demo的自定义文件夹 基本用法 git status 查看仓库状态 No commits yet 是指没有提交记录Untracked files 是指未跟踪到的文件，指有文件更改了但是没有commit git add . 将所有修改添加至暂存区 git commit -m &quot;描述&quot; 提交版本(即在这个历史节点下修改和做了什么) git add . &amp;&amp; git commit -m &quot;描述&quot; 同时执行两次操作这时候再查看仓库状态，可以看到nothing to commit git log 查看版本记录黄色的一串字母数字组合表示唯一标识 下面则是描述说明的内容 git log -p可以看到修改的具体信息 git log --oneline 一行显示 git checkout xxx 穿越到指定的历史节点,xxx表示上面的唯一标识,可以不用输入全部() git checkout - 回到原来的节点 三种状态 modefied 已修改 staged 已暂存(缓冲阶段) commited 已提交 git diff 比对当前内容和暂存区内容。 git diff HEAD 比对当前内容和最近一次提交。 git diff HEAD^ 比对当前内容和倒数第二次提交。 git diff HEAD^ HEAD 比对最近两次提交。标签tag 项目开发中，在版本提交时会有很多小版本，但是其中有一些节点很重要，比如完成某些重要的功能，可以在这个重要的节点使用标签做标记。 git tag -a 标签名 -m &quot;备注&quot; 添加tag (a=annotated有注释的) 默认加在最近的节点上面 git tag -a 标签名 -m &quot;备注&quot; 版本号 在历史节点添加标签，最后加上历史版本号 git tag 罗列所有tag command1 &amp;&amp; command2 ：组合命令 添加 -a 属性，才可以后接-m属性 git show 标签名 显示tag信息 git checkout 标签名 回到tag所在的提交 分支branch git branch $branchName 在当前节点创建分支 git checkout $branchName 切换到分支(可以跳转到不同分支) git log --all --graph 图形化显示 分支的作用主要是利用分支进行当前问题的处理（比如在某个版本分支出来进行调试bug，但是在master继续版本的推进，在bug修复后合并分支，这样bug得到了解决版本也进行了推进）。 合并分支 git checkout -b $branchName 创建并切换至分支 git merge $branchName 合并分支 合并一个文件要用人工进行处理orz 远程仓库 git remote add $remoteName $giturl添加远程仓库 git push -u $remoteName $branchNamepush到远程分支输入github的用户名密码就自动开始push了 git remote -v 打印远程仓库信息 fetch是下载地址，push是上传地址 git clone $giturl 克隆 git pull = git fetch &amp;&amp; git merge 根据表严肃老师的视频进行整理 http://biaoyansu.com/i/6593023230131]]></content>
      <tags>
        <tag>学习笔记</tag>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实现栈和队列]]></title>
    <url>%2F2018%2F07%2F17%2F%E5%AE%9E%E7%8E%B0%E6%A0%88%E5%92%8C%E9%98%9F%E5%88%97%2F</url>
    <content type="text"><![CDATA[栈栈在计算机中有很广泛的应用，比如括号的匹配用到了栈，系统栈，撤销操作。 栈也是一种线性结构。相比数组，栈对应的操作是数组的子集。只能从一端添加元素，从同一端取出元素，即栈顶。是一种后进先出的数据结构(Last In First Out (LIFO)) 栈的实现栈的实际底层实现有多种方式，如动态数组，链表等。 这里只实现栈的几个基本操作：入栈，出栈，查看栈顶元素，查看栈元素数量，查看栈是否为空。 定义Stack接口，基于动态数组实现栈，动态数组的实现见 https://homxuwang.github.io/2018/07/17/%E6%95%B0%E7%BB%84/ 首先在程序中创建上面链接中的Array类然后创建Stack接口Interface Stack&lt;E&gt;1234567public interface Stack&lt;E&gt; &#123; int getSize(); //获取元素个数 boolean isEmpty(); //判断是否为空 void push(E e); //进栈 E pop(); //出栈 E peek(); //查看栈顶元素&#125; ArrayStack基于动态数组实现的栈类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public class ArrayStack&lt;E&gt; implements Stack&lt;E&gt; &#123; Array&lt;E&gt; array; /** * 构造函数 * @param capacity 定义容量 */ public ArrayStack(int capacity)&#123; array = new Array&lt;&gt;(capacity); &#125; /** * 无参构造函数 */ public ArrayStack()&#123; array = new Array&lt;&gt;(); &#125; @Override public int getSize()&#123; return array.getSize(); &#125; @Override public boolean isEmpty() &#123; return array.isEmpty(); &#125; //获得栈的容量,这个方法与栈的接口无关,只有在使用动态数组的时候才有这个方法 public int getCapacity()&#123; return array.getCapacity(); &#125; @Override public void push(E e)&#123; array.addLast(e); &#125; @Override public E pop()&#123; return array.removeLast(); &#125; @Override public E peek()&#123; return array.getLast(); &#125; @Override public String toString()&#123; StringBuilder res = new StringBuilder(); res.append("Stack:"); res.append("["); for(int i = 0 ; i &lt; array.getSize(); i++)&#123; res.append(array.get(i)); if(i != array.getSize() - 1)&#123; res.append(","); &#125; &#125; res.append("] top"); return res.toString(); &#125;&#125; 在Main函数中进行测试：1234567891011public class Main &#123; public static void main(String[] args)&#123; ArrayStack&lt;Integer&gt; stack = new ArrayStack&lt;&gt;(); for(int i = 0 ; i &lt; 4 ; i++)&#123; stack.push(i); System.out.println(stack); &#125; stack.pop(); System.out.println(stack); &#125;&#125; 结果如下： 栈的复杂度分析ArrayStack void push(e) O(1) 均摊E pop() O(1) 均摊E peek() O(1)int getSize() O(1)boolean isEmpty() O(1) 队列队列也是一种线性结构。先进先出(First In First Out(FIFO)) 一般队列的实现定义Queue接口，基于动态数组实现队里，动态数组的实现见 https://homxuwang.github.io/2018/07/17/%E6%95%B0%E7%BB%84/ 基于Array类创建Queue接口 Interface Queue&lt;E&gt;1234567public interface Queue&lt;E&gt;&#123; int getSize(); //获取元素个数 boolean isEmpty; //判断队列是否为空 void enqueue(E e); //入队 E dequeue(); //出队 E getFront(); //查看队首元素&#125; 底层采用动态数组Array类来实现队列,创建队列类ArrayQueue：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class ArrayQueue&lt;E&gt; implements Queue&lt;E&gt; &#123; private Array&lt;E&gt; array; public ArrayQueue(int capacity)&#123; array = new Array&lt;&gt;(capacity); &#125; public ArrayQueue()&#123; array = new Array&lt;&gt;(); &#125; @Override public int getSize()&#123; return array.getSize(); &#125; @Override public boolean isEmpty()&#123; return array.isEmpty(); &#125; public int getCapacity()&#123; return array.getCapacity(); &#125; @Override public void enqueue(E e)&#123; array.addLast(e); &#125; @Override public E dequeue()&#123; return array.removeFirst(); &#125; @Override public E getFront()&#123; return array.getFirst(); &#125; @Override public String toString()&#123; StringBuilder res = new StringBuilder(); res.append("Queue:"); res.append("front["); for(int i = 0 ; i &lt; array.getSize() ; i ++)&#123; res.append(array.get(i)); if(i != array.getSize() - 1) res.append(","); &#125; res.append("]end"); return res.toString(); &#125;&#125; 简单测试：1234567891011public static void main(String[] args)&#123; ArrayQueue&lt;Integer&gt; queue = new ArrayQueue&lt;&gt;(); for(int i = 0 ; i &lt; 10 ; i++)&#123; queue.enqueue(i); System.out.println(queue); if(i % 3 == 2)&#123; queue.dequeue(); System.out.println(queue); &#125; &#125;&#125; 一般队列的复杂度分析void enqueue(E) O(1) 均摊E dequeue() O(n)E getFront() O(1)int getSize() O(1)boolean isEmpty() O(1) 可以看到一般队列的出队过程因为要往前挪动一个元素，导致其时间复杂度是O(n) 循环队列的实现在一般队列的基础上，引入front指向队列头,tail指向队列尾。front == tail 队列为空。在出队后，不用挪动元素，而是让front的指向后移，同时，添加元素时tail向后挪，指向第一个未添加元素的地方。在队列满的情况下，索引+1后对数组长度求余得到tail的位置。但是若tail指向的事最后一个未添加元素的位置时，再向其中添加元素，此时tail后挪后就导致tail == front而当front == tail时表示队列为空。所以用(tail + 1) % 数组容量 == front(tail + 1 % 数组容量 是因为如图，如果front == 0 ,tail == 7时也是满的) 表示队列已满，这时候进行扩容。即浪费一个空间。 循环队列底层不再使用上文将到的动态数组1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192public class LoopQueue&lt;E&gt; implements Queue&lt;E&gt; &#123; private E[] data; private int front, tail; //front表示队首所指的索引，tail指向队列最后一个元素的下一个位置所在的索引，也就是入队新的元素所存放的位置对应的索引 private int size; //队列中有多少元素(也可以用tail和front进行控制) //构造函数 public LoopQueue(int capacity)&#123; data = (E[])new Object[capacity + 1]; //因为上文提到要预留一个空间，所以这里+1 front = 0; tail = 0; size = 0; &#125; //无参构造函数 public LoopQueue()&#123; this(10); &#125; public int getCapacity()&#123; return data.length - 1; //真正能够存储的数据大小是length - 1，因为预留了一个空间 &#125; @Override public boolean isEmpty()&#123; return front == tail; &#125; @Override public int getSize()&#123; return size; &#125; @Override public void enqueue(E e)&#123; if((tail + 1) % data.length == front)&#123; //如果队列满了,则调用resize()函数 resize(getCapacity() * 2); &#125; data[tail] = e; tail = (tail + 1) % data.length; &#125; @Override public E dequeue()&#123; if(isEmpty())&#123; throw new IllegalArgumentException("Canno dequeuq from an empty queue"); &#125; E temp = data[front]; data[front] = null; front = (front + 1) % data.length; size --; if(size == getCapacity() / 4 &amp;&amp; getCapacity() / 2 != 0)&#123; //如果队列元素个数小于容量的1/4则进行缩容 resize(getCapacity() / 2); &#125; return temp; &#125; @Override public E getFront()&#123; if(isEmpty())&#123; throw new IllegalArgumentException("Queue is empty"); &#125; return data[front]; &#125; private void resize(int newCapacity)&#123; E[] newData = (E[])new Object[newCapacity + 1]; for(int i = 0 ; i &lt; size ; i++)&#123; newData[i] = data[(front + i) % data.length]; &#125; data = newData; front = 0; tail = size; &#125; /** *便于打印输出 */ @Override public String toString()&#123; StringBuilder res = new StringBuilder(); res.append(String.format("Queue: Size = %d , capacity = %d \n",size,getCapacity())); res.append("front["); for(int i = front ; i != tail ; i = (i + 1 ) % data.length)&#123; res.append(data[i]); if((i + 1) % data.length != tail) res.append(", "); &#125; res.append("] tail"); return res.toString(); &#125;&#125; 测试代码：1234567891011public static void main(String[] args)&#123; LoopQueue&lt;Integer&gt; queue = new LoopQueue&lt;&gt;(); for(int i = 0 ; i &lt; 10 ; i++)&#123; queue.enqueue(i); System.out.println(queue); if(i % 3 == 2)&#123; queue.dequeue(); System.out.println(queue); &#125; &#125;&#125; 代码结果：]]></content>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实现动态数组]]></title>
    <url>%2F2018%2F07%2F17%2F%E6%95%B0%E7%BB%84%2F</url>
    <content type="text"><![CDATA[因为数组开辟时是静态的。在这里底层使用静态数组二次封装一个属于自己的动态数组类Array Array类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245public class Array&lt;E&gt;&#123; private E[] data; //开辟一个数组 private int size; //指向第一个没有元素的索引，也代表数组中有多少个有效的元素 /** * 有参构造函数 * @param capacity 数组的容量 */ public Array(int capacity)&#123; data = (E[])new Object[capacity]; size = 0; &#125; /** * 默认构造函数,默认容量给10 */ public Array()&#123; this(10); &#125; /** * 获取数组中的元素个数 * @return元素个数 */ public int getSize()&#123; return size; &#125; /** * 获取数组的容量 * @return数组的大小 */ public int getCapacity()&#123; return data.length; &#125; /** * 返回数组是否为空 * @return 数组是否为空 */ public boolean isEmpty()&#123; return size == 0; &#125; /** * 插入新元素 * @param index 在index插入 * @param e 新元素的值 */ public void add(int index, E e)&#123; //index不能为负,index如果大于size说明数组不是紧密排列的,则不合法 if(index &lt; 0 || index &gt; size)&#123; throw new IllegalArgumentException("add() failed. Require index &gt;= 0 &amp;&amp; index &lt;= size"); &#125; if(size == data.length)&#123; //如果此时元素个数等于数组长度，则进行动态扩容 resize(2 * data.length); &#125; //将元素往后挪动 for(int i = size-1 ; i &gt;= index ; i-- )&#123; data[i+1] = data[i]; &#125; data[index] = e; size ++; &#125; /** * 向所有元素后添加一个新元素 * @param e 要插入的元素 */ public void addLast(E e)&#123; add(size,e); &#125; /** * 获取index位置的元素 * @param index * @return 元素位置 */ public void addFirst(E e)&#123; add(0,e); &#125; /** * 获取index位置的元素 * @param index 要获取的元素的索引 * @return 元素值 */ public E get(int index)&#123; if(index &lt; 0 || index &gt;= size)&#123; throw new IllegalArgumentException("Set failed,Index is illegal"); &#125; return data[index]; &#125; /** * 获取最后一个元素 * @return 最后一个元素 */ public E getLast()&#123; return get(size - 1); &#125; /** * 获取第一个元素 * @return 第一个元素 */ public E getFirst()&#123; return get(0)); &#125; /** * 修改Index索引位置的元素为e * @param index 要修改的元素的索引 * @param e 要修改的元素的值 */ void set(int index,E e)&#123; if(index &lt; 0 || index &gt;= size)&#123; throw new IllegalArgumentException("Get failed,Index is illegal"); &#125; data[index] = e; &#125; /** *查找数组中是否含有元素e * @param e 要查找的元素的值 * @return true or false */ public boolean contains(E e)&#123; for(int i = 0 ; i &lt; size ; i++)&#123; if(data[i].equals(e)) return true; &#125; return false; &#125; /** * 查找数组中元素e所在的索引,如果不存在则返回-1 * @param e 要查找的元素的值 * @return 返回索引或者-1 */ public int find(E e)&#123; for(int i = 0 ; i &lt; size ; i++)&#123; if(data[i].equals(e)) return i; &#125; return -1; &#125; /** * 从数组中删除第一个元素elem * @param elem * @return true or false */ public boolean removeElement(E elem)&#123; int index = find(elem); if(index != -1)&#123; remove(index); return true; &#125; return false; &#125; /** * 删除Index位置的元素,返回index位置的元素 * @param index 要删除的索引位置的值 * @return 返回删除的元素的值 */ public E remove(int index)&#123; if(index &lt; 0 || index &gt;= size)&#123; throw new IllegalArgumentException("Remove failed.Index is illegal"); &#125; E temp = data[index]; for(int i = index + 1 ; i &lt; size ; i++)&#123; data[i - 1] = data[i]; &#125; size --; data[size] = null; // loitering objects. 释放size位置的内容(非必须，因为访问不到) if(size == data.length / 4 &amp;&amp; data.length / 2 != 0)&#123; //如果删除元素过多，少于四分之一则数组缩小容量 resize(data.length / 2); &#125; return temp; &#125; /** * 删除第一个元素 * @return */ public E removeFirst()&#123; return remove(0); &#125; /** * 删除数组中最后一个元素 * @return */ public E removeLast()&#123; return remove(size-1); &#125; /** * 覆盖父类的方法,定义本类在打印输出时打印的信息 * @return 打印输出数组字符串 */ @Override public String toString()&#123; StringBuilder res = new StringBuilder(); res.append(String.format("Array: size = %d,capacity = %d\n",size,data.length)); res.append('['); for(int i = 0 ; i &lt; size ; i ++)&#123; res.append(data[i]); if(i != size - 1) res.append(","); &#125; res.append(']'); return res.toString(); &#125; /** * 扩容数组 * @param newCapacity 新的数组的大小 */ private void resize(int newCapacity)&#123; E[] newData =(E[]) new Object[newCapacity]; for(int i = 0 ;i &lt; size ; i++) &#123; newData[i] = data[i]; &#125; data = newData; &#125;&#125; 说明：1.实现动态数组的关键函数是类中的resize()函数，动态数组的底层实现有多种方式，比如本文讲到的使用静态数组进行扩容缩容，或者使用链表。动态数组的实现原理其实非常简单，假设数组的已经满了，那么如果继续添加元素，则会报出异常。实现动态数组的思路是，如果这时候数组满了，那么开辟一个新的数组，这个新数组的容量比原数组大(比如是原数组的1.5倍或者2倍,本文采用2倍)，然后将原数组的数据拷贝到新开辟的大数组中(这里需要循环),原来的数组data实际上是一个引用，在拷贝之后将data指向新的数组newData,这时候data这个成员变量和newData指向的是同一个空间，因为newData是封装在函数中，在函数执行结束后就失效了,而data是在整个类中的，它的生命周期是和类一样的。而原来的数组，因为没有东西指向它，java的垃圾回收机制会自动回收。这样扩容过程就结束了。 2.在remove()函数中，有一个缩容的操作，即当前的元素个数小到一定程度的时候，调用resize()函数，缩小数组的容量为当前数组容量的一半。这里是小于四分之一，并且缩小一半后的容量不能为0(所以在条件中判断data.length / 2 != 0)，这时再执行缩小操作。缩容的界限之所以选取1/4是因为如果removeLast()后刚好是数组的一半了，这时候进行缩容，然后又进行添加的时候，这时候又要进行扩容，这样如果有频繁的增删操作，在resize()函数中就会耗费大量的时间，所以采取Lazy解决方案。在删除元素后元素是1/2时，不着急进行缩容操作，而是等到元素为容量的1/4之一时再进行缩容，只缩容1/2，还留出1/4的空间。这样就防止了复杂度的震荡。 简单的复杂度分析·添加操作 addLast(e) O(1) addFirst(e) O(n) add(index,e) O(n/2) = O(n) //这里是简单的计算，严格计算需要使用概率论的知识，使用期望 resize() O(n) 所以添加操作的整体时间复杂度是O(n) ·删除操作 removeLast(e) O(1) removeFirst(e) O(n) remove(index,e) O(n/2) = O(n) resize() O(n) 整体时间复杂度是O(n) ·修改操作 set(index,e) O(1) ·查找操作 get(index) O(1) contains(e) O(n) find(e) O(n) 总结: ·增 O(n) ·删 O(n) ·改 已知索引O(1);未知索引O(n) ·查 已知索引O(1);未知索引O(n)]]></content>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法]]></title>
    <url>%2F2018%2F06%2F09%2F%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[最近在学习数据结构和算法，这里总结一下学习的排序算法。 选择排序 - SelectionSort基本思路:假设有一个数组（如图所示），进行从小到大的排序。首先在整个数组范围里，找出要放在第一个位置的数，也就是最小的数：1，然后将1和现在的第一名的位置8进行换位，经过交换以后，1所处的位置就是最终排序所在的位置，这样就继续在剩下的部分找此时最小的数，也就是2，然后把2和相应的第二个位置所在的元素进行交换，此时1和2两个元素也已经是最终排好序的结果。整个过程以此类推，继续在剩下的部分中找此时最小的元素，然后进行交换位置。。。。。 代码实现： 123456789101112131415161718192021222324252627282930313233343536public class SelectionSort &#123; // 算法类不允许产生任何实例 private SelectionSort()&#123;&#125; public static void sort(int[] arr)&#123; int n = arr.length; for( int i = 0 ; i &lt; n ; i ++ )&#123; // 寻找[i, n)区间里的最小值的索引 int minIndex = i; for( int j = i + 1 ; j &lt; n ; j ++ ) if( arr[j] &lt; arr[minIndex] ) minIndex = j; swap( arr , i , minIndex); &#125; &#125; private static void swap(int[] arr, int i, int j) &#123; int t = arr[i]; arr[i] = arr[j]; arr[j] = t; &#125; public static void main(String[] args) &#123; int[] arr = &#123;10,9,8,7,6,5,4,3,2,1&#125;; SelectionSort.sort(arr); for( int i = 0 ; i &lt; arr.length ; i ++ )&#123; System.out.print(arr[i]); System.out.print(' '); &#125; System.out.println(); &#125;&#125; 改进:12345678910111213141516171819202122232425262728import java.util.*;public class SelectionSort&#123; // 算法类不允许产生任何实例 private SelectionSort()&#123;&#125; public static void sort(Comparable[] arr)&#123; int n = arr.length; for( int i = 0 ; i &lt; n ; i ++ )&#123; // 寻找[i, n)区间里的最小值的索引 int minIndex = i; for( int j = i + 1 ; j &lt; n ; j ++ ) // 使用compareTo方法比较两个Comparable对象的大小 if( arr[j].compareTo( arr[minIndex] ) &lt; 0 ) minIndex = j; swap( arr , i , minIndex); &#125; &#125; private static void swap(Object[] arr, int i, int j) &#123; Object t = arr[i]; arr[i] = arr[j]; arr[j] = t; &#125;&#125; 插入排序 - InsertionSort基本思路：开始只考虑8这个元素的时候，它就已经排好序了。接着看6这个元素,接下来的步骤是把6与它前面的数组进行比较，放在合适的位置，当6与8比较时，6&lt;8，所以6在8前面位置。接着看2这个元素，2与它前面的数组进行比较，2&lt;8，所以2和8交换一次位置，2继续和6比较，2&lt;6，所以2和6交换位置，此时2在最前面的位置。接着看3这个元素，3比8小，所以交换一次位置，3又比6小，所以交换一次位置，3比2大所以不进行交换操作，3插入在2和6中间，这时前面的4个元素排序完成。以此类推。 代码实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344import java.util.*;public class InsertionSort&#123; private InsertionSort()&#123;&#125; public static void sort(Comparable[] arr)&#123; int n = arr.length; for (int i = 0; i &lt; n; i++) &#123; // 寻找元素arr[i]合适的插入位置 // 写法1// for( int j = i ; j &gt; 0 ; j -- )// if( arr[j].compareTo( arr[j-1] ) &lt; 0 )// swap( arr, j , j-1 );// else// break; // 写法2 for( int j = i; j &gt; 0 &amp;&amp; arr[j].compareTo(arr[j-1]) &lt; 0 ; j--) swap(arr, j, j-1); &#125; &#125; private static void swap(Object[] arr, int i, int j) &#123; Object t = arr[i]; arr[i] = arr[j]; arr[j] = t; &#125; // 测试InsertionSort public static void main(String[] args) &#123; int N = 20000; Integer[] arr = SortTestHelper.generateRandomArray(N, 0, 100000); SortTestHelper.testSort("bobo.algo.InsertionSort", arr); return; &#125;&#125; 插入排序和选择排序相比，如果满足了条件就有机会提前结束，所以它的排序效率理论上要比选择排序高但是实际上它的运行时间比选择排序要慢，这是因为其swap操作较多，浪费了时间，所以针对这个地方进行改进 改进代码思路:首先,对位置0处的元素不作处理。 接着，看位置1处的元素6，首先对元素6做一个副本保存起来，然后看元素6是否应该在当前位置，即让他与前面的元素进行对比，如果小于前面的元素，则当前元素位置的值赋值为前一个元素的值，前一个元素位置的值赋值为刚才保存起来的副本（即当前元素的值）。（其实这也是相当于一个交换操作，在满足前一个元素大于当前元素值的情况下，进行交换值操作） 接着，看位置2处的元素2，首先对元素2做一个副本保存起来，然后元素2与前一个元素8比较，2&lt;8，则将元素2处的值赋值为8；接着再比较位置1处的元素8与位置1处的元素6的大小，将位置1处的值赋值为元素6，接着将元素2放在第一个位置。这样就少进行了交换的操作。 接着，看元素3，首先对元素3做一个副本保存起来，然后看元素3该不该放在当前位置，发现3&lt;8，所以这个位置赋值为8；然后看3是不是该放在刚才元素8的位置，发现元素3比元素6小，所以元素6放在刚才元素8的位置；然后看元素3是不是该放在刚才元素6的位置，发现元素3比元素2大，所以元素3应该放在这个位置。 这样很多交换操作就通过赋值进行取代了，所以性能更好。 改进代码：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import java.util.*;public class InsertionSort&#123; // 我们的算法类不允许产生任何实例 private InsertionSort()&#123;&#125; public static void sort(Comparable[] arr)&#123; int n = arr.length; for (int i = 0; i &lt; n; i++) &#123; // 寻找元素arr[i]合适的插入位置 // 写法1// for( int j = i ; j &gt; 0 ; j -- )// if( arr[j].compareTo( arr[j-1] ) &lt; 0 )// swap( arr, j , j-1 );// else// break; // 写法2// for( int j = i; j &gt; 0 &amp;&amp; arr[j].compareTo(arr[j-1]) &lt; 0 ; j--)// swap(arr, j, j-1); // 写法3 Comparable e = arr[i]; int j = i; for( ; j &gt; 0 &amp;&amp; arr[j-1].compareTo(e) &gt; 0 ; j--) arr[j] = arr[j-1]; arr[j] = e; &#125; &#125; private static void swap(Object[] arr, int i, int j) &#123; Object t = arr[i]; arr[i] = arr[j]; arr[j] = t; &#125; // 测试InsertionSort public static void main(String[] args) &#123; int N = 20000; Integer[] arr = SortTestHelper.generateRandomArray(N, 0, 100000); SortTestHelper.testSort("bobo.algo.InsertionSort", arr); return; &#125;&#125; 归并排序 - MergeSort自顶向下的归并排序基本思路：所谓归并排序，有两大步，一步是归，一步是并。 当对一个数组进行排序的时候,先把这个数组分成一半，然后分别把左边的数组和右边的数组排序,之后再归并起来。在对左边数组和右边的数组进行排序的时候，再次分别把左边的数组和右边的数组分成一半，然后对每一个部分进行排序。一样的，对这每一个部分进行排序的时候，再次把他们分成一半，直到它们只含有一个元素的时候，已经是有序了。（蓝色部分） 这时候就对上面最终分割完成的各个部分进行归并。在归并到上一个层级之后，继续进行归并,逐层上升，进行归并，直到归并到最后一层的时候，整个数组就有序了。（红色部分） 可以看到，对图中的8各元素进行归并的时候，分成3级，第三级就可以把数组分成单个元素了，这样每次二分，就是log2 8 = 3，如果是N个元素，则有log2 N的层级。每一层要处理的元素个数是一样的，则整个归并过程是N*log(N)的时间复杂度。 那么问题是假设左半部分和右半部分已经排好了序，怎么把他们合并成一个有序的数组。这个过程需要为数组开辟一个相同大小的临时空间进行辅助,如下图。在合并成一个数组的过程中，需要3个索引：k是最终在归并过程中要跟踪的位置，i和j分别表示两个排好序的数组，当前要考虑的元素项。首先看1和2两个元素，谁应该放在k位置，进行对比后，较小的元素1放在k位置。这时候k++，j++。紧接着，元素2和元素4进行对比，较小的元素2放在当前的k位置，这时候k++,i++。继续元素3和元素4进行对比，较小的元素3放在当前的k位置，这时候k++,i++。继续元素6和元素4进行对比，较小的元素4放在当前的k位置，这时候k++,j++。。。。。算法过程中，需要维护k,i,j满足算法的定义。并且需要跟踪i,j的越界情况。这里定义l(left),r(right),和m(middle)分别为整个数组的最左边的元素，数组最右边的元素和中间位置的元素(这里指定为第一个数组的最后一个元素)。这里再做一个说明，这里的定义整个算法的数组是前闭后闭的数组。 代码实现：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263import java.util.*;public class MergeSort&#123; // 我们的算法类不允许产生任何实例 private MergeSort()&#123;&#125; // 将arr[l...mid]和arr[mid+1...r]两部分进行归并 private static void merge(Comparable[] arr, int l, int mid, int r) &#123; Comparable[] aux = Arrays.copyOfRange(arr, l, r+1); // 初始化，i指向左半部分的起始索引位置l；j指向右半部分起始索引位置mid+1 int i = l, j = mid+1; for( int k = l ; k &lt;= r; k ++ )&#123; if( i &gt; mid )&#123; // 如果左半部分元素已经全部处理完毕 arr[k] = aux[j-l]; j ++; &#125; else if( j &gt; r )&#123; // 如果右半部分元素已经全部处理完毕 arr[k] = aux[i-l]; i ++; &#125; else if( aux[i-l].compareTo(aux[j-l]) &lt; 0 )&#123; // 左半部分所指元素 &lt; 右半部分所指元素 arr[k] = aux[i-l]; i ++; &#125; else&#123; // 左半部分所指元素 &gt;= 右半部分所指元素 arr[k] = aux[j-l]; j ++; &#125; &#125; &#125; // 递归使用归并排序,对arr[l...r]的范围进行排序 private static void sort(Comparable[] arr, int l, int r) &#123; if (l &gt;= r) return; int mid = (l+r)/2; sort(arr, l, mid); sort(arr, mid + 1, r); merge(arr, l, mid, r); &#125; public static void sort(Comparable[] arr)&#123; int n = arr.length; sort(arr, 0, n-1); &#125; // 测试MergeSort public static void main(String[] args) &#123; // Merge Sort是我们学习的第一个O(nlogn)复杂度的算法 // 可以在1秒之内轻松处理100万数量级的数据 // 注意：不要轻易尝试使用SelectionSort, InsertionSort或者BubbleSort处理100万级的数据 // 否则，你就见识了O(n^2)的算法和O(nlogn)算法的本质差异：） int N = 1000000; Integer[] arr = SortTestHelper.generateRandomArray(N, 0, 100000); SortTestHelper.testSort("bobo.algo.MergeSort", arr); return; &#125;&#125; 自底向上的归并排序基本思路：自底向上的归并排序的基本原理是，给定一个数组，从左向右将数组依次划分为小段，如两个元素一个小段，然后进行归并排序。当这一轮归并排序完成之后，再四个元素一个小段进行归并排序，最后八个元素一个小段进行归并排序。以上面的数组为例，这时候就排序完成了。这种方法并不需要递归，只需要迭代就可以完成排序操作。 代码实现：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import java.util.*;public class MergeSortBU&#123; // 我们的算法类不允许产生任何实例 private MergeSortBU()&#123;&#125; // 将arr[l...mid]和arr[mid+1...r]两部分进行归并 private static void merge(Comparable[] arr, int l, int mid, int r) &#123; Comparable[] aux = Arrays.copyOfRange(arr, l, r+1); // 初始化，i指向左半部分的起始索引位置l；j指向右半部分起始索引位置mid+1 int i = l, j = mid+1; for( int k = l ; k &lt;= r; k ++ )&#123; if( i &gt; mid )&#123; // 如果左半部分元素已经全部处理完毕 arr[k] = aux[j-l]; j ++; &#125; else if( j &gt; r )&#123; // 如果右半部分元素已经全部处理完毕 arr[k] = aux[i-l]; i ++; &#125; else if( aux[i-l].compareTo(aux[j-l]) &lt; 0 )&#123; // 左半部分所指元素 &lt; 右半部分所指元素 arr[k] = aux[i-l]; i ++; &#125; else&#123; // 左半部分所指元素 &gt;= 右半部分所指元素 arr[k] = aux[j-l]; j ++; &#125; &#125; &#125; public static void sort(Comparable[] arr)&#123; int n = arr.length; // Merge Sort Bottom Up 无优化版本// for (int sz = 1; sz &lt; n; sz *= 2)// for (int i = 0; i &lt; n - sz; i += sz+sz)// // 对 arr[i...i+sz-1] 和 arr[i+sz...i+2*sz-1] 进行归并// merge(arr, i, i+sz-1, Math.min(i+sz+sz-1,n-1)); // Merge Sort Bottom Up 优化 // 对于小数组, 使用插入排序优化 for( int i = 0 ; i &lt; n ; i += 16 ) InsertionSort.sort(arr, i, Math.min(i+15, n-1) ); for( int sz = 16; sz &lt; n ; sz += sz ) for( int i = 0 ; i &lt; n - sz ; i += sz+sz ) // 对于arr[mid] &lt;= arr[mid+1]的情况,不进行merge if( arr[i+sz-1].compareTo(arr[i+sz]) &gt; 0 ) merge(arr, i, i+sz-1, Math.min(i+sz+sz-1,n-1) ); &#125; // 测试 MergeSort BU public static void main(String[] args) &#123; // Merge Sort BU 也是一个O(nlogn)复杂度的算法，虽然只使用两重for循环 // 所以，Merge Sort BU也可以在1秒之内轻松处理100万数量级的数据 // 注意：不要轻易根据循环层数来判断算法的复杂度，Merge Sort BU就是一个反例 int N = 1000000; Integer[] arr = SortTestHelper.generateRandomArray(N, 0, 100000); SortTestHelper.testSort("bobo.algo.MergeSortBU", arr); return; &#125;&#125; 自底向上的归并排序方法没有用到数组的索引，所以可以很好的对链表结构进行排序 快速排序 - QuickSort基本思路：以下图的数组为例，首先选定一个元素4，把它挪到当它在排好序的时候，应该处的位置，当它处在这个位置之后，使得这个数组有了一个性质：在4之前的数都小于4，在4之后的数都大于4。接下来，对小于4的子数组和大于4的子数组分别继续进行快速排序。这样逐渐递归，完成整个排序过程。 那么关键问题是怎么把4这个元素放在正确的改放的位置，这个过程称为Partition。通常使用数组的第一个元素作为分界的标志点，第一个位置记作l，然后逐渐遍历右边没有被访问的元素，在遍历的过程中，逐步整理让元素一部分是&gt;v一部分是&lt;v的,将大于v和小于v的分界点的索引记为j，当前访问的元素e的索引记作i。下面讨论i位置的e该如何处理，如图。如果e&gt;v则将它放在这个位置不变，i++继续讨论下一个元素。否则如果e&lt;v则将i位置的e与j+1位置的元素交换，j++,i++，继续考察下一个元素。依次继续进行操作，直至遍历完数组的所有元素。这时候要将v放在数组中合适的位置，即将索引l位置的v与索引j位置的元素进行交换。最终完成此次操作。 代码实现：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import java.util.*;public class QuickSort &#123; // 我们的算法类不允许产生任何实例 private QuickSort()&#123;&#125; // 对arr[l...r]部分进行partition操作 // 返回p, 使得arr[l...p-1] &lt; arr[p] ; arr[p+1...r] &gt; arr[p] private static int partition(Comparable[] arr, int l, int r)&#123; Comparable v = arr[l]; int j = l; // arr[l+1...j] &lt; v ; arr[j+1...i) &gt; v for( int i = l + 1 ; i &lt;= r ; i ++ ) if( arr[i].compareTo(v) &lt; 0 )&#123; j ++; swap(arr, j, i); &#125; swap(arr, l, j); return j; &#125; // 递归使用快速排序,对arr[l...r]的范围进行排序 private static void sort(Comparable[] arr, int l, int r)&#123; if( l &gt;= r ) return; int p = partition(arr, l, r); sort(arr, l, p-1 ); sort(arr, p+1, r); &#125; public static void sort(Comparable[] arr)&#123; int n = arr.length; sort(arr, 0, n-1); &#125; private static void swap(Object[] arr, int i, int j) &#123; Object t = arr[i]; arr[i] = arr[j]; arr[j] = t; &#125; // 测试 QuickSort public static void main(String[] args) &#123; // Quick Sort也是一个O(nlogn)复杂度的算法 // 可以在1秒之内轻松处理100万数量级的数据 int N = 1000000; Integer[] arr = SortTestHelper.generateRandomArray(N, 0, 100000); SortTestHelper.testSort("bobo.algo.QuickSort", arr); return; &#125;&#125; 堆排序见堆和堆排序 排序算法总结 平均时间复杂度 原地排序 额外空间 稳定排序 选择排序Selection Sort O(n^2) √ O(1) × 插入排序Insertion Sort O(n^2) √ O(1) √ 归并排序Merge Sort O(nlogn) × O(n) √ 快速排序Quick Sort O(nlogn) √ O(logn) × 堆排序Heap Sort O(nlogn) √ O(1) × 排序算法的稳定性:稳定排序是指对于相等的元素，在排序后，原来靠前的元素依然靠前。相等元素的相对位置没有发生改变。]]></content>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vue-cli 目录结构笔记 及 一个简单电商项目的网页架构思路]]></title>
    <url>%2F2018%2F06%2F05%2FVue-cli-%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84%E7%AC%94%E8%AE%B0-%E5%8F%8A-%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%94%B5%E5%95%86%E9%A1%B9%E7%9B%AE%E7%9A%84%E7%BD%91%E9%A1%B5%E6%9E%B6%E6%9E%84%E6%80%9D%E8%B7%AF%2F</url>
    <content type="text"><![CDATA[学习了vue有一小段时间，期间中断去学习了java并且补了一下数据结构的基础，有点断层。跟着视频用vue2.0做了一个电商的小项目。思路稍微清晰了一些，但是因为中途转学其他的缘故，有一些东西还是忘掉了，这里总结一下使用vue-cli搭建项目的一些经验和教训。 首先是vue-cli的目录结构，这个是基于webpack的脚手架目录： 12345678910111213141516171819202122232425262728293031.|-- build // 项目构建(webpack)相关代码| |-- build.js // 生产环境构建代码| |-- check-version.js // 检查node、npm等版本| |-- utils.js // 构建工具相关| |-- vue-loader.conf.js // css加载器配置| |-- webpack.base.conf.js // webpack基础配置| |-- webpack.dev.conf.js // webpack开发环境配置| |-- webpack.prod.conf.js // webpack生产环境配置|-- config // 项目开发环境配置| |-- dev.env.js // 开发环境变量| |-- index.js // 项目一些配置变量(包括监听变量，打包路径等)| |-- prod.env.js // 生产环境变量| |-- test.env.js // 测试环境变量|-- node_modules //存放依赖的目录|-- src // 源码目录| |-- assets // 静态资源（css文件，外部js文件）| |-- components // vue公共组件| |-- router // 路由配置| |-- App.vue // 根组件| |-- main.js // 入口文件，加载各种公共组件|-- static // 静态文件，比如一些图片，json数据等|-- test // 测试文件目录|-- .babelrc // ES6语法编译配置|-- .editorconfig // 定义代码格式|-- .gitignore // git上传需要忽略的文件格式|-- .postcssrc.js|-- README.md // 项目说明|-- index.html // 入口页面|-- package.json // 项目基本信息. 当然不同版本的项目目录或者文件大同小异，基本都包括在上面了。 接下来讲一个平时用的比较多的网页排版及vue的大体配置。在此之前先介绍几个文件： index.html一般只定义一个空的根节点，在main.js里面定义的实例将挂载在根节点下，内容都通过vue组件来填充。 App.vueApp.vue 是个根组件。一个vue文件包括template,script,style三部分。vue通常用es6来写，用export default导出。&lt;style&gt;&lt;/style&gt;默认是影响全局的，如需定义作用域只在该组件下起作用，需在标签上加scoped。如要引入外部css文件，首先需给项目安装css-loader依赖包。使用import引入，比如： 12345&lt;style&gt; import &apos;./assets/css/bootstrap.css&apos;&lt;/style&gt; main.jsmain.js是个入口文件。这里:template: &#39;&lt;App/&gt;&#39;表示用&lt;app&gt;&lt;/app&gt;替换index.html里面的&lt;div id=&quot;app&quot;&gt;&lt;/div&gt;。这么做的目的很简单，&lt;App /&gt;他就是App.vue，template就是选择vue实例要加载哪个模板。最新的vue-cli脚手架模板现在是这个形式。App.vue是主程序，其他所有的.vue都是放在App.vue中，所以只需要加载App.vue就完全可以把其他的东西加载出来。 routerrouter目录下的index.js即是路由配置文件 router中可以设置多个路由，但是这里要先引入相应的组件，在进行设置： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152//引入Vue框架import Vue from &apos;vue&apos;//引入路由依赖import Router from &apos;vue-router&apos;//引入各个页面组件import IndexPage from &apos;./components/index&apos;import DetailPage from &apos;./components/detail.vue&apos;import DetailAnaPage from &apos;./components/detail/analysis&apos;import DetailPubPage from &apos;./components/detail/publish&apos;import DetailCouPage from &apos;./components/detail/count&apos;import DetailForPage from &apos;./components/detail/forecast&apos;import OrderListPage from &apos;./components/orderList&apos;Vue.use(Router)export default new Router(&#123; mode: &apos;history&apos;, routes: [ &#123; path: &apos;/&apos;, component: IndexPage &#125;, &#123; path: &apos;/orderList&apos;, component: OrderListPage &#125;, &#123; path: &apos;/detail&apos;, component: DetailPage, redirect: &apos;detail/analysis&apos;, children: [ &#123; path: &apos;forecast&apos;, component: DetailForPage &#125;, &#123; path: &apos;analysis&apos;, component: DetailAnaPage &#125;, &#123; path: &apos;publish&apos;, component: DetailPubPage &#125;, &#123; path: &apos;count&apos;, component: DetailCouPage &#125; ] &#125; ]&#125;) 这里介绍一个基础的vue模板构建思路： App.vue如下，其中的router的配置可以参见上面的代码 当然这里只设置了简单的内容，具体的方法和数据及样式根据不同的需求进行补充即可。 当然这只是一种简单的设计思路，做项目时可以用这个做为参考，但是不要被限制。]]></content>
      <tags>
        <tag>前端</tag>
        <tag>Vue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据基础学习笔记（十）——图计算]]></title>
    <url>%2F2018%2F05%2F28%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%EF%BC%89%E2%80%94%E2%80%94%E5%9B%BE%E8%AE%A1%E7%AE%97%2F</url>
    <content type="text"><![CDATA[图结构数据•许多大数据都是以大规模图或网络的形式呈现•许多非图结构的大数据，也常常会被转换为图模型后进行分析•图数据结构很好地表达了数据之间的关联性•关联性计算是大数据计算的核心——通过获得数据的关联性，可以从噪音很多的海量数据中抽取有用的信息 传统图计算解决方案的不足之处很多传统的图计算算法都存在以下几个典型问题：（1）常常表现出比较差的内存访问局部性（2）针对单个顶点的处理工作过少（3）计算过程中伴随着并行度的改变 针对大型图（比如社交网络和网络图）的计算问题，可能的解决方案及其不足之处具体如下：• （1 ）为特定的图应用定制相应的分布式实现• （2 ）基于现有的分布式计算平台进行图计算• （3 ）使用单机的图算法库：比如BGL、LEAD、NetworkX、JDSL、Standford GraphBase和FGL等• （4 ）使用已有的并行图计算系统：比如，ParallelBGL和CGM Graph，实现了很多并行图算法 图计算通用软件• 针对大型图的计算，目前通用的图计算软件主要包括两种：– 第一种主要是 基于遍历算法 的、 实时的图数据库，如Neo4j、OrientDB、DEX和 Infinite Graph– 第二种则是 以图顶点为中心的、基于消息传递批处理的并行引擎，如GoldenOrb、Giraph、Pregel和Hama，这些图处理软件主要是基于BSP模型实现的并行图处理系统 一次BSP(Bulk Synchronous Parallel Computing Model，又称“大同步”模型)计算过程包括一系列全局超步（所谓的超步就是计算中的一次迭代），每个超步主要包括三个组件：• 局部计算：每个参与的处理器都有自身的计算任务• 通讯：处理器群相互交换数据• 栅栏同步(Barrier Synchronization)：当一个处理器遇到“路障”（或栅栏），会等到其他所有处理器完成它们的计算步骤 Pregel•谷歌公司在2003年到2004年公布了GFS、MapReduce和BigTable•谷歌在后Hadoop时代的新“三驾马车”•Caffeine•Dremel•Pregel•Pregel是一种基于BSP模型实现的并行图处理系统•为了解决大型图的分布式计算问题，Pregel搭建了一套可扩展的、有容错机制的平台，该平台提供了一套非常灵活的API，可以描述各种各样的图计算•Pregel作为分布式图计算的计算框架，主要用于图遍历、最短路径、PageRank计算等等 Pregel图计算模型有向图和顶点•Pregel计算模型以有向图作为输入•有向图的每个顶点都有一个String类型的顶点ID•每个顶点都有一个可修改的用户自定义值与之关联•每条有向边都和其源顶点关联，并记录了其目标顶点ID•边上有一个可修改的用户自定义值与之关联 •在每个超步S中，图中的所有顶点都会并行执行相同的用户自定义函数•每个顶点可以接收前一个超步(S-1)中发送给它的消息，修改其自身及其出射边的状态，并发送消息给其他顶点，甚至是修改整个图的拓扑结构•在这种计算模式中，“边”并不是核心对象，在边上面不会运行相应的计算，只有顶点才会执行用户自定义函数进行相应计算 顶点之间的消息传递采用消息传递模型主要基于以下两个原因：（1）消息传递具有足够的表达能力，没有必要使用远程读取或共享内存的方式（2）有助于提升系统整体性能 Pregel的计算过程•Pregel的计算过程是由一系列被称为“超步”的迭代组成的•在每个超步中，每个顶点上面都会并行执行用户自定义的函数，该函数描述了一个顶点V在一个超步S中需要执行的操作•该函数可以读取前一个超步(S-1)中其他顶点发送给顶点V的消息，执行相应计算后，修改顶点V及其出射边的状态，然后沿着顶点V的出射边发送消息给其他顶点，而且，一个消息可能经过多条边的传递后被发送到任意已知ID的目标顶点上去•这些消息将会在下一个超步(S+1)中被目标顶点接收，然后象上述过程一样开始下一个超步(S+1)的迭代过程•在Pregel计算过程中，一个算法什么时候可以结束，是由所有顶点的状态决定的•在第0个超步，所有顶点处于活跃状态•当一个顶点不需要继续执行进一步的计算时，就会把自己的状态设置为“停机”，进入非活跃状态•当一个处于非活跃状态的顶点收到来自其他顶点的消息时，Pregel计算框架必须根据条件判断来决定是否将其显式唤醒进入活跃状态•当图中所有的顶点都已经标识其自身达到“非活跃（inactive）”状态，并且没有消息在传送的时候，算法就可以停止运行 实例 Pregel的C++ APIPregel已经预先定义好一个基类——Vertex类：123456789101112template &lt;typename VertexValue, typename EdgeValue, typename MessageValue&gt;class Vertex &#123; public: virtual void Compute(MessageIterator* msgs) = 0; const string&amp; vertex_id() const; int64 superstep() const; const VertexValue&amp; GetValue(); VertexValue* MutableValue(); OutEdgeIterator GetOutEdgeIterator(); void SendMessageTo(const string&amp; dest_vertex, const MessageValue&amp; message); void VoteToHalt();&#125;; •在Vetex类中，定义了三个值类型参数，分别表示顶点、边和消息。每一个顶点都有一个给定类型的值与之对应•编写Pregel程序时，需要继承Vertex类，并且覆写Vertex类的虚函数Compute() 消息传递机制• 顶点之间的通讯是借助于消息传递机制来实现的，每条消息都包含了消息值和需要到达的目标顶点ID。用户可以通过Vertex类的模板参数来设定消息值的数据类型• 在一个超步S中，一个顶点可以发送任意数量的消息，这些消息将在下一个超步（S+1）中被其他顶点接收• 一个顶点V通过与之关联的出射边向外发送消息，并且，消息要到达的目标顶点并不一定是与顶点V相邻的顶点，一个消息可以连续经过多条连通的边到达某个与顶点V不相邻的顶点U，U可以从接收的消息中获取到与其不相邻的顶点V的ID Combiner• Pregel计算框架在消息发出去之前，Combiner可以将发往同一个顶点的多个整型值进行求和得到一个值，只需向外发送这个“求和结果”，从而实现了由多个消息合并成一个消息，大大减少了传输和缓存的开销• 在默认情况下，Pregel计算框架并不会开启Combiner功能• 当用户打算开启Combiner功能时，可以继承Combiner类并覆写虚函数Combine()• 此外，通常只对那些满足交换律和结合律的操作才可以去开启Combiner功能 Aggregator• Aggregator提供了一种全局通信、监控和数据查看的机制• 在一个超步S中，每一个顶点都可以向一个Aggregator提供一个数据，Pregel计算框架会对这些值进行聚合操作产生一个值，在下一个超步（S+1）中，图中的所有顶点都可以看见这个值• Aggregator的聚合功能，允许在整型和字符串类型上执行最大值、最小值、求和操作，比如，可以定义一个“Sum”Aggregator来统计每个顶点的出射边数量，最后相加可以得到整个图的边的数量• Aggregator还可以实现全局协同的功能，比如，可以设计“and”Aggregator来决定在某个超步中Compute()函数是否执行某些逻辑分支，只有当“and” Aggregator显示所有顶点都满足了某条件时，才去执行这些逻辑分支 拓扑改变• Pregel计算框架允许用户在自定义函数Compute()中定义操作，修改图的拓扑结构，比如在图中增加（或删除）边或顶点• 对于全局拓扑改变，Pregel采用了惰性协调机制• 对于本地的局部拓扑改变，是不会引发冲突的，顶点或边的本地增减能够立即生效，很大程度上简化了分布式编程 输入和输出• 在Pregel计算框架中，图的保存格式多种多样，包括文本文件、关系数据库或键值数据库等• 在Pregel中，“从输入文件生成得到图结构”和“执行图计算”这两个过程是分离的，从而不会限制输入文件的格式• 对于输出，Pregel也采用了灵活的方式，可以以多种方式进行输出 Pregel的体系结构Pregel的执行过程•在Pregel计算框架中，一个大型图会被划分成许多个分区，每个分区都包含了一部分顶点以及以其为起点的边•一个顶点应该被分配到哪个分区上，是由一个函数决定的，系统默认函数为hash(ID) mod N，其中，N为所有分区总数，ID是这个顶点的标识符；当然，用户也可以自己定义这个函数•这样，无论在哪台机器上，都可以简单根据顶点ID判断出该顶点属于哪个分区，即使该顶点可能已经不存在了 在理想的情况下（不发生任何错误），一个Pregel用户程序的执行过程如下：（1）选择集群中的多台机器执行图计算任务，有一台机器会被选为Master，其他机器作为Worker（2）Master把一个图分成多个分区，并把分区分配到多个Worker。一个Worker会领到一个或多个分区，每个Worker知道所有其他Worker所分配到的分区情况 （3）Master会把用户输入划分成多个部分。然后，Master会为每个Worker分配用户输入的一部分。如果一个Worker从输入内容中加载到的顶点，刚好是自己所分配到的分区中的顶点，就会立即更新相应的数据结构。否则，该Worker会根据加载到的顶点的ID，把它发送到其所属的分区所在的Worker上。当所有的输入都被加载后，图中的所有顶点都会被标记为“活跃”状态。 （4）Master向每个Worker发送指令，Worker收到指令后，开始运行一个超步。当一个超步中的所有工作都完成以后，Worker会通知Master，并把自己在下一个超步还处于“活跃”状态的顶点的数量报告给Master。上述步骤会被不断重复，直到所有顶点都不再活跃并且系统中不会有任何消息在传输，这时，执行过程才会结束。（5）计算过程结束后，Master会给所有的Worker发送指令，通知每个Worker对自己的计算结果进行持久化存储 容错性• Pregel采用检查点机制来实现容错。在每个超步的开始，Master会通知所有的Worker把自己管辖的分区的状态写入到持久化存储设备• Master会周期性地向每个Worker发送ping消息，Worker收到ping消息后会给Master发送反馈消息• 每个Worker上都保存了一个或多个分区的状态信息，当一个Worker发生故障时，它所负责维护的分区的当前状态信息就会丢失。Master监测到一个Worker发生故障“失效”后，会把失效Worker所分配到的分区，重新分配到其他处于正常工作状态的Worker集合上，然后，所有这些分区会从最近的某超步S开始时写出的检查点中，重新加载状态信息 Worker在一个Worker中，它所管辖的分区的状态信息是保存在内存中的。分区中的顶点的状态信息包括：•顶点的当前值•以该顶点为起点的出射边列表，每条出射边包含了目标顶点ID和边的值•消息队列，包含了所有接收到的、发送给该顶点的消息•标志位，用来标记顶点是否处于活跃状态在每个超步中，Worker会对自己所管辖的分区中的每个顶点进行遍历，并调用顶点上的Compute()函数，在调用时，会把以下三个参数传递进去：•该顶点的当前值•一个接收到的消息的迭代器•一个出射边的迭代器 •在Pregel中，为了获得更好的性能，“标志位”和输入消息队列是分开保存的•对于每个顶点而言，Pregel只保存一份顶点值和边值，但是，会保存两份“标志位”和输入消息队列，分别用于当前超步和下一个超步•如果一个顶点V在超步S接收到消息，那么，它表示V将会在下一个超步S+1中（而不是当前超步S中）处于“活跃”状态 •当一个Worker上的一个顶点V需要发送消息到其他顶点U时，该Worker会首先判断目标顶点U是否位于自己机器上•如果目标顶点U在自己的机器上，就直接把消息放入到与目标顶点U对应的输入消息队列中•如果发现目标顶点U在远程机器上，这个消息就会被暂时缓存到本地，当缓存中的消息数目达到一个事先设定的阈值时，这些缓存消息会被批量异步发送出去，传输到目标顶点所在的Worker上 Master•Master主要负责协调各个Worker执行任务，每个Worker会借助于名称服务系统定位到Master的位置，并向Master发送自己的注册信息，Master会为每个Worker分配一个唯一的ID•Master维护着关于当前处于“有效”状态的所有Worker的各种信息，包括每个Worker的ID和地址信息，以及每个Worker被分配到的分区信息•Master中保存这些信息的数据结构的大小，只与分区的数量有关，而与顶点和边的数量无关 •一个大规模图计算任务会被Master分解到多个Worker去执行，在每个超步开始时，Master都会向所有处于“有效”状态的Worker发送相同的指令，然后等待这些Worker的回应•如果在指定时间内收不到某个Worker的反馈，Master就认为这个Worker失效•如果参与任务执行的多个Worker中的任意一个发生了故障失效，Master就会进入恢复模式•在每个超步中，图计算的各种工作，比如输入、输出、计算、保存和从检查点中恢复，都会在“路障（barrier）”之前结束 •Master在内部运行了一个HTTP服务器来显示图计算过程的各种信息•用户可以通过网页随时监控图计算执行过程各个细节 •图的大小 •关于出度分布的柱状图 •处于活跃状态的顶点数量 •在当前超步的时间信息和消息流量 •所有用户自定义Aggregator的值 Aggregator• 每个用户自定义的Aggregator都会采用聚合函数对一个值集合进行聚合计算得到一个全局值• 每个Worker都保存了一个Aggregator的实例集，其中的每个实例都是由类型名称和实例名称来标识的• 在执行图计算过程的某个超步S中，每个Worker会利用一个Aggregator对当前本地分区中包含的所有顶点的值进行归约，得到一个本地的局部归约值• 在超步S结束时，所有Worker会将所有包含局部归约值的Aggregator的值进行最后的汇总，得到全局值，然后提交给Master• 在下一个超步S+1开始时，Master就会将Aggregator的全局值发送给每个Worker Pregel的应用实例——单源最短路径Dijkstra算法是解决单源最短路径问题的贪婪算法 Pregel非常适合用来解决单源最短路径问题，实现代码如下：12345678910111213141516class ShortestPathVertex : public Vertex&lt;int, int, int&gt; &#123; void Compute(MessageIterator* msgs) &#123; int mindist = IsSource(vertex_id()) ? 0 : INF; for (; !msgs-&gt;Done(); msgs-&gt;Next()) mindist = min(mindist, msgs-&gt;Value()); if (mindist &lt; GetValue()) &#123; *MutableValue() = mindist; OutEdgeIterator iter = GetOutEdgeIterator(); for (; !iter.Done(); iter.Next()) SendMessageTo(iter.Target(), mindist + iter.GetValue()); &#125; VoteToHalt(); &#125; &#125;; 超步1：•顶点0：没有收到消息，依然非活跃•顶点1：收到消息100（唯一消息），被显式唤醒，执行计算，mindist变为100，小于顶点值INF，顶点值修改为100，没有出射边，不需要发送消息，最后变为非活跃•顶点2：收到消息30，被显式唤醒，执行计算，mindist变为30，小于顶点值ZNF，顶点值修改为30，有两条出射边，向顶点3发送消息90（即：30+60），向顶点1发送消息90（即：30+60），最后变为非活跃•顶点3：没有收到消息，依然非活跃•顶点4：收到消息10，被显式唤醒，执行计算，mindist变为10，小于顶点值INF，顶点值修改为10，向顶点3发送消息60（即：10+50），最后变为非活跃剩余超步省略……当所有顶点非活跃，并且没有消息传递，就结束]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据基础学习笔记（九）——流计算]]></title>
    <url>%2F2018%2F05%2F22%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B9%9D%EF%BC%89%E2%80%94%E2%80%94%E6%B5%81%E8%AE%A1%E7%AE%97%2F</url>
    <content type="text"><![CDATA[什么是流数据• 近年来，在Web应用、网络监控、传感监测等领域，兴起了一种新的数据密集型应用——流数据，即数据以大量、快速、时变的流形式持续到达• 实例：PM2.5检测、电子商务网站用户点击流• 流数据具有如下特征：– 数据快速持续到达，潜在大小也许是无穷无尽的– 数据来源众多，格式复杂– 数据量大，但是不十分关注存储，一旦经过处理，要么被丢弃，要么被归档存储– 注重数据的整体价值，不过分关注个别数据– 数据顺序颠倒，或者不完整，系统无法控制将要处理的新到达的数据元素的顺序• 对静态数据和流数据的处理，对应着两种截然不同的计算模式：批量计算和实时计算•批量计算：充裕时间处理静态数据，如Hadoop•流数据不适合采用批量计算，因为流数据不适合用传统的关系模型建模•流数据必须采用实时计算，响应时间为秒级•在大数据时代，数据格式复杂、来源众多、数据量巨大，对实时计算提出了很大的挑战。因此，针对流数据的实时计算——流计算，应运而生 流计算的概念• 流计算秉承一个基本理念，即 数据的价值随着时间的流逝而降低，如用户点击流。因此，当事件出现时就应该立即进行处理，而不是缓存起来进行批量处理。为了及时处理流数据，就需要一个低延迟、可扩展、高可靠的处理引擎• 对于一个流计算系统来说，它应达到如下需求：– 高性能– 海量式– 实时性– 分布式– 易用性– 可靠性 流计算与Hadoop• Hadoop设计的初衷是面向大规模数据的批量处理• MapReduce是专门面向静态数据的批量处理的，内部各种实现机制都为批处理做了高度优化，不适合用于处理持续到达的动态数据• 可能会想到一种“变通”的方案来降低批处理的时间延迟——将基于MapReduce的批量处理转为小批量处理，将输入数据切成小的片段，每隔一个周期就启动一次MapReduce作业。但这种方式也无法有效处理流数据– 切分成小片段，可以降低延迟，但是也增加了附加开销，还要处理片段之间依赖关系– 需要改造MapReduce以支持流式处理结论：鱼和熊掌不可兼得，Hadoop擅长批处理，但是不适合流计算• 当前业界诞生了许多专门的流数据实时计算系统来满足各自需求：• 商业级：IBM InfoSphere Streams和IBM StreamBase• 开源流计算框架：– Twitter Storm：免费、开源的分布式实时计算系统，可简单、高效、可靠地处理大量的流数据– Yahoo! S4（Simple Scalable Streaming System）：开源流计算平台，是通用的、分布式的、可扩展的、分区容错的、可插拔的流式系统• 公司为支持自身业务开发的流计算框架：– Facebook Puma– Dstream（百度）– 银河流数据处理平台（淘宝） 流计算处理流程• 传统的数据处理流程，需要先采集数据并存储在关系数据库等数据管理系统中，之后由用户通过查询操作和数据管理系统进行交互• 传统的数据处理流程隐含了两个前提：– 存储的数据是旧的。存储的静态数据是过去某一时刻的快照，这些数据在查询时可能已不具备时效性了– 需要用户主动发出查询 • 流计算的处理流程一般包含三个阶段：数据实时采集、数据实时计算、实时查询服务 数据实时采集• 数据实时采集阶段通常采集多个数据源的海量数据，需要保证实时性、低延迟与稳定可靠• 以日志数据为例，由于分布式集群的广泛应用，数据分散存储在不同的机器上，因此需要实时汇总来自不同机器上的日志数据• 目前有许多互联网公司发布的开源分布式日志采集系统均可满足每秒数百MB的数据采集和传输需求，如： – Facebook的Scribe – LinkedIn的Kafka – 淘宝的Time Tunnel – 基于Hadoop的Chukwa和Flume • 数据采集系统的基本架构一般有以下三个部分：– Agent：主动采集数据，并把数据推送到Collector部分– Collector：接收多个Agent的数据，并实现有序、可靠、高性能的转发– Store：存储Collector转发过来的数据（对于流计算不存储数据） 数据实时计算• 数据实时计算阶段对采集的数据进行实时的分析和计算，并反馈实时结果• 经流处理系统处理后的数据，可视情况进行存储，以便之后再进行分析计算。在时效性要求较高的场景中，处理之后的数据也可以直接丢弃 实时查询服务• 实时查询服务：经由流计算框架得出的结果可供用户进行实时查询、展示或储存• 传统的数据处理流程，用户需要主动发出查询才能获得想要的结果。而在流处理流程中，实时查询服务可以不断更新结果，并将用户所需的结果实时推送给用户• 虽然通过对传统的数据处理系统进行定时查询，也可以实现不断地更新结果和结果推送，但通过这样的方式获取的结果，仍然是根据过去某一时刻的数据得到的结果，与实时结果有着本质的区别• 可见，流处理系统与传统的数据处理系统有如下不同：– 流处理系统处理的是实时的数据，而传统的数据处理系统处理的是预先存储好的静态数据– 用户通过流处理系统获取的是实时结果，而通过传统的数据处理系统，获取的是过去某一时刻的结果– 流处理系统无需用户主动发出查询，实时查询服务可以主动将实时结果推送给用户 开源流计算框架Storm• Twitter Storm是一个免费、开源的分布式实时计算系统，Storm对于实时计算的意义类似于Hadoop对于批处理的意义，Storm可以简单、高效、可靠地处理流数据，并支持多种编程语言 • Storm框架可以方便地与数据库系统进行整合，从而开发出强大的实时计算系统 • Twitter是全球访问量最大的社交网站之一，Twitter开发Storm流处理框架也是为了应对其不断增长的流数据实时处理需求 Storm的特点• Storm可用于许多领域中，如实时分析、在线机器学习、持续计算、远程RPC、数据提取加载转换等 • Storm具有以下主要特点：– 整合性– 简易的API– 可扩展性– 可靠的消息处理– 支持各种编程语言– 快速部署– 免费、开源 Storm设计思想• Storm主要术语包括Streams、Spouts、Bolts、Topology和Stream Groupings • Streams ：Storm将流数据Stream描述成一个无限的Tuple序列，这些Tuple序列会以分布式的方式并行地创建和处理 •每个tuple是一堆值，每个值有一个名字，并且每个值可以是任何类型•Tuple本来应该是一个Key-Value的Map，由于各个组件间传递的tuple的字段名称已经事先定义好了，所以Tuple只需要按序填入各个Value，所以就是一个Value List（值列表） • Spout：Storm认为每个Stream都有一个源头，并把这个源头抽象为Spout • 通常Spout会从外部数据源（队列、数据库等）读取数据，然后封装成Tuple形式，发送到Stream中。Spout是一个主动的角色，在接口内部有个nextTuple函，Storm框架会不停的调用该函数 • Bolt ：Storm将Streams的状态转换过程抽象为Bolt。Bolt即可以处理Tuple，也可以将处理后的Tuple作为新的Streams发送给其他Bolt • Bolt可以执行过滤、函数操作、Join、操作数据库等任何操作• Bolt是一个被动的角色，其接口中有一个execute(Tuple input)方法，在接收到消息之后会调用此函数，用户可以在此方法中执行自己的处理逻辑 • Topology ：Storm将Spouts和Bolts组成的网络抽象成Topology，它可以被提交到Storm集群执行。Topology可视为流转换图，图中节点是一个Spout或Bolt，边则表示Bolt订阅了哪个Stream。当Spout或者Bolt发送元组时，它会把元组发送到每个订阅了该Stream的Bolt上进行处理• Topology里面的每个处理组件（Spout或Bolt）都包含处理逻辑， 而组件之间的连接则表示数据流动的方向• Topology里面的每一个组件都是并行运行的•在Topology里面可以指定每个组件的并行度，Storm会在集群里面分配那么多的线程来同时计算•在Topology的具体实现上，Storm中的Topology定义仅仅是一些Thrift结构体（二进制高性能的通信中间件），支持各种编程语言进行定义 • Stream Groupings ：Storm中的Stream Groupings用于告知Topology如何在两个组件间（如Spout和Bolt之间，或者不同的Bolt之间）进行Tuple的传送。每一个Spout和Bolt都可以有多个分布式任务，一个任务在什么时候、以什么方式发送Tuple就是由Stream Groupings来决定的 目前，Storm中的Stream Groupings有如下几种方式： (1)ShuffleGrouping：随机分组，随机分发Stream中的Tuple，保证每个Bolt的Task接收Tuple数量大致一致(2)FieldsGrouping：按照字段分组，保证相同字段的Tuple分配到同一个Task中(3)AllGrouping：广播发送，每一个Task都会收到所有的Tuple(4)GlobalGrouping：全局分组，所有的Tuple都发送到同一个Task中(5)NonGrouping：不分组，和ShuffleGrouping类似，当前Task的执行会和它的被订阅者在同一个线程中执行(6)DirectGrouping：直接分组，直接指定由某个Task来执行Tuple的处理 Storm框架设计•Storm运行任务的方式与Hadoop类似：Hadoop运行的是MapReduce作业，而Storm运行的是“Topology”•但两者的任务大不相同，主要的不同是：MapReduce作业最终会完成计算并结束运行，而Topology将持续处理消息（直到人为终止） • Storm集群采用“Master—Worker”的节点方式：– Master节点运行名为“Nimbus”的后台程序（类似Hadoop中的“JobTracker”），负责在集群范围内分发代码、为Worker分配任务和监测故障– Worker节点运行名为“Supervisor”的后台程序，负责监听分配给它所在机器的工作，即根据Nimbus分配的任务来决定启动或停止Worker进程，一个Worker节点上同时运行若干个Worker进程• Storm使用Zookeeper来作为分布式协调组件，负责Nimbus和多个Supervisor之间的所有协调工作。借助于Zookeeper，若Nimbus进程或Supervisor进程意外终止，重启时也能读取、恢复之前的状态并继续工作，使得Storm极其稳定 worker进程 (1)Worker进程:每个worker进程都属于一个特定的Topology，每个Supervisor节点的worker可以有多个，每个worker对Topology中的每个组件（Spout或Bolt）运行一个或者多个executor线程来提供task的运行服务(2)Executor：executor是产生于worker进程内部的线程，会执行同一个组件的一个或者多个task。(3)Task:实际的数据处理由task完成Worker、Executor和Task的关系 • 基于这样的架构设计，Storm的工作流程如下图所示：•所有Topology任务的提交必须在Storm客户端节点上进行，提交后，由Nimbus节点分配给其他Supervisor节点进行处理•Nimbus节点首先将提交的Topology进行分片，分成一个个Task，分配给相应的Supervisor，并将Task和Supervisor相关的信息提交到Zookeeper集群上•Supervisor会去Zookeeper集群上认领自己的Task，通知自己的Worker进程进行Task的处理 Spark StreamingSpark Streaming设计•Spark Streaming可整合多种输入数据源，如Kafka、Flume、HDFS，甚至是普通的TCP套接字。经处理后的数据可存储至文件系统、数据库，或显示在仪表盘里 Spark Streaming的基本原理是将实时输入数据流以时间片（秒级）为单位进行拆分，然后经Spark引擎以类似批处理的方式处理每个时间片数据 Spark Streaming最主要的抽象是DStream（Discretized Stream，离散化数据流），表示连续不断的数据流。在内部实现上，Spark Streaming的输入数据按照时间片（如1秒）分成一段一段的DStream，每一段数据转换为Spark中的RDD，并且对DStream的操作都最终转变为对相应的RDD的操作 Spark Streaming与Storm的对比•Spark Streaming和Storm最大的区别在于，Spark Streaming无法实现毫秒级的流计算，而Storm可以实现毫秒级响应 •Spark Streaming构建在Spark上，一方面是因为Spark的低延迟执行引擎（100ms+）可以用于实时计算，另一方面，相比于Storm，RDD数据集更容易做高效的容错处理 •Spark Streaming采用的小批量处理的方式使得它可以同时兼容批量和实时数据处理的逻辑和算法，因此，方便了一些需要历史数据和实时数据联合分析的特定应用场合 Samza1.作业 一个作业（Job）是对一组输入流进行处理转化成输出流的程序。 2.分区 •Samza的流数据单位既不是Storm中的元组，也不是Spark Streaming中的DStream，而是一条条消息•Samza中的每个流都被分割成一个或多个分区，对于流里的每一个分区而言，都是一个有序的消息序列，后续到达的消息会根据一定规则被追加到其中一个分区里 3.任务 •一个作业会被进一步分割成多个任务（Task）来执行，其中，每个任务负责处理作业中的一个分区•分区之间没有定义顺序，从而允许每一个任务独立执行•YARN调度器负责把任务分发给各个机器，最终，一个工作中的多个任务会被分发到多个机器进行分布式并行处理 4.数据流图 •一个数据流图是由多个作业构成的，其中，图中的每个节点表示包含数据的流，每条边表示数据传输•多个作业串联起来就完成了流式的数据处理流程•由于采用了异步的消息订阅分发机制，不同任务之间可以独立运行 系统架构•Samza系统架构主要包括•流数据层（Kafka）•执行层（YARN）•处理层（Samza API）•流处理层和执行层都被设计成可插拔的，开发人员可以使用其他框架来替代YARN和Kafka 处理分析过程如下： •Samza客户端需要执行一个Samza作业时，它会向YARN的ResouceManager提交作业请求 •ResouceManager通过与NodeManager沟通为该作业分配容器（包含了CPU、内存等资源）来运行Samza ApplicationMaster •Samza ApplicationMaster进一步向ResourceManager申请运行任务的容器 •获得容器后，Samza ApplicationMaster与容器所在的NodeManager沟通，启动该容器，并在其中运行Samza Task Runner •Samza Task Runner负责执行具体的Samza任务，完成流数据处理分析 Storm、Spark Streaming和Samza的应用场景•从编程的灵活性来讲，Storm是比较理想的选择，它使用Apache Thrift，可以用任何编程语言来编写拓扑结构（Topology） •当需要在一个集群中把流计算和图计算、机器学习、SQL查询分析等进行结合时，可以选择Spark Streaming，因为，在Spark上可以统一部署Spark SQL，Spark Streaming、MLlib，GraphX等组件，提供便捷的一体化编程模型 •当有大量的状态需要处理时，比如每个分区都有数十亿个元组，则可以选择Samza。当应用场景需要毫秒级响应时，可以选择Storm和Samza，因为Spark Streaming无法实现毫秒级的流计算]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据基础学习笔记（八）——Spark]]></title>
    <url>%2F2018%2F05%2F17%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AB%EF%BC%89%E2%80%94%E2%80%94Spark%2F</url>
    <content type="text"><![CDATA[Spark的特点•运行速度快：使用DAG执行引擎以支持循环数据流与内存计算 •容易使用：支持使用Scala、Java、Python和R语言进行编程，可以通过Spark Shell进行交互式编程 •通用性：Spark提供了完整而强大的技术栈，包括SQL查询、流式计算、机器学习和图算法组件 •运行模式多样：可运行于独立的集群模式中，可运行于Hadoop中，也可运行于Amazon EC2等云环境中，并且可以访问HDFS、Cassandra、HBase、Hive等多种数据源 Scala简介Scala是一门现代的多范式编程语言，运行于Java平台（JVM，Java 虚拟机），并兼容现有的Java程序 Scala的特性： •Scala具备强大的并发性，支持函数式编程，可以更好地支持分布式系统 •Scala语法简洁，能提供优雅的API Scala兼容Java，运行速度快，且能融合到Hadoop生态圈中 Scala是Spark的主要编程语言，但Spark还支持Java、Python、R作为编程语言 Scala的优势是提供了REPL（Read-Eval-Print Loop，交互式解释器），提高程序开发效率 Spark与Hadoop的对比Hadoop存在如下一些缺点： •表达能力有限 •磁盘IO开销大 •延迟高 •任务之间的衔接涉及IO开销 •在前一个任务执行完成之前，其他任务就无法开始，难以胜任复杂、多阶段的计算任务 Spark在借鉴Hadoop MapReduce优点的同时，很好地解决了MapReduce所面临的问题 相比于Hadoop MapReduce，Spark主要具有如下优点： •Spark的计算模式也属于MapReduce，但不局限于Map和Reduce操作，还提供了多种数据集操作类型，编程模型比Hadoop MapReduce更灵活 •Spark提供了内存计算，可将中间结果放到内存中，对于迭代运算效率更高 Spark基于DAG的任务调度执行机制，要优于Hadoop MapReduce的迭代执行机制 •使用Hadoop进行迭代计算非常耗资源 •Spark将数据载入内存后，之后的迭代计算都可以直接使用内存中的中间结果作运算，避免了从磁盘中频繁读取数据 Spark生态系统在实际应用中，大数据处理主要包括以下三个类型： •复杂的批量数据处理：通常时间跨度在数十分钟到数小时之间 •基于历史数据的交互式查询：通常时间跨度在数十秒到数分钟之间 •基于实时数据流的数据处理：通常时间跨度在数百毫秒到数秒之间 当同时存在以上三种场景时，就需要同时部署三种不同的软件 •比如: MapReduce / Impala / Storm这样做难免会带来一些问题： •不同场景之间输入输出数据无法做到无缝共享，通常需要进行数据格式的转换 •不同的软件需要不同的开发和维护团队，带来了较高的使用成本 •比较难以对同一个集群中的各个系统进行统一的资源协调和分配 •Spark的设计遵循“一个软件栈满足不同应用场景”的理念，逐渐形成了一套完整的生态系统 •既能够提供内存计算框架，也可以支持SQL即席查询、实时流式计算、机器学习和图计算等 •Spark可以部署在资源管理器YARN之上，提供一站式的大数据解决方案 •因此，Spark所提供的生态系统足以应对上述三种场景，即同时支持批处理、交互式查询和流数据处理 Spark生态系统已经成为伯克利数据分析软件栈BDAS（Berkeley Data Analytics Stack）的重要组成部分 Spark的生态系统主要包含了Spark Core、Spark SQL、Spark Streaming、MLLib和GraphX 等组件 Spark运行架构•RDD：是Resillient Distributed Dataset（弹性分布式数据集）的简称，是分布式内存的一个抽象概念，提供了一种高度受限的共享内存模型 •DAG：是Directed Acyclic Graph（有向无环图）的简称，反映RDD之间的依赖关系 •Executor：是运行在工作节点（WorkerNode）的一个进程，负责运行Task •Application：用户编写的Spark应用程序 •Task：运行在Executor上的工作单元 •Job：一个Job包含多个RDD及作用于相应RDD上的各种操作 •Stage：是Job的基本调度单位，一个Job会分为多组Task，每组Task被称为Stage，或者也被称为TaskSet，代表了一组关联的、相互之间没有Shuffle依赖关系的任务组成的任务集 •Spark运行架构包括集群资源管理器（Cluster Manager）、运行作业任务的工作节点（Worker Node）、每个应用的任务控制节点（Driver）和每个工作节点上负责具体任务的执行进程（Executor） •资源管理器可以自带或Mesos或YARN 与Hadoop MapReduce计算框架相比，Spark所采用的Executor有两个优点： •一是利用多线程来执行具体的任务，减少任务的启动开销 •二是Executor中有一个BlockManager存储模块，会将内存和磁盘共同作为存储设备，有效减少IO开销 •一个Application由一个Driver和若干个Job构成，一个Job由多个Stage构成，一个Stage由多个没有Shuffle关系的Task组成 •当执行一个Application时，Driver会向集群管理器申请资源，启动xecutor，并向Executor发送应用程序代码和文件，然后在Executor上执行Task，运行结束后，执行结果会返回给Driver，或者写到HDFS或者其他数据库中 Spark运行基本流程（1）首先为应用构建起基本的运行环境，即由Driver创建一个SparkContext，进行资源的申请、任务的分配和监控 （2）资源管理器为Executor分配资源，并启动Executor进程 （3）SparkContext根据RDD的依赖关系构建DAG图，DAG图提交给DAGScheduler解析成Stage，然后把一个个TaskSet提交给底层调度器TaskScheduler处理；Executor向SparkContext申请Task，Task Scheduler将Task发放给Executor运行，并提供应用程序代码 （4）Task在Executor上运行，把执行结果反馈给TaskScheduler，然后反馈给DAGScheduler，运行完毕后写入数据并释放所有资源 总体而言，Spark运行架构具有以下特点： （1）每个Application都有自己专属的Executor进程，并且该进程在Application运行期间一直驻留。Executor进程以多线程的方式运行Task （2）Spark运行过程与资源管理器无关，只要能够获取Executor进程并保持通信即可 （3）Task采用了数据本地性和推测执行等优化机制 RDD1.设计背景 •许多迭代式算法（比如机器学习、图算法等）和交互式数据挖掘工具，共同之处是，不同计算阶段之间会重用中间结果 •目前的MapReduce框架都是把中间结果写入到HDFS中，带来了大量的数据复制、磁盘IO和序列化开销 •RDD就是为了满足这种需求而出现的，它提供了一个抽象的数据架构，我们不必担心底层数据的分布式特性，只需将具体的应用逻辑表达为一系列转换处理，不同RDD之间的转换操作形成依赖关系，可以实现管道化，避免中间数据存储 2.RDD 概念 •一个RDD就是一个分布式对象集合，本质上是一个只读的分区记录集合，每个RDD可分成多个分区，每个分区就是一个数据集片段，并且一个RDD的不同分区可以被保存到集群中不同的节点上，从而可以在集群中的不同节点上进行并行计算 •RDD提供了一种高度受限的共享内存模型，即RDD是只读的记录分区的集合，不能直接修改，只能基于稳定的物理存储中的数据集创建RDD，或者通过在其他RDD上执行确定的转换操作（如map、join和group by）而创建得到新的RDD •RDD提供了一组丰富的操作以支持常见的数据运算，分为“动作”（Action）和“转换”（Transformation）两种类型 •RDD提供的转换接口都非常简单，都是类似map、filter、groupBy、join等粗粒度的数据转换操作，而不是针对某个数据项的细粒度修改（不适合网页爬虫） •表面上RDD的功能很受限、不够强大，实际上RDD已经被实践证明可以高效地表达许多框架的编程模型（比如MapReduce、SQL、Pregel） •Spark用Scala语言实现了RDD的API，程序员可以通过调用API实现对RDD的各种操作 RDD典型的执行过程如下： •RDD读入外部数据源进行创建 •RDD经过一系列的转换（Transformation）操作，每一次都会产生不同的RDD，供给下一个转换操作使用 •最后一个RDD经过“动作”操作进行转换，并输出到外部数据源这一系列处理称为一个Lineage（血缘关系），即DAG拓扑排序的结果 优点：惰性调用、管道化、避免同步等待、不需要保存中间结果、每次操作变得简单 3.RDD特性 Spark采用RDD以后能够实现高效计算的原因主要在于： （1）高效的容错性 •现有容错机制：数据复制或者记录日志 •RDD：血缘关系、重新计算丢失分区、无需回滚系统、重算过程在不同节点之间并行、只记录粗粒度的操作 （2）中间结果持久化到内存，数据在内存中的多个RDD操作之间进行传递，避免了不必要的读写磁盘开销 （3）存放的数据可以是Java对象，避免了不必要的对象序列化和反序列化 4.RDD之间的依赖关系 窄依赖表现为一个父RDD的分区对应于一个子RDD的分区或多个父RDD的分区对应于一个子RDD的分区 •宽依赖则表现为存在一个父RDD的一个分区对应一个子RDD的多个分区 5.Stage的划分 Spark通过分析各个RDD的依赖关系生成了DAG，再通过分析各个RDD中的分区之间的依赖关系来决定如何划分Stage，具体划分方法是： •在DAG中进行反向解析，遇到宽依赖就断开 •遇到窄依赖就把当前的RDD加入到Stage中 •将窄依赖尽量划分在同一个Stage中，可以实现流水线计算被分成三个Stage，在Stage2中，从map到union都是窄依赖，这两步操作可以形成一个流水线操作 流水线操作实例分区7通过map操作生成的分区9，可以不用等待分区8到分区10这个map操作的计算结束，而是继续进行union操作，得到分区13，这样流水线执行大大提高了计算的效率 Stage的类型包括两种：ShuffleMapStage和ResultStage，具体如下： （1）ShuffleMapStage：不是最终的Stage，在它之后还有其他Stage，所以，它的输出一定需要经过Shuffle过程，并作为后续Stage的输入；这种Stage是以Shuffle为输出边界，其输入边界可以是从外部获取数据，也可以是另一个ShuffleMapStage的输出，其输出可以是另一个Stage的开始；在一个Job里可能有该类型的Stage，也可能没有该类型Stage； （2）ResultStage：最终的Stage，没有输出，而是直接产生结果或存储。这种Stage是直接输出结果，其输入边界可以是从外部获取数据，也可以是另一个ShuffleMapStage的输出。在一个Job里必定有该类型Stage。因此，一个Job含有一个或多个Stage，其中至少含有一个ResultStage。 6.RDD运行过程 通过上述对RDD概念、依赖关系和Stage划分的介绍，结合之前介绍的Spark运行基本流程，再总结一下RDD在Spark架构中的运行过程： （1）创建RDD对象； （2）SparkContext负责计算RDD之间的依赖关系，构建DAG； （3）DAGScheduler负责把DAG图分解成多个Stage，每个Stage中包含了多个Task，每个Task会被TaskScheduler分发给各个WorkerNode上的Executor去执行。 Spark SQL设计Spark SQL在Hive兼容层面仅依赖HiveQL解析、Hive元数据，也就是说，从HQL被解析成抽象语法树（AST）起，就全部由Spark SQL接管了。Spark SQL执行计划生成和优化都由Catalyst（函数式关系查询优化框架）负责 •Spark SQL增加了SchemaRDD（即带有Schema信息的RDD），使用户可以在Spark SQL中执行SQL语句，数据既可以来自RDD，也可以是Hive、HDFS、Cassandra等外部数据源，还可以是JSON格式的数据 •Spark SQL目前支持Scala、Java、Python三种语言，支持SQL-92规范]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据基础学习笔记（七）——数据仓库Hive]]></title>
    <url>%2F2018%2F05%2F01%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%83%EF%BC%89%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93Hive%2F</url>
    <content type="text"><![CDATA[数据仓库概念数据仓库（Data Warehouse）是一个面向主题的（Subject Oriented）、集成的（Integrated）、相对稳定的（Non-Volatile）、反映历史变化（Time Variant）的数据集合，用于支持管理决策。 Hive简介•Hive是一个构建于Hadoop顶层的数据仓库工具 •支持大规模数据存储、分析，具有良好的可扩展性 •某种程度上可以看作是用户编程接口，本身不存储和处理数据 •依赖分布式文件系统HDFS存储数据 •依赖分布式并行计算模型MapReduce处理数据 •定义了简单的类似SQL 的查询语言——HiveQL •用户可以通过编写的HiveQL语句运行MapReduce任务 •可以很容易把原来构建在关系数据库上的数据仓库应用程序移植到Hadoop平台上 •是一个可以提供有效、合理、直观组织和使用数据的分析工具 Hive具有的特点非常适用于数据仓库 1 采用批处理方式处理海量数据 •Hive需要把HiveQL语句转换成MapReduce任务进行运行 •数据仓库存储的是静态数据，对静态数据的分析适合采用批处理方式，不需要快速响应给出结果，而且数据本身也不会频繁变化 2 提供适合数据仓库操作的工具 •Hive本身提供了一系列对数据进行提取、转换、加载（ETL）的工具，可以存储、查询和分析存储在Hadoop中的大规模数据 •这些工具能够很好地满足数据仓库各种应用场景 Hive与Hadoop生态系统中其他组件的关系•Hive 依赖于HDFS 存储数据 •Hive 依赖于MapReduce 处理数据 • 在某些场景下Pig 可以作为Hive 的替代工具 •HBase 提供数据的实时访问 Hive 与传统数据库的对比分析Hive在很多方面和传统的关系数据库类似，但是它的底层依赖的是HDFS和MapReduce，所以在很多方面又有别于传统数据库 Hive 在企业中的部署和应用 Hive在企业大数据分析平台中的应用 Hive 在Facebook 公司中的应用 •基于Oracle的数据仓库系统已经无法满足激增的业务需求 •Facebook公司开发了数据仓库工具Hive，并在企业内部进行了大量部署 Hive系统架构•用户接口模块包括CLI、HWI、JDBC、ODBC、Thrift Server •驱动模块（Driver）包括编译器、优化器、执行器等，负责把HiveSQL语句转换成一系列MapReduce作业 •元数据存储模块（Metastore）是一个独立的关系型数据库（自带derby数据库，或MySQL数据库） Hive HA基本原理问题：在实际应用中，Hive也暴露出不稳定的问题 解决方案：Hive HA（High Availability） •由多个Hive实例进行管理的，这些Hive实例被纳入到一个资源池中，并由HAProxy提供一个统一的对外接口 •对于程序开发人员来说，可以把它认为是一台超强“Hive” Hive工作原理SQL语句转换成MapReduce作业的基本原理 join的实现原理 group by 的实现原理 存在一个分组（Group By）操作，其功能是把表Score的不同片段按照rank和level的组合值进行合并，计算不同rank和level的组合值分别有几条记录：select rank, level ,count(*) as value from score group by rank, level Hive中SQL查询转换成MapReduce作业的过程•当用户向Hive输入一段命令或查询时，Hive需要与Hadoop交互工作来完成该操作： •驱动模块接收该命令或查询编译器 •对该命令或查询进行解析编译 •由优化器对该命令或查询进行优化计算 •该命令或查询通过执行器进行执行 第1步：由Hive驱动模块中的编译器对用户输入的SQL语言进行词法和语法解析，将SQL语句转化为抽象语法树的形式 第2步：抽象语法树的结构仍很复杂，不方便直接翻译为MapReduce算法程序，因此，把抽象语法书转化为查询块 第3步：把查询块转换成逻辑查询计划，里面包含了许多逻辑操作符 第4步：重写逻辑查询计划，进行优化，合并多余操作，减少MapReduce任务数量 第5步：将逻辑操作符转换成需要执行的具体MapReduce任务 第6步：对生成的MapReduce任务进行优化，生成最终的MapReduce任务执行计划 第7步：由Hive驱动模块中的执行器，对最终的MapReduce任务进行执行输出 几点说明： • 当启动MapReduce程序时，Hive本身是不会生成MapReduce算法程序的 • 需要通过一个表示“Job执行计划”的XML文件驱动执行内置的、原生的Mapper和Reducer模块 • Hive通过和JobTracker通信来初始化MapReduce任务，不必直接部署在JobTracker所在的管理节点上执行 • 通常在大型集群上，会有专门的网关机来部署Hive工具。网关机的作用主要是远程操作和管理节点上的JobTracker通信来执行任务 • 数据文件通常存储在HDFS上，HDFS由名称节点管理 ImpalaImpala简介• Impala是由Cloudera公司开发的新型查询系统，它提供SQL语义，能查询存储在Hadoop的HDFS和HBase上的PB级大数据，在性能上比Hive高出3~30倍 • Impala的运行需要依赖于Hive的元数据 • Impala是参照 Dremel系统进行设计的 • Impala采用了与商用并行关系数据库类似的分布式查询引擎，可以直接与HDFS和HBase进行交互查询 • Impala和Hive采用相同的SQL语法、ODBC驱动程序和用户接口 Impala系统架构Impala和Hive、HDFS、HBase等工具是统一部署在一个Hadoop平台上的Impala主要由Impalad，State Store和CLI三部分组成 图中虚线组件是Impala的组件 Impala主要由Impalad，State Store和CLI三部分组成 Impalad • 负责协调客户端提交的查询的执行 • 包含Query Planner、Query Coordinator和Query Exec Engine三个模块 • 与HDFS的数据节点（HDFS DN）运行在同一节点上 • 给其他Impalad分配任务以及收集其他Impalad的执行结果进行汇总 • Impalad也会执行其他Impalad给其分配的任务，主要就是对本地HDFS和HBase里的部分数据进行操作 State Store • 会创建一个statestored进程 • 负责收集分布在集群中各个Impalad进程的资源信息，用于查询调度 CLI • 给用户提供查询使用的命令行工具 • 还提供了Hue、JDBC及ODBC的使用接口 说明：Impala中的元数据直接存储在Hive中。Impala采用与Hive相同的元数据、SQL语法、ODBC驱动程序和用户接口，从而使得在一个Hadoop平台上，可以统一部署Hive和Impala等分析工具，同时支持批处理和实时查询 Impala查询执行过程 Impala执行查询的具体过程： • 第0步，当用户提交查询前，Impala先创建一个负责协调客户端提交的查询的Impalad进程，该进程会向Impala State Store提交注册订阅信息，State Store会创建一个statestored进程，statestored进程通过创建多个线程来处理Impalad的注册订阅信息。 • 第1步，用户通过CLI客户端提交一个查询到impalad进程，Impalad的Query Planner对SQL语句进行解析，生成解析树；然后，Planner把这个查询的解析树变成若干PlanFragment，发送到Query Coordinator • 第2步，Coordinator通过从MySQL元数据库中获取元数据，从HDFS的名称节点中获取数据地址，以得到存储这个查询相关数据的所有数据节点。 • 第3步，Coordinator初始化相应impalad上的任务执行，即把查询任务分配给所有存储这个查询相关数据的数据节点。 • 第4步，Query Executor通过流式交换中间输出，并由Query Coordinator汇聚来自各个impalad的结果。 • 第5步，Coordinator把汇总后的结果返回给CLI客户端。 Impala与Hive的比较 Hive与Impala的 不同点总结如下： Hive适合于长时间的批处理查询分析，而Impala适合于实时交互式SQL查询 Hive依赖于MapReduce计算框架，Impala把执行计划表现为一棵完整的执行计划树，直接分发执行计划到各个Impalad执行查询 Hive在执行过程中，如果内存放不下所有数据，则会使用外存，以保证查询能顺序执行完成，而Impala在遇到内存放不下数据时，不会利用外存，所以Impala目前处理查询时会受到一定的限制 Hive与Impala的 相同点总结如下： Hive与Impala使用相同的存储数据池，都支持把数据存储于HDFS和HBase中 Hive与Impala使用相同的元数据 Hive与Impala中对SQL的解释处理比较相似，都是通过词法分析生成执行计划 总结 •Impala的目的不在于替换现有的MapReduce工具 •把Hive与Impala配合使用效果最佳 •可以先使用Hive进行数据转换处理，之后再使用Impala在Hive处理后的结果数据集上进行快速的数据分析 转自林子雨老师的公开课 视频地址：http://www.icourse163.org/learn/XMU-1002335004#/learn/content?type=detail&amp;id=1003836807&amp;cid=1004616536&amp;replay=true]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据基础学习笔记（六）——MapReduce]]></title>
    <url>%2F2018%2F04%2F24%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AD%EF%BC%89%E2%80%94%E2%80%94MapReduce%2F</url>
    <content type="text"><![CDATA[MapReduce体系结构MapReduce主要有以下4个部分组成： 1 ）Client •用户编写的MapReduce程序通过Client提交到JobTracker端 •用户可通过Client提供的一些接口查看作业运行状态 2 ）JobTracker •JobTracker负责资源监控和作业调度 •JobTracker 监控所有TaskTracker与Job的健康状况，一旦发现失败，就将相应的任务转移到其他节点 •JobTracker 会跟踪任务的执行进度、资源使用量等信息，并将这些信息告诉任务调度器（TaskScheduler），而调度器会在资源出现空闲时，选择合适的任务去使用这些资源 3 ）TaskTracker •TaskTracker 会周期性地通过“心跳”将本节点上资源的使用情况和任务的运行进度汇报给JobTracker，同时接收JobTracker 发送过来的命令并执行相应的操作（如启动新任务、杀死任务等） •TaskTracker 使用“slot”等量划分本节点上的资源量（CPU、内存等）。一个Task 获取到一个slot 后才有机会运行，而Hadoop调度器的作用就是将各个TaskTracker上的空闲slot分配给Task使用。slot 分为Map slot 和Reduce slot 两种，分别供MapTask 和Reduce Task使用 4 ）Task Task 分为Map Task 和Reduce Task 两种，均由TaskTracker 启动 MapReduce工作流程概述MapReduce把一个大的数据集拆分成多个小数据块在多台机器上并行处理，也就是说，一个大的MapReduce作业，首先会被拆分成许多个Map任务在多台机器上并行执行，每个Map任务通常运行在数据存储的节点上，这样，计算和数据就可以放在一起运行，不需要额外的数据传输开销。当Map任务结束后，会生成以形式表示的许多中间结果。然后，这些中间结果会被分发到多个Reduce任务在多台机器上并行执行，具有相同Key的会被发送到同一个Reduce任务那里，Reduce任务会对中间结果进行会中计算得到最后的结果，并输出到分布式文件系统中。 •不同的Map任务之间不会进行通信 •不同的Reduce任务之间也不会发生任何信息交换 •用户不能显式地从一台机器向另一台机器发送消息 •所有的数据交换都是通过MapReduce框架自身去实现的 MapReduce各个执行阶段MapReduce的算法执行过程： 1）MapReduce框架使用InputFormat模块做Map前的预处理，比如验证输入的格式是否符合输入定义，然后，将输入文件切分为逻辑上的多个InputSplit，这是MapReduce对文件进行处理和运算的输入单位，只是一个逻辑概念，每个InputSplit并没有对文件进行实际切割，只是记录了要处理的数据的位置和长度。 2）因为InuptSplit是逻辑切分而非物理切分，所以还需要通过RecordReader（RR）根据InputSplit中的信息来处理InputSplit中的具体记录，加载数据并转换为适合Map任务读取的键值对，输入给Map任务。 3）Map任务会根据用户自定义的映射规则，输出一系列的作为中间结果。 4）为了让Reduce可以并行处理Map的结果，需要对Map的输出进行一定的分区（Portition）、排序（Sort）、合并（Combine）、归并（Merge）等操作，得到形式的中间结果，在交给对应的Reduce进行处理，这个过程称为Shuffle。从无序的到有序的，这个过程用Shuffle（洗牌）来称呼是非常形象的。 5）Reduce以一系列中间结果作为输入，执行用户定义的逻辑，输出结果给OutputFormat模块。 6）OutputFormat模块会验证输出目录是否已经存在以及输出结果类型是否符合配置文件中的配置类型，如果都满足，就输出Reduce的结果到分布式文件系统。 HDFS 以固定大小的block 为基本单位存储数据，而对于MapReduce 而言，其处理单位是split。split 是一个逻辑概念，它只包含一些元数据信息，比如数据起始位置、数据长度、数据所在节点等。它的划分方法完全由用户自己决定。 Map 任务的数量 •Hadoop为每个split创建一个Map任务，split 的多少决定了Map任务的数目。大多数情况下，理想的分片大小是一个HDFS块 Reduce 任务的数量 •最优的Reduce任务个数取决于集群中可用的reduce任务槽(slot)的数目 •通常设置比reduce任务槽数目稍微小一些的Reduce任务个数（这样可以预留一些系统资源处理可能发生的错误 Shuffle过程原理1.Shuffle 过程简介 2.Map 端的Shuffle 过程 Map的输出结果首先被写入缓存，当缓存满时，就启动溢写操作，把缓存中的数据写入磁盘文件，并清空缓存。当启动溢写操作时，首先需要把缓存中的数据进行分区，然后对每个分区的数据进行排序（Sort）和合并（Combine），之后再写入磁盘文件。每次溢写操作会生成一个新的磁盘文件，随着Map任务的执行，磁盘中就会生成多个溢写文件。在Map任务全部结束之前，这些溢写文件会被归并（Merge）成一个大的磁盘文件，然后通知相应的Reduce任务来领取属于自己处理的数据。 •每个Map任务分配一个缓存 •MapReduce默认100MB缓存 •设置溢写比例0.8 •分区默认采用哈希函数 •排序是默认的操作 •排序后可以合并（Combine） •合并不能改变最终结果 •在Map任务全部结束之前进行归并 •归并得到一个大的文件，放在本地磁盘 •文件归并时，如果溢写文件数量大于预定值（默认是3）则可以再次启动Combiner，少于3不需要 •JobTracker会一直监测Map任务的执行，并通知Reduce任务来领取数据 合并（Combine）和归并（Merge）的区别：两个键值对&lt;“a”,1&gt;和&lt;“a”,1&gt;，如果合并，会得到&lt;“a”,2&gt;，如果归并，会得到&lt;“a”,&gt; 3.Reduce 端的Shuffle 过程 Reduce任务从Map端的不同Map及其领回属于自己处理的那部分数据，然后对数据进行归并（Merge）后交给Reduce处理。 •Reduce任务通过RPC向JobTracker询问Map任务是否已经完成，若完成，则领取数据 •Reduce领取数据先放入缓存，来自不同Map机器，先归并，再合并，写入磁盘 •多个溢写文件归并成一个或多个大文件，文件中的键值对是排序的 •当数据很少时，不需要溢写到磁盘，直接在缓存中归并，然后输出给Reduce MapReduce应用程序执行过程 参考资料：林子雨老师的MOOC课程：https://www.icourse163.org/learn/XMU-1002335004#/learn/content?type=detail&amp;id=1003836797&amp;cid=1004616527&amp;replay=true]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据基础学习笔记（五）——云数据库架构]]></title>
    <url>%2F2018%2F04%2F17%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89%E2%80%94%E2%80%94%E4%BA%91%E6%95%B0%E6%8D%AE%E5%BA%93%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[不同的云数据库产品采用的系统架构差异很大，这里以阿里巴巴集团核心系统数据库团队开发的UMP(Unified MySQL Platform)系统为例进行介绍。 UMP系统概述•UMP系统是低成本和高性能的MySQL云数据库方案 总的来说，UMP系统架构设计遵循了以下原则： •保持单一的系统对外入口，并且为系统内部维护单一的资源池（CPU、内存、带宽、磁盘等放在一个统一的资源池，供上部组件调用） •消除单点故障，保证服务的高可用性（设置多个管家（Controller）） •保证系统具有良好的可伸缩，能够动态地增加、删减计算与存储节点 •保证分配给用户的资源也是弹性可伸缩的，资源之间相互隔离，确保应用和数据安全（多租户之间隔离，当一个用户使用过多资源时，对其进行限制，以免影响其他用户的使用） UMP系统架构 Mnesia•Mnesia是一个分布式数据库管理系统 •Mnesia支持事务，支持透明的数据分片，利用两阶段锁实现分布式事务，可以线性扩展到至少50个节点 •Mnesia的数据库模式(schema)可在运行时动态重配置，表能被迁移或复制到多个节点来改进容错性 •Mnesia的这些特性，使其在开发云数据库时被用来提供分布式数据库服务 RabbitMQ •RabbitMQ是一个工业级的消息队列产品（功能类似于IBM公司的消息队列产品IBM Websphere MQ），作为消息传输中间件来使用，可以实现可靠的消息传送 •UMP集群中各个节点之间的通信，不需要建立专门的连接，都是通过读写队列消息来实现的 ZookeeperZookeeper是高效和可靠的协同工作系统，提供分布式锁之类的基本服务（比如统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等），用于构建分布式应用，减轻分布式应用程序所承担的协调任务 在UMP系统中，Zookeeper主要发挥三个作用： •作为全局的配置服务器。UMP系统把运行的应用系统的配置信息完全交给Zookeeper来管理，把配置信息保存在Zookeeper的某个目录节点中，然后将所有需要修改的服务器对这个目录节点设置监听，也就是监控配置信息的状态，一旦配置信息发生变化，每台服务器就会收到Zookeeper的通知，然后从Zookeeper获取新的配置信息。 •提供分布式锁（选出一个集群的“总管”）。UMP急群众部署了多个Controller服务器，为了保证系统的正确运行，对于有些操作，在某一时刻，只能由一个服务器去执行，而不能同时执行。礼物，一个MySQL实例发生故障以后，需要进行主备切换，有另一个正常的服务器来代替当前发生故障的服务器，如果这个时候所有的Controller服务器都去跟踪处理并且发起主备切换流程，那么，整个系统就会进入混乱状态。因此，在同一时间，必须从集群的多个Controller服务器中选举出一个“总管”，由这个“总管”负责发起各种系统任务。Zookeeper的分布式锁功能能够帮助选出一个“总管”，让这个“总管”来管理集群。 •监控所有MySQL实例。急群众运行MySQL实例的服务器发生故障时，必须被及时监听到，然后使用其他正常服务器来替代故障服务器。UMP系统借助Zookeeper实现对所有MySQL实例的监控。每个MySQL实例在启动时都会在Zookeeper上创建一个临时类型的目录节点，当某个MySQL实例挂掉时，这个临时类型的目录节点也随之被删除，后台监听进程可以捕获到这种变化，从而知道这个MySQL实例不再可用。 LVS•LVS(Linux Virtual Server)即Linux虚拟服务器，是一个虚拟的服务器集群系统 •UMP系统借助于LVS来实现集群内部的负载均衡 •LVS集群采用IP负载均衡技术和基于内容请求分发技术 •调度器是LVS集群系统的唯一入口点，调度器具有很好的吞吐率，将请求均衡地转移到不同的服务器上执行，且调度器自动屏蔽掉服务器的故障，从而将一组服务器构成一个高性能的、高可用的虚拟服务器 •整个服务器集群的结构对客户是透明的，而且无需修改客户端和服务器端的程序 Controller服务器•Controller服务器向UMP集群提供各种管理服务，实现集群成员管理、元数据存储、MySQL实例管理、故障恢复、备份、迁移、扩容等功能 •Controller服务器上运行了一组Mnesia分布式数据库服务，其中存储了各种系统元数据，主要包括集群成员、用户的配置和状态信息，以及用户名到后端MySQL实例地址的映射关系（或称为“路由表”）等 •当其它服务器组件需要获取用户数据时，可以向Controller服务器发送请求获取数据 •为了避免单点故障，保证系统的高可用性，UMP系统中部署了多台Controller服务器，然后，由Zookeeper的分布式锁功能来帮助选出一个“总管”，负责各种系统任务的调度和监控 Web 控制台Web控制台向用户提供系统管理界面 Proxy 服务器Proxy服务器向用户提供访问MySQL数据库的服务，它完全实现了MySQL协议，用户可以使用已有的MySQL客户端连接到Proxy服务器，Proxy服务器通过用户名获取到用户的认证信息、资源配额的限制(例如QPS、IOPS（I/O Per Second）、最大连接数等)，以及后台MySQL实例的地址，然后，用户的SQL查询请求会被转发到相应的MySQL实例上。 除了数据路由的基本功能外，Proxy服务器中还实现了很多重要的功能，主要包括屏蔽MySQL实例故障、读写分离、分库分表、资源隔离、记录用户访问日志等 Agent 服务器Agent服务器部署在运行MySQL进程的机器上，用来管理每台物理机上的MySQL实例，执行主从切换、创建、删除、备份、迁移等操作，同时，还负责收集和分析MySQL进程的统计信息、慢查询日志（Slow Query Log）和bin-log 日志分析服务器日志分析服务器存储和分析Proxy服务器传入的用户访问日志，并支持实时查询一段时间内的慢日志和统计报表 信息统计服务器信息统计服务器定期将采集到的用户的连接数、QPS数值以及MySQL实例的进程状态用RRDtool进行统计，可以在 Web界面上可视化展示统计结果，也可以把统计结果作为今后实现弹性的资源分配和自动化的MySQL实例迁移的依据 愚公系统愚公系统是一个全量复制结合bin-log分析进行增量复制的工具，可以实现在不停机的情况下动态扩容、缩容和迁移]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据基础学习笔记（四）——HBase相关知识（三）]]></title>
    <url>%2F2018%2F04%2F03%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89%E2%80%94%E2%80%94HBase%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%EF%BC%88%E4%B8%89%EF%BC%89%2F</url>
    <content type="text"><![CDATA[HBase应用方案HBase实际应用中的性能优化方法行键（Row Key ） 行键是按照 字典序存储，因此，设计行键时，要充分利用这个排序特点，将经常一起读取的数据存储到一块，将最近可能会被访问的数据放在一块。 举个例子：如果最近写入HBase表中的数据是最可能被访问的，可以考虑将时间戳作为行键的一部分，由于是字典序排序，所以可以使用Long.MAX_VALUE -timestamp作为行键，这样能保证新写入的数据在读取时可以被快速命中。 InMemory创建表的时候，可以通过HColumnDescriptor.setInMemory(true)将表放到Region服务器的缓存中，保证在读取的时候被cache命中。 Max Version创建表的时候，可以通过HColumnDescriptor.setMaxVersions(int maxVersions)设置表中数据的最大版本，如果只需要保存最新版本的数据，那么可以设置setMaxVersions(1)。 Time To Live创建表的时候，可以通过HColumnDescriptor.setTimeToLive(int timeToLive)设置表中数据的存储生命期，过期数据将自动被删除，例如如果只需要存储最近两天的数据，那么可以设置setTimeToLive(2 24 60 * 60)。 HBase性能监视Master-status(自带) •HBase Master默认基于Web的UI服务端口为60010，HBase region服务器默认基于Web的UI服务端口为60030.如果master运行在名为master.foo.com的主机中，mater的主页地址就是http://master.foo.com:60010，用户可以通过Web浏览器输入这个地址查看该页面 •可以查看HBase集群的当前状态 Ganglia Ganglia是UC Berkeley发起的一个开源集群监视项目，用于监控系统性能 OpenTSDB OpenTSDB可以从大规模的集群（包括集群中的网络设备、操作系统、应用程序）中获取相应的metrics并进行存储、索引以及服务，从而使得这些数据更容易让人理解，如web化，图形化等 Ambari Ambari 的作用就是创建、管理、监视 Hadoop 的集群 在HBase之上构建SQL引擎NoSQL区别于关系型数据库的一点就是NoSQL不使用SQL作为查询语言，至于为何在NoSQL数据存储HBase上提供SQL接口，有如下原因： 易使用。使用诸如SQL这样易于理解的语言，使人们能够更加轻松地使用HBase。 减少编码。使用诸如SQL这样更高层次的语言来编写，减少了编写的代码量。方案： 1.Hive整合HBase2.Phoenix 1.Hive 整合HBase Hive与HBase的整合功能从Hive0.6.0版本已经开始出现，利用两者对外的API接口互相通信，通信主要依靠hive_hbase-handler.jar工具包(Hive Storage Handlers)。由于HBase有一次比较大的版本变动，所以并不是每个版本的Hive都能和现有的HBase版本进行整合，所以在使用过程中特别注意的就是两者版本的一致性。 2.Phoenix Phoenix由Salesforce.com开源，是构建在Apache HBase之上的一个SQL中间层，可以让开发者在HBase上执行SQL查询。 构建HBase二级索引二级索引，又叫辅助索引 HBase只有一个针对行健的索引访问HBase表中的行，只有三种方式： •通过单个行健访问 •通过一个行健的区间来访问 •全表扫描 使用其他产品为HBase行健提供索引功能： •Hindex二级索引 •HBase+Redis •HBase+solr 原理：采用HBase0.92版本之后引入的Coprocessor特性 Coprocessor构建二级索引 •Coprocessor提供了两个实现：endpoint和observer，endpoint相当于关系型数据库的存储过程，而observer则相当于触发器 •observer允许我们在记录put前后做一些处理，因此，而我们可以在插入数据时同步写入索引表 •Coprocessor构建二级索引•缺点：每插入一条数据需要向索引表插入数据，即耗时是双倍的，对HBase的集群的压力也是双倍的 优点：非侵入性：引擎构建在HBase之上，既没有对HBase进行任何改动，也不需要上层应用做任何妥协 Hindex二级索引 Hindex 是华为公司开发的纯 Java 编写的HBase二级索引，兼容 Apache HBase 0.94.8。当前的特性如下： •多个表索引 •多个列索引 •基于部分列值的索引 HBase+Redis •Redis+HBase方案 •Coprocessor构建二级索引 •Redis做客户端缓存 •将索引实时更新到Redis等KV系统中，定时从KV更新索引到HBase的索引表中 Solr+HBase Solr是一个高性能，采用Java5开发，基于Lucene的全文搜索服务器。同时对其进行了扩展，提供了比Lucene更为丰富的查询语言，同时实现了可配置、可扩展并对查询性能进行了优化，并且提供了一个完善的功能管理界面，是一款非常优秀的全文搜索引擎。]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据基础学习笔记（三）——HBase相关知识（二）]]></title>
    <url>%2F2018%2F04%2F02%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89%E2%80%94%E2%80%94HBase%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[HBase功能组件• HBase的实现包括三个主要的功能组件：– （1）库函数：链接到每个客户端– （2）一个Master主服务器（充当管家的作用）– （3）许多个Region服务器 • 主服务器Master负责管理和维护HBase表的分区信息，维护Region服务器列表，分配Region，负载均衡 • 一个大的表会被分成很多个Region，Region服务器负责存储和维护分配给自己的Region，处理来自客户端的读写请求 • 客户端并不是直接从Master主服务器上读取数据，而是在获得Region的存储位置信息后，直接从Region服务器上读取数据 • 客户端并不依赖Master，而是通过Zookeeper来获得Region位置信息，大多数客户端甚至从来不和Master通信，这种设计方式使得Master负载很小 表和Region •开始只有一个Region，后来不断分裂 •Region拆分操作非常快，接近瞬间，因为拆分之后的Region读取的仍然是原存储文件，直到“合并”过程把存储文件异步地写到独立的文件之后，才会读取新文件 这种拆分只是逻辑上的拆分，只是数据的指向发生了变化，它的实际存储还是在原来的旧的Region中的数据。当读新的Region时，后台会有一个合并操作，会把拆分的数据进行重新操作，最终会写到新的文件中去。 一个Region只能存到一个Region服务器上。 Region的定位那么有一个问题，当一个Region被拆成很多个Region时，这些Region会把它打散，分布到不同的地方存储，那么怎么知道它被存到哪里去了呢？ •元数据表，又名.META.表，存储了Region和Region服务器的映射关系 •当HBase表很大时， .META.表也会被分裂成多个Region •根数据表，又名-ROOT-表，记录所有元数据的具体位置 •-ROOT-表只有唯一一个Region，名字是在程序中被写死的 •Zookeeper文件记录了-ROOT-表的位置 •一个-ROOT-表最多只能有一个Region，也就是最多只能有128MB，按照每行（一个映射条目）占用1KB内存计算，128MB空间可以容纳128MB/1KB=2^17 行，也就是说，一个-ROOT-表可以寻址2^17 个.META.表的Region。 •同理，每个.META.表的 Region可以寻址的用户数据表的Region个数是128MB/1KB=2^17 。 •最终，三层结构可以保存的Region数目是(128MB/1KB) × (128MB/1KB) = 2^34 个Region 所以三层架构能够满足企业的需求。 客户端访问数据时的“三级寻址”•为了加速寻址，客户端会缓存位置信息，同时，需要解决缓存失效问题 •寻址过程客户端只需要询问Zookeeper服务器，不需要连接Master服务器 这里的缓存机制采用的是惰性缓存，如果在使用缓存获取数据时，获取不到数据，那么就失效了，这时候再次进行三级寻址过程，以解决缓存失效问题。 HBase运行机制 • 1. 客户端– 客户端包含访问HBase的接口，同时在缓存中维护着已经访问过的Region位置信息，用来加快后续数据访问过程 • 2. Zookeeper服务器– Zookeeper可以帮助选举出一个Master作为集群的总管，并保证在任何时刻总有唯一一个Master在运行，这就避免了Master的“单点失效”问题Zookeeper是一个很好的集群管理工具，被大量用于分布式计算，提供配置维护、域名服务、分布式同步、组服务等。提供管家的功能，维护整个HBase集群。虽然有很多备用的Master，但是它保证只有一个Master是运行的。 • 3. Master• 主服务器Master主要负责表和Region的管理工作：– 管理用户对表的增加、删除、修改、查询等操作– 实现不同Region服务器之间的负载均衡– 在Region分裂或合并后，负责重新调整Region的分布– 对发生故障失效的Region服务器上的Region进行迁移 • 4. Region服务器– Region服务器是HBase中最核心的模块，负责维护分配给自己的Region，并响应用户的读写请求 Region服务器工作原理1.用户读写数据过程 •用户写入数据时，被分配到相应Region服务器去执行 •用户数据首先被写入到MemStore和Hlog中 •只有当操作写入Hlog之后，commit()调用才会将其返回给客户端 •当用户读取数据时，Region服务器会首先访问MemStore缓存，如果找不到，再去磁盘上面的StoreFile中寻找 2.缓存的刷新 •系统会周期性地把MemStore缓存里的内容刷写到磁盘的StoreFile文件中，清空缓存，并在Hlog里面写入一个标记 •每次刷写都生成一个新的StoreFile文件，因此，每个Store包含多个StoreFile文件 •每个Region服务器都有一个自己的HLog 文件，每次启动都检查该文件，确认最近一次执行缓存刷新操作之后是否发生新的写入操作；如果发现更新，则先写入MemStore，再刷写到StoreFile，最后删除旧的Hlog文件，开始为用户提供服务 3.StoreFile 的合并 •每次刷写都生成一个新的StoreFile，数量太多，影响查找速度 •调用Store.compact()把多个合并成一个 •合并操作比较耗费资源，只有数量达到一个阈值才启动合并 Store工作原理•Store是Region服务器的核心 •多个StoreFile合并成一个 •单个StoreFile过大时，又触发分裂操作，1个父Region被分裂成两个子Region HLog工作原理• 分布式环境必须要考虑系统出错。HBase采用HLog保证系统恢复 • HBase系统为每个Region服务器配置了一个HLog文件，它是一种预写式日志（Write Ahead Log） • 用户更新数据必须首先写入日志后，才能写入MemStore缓存，并且，直到MemStore缓存内容对应的日志已经写入磁盘，该缓存内容才能被刷写到磁盘 • Zookeeper会实时监测每个Region服务器的状态，当某个Region服务器发生故障时，Zookeeper会通知Master • Master首先会处理该故障Region服务器上面遗留的HLog文件，这个遗留的HLog文件中包含了来自多个Region对象的日志记录 • 系统会根据每条日志记录所属的Region对象对HLog数据进行拆分，分别放到相应Region对象的目录下，然后，再将失效的Region重新分配到可用的Region服务器中，并把与该Region对象相关的HLog日志记录也发送给相应的Region服务器 • Region服务器领取到分配给自己的Region对象以及与之相关的HLog日志记录以后，会重新做一遍日志记录中的各种操作，把日志记录中的数据写入到MemStore缓存中，然后，刷新到磁盘的StoreFile文件中，完成数据恢复 • 共用日志优点：提高对表的写操作性能；缺点：恢复时需要分拆日志 本笔记参考自厦门大学林子雨老师的公开课：https://www.icourse163.org/course/XMU-1002335004]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据基础学习笔记（二）——HBase相关知识（一）]]></title>
    <url>%2F2018%2F04%2F02%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94Hbase%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[Hbase简介HBase是一个高可靠、高性能、面向列、可伸缩的分布式数据库，是谷歌BigTable的开源实现，主要用来存储非结构化和半结构化的松散数据。HBase的目标是处理非常庞大的表，可以通过水平扩展的方式，利用廉价计算机集群处理由超过10亿行数据和数百万列元素组成的数据表。 底层的分布式文件系统用来存储完全非结构化的数据。 Hbase是架构在底层的分布式文件系统HDFS基础之上的同时MR可以对Hbase的数据进行处理。同时Hive和Pig等都可以访问Hbase中的数据。 从上图可以看出，BigTable和HBase的底层技术的对比。 为什么要设计HBase这个数据产品呢？•Hadoop可以很好地解决大规模数据的离线批量处理问题，但是，受限于HadoopMapReduce编程框架的高延迟数据处理机制，使得Hadoop无法满足大规模数据实时处理应用的需求 •HDFS面向批量访问模式，不是随机访问模式 •传统的通用关系型数据库无法应对在数据规模剧增时导致的系统扩展性和性能问题（分库分表也不能很好解决） •传统关系数据库在数据结构变化时一般需要停机维护；空列浪费存储空间 •因此，业界出现了一类面向半结构化数据存储和处理的高可扩展、低写入/查询延迟的系统，例如，键值数据库、文档数据库和列族数据库（如BigTable和HBase等） •HBase已经成功应用于互联网服务领域和传统行业的众多在线式数据分析处理系统中 HBase与传统关系数据库的对比分析• HBase与传统的关系数据库的区别主要体现在以下几个方面： • （1）数据类型：关系数据库采用关系模型，具有丰富的数据类型（整型，字符型等等）和存储方式，HBase则采用了更加简单的数据模型，它把数据存储为未经解释的字符串（也就是Bytes数组） • （2）数据操作：关系数据库中包含了丰富的操作（增删改查），其中会涉及复杂的多表连接。HBase操作则不存在复杂的表与表之间的关系，只有简单的插入、查询、删除、清空等，因为HBase在设计上就避免了复杂的表和表之间的关系 • （3）存储模式：关系数据库是基于行模式存储的。HBase是基于列存储的，每个列族都由几个文件保存，不同列族的文件是分离的 • （4）数据索引：关系数据库通常可以针对不同列构建复杂的多个索引，以提高数据访问性能。HBase只有一个索引——行键，通过巧妙的设计，HBase中的所有访问方法，或者通过行键访问，或者通过行键扫描，从而使得整个系统不会慢下来 • （5）数据维护：在关系数据库中，更新操作会用最新的当前值去替换记录中原来的旧值，旧值被覆盖后就不会存在。而在HBase中执行更新操作时，并不会删除数据旧的版本，而是生成一个新的版本，旧有的版本仍然保留，只有在过了设置的参数期限之后，在系统后台清理的时候才会清理掉 • （6）可伸缩性：关系数据库很难实现横向扩展，纵向扩展（如添加内存，改进CPU等等）的空间也比较有限。相反，HBase和BigTable这些分布式数据库就是为了实现灵活的水平扩展而开发的，能够轻易地通过在集群中增加或者减少硬件数量来实现性能的伸缩 HBase的访问接口以后在使用Hbase的时候，可以通过哪些方式访问HBase数据库？见下图： HBase数据模型• HBase是一个稀疏、多维度、排序的映射表，这张表的索引是行键、列族、列限定符和时间戳 • 每个值是一个未经解释的字符串，没有数据类型 • 用户在表中存储数据，每一行都有一个可排序的行键和任意多的列 • 表在水平方向由一个或者多个列族组成，一个列族中可以包含任意多个列，同一个列族里面的数据存储在一起 • 列族支持动态扩展，可以很轻松地添加一个列族或列，无需预先定义列的数量以及类型，所有列均以字符串形式存储，用户需要自行进行数据类型转换 • HBase中执行更新操作时，并不会删除数据旧的版本，而是生成一个新的版本，旧有的版本仍然保留（这是和HDFS只允许追加不允许修改的特性相关的） • 表：HBase采用表来组织数据，表由行和列组成，列划分为若干个列族 • 行：每个HBase表都由若干行组成，每个行由行键（row key）来标识。 • 列族：一个HBase表被分组成许多“列族”（Column Family）的集合，它是基本的访问控制单元（支持动态拓展） • 列限定符：列族里的数据通过列限定符（或列）来定位 • 单元格：在HBase表中，通过行、列族和列限定符确定一个“单元格”（cell），单元格中存储的数据没有数据类型，总被视为字节数组byte[] • 时间戳：每个单元格都保存着同一份数据的多个版本，这些版本采用时间戳进行索引 HBase的数据坐标HBase中需要根据行键、列族、列限定符和时间戳来确定一个单元格，因此，可以视为一个“四维坐标”，即[行键, 列族, 列限定符, 时间戳] 概念视图HBase在概念上和实际的底层存储是有区分的，在概念上HBase只是一个表，如下面只给了一个行键： 如这一个行键给了两个列族，第一个列族contents中冒号前面的contents是列族的名称，冒号后面的html是列的名称，引号中的内容就是这一列的数据。一个时间戳并不一定会在所有列族插入数据，从图中就可以看出。所以这就导致了HBase的稀疏表的特性。这只是在概念上的视图。 物理视图实际上在实际存储中，并不是按上述的方式去存的。在底层存储时，是按列族为单位进行存储的。 上图是在实际存储时，存储在底层的实际的表。并没有像概念视图中存储了很多的空数据。所以概念视图和物理视图上是有区分的。 面向列的存储 传统的数据库，以行为单位进行存储，一行包括ID,姓名，年龄，性别，IP，操作等。但是按列存储，里面的姓名、年龄等进行单独存储。 它们各自的优缺点： 另外，使用列式存储，数据可以达到很高的数据压缩率。而行式存储，很难压缩。 本笔记参考自厦门大学林子雨老师的公开课：https://www.icourse163.org/course/XMU-1002335004]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据基础学习笔记（一）——Hadoop相关知识]]></title>
    <url>%2F2018%2F04%2F01%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[HadoopHadoop的应用现状和构成简介下图为Hadoop在企业中的 应用架构 访问层不用多说，满足企业的数据分析、数据挖掘和数据实时查询功能。为了满足访问层的需求，大数据层的各个技术对其进行支撑。（1）离线分析：大量数据拿过来之后进行批量处理。其中MR是MapReduce的简称，Hive数据仓库和Pig也可以进行离线数据分析。（2）实时查询：其中Hbase是一个可以支持几十亿行数据的非常好的分布式数据库。（3）BI分析：Mahout是Hadoop平台上的一款数据挖掘应用。可以把各种数据挖掘，机器学习和商务智能的算法用MapReduce实现。否则开发人员要自己用MapReduce写决策树算法。 下图为一些大数据计算模式及其代表产品 下图为Hadoop项目结构 YARNz专门负责调度内存，CPU，带宽等计算资源。而上面的事完成具体的计算工作的。 Tez会把很多的MapReduce作业进行分析优化，构建成一个有向无环图，保证获得最好的处理效率。 Spark与MapReduce类似，也是进行相应的计算。但是Spark是基于内存的，而MapReduce是基于磁盘的计算。MR在计算时，先把数据写到磁盘中，然后c处理结束后再写到分布式文件系统中。所以Spark的性能要高。 Pig实现流数据处理，较MR属于轻量级。它也支持类似于SQL的语句。是一种轻量级的脚本语言。 Oozie是一个工作流管理系统，可以把一个工作分成不同的工作环节。 Zookeeper提供分布式协调一致性服务。 Hbase是一个非关系型数据库，可以支持随机读写。 Flume是专门负责日志收集的，分析一些实时生成的数据流。 Sqoopy用于在Hadoop与传统数据库之间进行数据传递（导入导出等）。可以把之前存到关系型数据库（如Oracle）中的数据导入到HDFS、Hive或者Hbase中，反之亦可。 Ambari是一个安装部署工具，可以在一个集群上面智能化的管理一整套Hadoop上的各个套件。 Hadoop各组件的功能如下： Hadoop集群的节点类型Hadoop框架中最核心的设计是为海量数据提供存储的HDFS和对数据进行计算的MapReduce MapReduce的作业主要包括：（1）从磁盘或从网络读取数据，即IO密集工作；（2）计算数据，即CPU密集工作 •Hadoop集群的整体性能取决于CPU、内存、网络以及存储之间的性能平衡。因此运营团队在选择机器配置时要针对不同的工作节点选择合适硬件类型•一个基本的Hadoop集群中的节点主要有: •NameNode：负责协调集群中的数据存储 •DataNode：存储被拆分的数据块 •JobTracker：协调数据计算任务 •TaskTracker：负责执行由JobTracker指派的任务 •SecondaryNameNode：帮助NameNode收集文件系统运行的状态信息 HDFS全称：Hadoop Distributed File System.解决海量数据的分布式存储问题。 分布式文件系统的结构分布式文件系统在物理结构上是由计算机集群中的多个节点构成的，这些节点分为两类，一类叫“主节点”(Master Node)或者也被称为“名称结点”(NameNode)，另一类叫“从节点”（Slave Node）或者也被称为“数据节点”(DataNode) HDFS的三个节点：Namenode，Datanode，Secondary Namenode Namenode：HDFS的守护进程，用来管理文件系统的命名空间，负责记录文件是如何分割成数据块，以及这些数据块分别被存储到那些数据节点上，它的主要功能是对内存及IO进行集中管理。 Datanode：文件系统的工作节点，根据需要存储和检索数据块，并且定期向namenode发送他们所存储的块的列表。 Secondary Namenode：辅助后台程序，与NameNode进行通信，以便定期保存HDFS元数据的快照。 HDFS采用了主从（Master/Slave）结构模型，一个HDFS集群包括一个名称节点（NameNode）和若干个数据节点（DataNode）（如图所示）。名称节点作为中心服务器，负责管理文件系统的命名空间及客户端对文件的访问。集群中的数据节点一般是一个节点运行一个数据节点进程，负责处理文件系统客户端的读/写请求，在名称节点的统一调度下进行数据块的创建、删除和复制等操作。每个数据节点的数据实际上是保存在本地Linux文件系统中的。 HDFS的缺点： 1.不适合低延迟的数据访问2.无法高效存储大量小文件3.不支持多用户写入及任意修改文件 名称节点和数据节点 在HDFS中，名称节点（NameNode）负责管理分布式文件系统的命名空间（Namespace），保存了两个核心的数据结构，即FsImage和EditLog。 •FsImage用于维护文件系统树以及文件树中所有的文件和文件夹的元数据 •操作日志文件EditLog中记录了所有针对文件的创建、删除、重命名等操作 •名称节点记录了每个文件中各个块所在的数据节点的位置信息。 客户端在访问数据时，先通过名称节点，获取元数据信息，从而知道被访问的数据存到哪些数据节点，获得数据块具体存储位置的信息之后，客户端就会到各个机器上去获取它所需要的数据。写入操作类似，客户端先访问名称节点，一个大文件（如1TB,2TB）要怎么写，然后名称节点会告诉它，把文件分成多少块，每个块放到哪个数据节点上。 FsImage 文件•FsImage文件包含文件系统中所有目录和文件inode的序列化形式。每个inode是一个文件或目录的元数据的内部表示，并包含此类信息：文件的复制等级、修改和访问时间、访问权限、块大小以及组成文件的块。对于目录，则存储修改时间、权限和配额元数据 •FsImage文件没有记录块存储在哪个数据节点。而是由名称节点把这些映射保留在内存中，当数据节点加入HDFS集群时，数据节点会把自己所包含的块列表告知给名称节点，此后会定期执行这种告知操作，以确保名称节点的块映射是最新的。 在名称节点启动的时候，它会将FsImage文件中的内容加载到内存中，之后再执行EditLog文件中的各项操作，使得内存中的元数据和实际的同步，存在内存中的元数据支持客户端的读操作。 •一旦在内存中成功建立文件系统元数据的映射，则创建一个新的FsImage文件和一个空的EditLog文件 •名称节点起来之后，HDFS中的更新操作会重新写到EditLog文件中，因为FsImage文件一般都很大（GB级别的很常见），如果所有的更新操作都往FsImage文件中添加，这样会导致系统运行的十分缓慢，但是，如果往EditLog文件里面写就不会这样，因为EditLog 要小很多。每次执行写操作之后，且在向客户端发送成功代码之前，edits文件都需要同步更新 第二名称节点第二名称节点是HDFS架构中的一个组成部分，它是用来保存名称节点中对HDFS 元数据信息的备份，并减少名称节点重启的时间。SecondaryNameNode一般是单独运行在一台机器上。 SecondaryNameNode的工作情况： （1）SecondaryNameNode会定期和NameNode通信，请求其停止使用EditLog文件，暂时将新的写操作写到一个新的文件edit.new上来，这个操作是瞬间完成，上层写日志的函数完全感觉不到差别； （2）SecondaryNameNode通过HTTP GET方式从NameNode上获取到FsImage和EditLog文件，并下载到本地的相应目录下； （3）SecondaryNameNode将下载下来的FsImage载入到内存，然后一条一条地执行EditLog文件中的各项更新操作，使得内存中的FsImage保持最新；这个过程就是EditLog和FsImage文件合并； （4）SecondaryNameNode执行完（3）操作之后，会通过post方式将新的FsImage文件发送到NameNode节点上； （5）NameNode将从SecondaryNameNode接收到的新的FsImage替换旧的FsImage文件，同时将edit.new替换EditLog文件，通过这个过程EditLog就变小。 数据节点（DataNode）数据节点是分布式文件系统HDFS的工作节点，负责数据的存储和读取，会根据客户端或者是名称节点的调度来进行数据的存储和检索，并且向名称节点定期发送自己所存储的块的列表 •每个数据节点中的数据会被保存在各自节点的本地Linux文件系统中 HDFS存储原理冗余数据保存作为一个分布式文件系统，为了保证系统的容错性和可用性，HDFS采用了多副本方式对数据进行冗余存储，通常一个数据块的多个副本会被分布到不同的数据节点上，数据块1被分别存放到数据节点A和C上，数据块2被存放在数据节点A和B上。这种多副本方式具有以下几个优点：（1） 加快数据传输速度 （2） 容易检查数据错误 （3） 保证数据可靠性 数据存取策略数据存放•第一个副本：放置在上传文件的数据节点；如果是集群外提交，则随机挑选一台磁盘不太满、CPU不太忙的节点 •第二个副本：放置在与第一个副本不同的机架的节点上 •第三个副本：与第一个副本相同机架的其他节点上 •更多副本：随机节点 数据读取•HDFS提供了一个API可以确定一个数据节点所属的机架ID，客户端也可以调用API获取自己所属的机架ID •当客户端读取数据时，从名称节点获得数据块不同副本的存放位置列表，列表中包含了副本所在的数据节点，可以调用API来确定客户端和这些数据节点所属的机架ID，当发现某个数据块副本对应的机架ID和客户端对应的机架ID相同时，就优先选择该副本读取数据，如果没有发现，就随机选择一个副本读取数据 数据错误与恢复HDFS具有较高的容错性，可以兼容廉价的硬件，它把硬件出错看作一种常态，而不是异常，并设计了相应的机制检测数据错误和进行自动恢复，主要包括以下几种情形：名称节点出错、数据节点出错和数据出错。 名称节点出错名称节点保存了所有的元数据信息，其中，最核心的两大数据结构是FsImage和Editlog，如果这两个文件发生损坏，那么整个HDFS实例将失效。因此，HDFS设置了备份机制，把这些核心文件同步复制到备份服务器SecondaryNameNode上。当名称节点出错时，就可以根据备份服务器SecondaryNameNode中的FsImage和Editlog数据进行恢复。 数据节点出错•每个数据节点会定期向名称节点发送“心跳”信息，向名称节点报告自己的状态 •当数据节点发生故障，或者网络发生断网时，名称节点就无法收到来自一些数据节点的心跳信息，这时，这些数据节点就会被标记为“宕机”，节点上面的所有数据都会被标记为“不可读”，名称节点不会再给它们发送任何I/O请求 •这时，有可能出现一种情形，即由于一些数据节点的不可用，会导致一些数据块的副本数量小于冗余因子•名称节点会定期检查这种情况，一旦发现某个数据块的副本数量小于冗余因子，就会启动数据冗余复制，为它生成新的副本 •HDFS和其它分布式文件系统的最大区别就是可以调整冗余数据的位置 数据出错•网络传输和磁盘错误等因素，都会造成数据错误 •客户端在读取到数据后，会采用md5和sha1对数据块进行校验，以确定读取到正确的数据 •在文件被创建时，客户端就会对每一个文件块进行信息摘录，并把这些信息写入到同一个路径的隐藏文件里面 •当客户端读取文件的时候，会先读取该信息文件，然后，利用该信息文件对每个读取的数据块进行校验，如果校验出错，客户端就会请求到另外一个数据节点读取该文件块，并且向名称节点报告这个文件块有错误，名称节点会定期检查并且重新复制这个块 本笔记的来源源自林子雨老师的MOOC课程和课件，地址：https://www.icourse163.org/course/XMU-1002335004]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
</search>
